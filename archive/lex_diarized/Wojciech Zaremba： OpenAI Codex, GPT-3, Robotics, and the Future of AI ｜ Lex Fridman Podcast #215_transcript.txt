Transcription for Wojciech Zaremba： OpenAI Codex, GPT-3, Robotics, and the Future of AI ｜ Lex Fridman Podcast #215.mp3:
Full transcript: The following is a conversation with Wojciech Zaremba, co founder of OpenAI, which is one of the top organizations in the world doing artificial intelligence research and development. Wojciech is the head of language and cogeneration teams, building and doing research on GitHub, Copilot, OpenAI, Codex, and GPT-3, and who knows? Four, five, six, n and n one. And he also previously led OpenAI's robotic efforts. These are incredibly exciting projects to me that deeply challenge and expand our understanding of the structure and nature of intelligence. The 21st century, I think, may very well be remembered for a handful of revolutionary AI systems and their implementations. GPT codecs and applications of language models and transformers in general to the language and visual domains may very well be at the core of these AI systems. To support this podcast, please check out our sponsors. Theyre listed in the description. This is the Lex Friedman podcast, and here is my conversation with Wojciech Zaremba. You mentioned that Sam Altman asked about the Fermi paradox, and the people at OpenAI had really sophisticated, interesting answers. So that's when you knew this is the right team to be working with. So let me ask you about the Fermi paradox about aliens. Why have we not found overwhelming evidence for aliens visiting Earth? I don't have a conviction in the answer, but rather kind of probabilistic perspective on what might be a, let's say, possible answers. It's also interesting that the question itself even can touch on the, you know, your typical question of what's the meaning of life? Because if you assume that, like, we don't see aliens because they destroy themselves, that kind of upweights the focus on making sure that we won't destroy ourselves. Yeah, at the moment, the place where I am actually with my belief, and these things also change over the time, is, I think that we might be alone in the universe, which actually makes life more or less a consciousness life more kind of valuable. And that means that we should more appreciate it. Have we always been alone? So what's your intuition about our galaxy, our universe? Is it just sprinkled with graveyards of intelligent civilizations? Or are we truly, is life, intelligent life, truly unique? At the moment, my belief that it is unique, but I would say I could also. There was some footage released with UFO objects, which makes me actually doubt my own belief. I can tell you one crazy answer that I have heard. Apparently, when you look actually at the limits of computation, you can compute more if the temperature of the universe would drop down. So one of the things that alias might want to do if they are truly optimizing to maximize amount of compute, which maybe can lead to more, let's say simulations or. So it's instead of wasting current entropy of the universe, because we, by living, we are actually somewhat wasting entropy, then you can wait for the universe to cool down such that you have more computation. That's kind of a funny answer. I'm not sure if I believe in it, but that would be one of the reasons why you don't see aliens. It's also possible. See, some people say that maybe there is not that much point in actually going to other galaxies if you can go inwards. So there is no limits of what could be an experience if we could connect machines to our brains while there are still some limits if we want to explore universe. Yeah, there could be a lot of ways to go inwards too. Once you figure out some aspect of physics we haven't figured out yet, maybe you can travel to different dimensions. I mean, travel in three dimensional space may not be the most fun kind of travel. There may be like just a huge amount of different ways to travel. And it doesn't require a spaceship going slowly in 3d space to space time. It also feels one of the problems is that speed of light is low and universe is vast. And it seems that actually most likely, if we want to travel very far, then we would, instead of actually sending spaceships with humans that wait a lot, we would send something similar to what Yuri Miller is working on. These are like huge sail which is at first powered. There is a shot of laser from an Earth and you can propel it to quarter of speed of light. And the sail itself contains a few grams of equipment. And that might be the way to actually transport matter through universe. But then when you think what would it mean for humans? It means that we would need to actually put their 3d printer and 3d print a human on other planet. I don't know, play them YouTube or let's say, or like a 3d print a huge human right away. Or maybe a womb or so. Yeah. With our current techniques of archaeology, if a civilization was born and died long enough ago on Earth, we wouldn't be able to tell. And so that makes me really sad. And so I think about Earth in that same way. How can we leave some remnants of, if we do destroy ourselves, how can we leave remnants for aliens in the future to discover? Like here's some nice stuff we've done, like Wikipedia and YouTube. Do we have it like in a satellite orbiting earth with a hard drive like, how do we say, how do we back up human civilization for the good parts, or all of it is good parts so that it can be preserved longer than our bodies can? That's kind of a, it's a difficult question. It also requires the difficult acceptance of the fact that we may die, and if we die, we may die suddenly as a civilization. So let's see. I think it kind of depends on the cataclysm we have observed in other parts of universe that births of gamma rays. These are high energy rays of light that actually can apparently kill entire galaxy. So there might be actually nothing, even to nothing to protect us from it. I'm also, when I'm looking actually at the past civilizations, so it's like Aztecs or so they disappear from the surface of the earth. And one can ask, why is it the case? And the way I'm thinking about it is, you know, that definitely they had some problem that they couldn't solve and maybe there was a flat and all of a sudden they couldn't drink, there was no potable water, and they all died. And I think that so far, the best solution to such a problems is, I guess, technology. So, I mean, if they would know that you can just boil water and then drink it after, then that would save their civilization. And even now, when we look actually at the current pandemic, it seems that once again, actually science comes to rescue and somehow science increases size of the action space. And I think that's a good thing. Yeah, but nature has a vastly larger action space. But still, it might be a good thing for us to keep on increasing action space. Okay, looking at past civilizations, yes, but looking at the destruction of human civilization, perhaps expanding the action space will add actions that are easily acted upon, easily executed, and as a result destroy us. So let's see, I was pondering why actually even we have negative impact on the globe. Because if you ask every single individual, they would like to have clean air, they would like healthy planet. But somehow it actually is not the case that as a collective, we are not going this direction. I think that there exists very powerful system to describe what we value. That's capitalism. It assigns actually monetary values to various activities. At the moment, the problem in the current system is that there are some things which we value, there is no cost assigned to it. So even though we value clean air or maybe we also value lack of distraction on, let's say, Internet or so, at the moment, these quantities, companies, corporations can pollute them for free. So in some sense, I wished or like and that's, I guess, purpose of politics, to align the incentive systems. And we are kind of maybe even moving in this direction. The first issue is even to be able to measure the things that we value, then we can actually assign the monetary value to them. Yeah. So it's getting the data and also probably through technology, enabling people to vote and to move money around in a way that is aligned with their values. And that's very much a technology question. So, like having one president and Congress and voting, that happens every four years or something like that, that's a very outdated idea. There could be some technological improvements to that kind of idea. So I'm thinking from time to time about these topics, but it also feels to me that it's a little bit like it's hard for me to actually make correct predictions. What is the appropriate thing to do? I extremely trust Sam Altman, our CEO, on these topics here. I'm more on the side of being, I guess, naive hippie. That, yeah, that's your life philosophy. Well, I think self doubt, and I think hippie implies optimism. Those two things are pretty good way to operate. I mean, still, it is hard for me to actually understand how the politics works or exactly how the things would play out. And Sam is really excellent with it. What do you think is rarest in the universe? You said we might be alone. What's hardest to build is another engineering way to ask that life, intelligence, or consciousness. So, like you said that we might be alone, which is the thing that's hardest to get to. Is it just the origin of life? Is it the origin of intelligence? Is it the origin of consciousness? So let me at first explain you my kind of mental model, what I think is needed for life to appear. So I imagine that at some point, there was this primordial zoop of amino acids and maybe some proteins in the ocean, and some proteins were turning into some other proteins through reaction. And you can almost think about this cycle of what turns into what, as there is a graph essentially describing which substance turns into some other substance. And essentially, life means that all the sudden in the graph has been created a cycle such that the same thing keeps on happening over and over again. That's what is needed for life to happen. And in some sense, you can think almost that you have this gigantic graph and it needs, like, a sufficient number of edges for the cycle to appear. Then, from perspective of intelligence and consciousness, my current intuition is that they might be quite intertwined. First of all, it might not be that it's like a binary thing that you have intelligence or consciousness. It seems to be more continuous component. Let's see if we look, for instance, on the event networks recognizing images, people are able to show that the activations of these networks correlate very strongly with activations in visual cortex of some monkeys. The same seems to be true about language models. Also, if you, for instance, look, if you train agent in 3d world, at first it barely recognizes what is going on. Over the time. It kind of recognizes foreground from background. Over the time, it kind of knows where there is a foot and it just follows it. Over the time, it actually starts having a 3d perception. So it is possible, for instance, to look inside of the head of an agent and ask what would it see if it looks to the right. And the crazy thing is, initially, when the agents are barely trained, these predictions are pretty bad. Over the time they become better and better. You can still see that if you ask what happens when the head is turned by 360 degrees, for some time they think that the different thing appears. And then at some stage they understand actually that the same thing is supposed to appear. So they get understanding of 3d structure. It's also very likely that they have inside some level of symbolic reasoning. They're particularly symbols for other agents. So when you look at delta agents, they collaborate together and they have some anticipation of if they would win battle. They have some expectations with respect to other agents. I might be too much anthropomorphizing how the things look for me. But then the fact that they have a symbol for other agents makes me believe that at some stage, as the, you know, as they are optimizing for skills, they would have also symbol to describe themselves. This is like a very useful symbol to have. And this particularly, I would call it like a self consciousness or self awareness. And still it might be different from the consciousness. So I guess the way how I'm understanding the word consciousness, I'd say the experience of drinking a coffee or let's say experience of being a. But that's the meaning of the word consciousness. It doesn't mean to be awake. Yeah, it feels. It might be also somewhat related to memory and recurrent connections. So it's kind of like, if you look at anesthetic drugs, they might be like they essentially disturb brainwaves such that maybe memory is not formed. So there's a lessening of consciousness when you do that. Correct. And so that's one way to intuit what is consciousness. There's also kind of another element here. It could be that it's this kind of self awareness module that you described, plus the actual subjective experience is a storytelling module that tells us a story about what we are experiencing. The crazy thing. So let's say, I mean, in meditation they teach people not to speak story inside of the head. And there is also some fraction of population who doesn't have actually narrator. I know people who don't have a narrator, and they have to use external people in order to kind of solve tasks that require internal narrator. So it seems that it's possible to have the experience without the talk. What are we talking about when we talk about the internal narrator? Is that the voice? When you like. Yeah, I thought that's what you are referring to. Well, I was referring more on it, like not an actual voice. I meant like there's some kind of like subjective experience. Feels like it's, it's fundamentally about storytelling to ourselves. It feels like, like the feeling is a story that is much, much simpler abstraction than the raw sensory information. So there feels like it's a very high level abstraction that is useful for me to feel like entity in this world. Most useful aspect of it is that because I'm conscious, I think there's an intricate connection to me, not one wanting to die. So like it's a useful hack to really prioritize not dying. Like those seem to be somehow connected. So I'm telling this story of like, it's richly feels like something to be me. And the fact that me exists in this world, I want to preserve me. And so that makes it a useful agent hack. So I will just refer maybe to the first part, as you said, about that kind of story of describing who you are. I was thinking about it. Even so, you know, obviously I like thinking about consciousness. I like thinking about AI as well. And I'm trying to see analogies of these things in AI. What would it correspond to? So, you know, OpenAI trained a model called GPT, which can generate pretty amusing text on arbitrary topic. And one way to control GPT is by putting into prefix at the beginning of the text some information. What would be the story about? You can have even chat with, you know, with GPT by saying that the chat is with Lex or Elon Musk or so. And GPT would just pretend to be you or Elon Musk or so. And it almost feels that this story that we give ourselves to describe our life, it's almost like things that you put into context of GPT. Yeah, to prime. But the context we provide to Gptheme is multimodal. So GPT itself is multimodal. GPT itself hasn't learned, actually, from experience of single human, but from the experience of humanity. It's a chameleon. You can turn it into anything. And in some sense, by providing context, it behaves as the thing that you wanted it to be. It's interesting that people have stories of who they are, and as you said, these stories, they help them to operate in the world. But it's also interesting, I guess, various people find it out through meditation or so, that there might be some patterns that you have learned when you were a kid that actually are not serving you anymore. And you also might be thinking that that's who you are, and that's actually just a story. Yeah, so it's a useful hack, but sometimes it gets us into trouble. It's a local optima. It's a local optima. You wrote that Stephen Hawking. He tweeted, Stephen Hawking asked, what breathes fire into equations, which meant, what makes, given mathematical equations, realize the physics of a universe. Similarly, I wonder what breathes fire into computation? What makes given computation conscious? Okay, so how do we engineer consciousness? How do you breathe fire and magic into the machine? So, it seems clear to me that not every computation is conscious. I mean, you can, let's say, just keep on multiplying one matrix over and over again, and maybe gigantic matrix, you can put a lot of computation. I don't think it would be conscious. So in some sense, the question is, what are the computations which could be conscious? I mean, so one assumption is that it has to do purely with computation, that you can abstract away matter other possibilities, that it's very important was the realization of computation, that it has to do with some force fields or so, and they bring consciousness. At the moment, my intuition is that it can be fully abstracted away. So, in case of computation, you can ask yourself, what are the mathematical objects or so that could bring such a properties. So, for instance, if we think about the models, AI models, what they truly try to do, or like models like GPT, is they try to predict next word or so. And this turns out to be equivalent to compressing text, because in some sense, compression means that you learn the model of reality, and you have just to remember what are your mistakes, the better you are in predicting the. And in some sense, when we look at our experience, also when you look, for instance, at the car driving, you know in which direction it will go, you are good, like in prediction. And I, you know, it might be the case that the consciousness is intertwined with compression it might be also the case that self consciousness has to do with compressor trying to compress itself. So, okay, I was just wondering, what are the objects in mathematics or computer science, which are mysterious, that could have to do with consciousness? I thought, you see, in mathematics, there is something called Gadel theorem, which means if you have sufficiently complicated mathematical system, it is possible to point the mathematical system back on itself. In computer science, there is something called halting problem. It's somewhat similar construction. So I thought that if we believe that, that under assumption that consciousness has to do with compression, then you could imagine that as they keep on compressing things, then at some point it actually makes sense for the compressor to compress itself. Meta. Compression. Yeah, consciousness is metacompression. That's an idea. And in some sense, the crazy. Thank you. So, but do you think if we think of Turing machine, a universal Turing machine, can that achieve consciousness? So is there something beyond our traditional definition of computation that's required? So it's a specific computation. And I said, this computation has to do with compression. And the compression itself, maybe other way of putting it, is like a. You are internally creating the model of reality in order, like you try insight to simplify reality in order to predict what's going to happen. And that also feels somewhat similar to how I think, actually, about my own conscious experience. So, clearly, I don't have access to reality. The only access to reality is through cable going to my brain, and my brain is creating a simulation of reality, and I have access to the simulation of reality. Are you by any chance aware of the Hutter Prize? Marcus Hutter, he made this prize for compression of Wikipedia pages. And there's a few qualities to it. One, I think has to be perfect compression, which makes. I think that little quirk, makes it much less applicable to the general task of intelligence, because it feels like intelligence is always going to be messy. Like, perfect compression feels like it's not the right goal, but it's nevertheless a very interesting goal. So, for him, intelligence equals compression. And so the smaller you make the file, given a large Wikipedia page, the more intelligent the system has to be. Yeah, that makes sense. So you can make perfect compression if you store errors. And I think that actually what he meant is you have algorithm plus errors. By the way, Hutter is a. He was PhD, advisor of Shenlec, who is a DeepMind, co founder. Yeah, yeah. So there's an interesting. And now he's a DeepMind, there's an interesting network of people. He's one of the people that I think seriously took on the task of what would an AGI system look like? I think for the longest time, the question of AGI was not taken seriously, or rather rigorously, and he did just that. Like, mathematically speaking, what would the model look like if you remove the constraints of it, having to be, having to have reasonable amount of memory, reasonable amount of running time, complexity, computation time, what would it look like? And essentially, it's a half math, half philosophical discussion of how would a reinforcement learning type of framework look like for an AGI. Yeah. So he developed the framework event to describe what's optimal with respect to reinforcement learning. Like, there is a theoretical framework which is, as you said, under assumption there is infinite amount of memory. And computer was actually one person before. His name is Solomonov. Hooter extended Solomonov work to reinforcement learning, but there exists the theoretical algorithm, which is optimal algorithm to build intelligence. And I can actually explain you the algorithm. Yes. Let's go, let's go. So the task itself. Can I just pause how absurd it is for brain in a skull trying to explain the algorithm for intelligence? Just go ahead. It is pretty crazy. It is pretty crazy that the brain itself is actually so small, and it can ponder. How to design algorithms that optimally solve the problem of intelligence. What's the algorithm? First of all, the task itself is described as you have infinite sequence of zeros and ones. Okay? You read N bits, and you are about to predict N plus one bit. So that's the task. And you could imagine that every task could be casted as such a task. So if, for instance, you have images and labels, you can just turn every image into a sequence of zeros and ones, then Label. You concatenate labels, and that's actually, you could start by having training data first, and then afterwards you have test data. So theoretically, any problem could be casted as the problem of predicting zeros and ones on this infinite tape. So let's say you read already N bits and you want to predict N plus one bit. And I will ask you to write every possible program that generates THese N bits of. And you can have you choose programming language. It can be python or c. And the difference between programming languages might be there is a difference by constant. Asymptotically, Your predictions will be equivalent. So you read N bits, you enumerate all the programs that produce these N bits in their output. And then in order to predict n plus one bit, you actually weight the programs according to their length. There is like some specific formula how you weigh them, and then the n plus one bit prediction is the prediction from each of these program according to that weight. Like statistically? Statistically. So the smaller the program, the more likely you are to pick its output. So that algorithm is grounded in the hope or the intuition that the simple answer is the right one. It's a formalization of it. Yeah. It also means like, if you would ask the question after how many years would sun explode? You can say it's more likely the answer is two to some power, because it's a shorter program. Yeah. I don't have a good intuition about how different the space of short programs are from the space of large programs. Like what is the universe where short programs, like, run things? So, as I said, the things have to agree with n bits. So even if you have, you need to start. Okay, if, if you have very short program and they're like a steals, if it's not perfect with prediction of n bits, you have to start errors. What are the errors? And that gives you the full program that agrees on n bits. Oh, so you don't agree perfectly with the n bits, and you store errors. That's like a longer, a longer program, slightly longer program, because it contains these extra bits of errors. That's fascinating. What's, what's your intuition about the programs that are able to do cool stuff like intelligence and consciousness? Are they perfectly like, is there if then statements in them? So is there a lot of exceptions at their story? So you could imagine if there would be tremendous amount of if statements, then they wouldn't be that short. In case of neural networks, you could imagine that what happens is when you start with an uninitialized neural network, it stores internally many possibilities how the problem can be solved. And SDD is magnifying some paths which are slightly similar to the correct answer. It's magnifying correct programs. And in some sense, SDD is a search algorithm in the program space, and the program space is represented by the wiring inside of the neural network. And there's an insane number of ways how the features can be computed. Let me ask you the high level basic question that's not so basic. What is deep learning? Is there a way you'd like to think of it that is different than like a generic textbook definition? The thing that I hinted just a second ago is maybe that closest to how I'm thinking these days about deep learning. So the statement is neural networks can represent some programs. It seems that various modules that we are actually adding up to are like, we want networks to be deep because we want multiple steps of the computation and deep learning provides a way to represent space of programs which is searchable, and it's searchable with stochastic gradient descent. So we have an algorithm to search over a humongous number of programs, and gradient descent kind of bubbles up the things that tend to give correct answers. A neural network with fixed weights that's optimized. Do you think of that as a single program? So there is a work by Christopher Olach where he. So he works on interpretability of neural networks, and he was able to identify inside of the neural network, for instance, a detector of a wheel for a car or the detector of a mask for a car. And then he was able to separate them out and assemble them together using a simple program for the detector, for a car detector. That's like, if you think of traditionally defined programs, that's like a function within a program that this particular neural network was able to find. And you can tear that out just like you can copy and paste some stack overflow that. So any program is a composition of smaller programs. Yeah, I mean, the nice thing about the neural networks is that it allows the things to be more fuzzy than in case of programs. In case of programs, you have this, like a branching this way or that way. And the neural networks, they have an easier way to be somewhere in between or to share things. What to use the most beautiful or surprising idea in deep learning, in the utilization of these neural networks, which, by the way, for people who are not familiar, neural networks is a bunch of, what would you say? It's inspired by the human brain. There's neurons, there's connection between those neurons, there's inputs and there's outputs, and there's millions or billions of those neurons. And the learning happens by adjusting the weights on the edges that connect these neurons. Thank you for giving definition that I supposed to do it, but I guess you have enough empathy to listeners to actually know that that might be useful. No, that's like. So I'm asking Plato of like, what is the meaning of life? He's not going to answer. You're being philosophical and deep and quite profound. Talking about the space of programs, which is, is very interesting, but also for people who just not familiar what the hell we're talking about when we talk about deep learning. Anyway, sorry. What is the most beautiful or surprising idea to you in all the time you've worked at deep learning, and you worked on a lot of fascinating projects, applications of neural networks. It doesn't have to be big and profound. It can be a cool trick. Yeah, I mean, I'm thinking about the trick, but, like, it's still amusing to me that it works at all. That let's say that the extremely simple algorithm stochastic gradient descent, which is something that I would be able to derive on the piece of paper to high school student, when put at the scale of thousands of machines, actually can create the behaviors which we called human like behaviors. So, in general, any application of stochastic gradient descent to neural networks is amazing to you? Or is there a particular application in natural language reinforcement learning? And also, would you attribute that success to? Is it just scale? What profound insight can we take from the fact that the thing works for gigantic sets of variables? I mean, the interesting thing is, these algorithms, they were invented decades ago, and people actually gave up on the idea. And back then, they thought that we need profoundly different algorithms, and they spent a lot of cycles on very different algorithms. And I believe that we have seen that various innovations that say, like transformer or dropout or so, they can pass the help. But it's also remarkable to me that this algorithm, from sixties or so, or, I mean, you can even say that the gradient descent was invented by Leibniz in, I guess, 18th century or so, that actually is the core of learning in the past. People are, it's almost like a. Out of the, maybe an ego. People are saying that it cannot be the case that such a simple algorithm is the, you know, could solve complicated problems. So they were in search for the other algorithms. And as I'm saying, like, I believe that actually, we are in the game where there is. There are actually, frankly, three levers. There is compute. There are algorithms, and there is data. If we want to build intelligent systems, we have to pull all three levers, and they are actually multiplicative. It's also interesting, you ask, is it only compute people internally? They did the studies to determine how much gains they were coming from different levers. And so far, we have seen that more gains came from compute than algorithms. But also, we are in the word that in case of compute, there is a kind of exponential increase in funding. And at some point, it's impossible to invest more. It's impossible to invest $10 trillion, as we are speaking about, let's say, all taxes in us. But you're talking about money. There could be innovation in the compute. That's true as well. So, I mean, there are a few pieces. So one piece is human brain is an incredible supercomputer, and they're like a. It has 100 trillion parameters. Or like, if you try to count various quantities in the brain. There are like a neuron synapses, there are small number of neurons, there is a lot of synapses. It's unclear even how to map synapses to parameters of neural networks, but it's clear that there are many more. So it might be the case that our networks are still somewhat small. It also might be the case that they are more efficient than brain or less efficient by some huge factor. I also believe that there will be, at the moment, we are at the stage that these neural networks, they require 1000 x or a huge factor of more data than humans do. And it will be a matter of. There will be algorithms that vastly decrease sample complexity. I believe so. But the place where we are heading today is dart domains, which contains million x more data. And even though computers might be 1000 times slower than humans in learning, that's not the problem. For instance, I believe that it should be possible to create superhuman therapists by, and there are even simple steps of doing it. And the core reason is there is just machine will be able to read way more transcripts of therapies, and then it should be able to speak simultaneously with many more people, and it should be possible to optimize it all in parallel. And, well, there's. Now you're touching on something I deeply care about and think is way harder than we imagine. What's the goal of a therapist? What's the goal of therapist? So, okay, so one goal. Now, this is terrifying to me, but there's a lot of people that contemplate suicide, suffer from depression, and they could significantly be helped with therapy. And the idea that an AI algorithm might be in charge of that, it's like a life and death task. It's, uh. The stakes are high. So one goal for a therapist, whether human or AI, is to prevent suicide. Ideation to prevent suicide. How do you achieve that? So, let's see. So, to be clear, I don't think that the current models are good enough for such a task, because it requires insane amount of understanding empathy, and the models are far from displaced. But it's. But do you think that understanding empathy, that signal is in the data? I think there is some signal in the data, yes. I mean, there are plenty of transcripts of conversations. And it is possible to. It is possible from it to understand personalities. It is possible from it to understand if conversation is friendly, amicable, antagonistic. I believe that given the fact that the models that we train now, they are chameleons, that they can have any personality, they might turn out to be better in understanding personality of other people than anyone else. And they empathetic to be empathetic. Yeah. Interesting. But I wonder if there's some level of multiple modalities required to be able to be empathetic of the human experience. Whether language is not enough to understand death, to understand fear, to understand childhood trauma, to understand wit and humor required when you're dancing with a person who might be depressed or suffering both humor and hope and love and all those kinds of things. So there's another underlying question, which is self supervised versus supervised. So can you get that from the data by just reading a huge number of transcripts? I actually. So I think that reading huge number of transcripts is step one. It's like at the same way as you cannot learn to dance if just from YouTube, by watching it, you have to actually try it out yourself. So I think that here that's a similar situation. I also wouldn't deploy the system in the high stakes situations right away, but kind of see gradually where it goes. And obviously, initially it would have to go hand in hand with humans. But at the moment, we are in the situation that actually, there is many more people who actually would like to have a therapy or speak with someone, then there are therapies out there. So fundamentally, I was thinking, what are the things that can vastly increase people's well being? Therapy is one of them. Being meditation is other one. I guess maybe human connection is the third one. And I guess, pharmacologically it's also possible, maybe direct brain stimulation or something like that. But these are pretty much options out there. Then let's say the way I'm thinking about the AGI endeavor is, by default, that's an endeavor to increase amount of wealth. And I believe that we can vastly increase amount of wealth for everyone and simultaneously. So, I mean, these are like two endeavors that make sense to me. One is like, essentially increase amount of wealth, and second one is increase overall human well being. And those are coupled together and they. Can, I would say these are different topics. One can help another. And, you know, therapist is a funny word, because I see friendship and love as therapy. I mean, so therapist, broadly defined as just friendship as a friend. So, like, therapist has a very kind of clinical sense to it. But what is human connection? You're like, not to get all Camus and Dostoevsky on you, but life is suffering, and we draw, we seek connection with other humans as we desperately try to make sense of this world in the deep, overwhelming loneliness that we feel inside. So I think connection has to do with understanding. And I think that almost like a lack of understanding causes suffering. If you speak with someone and you do, you feel ignored. That actually causes pain. If you are feeling deeply understood, that actually they might not even tell you what to do in life, but like a pure understanding or just being heard. Understanding is a kind of. It's a lot, you know, just being heard, feel like you're being heard. Like somehow that's alleviation, temporarily, of the loneliness that if somebody knows you're here with their body language, with the way they are, with the way they look at you, with the way they talk, you feel less alone for a brief moment. Yeah, very much agree. So I thought in the past about somewhat similar question to yours, which is, what is love? Rather what is connection? And obviously, I think about these things from AI perspective. What would it mean? So I said that intelligence has to do with some compression, which is more or less, like I can say, almost understanding of what is going around. It seems to me that other aspect is there seem to be reward functions. And you can have a, you know, reward for food, for maybe human connection, for, let's say, warmth, sex and so on. And it turns out that the various people might be optimizing slightly different reward functions. They essentially might care about different things. And in case of love, at least the love between two people, you can say that boundary between people dissolves to such extent that they end up optimizing each other reward functions. Yeah. That's interesting. Celebrate the success of each other. Yeah. In some sense, I would say love means helping others to optimize their reward functions. Not your reward functions, not the things that you think are important, but the things that the person cares about. You try to help them to optimize it. So love is, if you think of two reward functions, it's a condition. You combine them together. Yeah, pretty much. Maybe like with the weight and it depends, like the dynamic of the relationship. Yeah, I mean, you could imagine that if you are fully optimizing someone's reward function without yours, then maybe are creating codependency or something like that. I'm not sure what's the appropriate weight. But the interesting thing is I even think that the individual person, we ourselves, we are actually less of a unified insight. So, for instance, if you look at the donut, on the one level, you might think, oh, this looks tasty, I would like to eat it. On other level, you might tell yourself, I shouldn't be doing it because I want to gain muscles. So. And, you know, you might do it regardless, kind of against yourself. So it seems that even within ourselves, they're almost like a kind of intertwined Personas. And I believe that the self love means that the love between all these Personas, which also means being able to love, love yourself when we are angry or stressed or so combining all those. Reward functions of the different selves you have. Yeah. And accepting that they are there. Like, you know, often people, they have a negative self talk, or they say, I don't like when I'm angry. And like, I try to imagine, try to imagine if there would be like a small baby lex, like a five years old, who's angry, angry. And then you're like, you shouldn't be angry. Like, stop being angry. Yeah, but like, instead, actually, you want the lex to come over, give him a hug, and he's like, I say, it's fine, okay, you can be angry as long as he want, and then he would stop. Or maybe not, or maybe not. But you cannot expect it even. Yeah, but still, that doesn't explain the why of love. Like, why is love part of the human condition? Why is it useful to combine the reward functions? It seems like that doesn't. I mean, I don't think reinforcement learning frameworks can give us answers to why. Even the hutter framework has an objective function that's static. So we came to existence as a consequence of evolutionary process, and in some sense, the purpose of evolution is survival. And then this complicated optimization objective baked into us, let's say compression, which might help us operate in the real world and bake into us various reward functions. Yeah. Then to be clear, at the moment, we are operating in the regime which is somewhat out of distribution, where they even evolution optimize us. It's almost like love is a consequence of cooperation that we've discovered is useful. Correct. In some way. It's even the case if you. I just love the idea that love is like the outer distribution, or it's. Not out of distribution. It's like, as you said, it evolved for cooperation. Yes. And I believe that in some sense, cooperation ends up helping each of us individually. So it makes sense evolutionary. And there is in some sense, and love means there is this dissolution of boundaries that you have a shared reward function. And we evolve to actually identify ourselves with larger groups. So we can identify ourselves with a family, we can identify ourselves with a country to such extent that people are willing to give away their life for country. So we are wired, actually, even for love. And at the moment, I guess maybe it would be somewhat more beneficial if we would identify ourselves with all the humanity as a whole. So you can clearly see when people travel around the world, when they run into person from the same country, they say, oh, which city you are. And all of a sudden, they find all these similarities. They befriend those folks earlier than others. So there is some sense of the belonging. And I would say, I think it would be overall good thing to the world for people to move towards. I think it's even called open individualism, move toward the mindset of a larger and larger groups. The challenge there, that's a beautiful vision, and I share it to expand that circle of empathy, that circle of love towards the entirety of humanity. But then you start to ask, well, where do you draw the line? Because why not expand it to other conscious beings? And then finally, for our discussion, something I think about is, why not expand it to AI systems? We start respecting each other when the entity on the other side has the capacity to suffer, because then we develop a capacity to sort of empathize. And so I could see AI systems that are interacting with humans more and more, having conscious, like, displays. So, like, they display consciousness through language and through other means. And so then the question is, like, well, is that consciousness because they're acting conscious? And so, you know, the reason we don't like torturing animals is because they look like they're suffering when they're tortured. And if AI looks like it's suffering when it's tortured, how is that nothing requiring of the same kind of empathy from us and respect and rights than animals do and other humans do? I think it requires empathy as well. I mean, I would like, I guess, us or humanity or so make a progress in understanding what consciousness is, because I don't want just to be speaking about the philosophy, but rather actually make a scientific. To have a, like, you know, there was a time that people thought that there is a force of life, and the things that have this force, they are alive. And I think that there is actually a path to understand exactly what consciousness is. And in some sense, it might require essentially putting probes inside of a human brain, what neuralink does. So the goal there, I mean, there's several things with consciousness that make it a real discipline, which is, one is rigorous measurement of consciousness, and then the other is the engineering of consciousness, which may or may not be related. I mean, you could also run into trouble. Like, for example, in the United States, the Department dot Department of Transportation and a lot of different places put a value on human life. I think dots, values, $9 million per person, sort of. In that same way you can get into trouble if you put a number on how conscious of being is, because then you can start making policy. If a cow is 0.1 or like 10% as conscious as a human, then you can start making calculations and it might get you into trouble. But then again, that might be a very good way to do it. I would like to move to that place that actually we have scientific understanding what consciousness is, and then we'll be able to actually assign value. And I believe that there is even the path for the experimentation in it. So we said that you could put the probes inside of the brain. There is actually a few other things that you could do with devices like neuralinken. So you could imagine that the way even to measure if AI system is conscious is by literally just plugging into the brain. I mean, that assumes that it's kind of easy, but the plug into the brain and asking person if they feel that their consciousness expanded. This direction, of course, has some issues. You can say if someone takes a psychedelic drug, they might feel that their consciousness expanded even though the drug itself is not conscious, right? So, like, you can't fully trust the self report of a person saying their consciousness is expanded or not. Let me ask you a little bit about psychedelics, because there've been a lot of excellent research on different psychedelics. Psilocybin, MDMA, even DMT drugs in general. Marijuana too. What do you think psychedelics do to the human mind? It seems they take the human mind to some interesting places. Is that just a little hack, a visual hack, or is there some profound expansion of the mind? So let's see. I don't believe in magic. I believe in that. I believe in science, in causality still, let's say. And then, as I said, I think that brain, that our subjective experience of reality is we live in the simulation run by our brain and the simulation that our brain runs. They can be very pleasant or very hellish drugs. They are changing some hyperparameters of the simulation. It is possible, thanks to change of these hyperparameters, to actually look back on your experience and even see that the given things that we took for granted, they are changeable. So they allow to have an amazing perspective. There is also, for instance, the fact that after DMT, people can see the full movie inside of their head gives me further belief that the brain can generate the full movie, that the brain is actually learning the model of reality to such extent that it tries to predict what's going to happen next. Yeah. Very high resolution, so it can replay reality. Is that true? Extremely high resolution, yeah. It's also kind of interesting to me that somehow there seems to be some similarity between these drugs and meditation itself. And I actually started even these days to think about meditation as a psychedelic. Do you practice meditation? I practice meditation. I mean, I once, few times on the retreats. And it feels after, like after second or third day of meditation, there is almost like a sense of tripping. What does a meditation retreat entail? So you wake up early in the morning and you meditate for extended period of time alone. Sorry. To keep in mind. Yeah. So it's optimized. Even though there are other people, it's optimized for isolation. So you don't speak with anyone, you don't actually look into other people's eyes and you sit on the chair, say Vipassana. Meditation tells you to focus on the breath. So you try to put all attention into breathing and breathing in and breathing out. And crazy thing is that as you focus attention like that, after some time there's starts coming back like some memories that you completely forgotten. It almost feels like you have a mailbox and then you are just archiving email one by one. And at some point, at some point there is a amazing feeling of getting to mailbox. Zero, zero emails. And it's very pleasant. It's kind of, it's, it's, it's crazy to me that, that once you resolve these inner stories or like inner traumas, then once there is nothing left, the default state of human mind is extremely peaceful and happy. Like some sense. It feels that. It feels, at least to me, that way, how when I was a child, that I can look at any object and it's very beautiful. I have a lot of curiosity about the simple things and that's where the usual meditation takes me. Are you, what are you experiencing? Are you just taking in simple sensory information and they're just enjoying the rawness of that sensory information. So there's no, there's no memories or all that kind of stuff you're just enjoying being. Yeah, pretty much. I mean, still there is a, it's, it's thoughts are slowing down, sometimes they pop up, but it's also somehow the extended meditation takes you to the space that they are way more friendly, way more positive. There is also this thing that it almost feels that. It almost feels that we are constantly getting a little bit of a reward function and we are just spreading this reward function on various activities. But if you stay still for extended period of time, it kind of accumulates, accumulates, accumulates. And there is a sense. There is a sense that some point it passes some threshold and it feels as drop is falling into kind of ocean of love and bliss. And that's like a. This is like a very pleasant. And that's what I'm saying. Like that corresponds to the subjective experience. Some people, I guess in spiritual community, they describe it that that's the reality. And I would say, I believe that there are all sorts of subjective experience that one can have. And I believe that, for instance, meditation might take you to the subjective experiences, which are very pleasant, collaborative, and I would like a world to move toward a more collaborative place. Yeah, I would say that's very pleasant. And I enjoy doing stuff like that. I wonder how that maps to your mathematical model of love with the reward function combining a bunch of things. It seems like our life then is we have this reward function and we're accumulating a bunch of stuff in it with weights. It's like multi objective. And what meditation is, is you just remove them. Remove them until the weight on one or just a few is very high. And that's where the pleasure comes from. Yeah. So something similar how I'm thinking about this. So I told you that there is this like, there is a story of who you are. And I think almost about it as a, you know, text prepended to GPT. Yeah. And some people refer to it as ego. Okay. It's like a story who you are. Okay, so ego is the prompt for GPT-3 or GPT. Yes. Yes. And that's description of you. And then with meditation, you can get to the point that actually you experience things without the prompt and you experience things like, as they are, you are not biased over the description how they supposed to be. That's very pleasant. And then with respect to the reward function, it's possible to get to the point that there is dissolution of self. And therefore you can say that you are having your brain attempts to simulate the reward function of everyone else or everything. There is this love which feels like a oneness with everything. And that's also, you know, very beautiful, very pleasant. At some point you might have a lot of altruistic thoughts during that moment, and then the self always comes back. How would you recommend, if somebody is interested in meditation, like a big thing to take on as a project, would you recommend a meditation retreat? How many days? What kind of thing would you recommend? I think that actually retreat is the way to go. It almost feels that, as I said, like a meditation is a psychedelic, but when you take it in the small dose, you might barely feel it. Once you get the high dose, actually, you're gonna feel it. So even cold turkey, if you haven't really seriously meditated for a prolonged period of time, just go to a retreat? Yeah. How many days start? Weekend? One weekend. So like two, three days. And it's like, it's interesting that first or second day it's hard. And at some point it becomes easy. There's a lot of seconds in a day. How hard is the meditation retreat? Just sitting there in a chair. So the thing is, actually, it literally just depends on your. On your own framing. Like, if you are in the mindset that you are waiting for it to be over or you are waiting for nirvana to happen, it will be very unpleasant. And in some sense, even the difficulty, it's not even in the lack of being able to speak with others. Like, you are sitting there, your legs will hurt from sitting. In terms of the practical things, do you experience kind of discomfort, like physical discomfort of just sitting? Like your butt being numb, your legs being sore, all that kind of stuff? Yes. You experience it and then the. That they teach you to observe it, rather. And it's like the crazy thing is you at first might have a feeling toward trying to escape it, and that becomes very apparent that that's extremely unpleasant. And then you just observe it, and at some point it just becomes. It just is. It's like, I remember with Ilya told me some time ago that he takes a cold shower, and his mindset of taking a cold shower was to embrace suffering. Yeah, excellent. I do the same. This is your style? Yeah, it's my style. I like this. So my style is actually. I also sometimes take cold showers. It is purely observing how the water goes through my body. Like, purely being present, not trying to escape from there. Yeah. And I would say then it actually becomes pleasant. It's not like, ah, well, that, that's interesting. Um, I I'm also. That mean, that's. That's the way to deal with anything really difficult, especially in the physical space, is to observe it, to say it's pleasant. Hmm. It's a. I would use a different word. You're, um, you're accepting of the full beauty of reality, I would say. Cause say pleasant, but yeah, in some sense it is pleasant. That's the only way to deal with a cold shower is to become an observer and to find joy in it. Same with, like, really difficult physical exercise or like, running for a really long time. Endurance events, just anytime you're exhausted, any kind of pain. I think the only way to survive it is not to resist. It is to observe it. You mentioned Ilya. Ilya Setsgever, he's our chief scientist, but also he's a very close friend of mine. He co founded OpenAI with you. I've spoken with him a few times. He's brilliant. I really enjoy talking to him. His mind, just like yours, works in fascinating ways. Both of you are not able to define deep learning simply, what's it like having him as somebody you have technical discussions with in space of machine learning, deep learning, AI, but also life. What's it like when these two agents get into a self play situation in a room? What's it like collaborating with him? So I believe that we have extreme respect to each other. So I love Ilya's insight, both, like, I guess, about consciousness, life, AI. But in terms of the. It's interesting to me because you're a brilliant thinker in the space of machine learning, like intuition, like digging deep in what works, what doesn't, why it works, why it doesn't. And so is Ilya. I'm wondering if there's interesting deep discussions you've had with him in the past, or disagreements that were very productive. So I can say I also understood over the time, where are my strengths? So, obviously we have plenty of AI discussions and. And I myself have plenty of ideas, but I consider Ilya one of the most prolific AI scientists in the entire world. And I think that I realized that maybe my super skill is being able to bring people to collaborate together, that I have some level of empathy that is unique in AI world, and that might come from either meditation, psychedelics, let's say. I read just hundreds of books on this topic, so. And I also went through a journey of, you know, I develop all sorts of algorithms, so I think that maybe I can. That's my superhuman skill. Ilya is one of the best AI scientists, but then I'm pretty good in assembling teams, and I'm also not holding to people. Like, I'm growing people, and then people become managers at Openaiden that grew, many of them, like a research manager. So you find places where you're excellent, and he finds, like, his deep scientific insights is where he is, and you find ways you can, the puzzle pieces fit together. Correct. Ultimately, for instance, let's say, ilya, he doesn't manage people. That's not what he likes, or so I like hanging out with people. By default. I'm an extrovert and I care about people. Oh, interesting. Okay. Okay, cool. So that fits perfectly together. But I mean, I also just like your intuition about various problems in machine learning. He's definitely one I really enjoy. I remember talking to him about something I was struggling with, which is coming up with a good model for pedestrians, for human beings across the street in the context of autonomous vehicles. And he immediately started to, like, formulate a framework within which you can evolve a model for pedestrians, like, through self play, all that kind of mechanisms. The depth of thought on a particular problem, especially problems he doesn't know anything about, is fascinating to watch. It makes you realize, like, yeah, the limits that the human intellect might be limitless, or it's just impressive to see a descendant of ape come up with clever ideas. Yeah, I mean, so even in the space of deep learning, when you look at various people, there are people now who invented some breakthroughs once, but there are very few people who did it multiple times. And you can think, if someone invented it once, that might be just a sheer luck. And if someone invented it multiple times, you know, if a probability of inventing it once is one over a million, then probability of inventing it twice or three times would be one over a million square or to the power of three, which would be just impossible. So it literally means that it's given that it's not the luck. And Ilya is one of these few people who, who have a lot of these inventions in his arsenal. It also feels that, for instance, if you think about folks like Gauss or Euler, at first they read a lot of books, and then they did thinking, and then they figure out math, and that's how it feels with Ilya. At first, he read stuff, and then he spent his thinking cycles. And that's a really good way to put it. When I talk to him, I see thinking. He's actually thinking. He makes me realize that there's deep thinking that the human mind can do. Most of us are not thinking deeply. You really have to put a lot of effort to think deeply. Really put myself in a place where I think deeply about a problem. It takes a lot of effort. It's like a. It's like an airplane taking off or something. You have to achieve deep focus. He's just, he's. What is it? His brain is like a vertical takeoff in terms of airplane analogy. So it's interesting, but it. I mean, Cal Newport talks about this as ideas of deep work. It's, you know, most of us don't work much at all in terms of like, like deeply think about particular problems, whether it's a math, engineering, all that kind of stuff. You want to go to that place often, and that's real hard work. And some of us are better than others at that. So I think that the big piece has to do with actually even engineering your environment that. Such that it's conducive to that. Yeah. So, um, see, both Ilya and I, on the frequent basis we kind of disconnect ourselves from the world in order to be able to do extensive amount of thinking. Yes. So Ilya, usually he just leaves iPad at hand. He loves his iPad. And for me, I'm even sometimes just going for a few days to different location to Airbnb. I'm turning off my phone and there is no access to me. And that's extremely important for me to be able to actually just formulate new thoughts, to do deep work rather than to be reactive. And the older I am, the more of these random tasks are at hand. Before I go on to that thread, let me return to our friend GPT. Let me ask you another ridiculously big question. Can you give an overview of what GPT-3 is? Or like you say in your Twitter bio, GPT n one, how it works and why it works? So GPT-3 is a humongous neural network. Let's assume that we know what is neural network definition and it is trained on the entire Internet and just to predict next word. So let's say it sees part of the article, and the only task that it has at hand it is to say, what would be the next word? What would be the next word? And it becomes really exceptional at the task of figuring out what's the next word. So you might ask, why would this be an important task? Why would it be important to predict what's the next word of? And it turns out that a lot of problems can be formulated as a text completion problem. So GPT is purely learning to complete the text. But you could imagine, for instance, if you are asking a question, who is president of United States? Then GPT can give you an answer to it. It turns out that many more things can be formulated this way. Format text in the way that you have sentence in English. You make it even look like some content of a website elsewhere, which would be teaching people how to translate things between languages. So it would be en text in English fr colon. And then you ask model to continue. And it turns out that such a model is predicting translation from English to French. And the crazy thing is that this model can be used for way more sophisticated tasks. So you can format text such that it looks like a conversation between two people, and that might be a conversation between you and Elon Musk. And because the model read all the text about Elon Musk, it will be able to predict Elon Musk words as it would be Elon Musk. It will speak about colonization of Mars, about sustainable future, and so on. And it's also possible to even give arbitrary personality to the model. You can say, here is a conversation with a friendly AI bot, and the model will complete the text as a friendly AI bot. So, I mean, how do I express how amazing this is? So, just to clarify a conversation, generating a conversation between me and Elon Musk, it wouldn't just generate good examples of what Elon would say. It would get the syntax all correct. So, like, interview style, you would say, like, Elon Cohen and Lex Cohen. Like it. It's not just like inklings of semantic correctness. It's like the whole thing. Grammatical, syntactic, semantic. It's just really, really impressive generalization. Yeah. I mean, I also want to, you know, provide some caveats so it can generate few paragraphs of coherent text, but as you go to longer pieces, it actually goes off the rails. Okay. If you would try to write a book, it won't work out this way. What way does it go off the rails, by the way? Is there interesting ways in which it goes off the rails? What falls apart first? So the model is trained on all the existing data that is out there, which means that it is not trained on its own mistakes. So, for instance, if it would make a mistake, then to give you an example. So let's say I have a conversation with a model pretending that is Elon Musk, and then I start putting some. I start actually making up things which are not factual, I would say, sounds. Like Twitter, but I got you. Sorry. Yeah. Okay. I don't know. I would say that Elon is my wife, and the model will just keep on carrying it on and. As if it's true. Yes. And in some sense, if you would have a normal conversation with Elon, he would be, what the fuck? Yeah, there would be some feedback. So the model is trained on things that humans have written, but through the generation process, there's no human in the loop feedback. Correct. That's fascinating. Makes sense. So it's magnifies. Like, the errors get magnified and magnified. And it's also interesting. I mean, first of all, humans have the same problem. It's just that we make fewer errors and magnify the errors slower. I think that actually what happens with humans is if you have a wrong belief about the world as a kid, then very quickly we'll learn that it's not correct because you are grounded in reality and you are learning from your new experience. But do you think the model can correct itself too, won't it, through the power of the representation? And so the absence of Elon Musk being your wife, information on the Internet, won't it correct itself? There won't be examples like that. So the errors will be subtle at first. Subtle at first. And in some sense, you can also say that the data that is nothing out there is a data which would represent how the human learns, and maybe model would be trained on such a data, then it would be better off. How intelligent is GPT-3 do you think? Like, when you think about the nature of intelligence, it seems exceptionally impressive. But then if you think about the big AGI problem, is this footsteps along the way to AGI. So let's see. It seems that intelligence itself is. There are multiple axes of it, and I would expect that the systems that we are building, they might end up being superhuman on some axis and subhuman on some other axis. It would be surprising to me, on all axis simultaneously, they would become superhuman. Of course, people ask this question, is GPT a spaceship that, that would take us to moon, or are we building a ladder to heaven? That we are just building bigger and bigger ladder? And we don't know in some sense, which one of these two. Which one is better? I'm trying to. I like stairway to heaven. It's a good song. So I'm not exactly sure which one is better. But you're saying, like, the spaceship to the moon is actually effective. Correct. So people who criticize GPT, yeah, they say, the other guy is just building a taller ladder and it will never reach the moon. And at the moment, I would say, the way I'm thinking is, this is like a scientific question. And I'm also, in heart, I'm a builder creator. And like, I'm thinking, let's try out. Let's see how far it goes. And so far, we see constantly that there is a progress. Yeah. So do you think GPT four, GPT five, GPT N plus one, will. There'll be a phase shift, like a transition to a place where we'll be truly surprised? Then again, like, GPT-3 is already very truly surprising. The people that criticize GPT-3 as a stair, as a, what is it? Ladder to heaven? I think too quickly get accustomed to how impressive it is that the prediction of the next word can achieve such depth of semantics, accuracy of syntax, grammar and semantics. Do you think GPT four and five and six will continue to surprise us? I mean, definitely there will be more impressive models. There is a question, of course, if there will be a phase shift. And also even the way I'm thinking about these models is that when we build these models, we see some level of the capabilities, but we don't even fully understand everything that the model can do. And actually, one of the best things to do is to allow other people to probe the model, to even see what is possible. Hence using GPT as an API and opening it up to the world. Yeah, I mean, so when I'm thinking from perspective of obviously various people that have concerns about AGI, including myself, and when I'm thinking from perspective, what's the strategy even to deploy these things to the world? The one strategy that I have seen many times working is the iterative deployment, that you deploy slightly better versions and you allow other people to criticize you. So you actually are tried out. You see where there are fundamental issues, and it's almost, you don't want to be in that situation that you are holding into powerful system, and there's like a huge overhang, then you deploy it and it might have a random, chaotic impact on the world. So you, you actually want to be in the situation that they are gradually deploying systems. I asked this question of Ilya. Let me ask you this question. I've been reading a lot about Stalin and power. If you're in possession of a system that's like AGI, that's exceptionally powerful, do you think your character and integrity might become corrupted? Like, famously, power corrupts and absolute power corrupts absolutely. So I believe that you want, at some point, to work toward distributing the power. I think that you want to be in the situation that actually, AGI is not controlled by a small number of people, but essentially by a larger collective. So the thing is, that requires a George Washington style move in the ascent to power. There's always a moment when somebody gets a lot of power and they have to have the integrity and the moral compass to give away that power, that humans have been good and bad throughout history at this particular step. And I wonder, I wonder, we blind ourselves, for example, between nations, a race towards AI, race between nations. We might blind ourselves and justify to ourselves the development of AI without distributing the power, because we want to defend ourselves. Against China, against Russia, that kind of logic. And I wonder how we design governance mechanisms that prevent us from becoming power hungry and in the process destroying ourselves. So let's see. I have been thinking about this topic quite a bit, but I also want to admit that once again, I actually want to rely way more on Sam Altman on it herald and excellent block on how even to distribute wealth. And he proposed in his block to tax equity of the companies rather than profit and to distribute it. And this is an example of Washington move. I guess I personally have insane trust in some. He already spent plenty of money running universal basic income project. That gives me, I guess, maybe some level of trust to him. But I also, I guess, love him as a friend. Yeah. I wonder because we're sort of summoning a new set of technologies. I wonder if we'll be cognizant like you're describing the process of OpenAI, but it could also be at other places, like in the us government. Right. Both China and the US are now full steam ahead on autonomous weapons systems development. And that's really worrying to me because in the framework of something being a national security danger or a military danger, you can do a lot of pretty dark things that blind our moral compass. And I think AI will be one of those things. In some sense, the mission and the work you're doing at OpenAI is like the counterbalance to that. So you want to have more open AI and less autonomous weapon systems. I like these statements, to be clear, this interesting, and I'm thinking about it myself, but this is a place that I, I put my trust actually in Sam's hands because it's extremely hard for me to reason about it. Yeah. I mean, one important statement to make is it's good to think about this. Yeah. No question about it. No question. Even like low level, quote unquote engineer. Like, there's such a. I remember I programmed a car RC cardinal. They went really fast, like 30, 40 miles an hour. And I remember I was sleep deprived. So I programmed it pretty crappily. And it, like the code froze. So it's doing some basic computer vision and it's going around on track, but it's going full speed. And there was a bug in the code that the car just went. It didn't turn, it went straight full speed and smash into the wall. And I remember thinking the seriousness with which you need to approach the design of artificial intelligence systems and the programming of artificial intelligence systems is high because the consequences are high. Like that little car smashing into the wall for some reason I immediately thought of like an algorithm that controls nuclear weapons having the same kind of bug. And so, like, the lowest level engineer and the CEO of a company all need to have the seriousness in approaching this problem and thinking about the worst case consequences. So I think that is true. I mean, what I also recognize in myself and others even asking this question, is that it evokes a lot of fear. And fear itself ends up being actually quite debilitating. The place where I arrived at the moment might sound cheesy or so, but it's almost to build things out of love rather than fear. Yeah, I can focus on how I can maximize the value, how the systems that I'm building might be useful. I'm not saying that the fear doesn't exist out there and it totally makes sense to minimize it, but I don't want to be working because I'm scared. I want to be working out of passion, out of curiosity, out of the, you know, looking forward for the positive. Future with the definition of love arising from rigorous practice of empathy. So not just like your own conception of what is good for the world, but always listening to others. Correct. Like at the loft where I'm considering reward functions of others, others to limit. To infinity is like a sum like one to n, where n is 7 billion or whatever. It is not. Not projecting my reward functions on others. Yeah, exactly. Okay, can we just take a step back to something else super cool, which is OpenAI codecs. Can you give an overview of what OpenAI codecs and GitHub copilot is, how it works, and why the hell it works so well. So with GPT-3 we noticed that the system, you know, the system training on all the language out there started having some rudimentary coding capabilities. So we're able to ask it, you know, to implement addition function between two numbers, and indeed it can write Python or JavaScript code for that. Then we thought we might as well just go full steam ahead and try to create a system that is actually good at what we are doing every day ourselves, which is programming. We optimize models for proficiency in coding. We actually even created models that both have a comprehension of language and code, and Codex is API for these models. So it's first pre trained on language, and then I don't know if you can say fine tuned because there's a lot of code, but it's language and. Code, it's language and code. It's also optimized for various things like, let's say low latency and so on. Codex is the API that's similar to GPT-3. We expect that there will be proliferation of the potential products that can use coding capabilities. And I can, I can speak about it in a second. Copilot is the first product and developed by GitHub. So as we're building models, we wanted to make sure that these models are useful. And we worked together with GitHub on building the first product. Copilot is actually as you code, it suggests you code completions. And we have seen in the past, there are various tools that can suggest how to few characters of the code or the line of code. The thing about copilot is it can generate ten lines of code. It's often the way how it works is you often write in the comment what you want to happen, because people in comments, they describe what happens next. So these days when I code, instead of going to Google to search for the appropriate code to solve my problem, I say, oh, for this array, could you smooth it? And then it imports some appropriate libraries and say it uses numpy convolution or so that I was not even aware that exists, and it does the appropriate thing. So you write a comment, maybe the header of a function, and it completes the function. Of course, you don't know what is the space of all the possible small programs it can generate. What are the failure cases, how many edge cases, how many subtle errors there are, how many big errors there are, it's hard to know. But the fact that it works at all in a large number of cases is incredible. It's a kind of search engine into code that's been written on the Internet. Correct. For instance, when you search things online, then usually you get to some particular case. If you go to stack overflow, people describe the one particular situation and then they seek for a solution. But in case of copilot, it's aware of your entire context. And in context is, oh, these are the libraries that you are using. That's the set of the variables that is initialized and on the spot. It can actually tell you what to do. So the interesting thing is, and we think that the copilot is one possible product using codex, but there is a place for many more. So internally we tried out to create other fun products. It turns out that a lot of tools out there, let's say Google Calendar or Microsoft Word or so they all have internal API to build plugins around them. So there is a way in, the sophisticated way to control calendar or Microsoft Word today, if you want, if you want more complicated behaviors from these programs, you have to add a new button for every behavior. But it is possible to use Codex and tell, for instance, to calendar. Could you schedule an appointment with Lex next week after 02:00 p.m. and either writes corresponding piece of code? And that's the thing that actually you want. So interesting. So what you figure out is there's a lot of programs with which you can interact through code, and so there you can generate that code from natural language. That's fascinating. And that's somewhat like also closest to what was the promise of Siri or Alexa. So previously all these behaviors, they were hard coded. And it seems that codex on the fly can pick up the API of, let's say, given software, and then it can turn language into use of this API. So without hard coding, it can translate to machine language corrected to. So for example, this would be really exciting for me, like for Adobe products like Photoshop, which I think actionscript, I think there's a scripting language that communicates with them, same with premiere. And you could imagine that that allows even to do coding by voice on your phone. So for instance, in the past, as of today, I'm not editing word documents on my phone, because it's just the keyboard is too small. But if I would be able to tell to my phone, make the header large, then move the paragraphs around, and that's actually what I want. So I can tell you one more cool thing, or even how I'm thinking about Codex. So if you look actually at the evolution of computers, we started with a very primitive interfaces, which is a punch card and punch card, essentially you make a holes in the, in the plastic card to indicate zeros and ones. And during that time there was a small number of specialists who were able to use computers. And by the way, people even suspected that there is no need for many more people to use computers. But then we moved from punch cards to at first assembly and circumental, and these programming languages, they were slightly higher level, they allowed many more people to code, and they also led to more of a proliferation of technology. And further on, there was a jump from c to Java and Python. And every time it has happened, more people are able to code and we build more technology, and it's even hard to imagine now if someone will tell you that you should write code in assembly instead of let's say, Python or Java or JavaScript. And Codex is yet another step toward bringing computers closer to humans, such that you communicate with a computer with your own language, rather than with a specialized language, and I think that it will lead to increase of number of people who can code. Yeah. And the kind of technologies that those people will create is. It's innumerable. It could, you know, it could be a huge number of technologies we're not predicting at all, because that's less and less requirement of having a technical mind, a programming mind. You're not opening it to the world of other kinds of minds, creative minds, artistic minds, all that kind of stuff. I would like, for instance, biologists who work on DNA to be able to program and not need to spend a lot of time learning it. And I believe that's a good thing to the world. And I would actually add, I would add, so at the moment, I'm a managing codex team and also language team, and I believe that there is like a plenty of brilliant people out there and they should apply. Oh, okay. Yeah. Awesome. So what's the language in the codexes? So those are kind of their overlapping teams. It's like GPT, the raw language, and then the codecs is applied to programming. Correct. And they are quite intertwined. There are many more teams involved, making these models extremely efficient and deployable. For instance, there are people who are working to make our data centers amazing, or there are people who work on putting these models into production, or even pushing it at the very limit of the scale. So all aspects, from the infrastructure to the actual machine learning. So I'm just saying there are multiple teams. While the team working on Codex and language, I guess I'm directly managing them. I would love to hire more. If you're interested in machine learning. This is probably one of the most exciting problems and systems to be working on, because it's actually, it's pretty cool. But the program synthesis, the generating of programs, is very interesting, very interesting problem that has echoes of reasoning and intelligence in it. And I think there's a lot of fundamental questions that you might be able to sneak up to by generating programs. Yeah, one more exciting thing about the programs is that. So I said that in case of language, one of the troubles is even evaluating language. So when the things are made up, you need somehow either a human to say that this doesn't make sense, or so in case of program, there is this one extra lever that we can actually execute programs and see what they evaluate to. So the process might be somewhat more automated in order to improve the qualities of generations. That's fascinating. Wow, that's really interesting. So, for the language, you know, the simulation, to actually execute it, that's the human mind yeah, for programs there is a, there is a computer on which you can evaluate it. Wow, that's a brilliant little insight that the thing compiles and runs, that's first and second you can evaluate like do automated unit testing. And in some sense it seems to me that we will be able to make a tremendous progress. You know, we are in the paradigm that there is way more data and there is like a transcription of millions of software engineers. Yeah, yeah. So I mean, you just mean. Because I was going to ask you about reliability. The thing about programs is you don't know if they're going to like a program that's controlling a nuclear power plant has to be very reliable. So I wouldn't start with controlling nuclear power plant maybe one day, but that's not actually, that's not on the current roadmap, that's not that step one, you. Know, it's the russian thing. You just want to go to the most powerful destructive thing right away run by JavaScript. But I got you. So this is a lower impact, but nevertheless, what you make me realize, it is possible to achieve some levels of reliability by doing testing. And you could imagine that maybe there are ways for model to write event code for testing itself and so on, and there exists ways to create the feedback loops that the model could keep on improving. By writing programs that generate. Tests for the instance, for the instance. And that's how we get consciousness, because it's meta compression. That's what you're going to write. That's the comment, that's the prompt that generates consciousness. Compressor of compressors. You just write that. Do you think the code that generates consciousness would be simple? So let's see. I mean, ultimately the core idea behind will be simple, but there will be also decent amount of engineering involved. In some sense it seems that spreading these models on many machines, it's not that trivial. And we find all sorts of innovations that make our models more efficient. I believe that first models that I guess are conscious are like a truly intelligent, they will have all sorts of tricks. But then again there's Richard Sutton argument that maybe the tricks are temporary things. Yeah, they might be temporary things. And in some sense it's also even important to know that even the cost of a trick. So sometimes people are eager to put the trick while forgetting that there is a cost of maintenance, like long term cost, long term cost or maintenance, or maybe even flexibility of code to actually implement new ideas. So even if you have something that gives you two x, but it requires a 1000 lines of code, I'm not sure if it's actually worth it. So in some sense, if it's five lines of code and two x, I would take it. And we see many of this, but also that requires some level of, I guess, lack of attachment to code that we are willing to remove it. Yeah. So you led the OpenAI robotics team. Can you give an overview of, of the cool things you're able to accomplish? What are you most proud of? So when we started robotics, we knew that actually reinforcement learning works, and it is possible to solve fairly complicated problems. Like, for instance, Alphago is an evidence that it is possible to build superhuman go players. Dota two is an evidence that it's possible to build superhuman agents. Playing Delta. So I asked myself a question, you know, what about robots out there? Could we train machines to solve arbitrary tasks in the physical world? Our approach was, I guess, let's pick a complicated problem that if we would solve it, that means that we made some significant progress the domain, and then we went after the problem. So we noticed that actually the robots out there, they are kind of, at the moment, minds per task. So you can have a robot that it's like, if you have a robot opening a battle, it's very likely that the end factor is a battle opener. And in some sense, that's a hack to be able to solve a task, which makes any task easier, and ask myself, so what would be a robot that can actually solve many tasks? And we conclude that a human hands have such a quality that indeed they are. You know, you have five kind of tiny arms attached. Individually, they can manipulate pretty broad spectrum of objects. So we went after a single hand, like trying to solve Rubik's cube single handed. We picked this task because we thought that there is no way to hard code it. And also we picked a robot on which it would be hard to hard code it. And we went after the solution such that it could generalize to other problems. And just to clarify, it's one robotic hand solving the Rubik's cube. The hard part isn't the solution to the Rubik's cube is the manipulation of the. Of, like, having it not fall out of the hand, having it use the five baby arms to. What is it, like, rotate different parts of the Rubik's cube to achieve the solution? Correct? Yeah. So what was the hardest part about that? What was the approach taken there? What are you most proud of? Obviously, we have, like, a strong belief in reinforcement learning. And one path it is to do reinforcement learning the real world. Other path is to the simulation in some sense. The tricky part about the real world is, at the moment, our models, they require a lot of data. There is essentially no data. And we decided to go through the path of the simulation. And in simulation, you can have infinite amount of data. The tricky part is the fidelity of the simulation, and also, can you, in simulation, represent everything that you represent otherwise in the real world? And it turned out that because there is lack of fidelity, it is possible. What we arrived at is training a model that doesn't solve one simulation, but it actually solves the entire range of simulations, which vary in terms of, like, what's the exactly, the friction of the cube or the weight or so. And the single AI that can solve all of them ends up working well with the reality. How do you generate the different simulations? So there's plenty of parameters out there. We just pick them randomly. And in simulation, model just goes for thousands of years and keeps on solving Rubik's cube in each of them. And the thing is, the neural network that we used, it has a memory, and as it presses, for instance, the side of the cube, it can sense. Oh, that's actually, this side was difficult to press. I should press stronger. And throughout this process, kind of learns even how to, how to solve this particular instance of the Rubik's cube by given mass. It's kind of like a, you know, sometimes when you go to a gym and after. After bench press, you try to lift the glass and you kind of forgot, and your hand goes, like, up right away because kind of you got used to. To maybe different weight, and it takes a second to adjust. Yeah. And this kind of memory, the model gained through that process of interacting with the cube in the simulation. I appreciate you speaking to the audience with a bench press. All the bros in the audience probably working out right now, there's probably somebody listening to this actually doing bench press. So maybe put the bar down and pick up the water bottle, and you'll know exactly what Jack is talking about. Okay, so what was the hardest part of getting the whole thing to work? So, the hardest part is at the moment, when it comes to physical world, when it comes to robots, they require maintenance. It's hard to replicate them million times. It's also. It's hard to replay things exactly. I remember this situation, that one guy at our company, he had a model that performs way better than other models in solving Rubik's cube, and he didn't know what's going on. Why is that? It turned out that he was running it from his laptop. That had better cpu or better maybe local gpu as well. And because of that, there was less of a latency, and the model was the same, and that actually made solving Rubik's cube more reliable. So in some sense, there might be some saddlebags like that when it comes to running things in the real world. Even hinting on that, you could imagine that the initial models, you would like to have models which are insanely huge neural networks, and you would like to give them even more time for thinking. When you have these real time systems, then you might be constrained, actually, by the amount of latency. And ultimately, I would like to build a system that it is worth for you to wait five minutes, because it gives you the answer that you are willing to wait for five minutes. Latency is a very unpleasant constraint under which to operate. Correct. Also, there is actually one more thing which is tricky about robots. There is actually not much data. So the data that I'm speaking about would be data of first person experience from the robot. And, like, gigabytes of data like that. If we would have gigabytes of data like that of robots solving various problems, it would be very easy to make a progress on robotics. And you can see that in case of text or code, there is a lot of data, like a first person perspective. Data on writing code. Yeah. So you had this. You mentioned this really interesting idea that if you were to build, like, a successful robotics company, so OpenAI's mission is much bigger than robotics. This is one of the things you've worked on. But if it was a robotics company, that you wouldn't so quickly dismiss. Supervised learning. Correct. That you would build a robot that was perhaps, what, like an empty shell, like dumb. And they would operate under teleoperation. So you would invest. That's just one way to do it. Invest in human supervisor, like direct human control of the robots as it's learning, and over time, add more and more automation. That's correct. So let's say that's how I would build a robotics company today. If I would be building robotics company, which is spent $10 million or so recording human trajectories, controlling a robot, after. You find a thing that the robot should be doing that there's a market fit for, you can make a lot of money with that product. Correct? Correct. Yeah. So I would record data, and then I would essentially train supervised learning model on it. That might be the path today. Long term. I think that actually what is needed is to train powerful models over video. So you have seen maybe models that can generate images like Dali, and people are looking into models generating videos. They're like various algorithmic questions, even how to do it. And it's unclear if there is enough compute for this purpose. But I suspect that the models, which would have a level of understanding of video, same as GPT, has a level of understanding of text, could be used to train robots to solve tasks. They would have a lot of common. Sense if one day, I'm pretty sure one day, there will be a robotics company. By robotics company, I mean the primary source of income is from robots that is worth over $1 trillion. What do you think that company will do? I think self driving cars. No, it's interesting because my mind went to personal robotics robots in the home. It seems like there's much more market opportunity there. I think it's very difficult to achieve. I mean, this might speak to something important, which is, I understand self driving much better than I understand robotics in the home. So I understand how difficult it is to actually solve self driving to a level, not just the actual computer vision and the control problem and just the basic problem, self driving, but creating a product that would undeniably be. That would cost less money. Like, it would save you a lot of money, like orders of magnitude less money that could replace Uber drivers, for example. So car sharing, that's autonomous, that creates a similar or better experience in terms of how quickly you get from a to b, or just whatever, the pleasantness of the experience, the efficiency of the experience, the value of the experience, and at the same time, the car itself costs cheaper, I think that's very difficult to achieve. I think there's a lot more low. Hanging fruit in the home that could be. I also want to give you perspective on, like, how challenging it would be at home, or like, it maybe kind of depends on the exact problem that you'd be solving. Like, if we're speaking about these robotic arms and hence these things, they cost tens of thousands of dollars or maybe hundred k. And, you know, maybe, obviously, maybe there would be economy of scale, these things would be cheaper. But actually, for any household to buy it, the price would have to go down to maybe thousand bucks. Yeah, I personally think that. So self driving car provides a clear service. I don't think robots in the home, there'll be a trillion dollar company will just be all about service. Meaning it will not necessarily be about, like, a robotic arm that helps you, I don't know, open a bottle or wash the dishes or any of that kind of stuff. It has to be able to take care of that whole. The therapist thing you mentioned, I think that's, of course, there's a line between what is a robot and what is not. Like, does it really need a body? But, you know, some AI system with some embodiment, I think. So the tricky part when you think, actually, what's the difficult part is when the robot has, like, when there is a diversity of the environment with which the robot has to interact, that becomes hard. So on one spectrum, you have industrial robots, as they are doing over and over the same thing. It is possible to some extent, to prescribe the movements. And with very small amount of intelligence, the movement can be repeated millions of times. There are also various pieces of industrial robots where it becomes harder and harder. For instance, in case of Tesla, might be a matter of putting a rack inside of a car. And because the rock kind of moves around, it's not that easy. It's not exactly the same every time ends up being the case that you need actually humans to do it while welding cars together. It's a very repetitive process. Then in case of self driving itself, that difficulty has to do with the diversity of the environment, but still the card itself. The problem that you are solving is you try to avoid even interacting with things. You are not touching anything around, because touching itself is hard. And then if you would have in the home robot that has to touch things, and if these things, they change the shape, if there is a huge variety of things to be touched, then that's difficult. If you are speaking about the robot, which there is, you know, head that it's smiling in some way, with cameras that doesn't, you know, touch things, that's relatively simple. Okay, so to both agree and to push back. So you're referring to touch, like soft robotics, like the actual touch. But I would argue that you could formulate just basic interaction between, like, non contact interaction is also a kind of touch, and that might be very difficult to solve. That's the basic. This not disagreement. But that's the basic open question to me with self driving cars and disagreement with Elon, which is how much interaction is required to solve self driving cars, how much touch is required? You said that in your intuition. Touch is not required. In my intuition, to create a product that's compelling to use, you're going to have to interact with pedestrians, not just avoid pedestrians, but interact with them. When we drive around in major cities, we're constantly threatening everybody's life with our movements, and that's how they respect us. There's a game theory going on with pedestrians, and I am afraid you can't just formulate autonomous driving as a collision avoidance problem. So I think it goes beyond collision avoidance is the first order approximation. But then, at least in case of Tesla, they are gathering data from people driving their cars. And I believe that's an example of supervised learning data that they can train their models on the. And they are doing it, which can give model this another level of behavior that is needed to actually interact with the real world. Yeah, it's interesting how much data is required to achieve that. What do you think of the whole Tesla autopilot approach? The computer vision based approach with multiple cameras, and there's a data engine, it's a multitask, multi headed neural network. And it's this fascinating process of similar to what you're talking about with the robotics approach, which is you deploy neural network, and then there's humans that use it, and then it runs into trouble in a bunch of places, and that stuff is sent back. So the deployment discovers a bunch of edge cases, and those edge cases are sent back for supervised annotation, thereby improving the neural network. And that's deployed again, it goes over and over until the network becomes really good at the task of driving, becomes safer and safer. What do you think of that kind of approach to robotics? I believe that's the way to go. So in some sense, even when I was speaking about collecting trajectories from humans, that's like a first step, and then you deploy the system, and then you have humans revising all the issues. And in some sense, like this approach converges to system that doesn't make mistakes, because for the cases where there are mistakes, you gather data, how to fix them, and the system will keep on improving. So there's a very, to me, difficult question of how hard that, you know, how long that converging takes, how hard it is. The other aspect of autonomous vehicles, this probably applies to certain robotics applications, is society, right, they put as the quality of the system converges. So one is a human factors perspective of psychology, of humans being able to supervise those, even with teleoperation, those robots. And the other is society willing to accept robots. Currently, society is much harsher on self driving cars than it is on human driven cars in terms of the expectation of safety. So the bar is set much higher than for humans. So if there's a death in an autonomous vehicle, that's seen as much more, much more dramatic than a death in a human driven vehicle. Part of the successful deployment of robots is figuring out how to make robots part of society. Both on the just the human side, on the media journalist side, and also on the policy government side. And that seems to be maybe you can put that into the objective function to optimize, but that is definitely a tricky one. And I wonder if that is actually the trickiest part for self driving cars or any system that's safety critical. It's not the algorithm, it's the society accepting it. Yeah, I would say I believe that the part of the process of deployment is actually showing people that the given things can be trusted. Yeah. And, you know, trust is also like a glass that is actually really easy to crack it and damage it. And I think that's actually very common with innovation, that there is some resistance toward it and it's just a natural progression. So in some sense, people will have to keep on proving that indeed these systems are worth being used. And I would say I also found out that often the best way to convince people is by letting them experience it. Yeah, absolutely. That's the case with Tesla Autopilot, for example. That's the case with. Yeah, with basically robots in general. It's kind of funny to hear people talk about robots. Like, there's a lot of fear, even with, like legged robots, but when they actually interact with them, there's joy. I love interacting with them. And the same with the car, with the robot. If it starts being useful, I think people immediately understand. And if the product is designed well, they fall in love. You're right. It's actually even similar. When I'm thinking about copilot, the GitHub copilot, there was a spectrum of responses that people had, and ultimately the important piece was to let people try it out. And then many people just loved it, especially like programmers. Yeah, programmers. But like some of them, you know, they came with a fear. Yeah. But then you try it out and you think, actually that's cool. Okay. And, you know, you can try to resist the same way as you could resist moving from punch cards to, let's say, c or so, and it's a little bit futile. So we talked about generation program, generation of language, even self supervised learning in the visual space for robotics and then reinforcement learning. What to you in like, this whole beautiful spectrum of AI, do you think is a good benchmark, a good test to strive for, to achieve intelligence? That's a strong test of intelligence. You know, it started with Alan Turing and the Turing test. Maybe you think natural language conversation is a good test. So, you know, it would be nice if, for instance, machine would be able to solve Riemann hypothesis in math, that would be. I think that would be very impressive. So theorem proving is that to you, proving theorems is a good. Oh, like, one thing that the machine did, you would say, damn it. Exactly. Okay. That would be quite impressive. I mean, the tricky part about the benchmarks is, you know, as we are getting closer with them, we have to invent new benchmarks. There is actually no ultimate benchmark out there. Yeah, see, my thought with the Riemann hypothesis would be the moment the machine proves it, we would say, okay, well, then the problem was easy. That's what happens. And I mean, in some sense, that's actually what happens over the years in AI, that, like, we get used to things very quickly. You know something, I talked to Rodney Brooks. I don't know if you know that is he called alphazero homework problem, because he was saying, like, there's nothing special about it. It's not a big leap. And I didn't. Well, he's coming from, one of the aspects that we referred to is he was part of the founding of iRobot, which deployed now tens of millions of robot in the home. So if you see robots that are actually in the homes of people as the legitimate instantiation of artificial intelligence, then, yes, maybe an AI that plays a silly game like going chess is not a real accomplishment, but to me, it's a fundamental leap. But I think we as humans then say, okay, well, then that game of chess or go wasn't that difficult compared to the thing that's currently unsolved. So my intuition is that from perspective of the evolution of these AI systems, we'll at first see the tremendous progress in digital space. And the main thing about digital space is also that there is a lot of recorded data, plus you can very rapidly deploy things to billions of people, while in case of physical space, the deployment part takes multiple years. You have to manufacture things, and delivering it to actual people is very hard. So I'm expecting that the first. And the prices in digital space of goods, they would go down to, let's say, marginal cost to zero. And also the question is how much of our life will be in digital? Because it seems like we're heading towards more and more of our lives being in the digital space. So, like, innovation in the physical space might become less and less significant. Like, why do you need to drive anywhere if most of your life is spent in virtual reality? I still would like to, at least at the moment. My impression is that I would like to have a physical contact with other people, and that's very important. To me, we don't have a way to replicate it in the computer. It might be the case that over the time it will change, like in. Ten years from now. Why not have like an arbitrary infinite number of people you can interact with? Some of them are real, some are not, with arbitrary characteristics that you can define based on your own preferences? I think that's maybe where we are heading and maybe I'm resisting the future. Yeah. I'm telling you, if I got to choose, if I could live in elder scrolls Skyrim versus the real world, I'm not so sure I would stay with the real world. Yeah. I mean, the question is, will VR be sufficient to get us there, or do you need to, you know, plug electrodes in the brain? And it would be nice if these electrodes wouldn't be invasive. Yeah. Or at least like provably non destructive. But in the digital space, do you think we'll be able to solve the touring test, the spirit of the touring test, which is, do you think we'll be able to achieve compelling natural language conversation between people, like have friends that are AI systems on the Internet? I totally think it's doable. Do you think the current approach to. GPT will take us there? So there is the amount there, the part of at first learning all the content out there, and I think that steel system should keep on learning as it speaks with you. Yeah, and I think that should work. The question is how exactly to do it. And, you know, obviously we have people at open air asking these questions and kind of at first pre training on all existing content is like a backbone and is a decent backbone. Do you think AI needs a body connecting to our robotics question to truly connect with humans? Or can most of the connection be in the digital space? So let's see. We know that there are people who met each other online and they felt in love. Yeah. So it seems that it's conceivable to establish connection, which is purely through Internet. Of course, it might be more compelling the more modalities you add. So it would be like you're proposing like a tinder, but for AI, you like, swipe right and left and half the systems are AI and the other is humans, and you don't know which is which. That would be our formulation of Turing test. The moment AI is able to achieve more, swipe right or left, whatever, the moment it's able to be more attractive than other humans, it passes the Turing test. Then you would pass the Turing test in attractiveness. That's right. Well, no, like attractiveness just to clarify, conversation, not just visual. Right, right. It's also attractiveness with wit and humor and whatever. Whatever makes conversations pleasant for humans. Okay. All right, so you're saying it's possible to achieve in the digital space in some sense? I would almost ask that question, why wouldn't that be possible? Right. Well, I have this argument with my dad all the time. He thinks that touch and smell are. Really important, so they can be very important. And I'm saying the initial systems, they won't have it. Still, I wouldn't. There are people being born without these senses, and, you know, I believe that they can still fall in love and have meaningful life. Yeah. I wonder if it's possible to go close to all the way by just training on transcripts of conversations. Like, I wonder how far that takes us. So I think that actually still, you want images like I would like. So I don't have kids, but, like, I could imagine having AI tutorial. It has to see, you know, kids drawing some pictures on the paper and. Also facial expressions and all that kind of stuff. We use dogs and humans use their eyes and to communicate with each other. I think that's a really powerful mechanism of communication. Body language too, that words are much lower bandwidth. And for body language, we still, we kind of have a system that displays an image of its artificial expression on the computer. It doesn't have to move mechanical pieces or so. So I think that there is like, kind of a progression. You can imagine that text might be the simplest to tackle, but this is not a complete human experience at all. You expand it to, let's say, images, both for input and output. And what you describe is actually the final, I guess, frontier. What makes us human, the fact that we can touch each other or smell or so. And it's the hardest from perspective of data and deployment. And I believe that these things might happen gradually. Are you excited by that possibility, this particular application of human to AI, friendship and interaction? So let's see, like, would you, do. You look forward to a world, you said you're living with a few folks and you're very close friends with them. Do you look forward to a day where one or two of those friends are AI systems? So if the system would be truly wishing me well, rather than being in the situation that it optimizes for my time to interact with the system, the. Line between those is, it's a gray. It's a gray area. I think that's the distinction between love and possession. And these things, they might be often correlated for humans but it's like you, you might find that there are like some friends with whom you haven't spoke for months. Yeah. And then, you know, you pick up the phone, it's as the time hasn't passed, they are not holding to you. And I will. I wouldn't like to have AI system that, you know, it's, it's trying to convince me to spend time with it. I would like the system to optimize for what I care about and help me in achieving my own goals. But there's some, I mean, I don't know, there's some manipulation, there's some possessiveness, there's some insecurities, there's fragility. All those things are necessary to form a close friendship over time, to go through some dark shit together, some bliss and happiness together. I feel like there's a lot of greedy, self centered behavior within that process. My intuition, but I might be wrong, is that human computer interaction doesn't have to go through computer being greedy, possessive and so on. It is possible to train systems, maybe, that they actually, you know, they are I guess, prompted or fine tuned or so to truly optimize for what you care about. And you could imagine that, you know, the way, how the process would look like is at some point we as a humans, we look at the transcript of the conversation or like an entire interaction and we say, actually here there was more loving way to go about it. And we supervise system toward being more loving. Or maybe we train the system such that it has a reward function toward being more loving. Yeah. Or maybe the possibility of the system being an asshole and manipulative and possessive every once in a while is a feature, not a bug. Because some of the happiness that we experience when two souls meet each other, when two humans meet each other, is a kind of break from the assholes in the world. And so you need assholes in AI as well because, like, it'll be like a breath of fresh air to discover an AI that the three previous AI's. You had are too friendly. No. Or cruel or whatever. It's like some kind of mix. And then this one is just right. But you need to experience the full spectrum. Like, I think you need to be able to engineer assholes. So let's see, because there's some level to us being appreciated. To appreciate the human experience, we need the dark and the light. So that kind of reminds me, I met a while ago at the meditation retreat one woman, and, you know, beautiful, beautiful woman. And she had, she had a crutch. Okay. She had the trouble walking on one leg. I asked her what has happened? And she said that five years ago she was in Maui, Hawaii, and she was eating a salad and some snail fell into the salad. And apparently there are neurotoxic snails over there. And she got into coma for a year. And apparently there is high chance of even just dying. But she was in the comma. At some point, she regained partially consciousness. She was able to hear people in the room, people behave as she wouldn't be there. At some point, she started being able to speak, but she was mumbling, barely able to express herself. At some point, she got into wheelchair. Then at some point, she actually noticed that she can move her toe. And then she knew that she will be able to walk. And then, you know, that's where she was five years after. And she said that since then, she appreciates the fact that she can move her toe. And I was thinking, hmm, do I need to go through such experience to appreciate that I have. I can move my toe? Wow, that's really good story. A really deep example. Yeah. And in some sense, it might be the case that we don't see light if we haven't went through the darkness, but I wouldn't say that we should. We shouldn't assume that that's the case. We may be able to engineer shortcuts. Yeah. Ilya had this belief that maybe one has to go for a week or six months to some challenging camp to just experience, you know, a lot of difficulties, and then comes back and actually everything is bright, everything is beautiful. I'm with Ilya on this. It must be a russian thing. Where are you from originally? I'm polish. Polish, okay. I'm tempted to say that explains a lot, but, yeah, there's something about the Russian. The necessity of suffering. I believe. I believe suffering, or rather, struggle is necessary. I believe that struggle is necessary. I mean, in some sense, you even look at the story of any superhero, that movie. It's not that. It was like everything. Like, it goes easy, easy, easy, easy. I like how that's your ground. Truth is the story of superheroes. Okay, you mentioned that you used to do research at night and go to bed at like, 06:00 a.m. or 07:00 a.m. i still do that often. What sleep schedules have you tried to make for a productive and happy life? Like, is there. Is there some interesting, wild sleeping patterns that you engage that you found that works really well for you? I tried at some point, decreasing number of hours of sleep, like a gradually like, half an hour every few days, less. You know, I was hoping to just save time. That clearly didn't work for me. Like, at some point, there's, like, a phase shift, and I felt tired all the time. There was a time that I used to work during the nights. The nice thing about the nights is that no one disturbs you. And even I remember when I was meeting for the first time with Greg Brockman, his CTO and chairman of OpenAI. Our meeting was scheduled to 05:00 p.m. and I overstepped for the meeting. Mm hmm. Overslept for the meeting. Yeah. 05:00 p.m. yeah. Now you sound like me. That's hilarious. Okay. Yeah. And at the moment, in some sense, my sleeping schedule also has to do with the fact that I'm interacting with people. I sleep without an alarm. So. So, yeah, the. The team thing, you mentioned extrovert thing. Because most humans operate during a certain set of hours. You're forced to then operate at the same set of hours. But I'm not quite there yet. I found a lot of joy, just like you said, working through the night because it's quiet, because the world doesn't disturb you. And there's some aspect counter to everything you're saying. There's some joyful aspect to sleeping through the mess of the day, because people are having meetings and sending emails and there's drama. Meetings. I can sleep through all the meetings. You know, I have meetings every day, and they prevent me from having sufficient amount of time for focused work. And then I modified my calendar, and I said that I'm out of office Wednesday, Thursday, and Friday every day, and I'm having meetings only Monday and Tuesday. And that vastly, positively influenced my mood that I have. Literally, I get three days for fully focused work. Yeah. So there's better solutions to this problem than staying awake all night. Okay. You've been part of development of some of the greatest ideas in artificial intelligence. What would you say is your process for developing good, novel ideas? You have to be aware that clearly there are many other brilliant people around. So you have to ask yourself a question. Why they give an idea, let's say, wasn't tried by someone else. And in some sense, it has to do with kind of simple. It might sound simple, but like I thinking outside of the box. And what do I mean here? So, for instance, for a while, people in academia, they assumed that you have a fixed data set, and then you optimize the algorithms in order to get the best performance. And that was so in great assumption that no one thought about training models on anti Internet or like that. Maybe some people thought about it, but it felt to many as unfair. And in some sense that's almost like a, it's not my idea or so, but that's an example of breaking a typical assumption. So you want to be in the paradigm that you are breaking a typical. Assumption in the context of the AI community getting to pick your data set as cheating. Correct. And in some sense, so that was assumption that many people had out there. And then if you free yourself from assumptions, then they are likely to achieve something that others cannot do. And in some sense, if you are trying to do exactly the same things as others, it's very likely that you're going to have the same results. Yeah, but there's also that kind of tension which is asking yourself the question, why haven't others done this? Because I mean, I get a lot of good ideas, but I think probably most of them suck when they meet reality. So actually I think the other big piece is getting into habit of generating ideas, training your brain towards generating ideas, and not even suspending judgment of the ideas. So in some sense I noticed myself that even if I'm in the process of generating ideas, if I tell myself, oh, that was a bad idea, then that actually interrupts the process and I cannot generate more ideas because I'm actually focused on the negative part why it won't work. Yes, but I created also environment in the way that it's very easy for me to store new ideas. So for instance, next to my bed I have a voice recorder and it happens to me often. Like I wake up in the, during the night and I have some idea in the past I was writing them down on my phone, but that means, you know, turning off this, turning on the screen and that wakes me up, or like pulling a paper which requires, you know, turning on the light. These days I just start recording it. What do you think? I don't know if you know who Jim Keller is. I know Jim Keller. He's a big proponent of thinking hard on a problem right before sleep, so that he can sleep through it and solve it in his sleep, or like come up with radical stuff in his sleep. He was trying to get me to do this. So it happened from my experience perspective, it happened to me many times during the high school days when I was doing mathematics that I had the solution to math problem. As I woke up at the moment regarding thinking hard about the given problem is I'm trying to actually devote substantial amount of time to think about important problems, not just before the sleep, like I'm organizing amount of the huge chunks of time such that I'm not constantly working on the urgent problems, but I actually have time to think about the important one. So you do it naturally. But his idea is that you kind of prime your brain to make sure that that's the focus. You know, oftentimes people have other worries in their life. That's nothing fundamentally deep problems like, I don't know, just stupid drama in your life. And even at work, all that kind of stuff. He wants to kind of pick the most important problem that you're thinking about and go to bed on that. I think that's wise. I mean, the other thing that comes to my mind is also I feel the most fresh in the morning. So during the morning I try to work on the most important things rather than just being pulled by urgent things or checking email or. So what do you do with the cause? I've been doing the voice recorder thing too, but I end up recording so many messages, it's hard to organize. I have the same problem now. I have heard that Google Pixel is really good in transcribing text. And I might get a Google Pixel just for the sake of transcribing text. Yeah. People listening to this. If you have a good voice recorder suggestion that transcribes, please let me know. Some of it is this has to do with OpenAI codecs too. Like, some of it is simply like the friction. I need apps that remove that friction between voice and the organization of the resulting transcripts and all that kind of stuff. But yes, you're right. Absolutely. During, for me, it's walking sleep too. But walking and running, especially running, get a lot of thoughts during running. And there's no good mechanism for recording thoughts. So one more thing that I do, I have a separate phone which has no apps. Maybe it has like audible or let's say Kindle. No one has this phone number. This kind of my meditation phone. Yeah. And I try to expand the amount of time that's the phone that I'm having. It has also Google Maps if I need to go somewhere. And I also use this phone to write down ideas. Ah, that's a really good idea. That's a really good idea. Often, actually, what I end up doing is even sending a message from that phone to the other phone. So that's actually my way of recording messages. Or I just put them into notes. I love it. What advice would you give to a young person, high school, college, about how to be successful. You've done a lot of incredible things in the past decade. So maybe. Maybe you have some. Something. There might be something. There might be something. I mean, might sound like a simplistic or so, but I would say, literally, just follow your passion. Double down on it. And if you don't know what's your passion, just figure out what could be a. What could be a passion. So this, that might be an exploration. When I was in elementary school was math and chemistry, and I remember for some time I gave up on math because my school teacher, she told me that I'm dumb. And I guess maybe an advice would be, just ignore people. If they tell you that you're dumb, you're dumb. You mentioned something offline about chemistry and explosive. What was that about? So let's see. So a story goes like that. I can. I got into chemistry, maybe I was like a second grade of my elementary school. Third grade, I started going to chemistry classes. I really love building stuff. And I did all the experiments that they describe in the book, how to create oxygen with vinegar and baking soda or so. Okay, so I did all the experiments, and at some point I was. So what's next? What can I do? And explosives, they also. It's like you have a clear reward signal, you know, if the thing worked or not. So I remember at first, I got interested in producing hydrogen. That was kind of funny experiment from school. You can just burn it. And then I moved to nitroglycerin, so that's also relatively easy to synthesize. I started producing essential dynamite. And that one, I think it would be friend. I remember there was a. No, there was at first, like, maybe two attempts that I went with a friend to detonate what we built, and it didn't work out. And, like a third time, he was like, ah, it won't work. Like, let's don't waste time. And we were. I was carrying this, this, you know, that tube with dynamite, I don't know, pound or so dynamite in my backpack. We're, like, riding on the bike to the edges of the city. Yeah. And attempt number three. This would be attempt number three. Attempt number three. And now we. We dig a hole to put it inside. It actually had the, you know, electrical detonator. We draw a cable behind the tree. I even. I never had. I haven't ever seen, like, an explosion before. So I thought that there will be a lot of sound and. But, you know, we're, like, laying down and I'm holding the cable and the battery. At some point, you know, it kind of like a three to one. And I just connected it and it felt like at the ground shaked. It was like a. More like a sound. And then the soil started kind of lifting up and started falling on us. Yeah. Wow. And then now the friend said, let's make sure next time we have helmets. But also, you know, kind of. I'm happy that nothing happened to me. It could have been the case that I lost the limb or so. Yeah. But that's childhood of an engineering mind with a strong reward signal of an explosion. I love it. There's some aspect of chemists, the chemist I know, like my dad, with plasma chemistry, plasma physics, he was very much into explosives, too. It's a worrying quality of people that work in chemistry that they love. I think it is. That exactly is the strong signal that the thing worked. There is no doubt. There's no doubt there's some magic. It's almost like a reminder that physics works, that chemistry works. It's cool. It's almost like a little glimpse at nature that you yourself engineer. That's why I really like artificial intelligence, especially robotics, is you create a little piece of nature. And in some sense, even for me with explosives, the motivation was creation rather than distraction. Yes, exactly. In terms of advice, I forgot to ask about just machine learning and deep learning. For people who are specifically interested in machine learning, how would you recommend they get into the field? So I would say re implement everything. And also there is plenty of courses, so, like, from scratch, so on different levels of abstraction in some sense, but I would say re implement something from scratch, re implement something from a paper, re implement something, you know, from podcasts that you have heard about. I would say that's a powerful way to understand things. So it's often the case that you read the description and you think you understand, but you truly understand once you build it, then you actually know what really mattered in the description. Is there particular topics that you find people just fall in love with? I've seen. I tend to really enjoy reinforcement learning because it's much easier to get to a point where you feel like you created something special, like fun games kind. Of things that are rewarding. It's rewarding, yeah. As opposed to, like, re implementing from scratch, more like supervised learning kind of things. It's. Yeah. So, you know, if. If someone would optimize for things to be rewarding, then it feels that the things that are somewhat generative, they have such a property. So, yes, you have, for instance, adversarial networks, or you have just even generated language models. And you can even see internally, we have seen this thing with our releases. So we released recently two models. There is one model called Dali that generates images. And there is other model called clip, that actually you provide various possibilities. What could be the answer to what is on the picture? And it can tell you which one is the most likely. And in some sense, in case of the first one, Dali, it is very easy for you to understand that actually there is magic going on. And in case of the second one, even though it is insanely powerful, and, you know, people from vision community, as they started probing it inside, they actually understood how far it goes. It's difficult for person at first to see how well it works. And that's the same as you said, that in case of supervised learning models, you might not kind of see, or it's not that easy for you to understand the strength, even though you don't. Believe in magic, to see the magic. To see the magic. It's a generative that's really brilliant. So anything that's generative, because then you are at the core of the creation. You get to experience creation without much effort, unless you have to do it from scratch. But. And it feels that, you know, humans are wired. There is some level of reward for creating stuff. Yeah. Like, of course, different people have a different weight on this reward. Yeah. In the big objective function. In the big objective function of a person. Of a person. You wrote that beautiful is what you intensely pay attention to. Even a cockroach is beautiful if you look very closely. Can you expand on this? What is beauty? So what I wrote here actually corresponds to my subjective experience that I had through extended periods of meditation. It's pretty crazy that at some point, the meditation gets you to the place that you have really increased focus, increased attention. And then you look at the very simple objects that were all the time around. You can look at the table or on a pen or at that nature, and you notice more and more details, and it becomes very pleasant to look at it. And it, once again, it kind of reminds me my childhood, like, just pure joyous of being. It's also. I have seen even the reverse effect that by default, regardless of what we possess, we very quickly get used to it. And, you know, you can have a very beautiful house, and if you don't put sufficient effort, you're just gonna get used to it. And it doesn't bring any more joy regardless of what you have. Yeah, well, I actually. I find that material possessions get in the way of that experience of pure joy. So I've always. I've been very fortunate to just find joy in simple things. Just. Just like you're saying. Just like, I don't know, objects in my life. Just stupid objects like this cup like thing, you know, just objects sounds okay, I'm not being eloquent, but literally, objects in the world, they're just full of joy. Because it's like, I can't believe, one, I can't believe that I'm fortunate enough to be alive to experience these objects. And then two, I can't believe humans are clever enough to have built these objects. The hierarchy of pleasure that that provides is infinite. I mean, even if you look at the cup of water. So, you know, you see first, like, a level of, like a reflection of light. But then you think, no, man. There's like a trillions upon trillions of particles bouncing against each other. There is also the tension on the surface that if the back could, like, stand on it and move around. And you think it also has this, like a magical property that as you decrease temperature, it actually expands in volume, which allows for the legs to freeze on the surface and at the bottom to have actually not freeze, which allows for life like crazy. You look in detail at some object, and you think, actually, this table, that was just the figment of someone's imagination at some point. And then there was thousands of people involved to actually manufacture it and put it here. And by default, no one cares. And then you can start thinking about evolution, how it all started from single cell organisms that led to this table. And these thoughts, they give me life appreciation. Yeah, exactly. And even lack of thoughts, just the pure raw signal also gives the life appreciation. See, the thing is. And then that's coupled, for me, with the sadness that the whole ride ends. And perhaps is deeply coupled in that the fact that this experience, this moment ends gives it. Gives it an intensity that I'm not sure I would otherwise have. So in that same way, I try to meditate on my own death often. Do you think about your mortality? Are you afraid of death? So fear of death is like one of the most fundamental fears that each of us has. We might be not even aware of it. It requires to look inside, to even recognize that it's out there. There is still, let's say, this property of nature, that if things would last forever, then they would be also boring to us. The fact that the things change in some way gives any meaning to them. I also found out that it seems to be very healing to people to have these short experiences like I guess psychedelic experiences in which they experience death of self, in which they let go of this fear. And then I maybe can even increase the appreciation of the moment. It seems that many people, they, they, they can easily comprehend fine the fact that the money is finite, while they don't see that time is finite. I have this, like, a discussion with Ilya from time to time. He's saying, you know, man, like, the life will pass. Very passed. At some point I will be 40, 50, 60, 70, and then it's over. This is true. Which also makes me believe that, you know, that every single moment, it is so unique that should be appreciated. And it also makes me think that I should be acting on my life, because otherwise it will pass. I also like this framework of thinking from Jeff Bezos on regret minimization, that, like, I would like, if I will be at that deathbed, to look back on my life and. And not regret that I haven't done something. It's usually you might regret that you haven't tried. I'm fine with failing, but I haven't tried. What's the nietzsche? Eternal recurrence. Try to live a life that if you had to live it infinitely many times, that would be the. You'd be okay with that kind of life. So try to live it optimally. I can say that it's almost, like, unbelievable to me where I am in my life. I'm extremely grateful for, actually, people whom I met. I would say, I think that I'm decently smart and so on. But I think that actually, to a great extent, where I am has to do with the people who I met. Would you be okay if, after this conversation, you died? So if I'm dead, then it kind of. I don't have a choice anymore. So in some sense, there's, like, a. Plenty of things that I would like to try out in my life. I feel that, you know, I'm gradually going one by one, and I'm just doing them. I think that the list will be always infinite. Yeah. So might as well go today. Yeah. I mean, to be clear, I'm not looking forward to die. I would say, if there is no choice, I would accept it. But, like, in some sense, I'm. If there would be a choice, if there would be possibility to leave, I would fight for living. I find it's more honest and real to think about, you know, dying today. At the end of the day, that seems to me, at least to my brain, more honest slap in the face, as opposed to, I still have ten years, like, today, then I'm much more about appreciating the cup and the table and so on and less about, like, silly worldly accomplishments and all those kinds of things. We have in the company. A person who say at some point found out that they have cancer. And that also gives, you know, huge perspective with respect to what matters now. Yeah. And, you know, often people in situations like that, they conclude that actually what matters is human connection and love, and. That'S people conclude, also, if you have kids, it's kids as family. You, I think, tweeted, we don't assign the minus infinity reward to our death. Such a reward would prevent us from taking any risk. We wouldn't be able to cross the road in fear of being hit by a car. So in the objective function you mentioned, fear of death might be fundamental to the human condition. So, as I said, let's assume that there are, like, reward functions in our brain, and I. And the interesting thing is even realization how different reward functions can play with your behavior. As a matter of fact, I wouldn't say that you should assign infinite negative reward to anything because that messes up the math. The math doesn't work out. It doesn't work out. And as you said, even government or some insurance companies said they assign $9 million to human life. Yeah. And I'm just saying it with respect to. That might be a hard statement to ourselves, but in some sense that there is a finite value of our own life. I'm trying to put it from perspective of being less, of being more ego less, and realizing fragility of my own life. And in some sense, the fear of death might prevent you from acting, because anything can cause death. Yeah. And I'm sure, actually, if you were to put death in the objective function, there's probably so many aspects to death and fear of death and realization of death and mortality. There's just whole components of finiteness of not just your life, but every experience and so on that you're going to have to formalize mathematically. And also, you know, that might lead to you spending a lot of compute cycles on this, like, deliberating this terrible future instead of experiencing now. And in some sense, it's also kind of unpleasant simulation to run in your head. Yeah. Do you. Do you think there's an objective function that describes the entirety of human life? So, you know, usually the way you ask that is, what is the meaning of life? Is there a universal objective functions that captures the why of life? So, yeah, I mean, I suspect that they will ask this question, but it's also a question that I asked myself many, many times. See, I can tell you a framework that I have these days to think about these questions. So I think that fundamentally, meaning of life has to do with some of our reward functions that we have in brain. And they might have to do with, let's say, for instance, curiosity or human connection, which might mean understanding others. It's also possible for a person to slightly modify their reward function. Usually they mostly stay fixed, but it's possible to modify a reward function. And you can pretty much choose. So in some sense, the reward functions, optimizing reward functions, they will give you life satisfaction. Is there some randomness in the function? I think when you are born, there is some randomness. Like, you can see that some people, for instance, they care more about building stuff. Some people care more about caring for others. Some people, there are all sorts of default reward functions. And then in some sense, you can ask yourself, what is the satisfying way for you to go after this reward function? And you just go after this reward function? And some people also ask, are these reward functions real? I almost think about it as, let's say, if you would have to discover mathematics. In mathematics, you are likely to run into various objects, like a complex numbers or differentiation. Some other objects, and these are very natural objects that arise. And similarly, the reward functions that we are having in our brain, they are somewhat very natural that there is a reward function for understanding, like a comprehension, curiosity and so on. And so in some sense, they are in same way natural as they're natural objects in mathematics. Interesting. So, you know, there's the old sort of debate. Is mathematics invented or discovered? You're saying reward functions are discovered. So nature. So nature provided some, you can still, let's say, expand it throughout the life. Some of the reward functions, they might be futile. Like, for instance, there might be a reward function. Maximize amount of wealth. Yeah. And this is more like a learning reward function. But we know also that some reward functions, if you optimize them, you won't be quite satisfied. Well, I don't know which part of your reward function resulted in you coming today, but I am deeply appreciative that you did spend your valuable time with me. Wojciech is really fun talking to you. You're brilliant. You're a good human being. An honor to meet you and an honor to talk to you. Thanks for talking today, brother. Thank you, Lex, a lot. I appreciate your questions. Curiosity. I had a lot of time being here. Thanks for listening to this conversation with Wojcieg. Zaremba to support this podcast. Please check out our sponsors in the description. And now let me leave you with some words from Arthur C. Clarke, who is the author of 2001 A Space Odyssey. It may be that our role on this planet is not to worship God, but to create him. Thank you for listening and I hope to see you next time.

Utterances:
Speaker A: The following is a conversation with Wojciech Zaremba, co founder of OpenAI, which is one of the top organizations in the world doing artificial intelligence research and development. Wojciech is the head of language and cogeneration teams, building and doing research on GitHub, Copilot, OpenAI, Codex, and GPT-3, and who knows? Four, five, six, n and n one. And he also previously led OpenAI's robotic efforts. These are incredibly exciting projects to me that deeply challenge and expand our understanding of the structure and nature of intelligence. The 21st century, I think, may very well be remembered for a handful of revolutionary AI systems and their implementations. GPT codecs and applications of language models and transformers in general to the language and visual domains may very well be at the core of these AI systems. To support this podcast, please check out our sponsors. Theyre listed in the description. This is the Lex Friedman podcast, and here is my conversation with Wojciech Zaremba. You mentioned that Sam Altman asked about the Fermi paradox, and the people at OpenAI had really sophisticated, interesting answers. So that's when you knew this is the right team to be working with. So let me ask you about the Fermi paradox about aliens. Why have we not found overwhelming evidence for aliens visiting Earth?
Speaker B: I don't have a conviction in the answer, but rather kind of probabilistic perspective on what might be a, let's say, possible answers. It's also interesting that the question itself even can touch on the, you know, your typical question of what's the meaning of life? Because if you assume that, like, we don't see aliens because they destroy themselves, that kind of upweights the focus on making sure that we won't destroy ourselves. Yeah, at the moment, the place where I am actually with my belief, and these things also change over the time, is, I think that we might be alone in the universe, which actually makes life more or less a consciousness life more kind of valuable. And that means that we should more appreciate it.
Speaker A: Have we always been alone? So what's your intuition about our galaxy, our universe? Is it just sprinkled with graveyards of intelligent civilizations? Or are we truly, is life, intelligent life, truly unique?
Speaker B: At the moment, my belief that it is unique, but I would say I could also. There was some footage released with UFO objects, which makes me actually doubt my own belief. I can tell you one crazy answer that I have heard. Apparently, when you look actually at the limits of computation, you can compute more if the temperature of the universe would drop down. So one of the things that alias might want to do if they are truly optimizing to maximize amount of compute, which maybe can lead to more, let's say simulations or. So it's instead of wasting current entropy of the universe, because we, by living, we are actually somewhat wasting entropy, then you can wait for the universe to cool down such that you have more computation. That's kind of a funny answer. I'm not sure if I believe in it, but that would be one of the reasons why you don't see aliens. It's also possible. See, some people say that maybe there is not that much point in actually going to other galaxies if you can go inwards. So there is no limits of what could be an experience if we could connect machines to our brains while there are still some limits if we want to explore universe.
Speaker A: Yeah, there could be a lot of ways to go inwards too. Once you figure out some aspect of physics we haven't figured out yet, maybe you can travel to different dimensions. I mean, travel in three dimensional space may not be the most fun kind of travel. There may be like just a huge amount of different ways to travel. And it doesn't require a spaceship going slowly in 3d space to space time.
Speaker B: It also feels one of the problems is that speed of light is low and universe is vast. And it seems that actually most likely, if we want to travel very far, then we would, instead of actually sending spaceships with humans that wait a lot, we would send something similar to what Yuri Miller is working on. These are like huge sail which is at first powered. There is a shot of laser from an Earth and you can propel it to quarter of speed of light. And the sail itself contains a few grams of equipment. And that might be the way to actually transport matter through universe. But then when you think what would it mean for humans? It means that we would need to actually put their 3d printer and 3d print a human on other planet. I don't know, play them YouTube or let's say, or like a 3d print a huge human right away. Or maybe a womb or so. Yeah.
Speaker A: With our current techniques of archaeology, if a civilization was born and died long enough ago on Earth, we wouldn't be able to tell. And so that makes me really sad. And so I think about Earth in that same way. How can we leave some remnants of, if we do destroy ourselves, how can we leave remnants for aliens in the future to discover? Like here's some nice stuff we've done, like Wikipedia and YouTube. Do we have it like in a satellite orbiting earth with a hard drive like, how do we say, how do we back up human civilization for the good parts, or all of it is good parts so that it can be preserved longer than our bodies can? That's kind of a, it's a difficult question. It also requires the difficult acceptance of the fact that we may die, and if we die, we may die suddenly as a civilization.
Speaker B: So let's see. I think it kind of depends on the cataclysm we have observed in other parts of universe that births of gamma rays. These are high energy rays of light that actually can apparently kill entire galaxy. So there might be actually nothing, even to nothing to protect us from it. I'm also, when I'm looking actually at the past civilizations, so it's like Aztecs or so they disappear from the surface of the earth. And one can ask, why is it the case? And the way I'm thinking about it is, you know, that definitely they had some problem that they couldn't solve and maybe there was a flat and all of a sudden they couldn't drink, there was no potable water, and they all died. And I think that so far, the best solution to such a problems is, I guess, technology. So, I mean, if they would know that you can just boil water and then drink it after, then that would save their civilization. And even now, when we look actually at the current pandemic, it seems that once again, actually science comes to rescue and somehow science increases size of the action space. And I think that's a good thing.
Speaker A: Yeah, but nature has a vastly larger action space.
Speaker B: But still, it might be a good thing for us to keep on increasing action space.
Speaker A: Okay, looking at past civilizations, yes, but looking at the destruction of human civilization, perhaps expanding the action space will add actions that are easily acted upon, easily executed, and as a result destroy us.
Speaker B: So let's see, I was pondering why actually even we have negative impact on the globe. Because if you ask every single individual, they would like to have clean air, they would like healthy planet. But somehow it actually is not the case that as a collective, we are not going this direction. I think that there exists very powerful system to describe what we value. That's capitalism. It assigns actually monetary values to various activities. At the moment, the problem in the current system is that there are some things which we value, there is no cost assigned to it. So even though we value clean air or maybe we also value lack of distraction on, let's say, Internet or so, at the moment, these quantities, companies, corporations can pollute them for free. So in some sense, I wished or like and that's, I guess, purpose of politics, to align the incentive systems. And we are kind of maybe even moving in this direction. The first issue is even to be able to measure the things that we value, then we can actually assign the monetary value to them.
Speaker A: Yeah. So it's getting the data and also probably through technology, enabling people to vote and to move money around in a way that is aligned with their values. And that's very much a technology question. So, like having one president and Congress and voting, that happens every four years or something like that, that's a very outdated idea. There could be some technological improvements to that kind of idea.
Speaker B: So I'm thinking from time to time about these topics, but it also feels to me that it's a little bit like it's hard for me to actually make correct predictions. What is the appropriate thing to do? I extremely trust Sam Altman, our CEO, on these topics here. I'm more on the side of being, I guess, naive hippie.
Speaker A: That, yeah, that's your life philosophy. Well, I think self doubt, and I think hippie implies optimism. Those two things are pretty good way to operate.
Speaker B: I mean, still, it is hard for me to actually understand how the politics works or exactly how the things would play out. And Sam is really excellent with it.
Speaker A: What do you think is rarest in the universe? You said we might be alone. What's hardest to build is another engineering way to ask that life, intelligence, or consciousness. So, like you said that we might be alone, which is the thing that's hardest to get to. Is it just the origin of life? Is it the origin of intelligence? Is it the origin of consciousness?
Speaker B: So let me at first explain you my kind of mental model, what I think is needed for life to appear. So I imagine that at some point, there was this primordial zoop of amino acids and maybe some proteins in the ocean, and some proteins were turning into some other proteins through reaction. And you can almost think about this cycle of what turns into what, as there is a graph essentially describing which substance turns into some other substance. And essentially, life means that all the sudden in the graph has been created a cycle such that the same thing keeps on happening over and over again. That's what is needed for life to happen. And in some sense, you can think almost that you have this gigantic graph and it needs, like, a sufficient number of edges for the cycle to appear. Then, from perspective of intelligence and consciousness, my current intuition is that they might be quite intertwined. First of all, it might not be that it's like a binary thing that you have intelligence or consciousness. It seems to be more continuous component. Let's see if we look, for instance, on the event networks recognizing images, people are able to show that the activations of these networks correlate very strongly with activations in visual cortex of some monkeys. The same seems to be true about language models. Also, if you, for instance, look, if you train agent in 3d world, at first it barely recognizes what is going on. Over the time. It kind of recognizes foreground from background. Over the time, it kind of knows where there is a foot and it just follows it. Over the time, it actually starts having a 3d perception. So it is possible, for instance, to look inside of the head of an agent and ask what would it see if it looks to the right. And the crazy thing is, initially, when the agents are barely trained, these predictions are pretty bad. Over the time they become better and better. You can still see that if you ask what happens when the head is turned by 360 degrees, for some time they think that the different thing appears. And then at some stage they understand actually that the same thing is supposed to appear. So they get understanding of 3d structure. It's also very likely that they have inside some level of symbolic reasoning. They're particularly symbols for other agents. So when you look at delta agents, they collaborate together and they have some anticipation of if they would win battle. They have some expectations with respect to other agents. I might be too much anthropomorphizing how the things look for me. But then the fact that they have a symbol for other agents makes me believe that at some stage, as the, you know, as they are optimizing for skills, they would have also symbol to describe themselves. This is like a very useful symbol to have. And this particularly, I would call it like a self consciousness or self awareness. And still it might be different from the consciousness. So I guess the way how I'm understanding the word consciousness, I'd say the experience of drinking a coffee or let's say experience of being a. But that's the meaning of the word consciousness. It doesn't mean to be awake. Yeah, it feels. It might be also somewhat related to memory and recurrent connections. So it's kind of like, if you look at anesthetic drugs, they might be like they essentially disturb brainwaves such that maybe memory is not formed.
Speaker A: So there's a lessening of consciousness when you do that.
Speaker B: Correct.
Speaker A: And so that's one way to intuit what is consciousness. There's also kind of another element here. It could be that it's this kind of self awareness module that you described, plus the actual subjective experience is a storytelling module that tells us a story about what we are experiencing.
Speaker B: The crazy thing. So let's say, I mean, in meditation they teach people not to speak story inside of the head. And there is also some fraction of population who doesn't have actually narrator. I know people who don't have a narrator, and they have to use external people in order to kind of solve tasks that require internal narrator. So it seems that it's possible to have the experience without the talk.
Speaker A: What are we talking about when we talk about the internal narrator? Is that the voice? When you like.
Speaker B: Yeah, I thought that's what you are referring to.
Speaker A: Well, I was referring more on it, like not an actual voice. I meant like there's some kind of like subjective experience. Feels like it's, it's fundamentally about storytelling to ourselves. It feels like, like the feeling is a story that is much, much simpler abstraction than the raw sensory information. So there feels like it's a very high level abstraction that is useful for me to feel like entity in this world. Most useful aspect of it is that because I'm conscious, I think there's an intricate connection to me, not one wanting to die. So like it's a useful hack to really prioritize not dying. Like those seem to be somehow connected. So I'm telling this story of like, it's richly feels like something to be me. And the fact that me exists in this world, I want to preserve me. And so that makes it a useful agent hack.
Speaker B: So I will just refer maybe to the first part, as you said, about that kind of story of describing who you are. I was thinking about it. Even so, you know, obviously I like thinking about consciousness. I like thinking about AI as well. And I'm trying to see analogies of these things in AI. What would it correspond to? So, you know, OpenAI trained a model called GPT, which can generate pretty amusing text on arbitrary topic. And one way to control GPT is by putting into prefix at the beginning of the text some information. What would be the story about? You can have even chat with, you know, with GPT by saying that the chat is with Lex or Elon Musk or so. And GPT would just pretend to be you or Elon Musk or so. And it almost feels that this story that we give ourselves to describe our life, it's almost like things that you put into context of GPT. Yeah, to prime.
Speaker A: But the context we provide to Gptheme is multimodal.
Speaker B: So GPT itself is multimodal. GPT itself hasn't learned, actually, from experience of single human, but from the experience of humanity. It's a chameleon. You can turn it into anything. And in some sense, by providing context, it behaves as the thing that you wanted it to be. It's interesting that people have stories of who they are, and as you said, these stories, they help them to operate in the world. But it's also interesting, I guess, various people find it out through meditation or so, that there might be some patterns that you have learned when you were a kid that actually are not serving you anymore. And you also might be thinking that that's who you are, and that's actually just a story.
Speaker A: Yeah, so it's a useful hack, but sometimes it gets us into trouble. It's a local optima.
Speaker B: It's a local optima.
Speaker A: You wrote that Stephen Hawking. He tweeted, Stephen Hawking asked, what breathes fire into equations, which meant, what makes, given mathematical equations, realize the physics of a universe. Similarly, I wonder what breathes fire into computation? What makes given computation conscious? Okay, so how do we engineer consciousness? How do you breathe fire and magic into the machine?
Speaker B: So, it seems clear to me that not every computation is conscious. I mean, you can, let's say, just keep on multiplying one matrix over and over again, and maybe gigantic matrix, you can put a lot of computation. I don't think it would be conscious. So in some sense, the question is, what are the computations which could be conscious? I mean, so one assumption is that it has to do purely with computation, that you can abstract away matter other possibilities, that it's very important was the realization of computation, that it has to do with some force fields or so, and they bring consciousness. At the moment, my intuition is that it can be fully abstracted away. So, in case of computation, you can ask yourself, what are the mathematical objects or so that could bring such a properties. So, for instance, if we think about the models, AI models, what they truly try to do, or like models like GPT, is they try to predict next word or so. And this turns out to be equivalent to compressing text, because in some sense, compression means that you learn the model of reality, and you have just to remember what are your mistakes, the better you are in predicting the. And in some sense, when we look at our experience, also when you look, for instance, at the car driving, you know in which direction it will go, you are good, like in prediction. And I, you know, it might be the case that the consciousness is intertwined with compression it might be also the case that self consciousness has to do with compressor trying to compress itself. So, okay, I was just wondering, what are the objects in mathematics or computer science, which are mysterious, that could have to do with consciousness? I thought, you see, in mathematics, there is something called Gadel theorem, which means if you have sufficiently complicated mathematical system, it is possible to point the mathematical system back on itself. In computer science, there is something called halting problem. It's somewhat similar construction. So I thought that if we believe that, that under assumption that consciousness has to do with compression, then you could imagine that as they keep on compressing things, then at some point it actually makes sense for the compressor to compress itself.
Speaker A: Meta. Compression. Yeah, consciousness is metacompression.
Speaker B: That's an idea. And in some sense, the crazy. Thank you.
Speaker A: So, but do you think if we think of Turing machine, a universal Turing machine, can that achieve consciousness? So is there something beyond our traditional definition of computation that's required?
Speaker B: So it's a specific computation. And I said, this computation has to do with compression. And the compression itself, maybe other way of putting it, is like a. You are internally creating the model of reality in order, like you try insight to simplify reality in order to predict what's going to happen. And that also feels somewhat similar to how I think, actually, about my own conscious experience. So, clearly, I don't have access to reality. The only access to reality is through cable going to my brain, and my brain is creating a simulation of reality, and I have access to the simulation of reality.
Speaker A: Are you by any chance aware of the Hutter Prize? Marcus Hutter, he made this prize for compression of Wikipedia pages. And there's a few qualities to it. One, I think has to be perfect compression, which makes. I think that little quirk, makes it much less applicable to the general task of intelligence, because it feels like intelligence is always going to be messy. Like, perfect compression feels like it's not the right goal, but it's nevertheless a very interesting goal. So, for him, intelligence equals compression. And so the smaller you make the file, given a large Wikipedia page, the more intelligent the system has to be.
Speaker B: Yeah, that makes sense. So you can make perfect compression if you store errors. And I think that actually what he meant is you have algorithm plus errors. By the way, Hutter is a. He was PhD, advisor of Shenlec, who is a DeepMind, co founder.
Speaker A: Yeah, yeah. So there's an interesting. And now he's a DeepMind, there's an interesting network of people. He's one of the people that I think seriously took on the task of what would an AGI system look like? I think for the longest time, the question of AGI was not taken seriously, or rather rigorously, and he did just that. Like, mathematically speaking, what would the model look like if you remove the constraints of it, having to be, having to have reasonable amount of memory, reasonable amount of running time, complexity, computation time, what would it look like? And essentially, it's a half math, half philosophical discussion of how would a reinforcement learning type of framework look like for an AGI.
Speaker B: Yeah. So he developed the framework event to describe what's optimal with respect to reinforcement learning. Like, there is a theoretical framework which is, as you said, under assumption there is infinite amount of memory. And computer was actually one person before. His name is Solomonov. Hooter extended Solomonov work to reinforcement learning, but there exists the theoretical algorithm, which is optimal algorithm to build intelligence. And I can actually explain you the algorithm.
Speaker A: Yes. Let's go, let's go.
Speaker B: So the task itself.
Speaker A: Can I just pause how absurd it is for brain in a skull trying to explain the algorithm for intelligence? Just go ahead.
Speaker B: It is pretty crazy. It is pretty crazy that the brain itself is actually so small, and it can ponder.
Speaker A: How to design algorithms that optimally solve the problem of intelligence. What's the algorithm?
Speaker B: First of all, the task itself is described as you have infinite sequence of zeros and ones. Okay? You read N bits, and you are about to predict N plus one bit. So that's the task. And you could imagine that every task could be casted as such a task. So if, for instance, you have images and labels, you can just turn every image into a sequence of zeros and ones, then Label. You concatenate labels, and that's actually, you could start by having training data first, and then afterwards you have test data. So theoretically, any problem could be casted as the problem of predicting zeros and ones on this infinite tape. So let's say you read already N bits and you want to predict N plus one bit. And I will ask you to write every possible program that generates THese N bits of. And you can have you choose programming language. It can be python or c. And the difference between programming languages might be there is a difference by constant. Asymptotically, Your predictions will be equivalent. So you read N bits, you enumerate all the programs that produce these N bits in their output. And then in order to predict n plus one bit, you actually weight the programs according to their length. There is like some specific formula how you weigh them, and then the n plus one bit prediction is the prediction from each of these program according to that weight.
Speaker A: Like statistically?
Speaker B: Statistically.
Speaker A: So the smaller the program, the more likely you are to pick its output. So that algorithm is grounded in the hope or the intuition that the simple answer is the right one.
Speaker B: It's a formalization of it.
Speaker A: Yeah.
Speaker B: It also means like, if you would ask the question after how many years would sun explode? You can say it's more likely the answer is two to some power, because it's a shorter program.
Speaker A: Yeah. I don't have a good intuition about how different the space of short programs are from the space of large programs. Like what is the universe where short programs, like, run things?
Speaker B: So, as I said, the things have to agree with n bits. So even if you have, you need to start. Okay, if, if you have very short program and they're like a steals, if it's not perfect with prediction of n bits, you have to start errors. What are the errors? And that gives you the full program that agrees on n bits.
Speaker A: Oh, so you don't agree perfectly with the n bits, and you store errors.
Speaker B: That's like a longer, a longer program, slightly longer program, because it contains these extra bits of errors.
Speaker A: That's fascinating. What's, what's your intuition about the programs that are able to do cool stuff like intelligence and consciousness? Are they perfectly like, is there if then statements in them? So is there a lot of exceptions at their story?
Speaker B: So you could imagine if there would be tremendous amount of if statements, then they wouldn't be that short. In case of neural networks, you could imagine that what happens is when you start with an uninitialized neural network, it stores internally many possibilities how the problem can be solved. And SDD is magnifying some paths which are slightly similar to the correct answer. It's magnifying correct programs. And in some sense, SDD is a search algorithm in the program space, and the program space is represented by the wiring inside of the neural network. And there's an insane number of ways how the features can be computed.
Speaker A: Let me ask you the high level basic question that's not so basic. What is deep learning? Is there a way you'd like to think of it that is different than like a generic textbook definition?
Speaker B: The thing that I hinted just a second ago is maybe that closest to how I'm thinking these days about deep learning. So the statement is neural networks can represent some programs. It seems that various modules that we are actually adding up to are like, we want networks to be deep because we want multiple steps of the computation and deep learning provides a way to represent space of programs which is searchable, and it's searchable with stochastic gradient descent. So we have an algorithm to search over a humongous number of programs, and gradient descent kind of bubbles up the things that tend to give correct answers.
Speaker A: A neural network with fixed weights that's optimized. Do you think of that as a single program?
Speaker B: So there is a work by Christopher Olach where he. So he works on interpretability of neural networks, and he was able to identify inside of the neural network, for instance, a detector of a wheel for a car or the detector of a mask for a car. And then he was able to separate them out and assemble them together using a simple program for the detector, for a car detector.
Speaker A: That's like, if you think of traditionally defined programs, that's like a function within a program that this particular neural network was able to find. And you can tear that out just like you can copy and paste some stack overflow that. So any program is a composition of smaller programs.
Speaker B: Yeah, I mean, the nice thing about the neural networks is that it allows the things to be more fuzzy than in case of programs. In case of programs, you have this, like a branching this way or that way. And the neural networks, they have an easier way to be somewhere in between or to share things.
Speaker A: What to use the most beautiful or surprising idea in deep learning, in the utilization of these neural networks, which, by the way, for people who are not familiar, neural networks is a bunch of, what would you say? It's inspired by the human brain. There's neurons, there's connection between those neurons, there's inputs and there's outputs, and there's millions or billions of those neurons. And the learning happens by adjusting the weights on the edges that connect these neurons.
Speaker B: Thank you for giving definition that I supposed to do it, but I guess you have enough empathy to listeners to actually know that that might be useful.
Speaker A: No, that's like. So I'm asking Plato of like, what is the meaning of life? He's not going to answer. You're being philosophical and deep and quite profound. Talking about the space of programs, which is, is very interesting, but also for people who just not familiar what the hell we're talking about when we talk about deep learning. Anyway, sorry. What is the most beautiful or surprising idea to you in all the time you've worked at deep learning, and you worked on a lot of fascinating projects, applications of neural networks. It doesn't have to be big and profound. It can be a cool trick.
Speaker B: Yeah, I mean, I'm thinking about the trick, but, like, it's still amusing to me that it works at all. That let's say that the extremely simple algorithm stochastic gradient descent, which is something that I would be able to derive on the piece of paper to high school student, when put at the scale of thousands of machines, actually can create the behaviors which we called human like behaviors.
Speaker A: So, in general, any application of stochastic gradient descent to neural networks is amazing to you? Or is there a particular application in natural language reinforcement learning? And also, would you attribute that success to? Is it just scale? What profound insight can we take from the fact that the thing works for gigantic sets of variables?
Speaker B: I mean, the interesting thing is, these algorithms, they were invented decades ago, and people actually gave up on the idea. And back then, they thought that we need profoundly different algorithms, and they spent a lot of cycles on very different algorithms. And I believe that we have seen that various innovations that say, like transformer or dropout or so, they can pass the help. But it's also remarkable to me that this algorithm, from sixties or so, or, I mean, you can even say that the gradient descent was invented by Leibniz in, I guess, 18th century or so, that actually is the core of learning in the past. People are, it's almost like a. Out of the, maybe an ego. People are saying that it cannot be the case that such a simple algorithm is the, you know, could solve complicated problems. So they were in search for the other algorithms. And as I'm saying, like, I believe that actually, we are in the game where there is. There are actually, frankly, three levers. There is compute. There are algorithms, and there is data. If we want to build intelligent systems, we have to pull all three levers, and they are actually multiplicative. It's also interesting, you ask, is it only compute people internally? They did the studies to determine how much gains they were coming from different levers. And so far, we have seen that more gains came from compute than algorithms. But also, we are in the word that in case of compute, there is a kind of exponential increase in funding. And at some point, it's impossible to invest more. It's impossible to invest $10 trillion, as we are speaking about, let's say, all taxes in us.
Speaker A: But you're talking about money. There could be innovation in the compute.
Speaker B: That's true as well. So, I mean, there are a few pieces. So one piece is human brain is an incredible supercomputer, and they're like a. It has 100 trillion parameters. Or like, if you try to count various quantities in the brain. There are like a neuron synapses, there are small number of neurons, there is a lot of synapses. It's unclear even how to map synapses to parameters of neural networks, but it's clear that there are many more. So it might be the case that our networks are still somewhat small. It also might be the case that they are more efficient than brain or less efficient by some huge factor. I also believe that there will be, at the moment, we are at the stage that these neural networks, they require 1000 x or a huge factor of more data than humans do. And it will be a matter of. There will be algorithms that vastly decrease sample complexity. I believe so. But the place where we are heading today is dart domains, which contains million x more data. And even though computers might be 1000 times slower than humans in learning, that's not the problem. For instance, I believe that it should be possible to create superhuman therapists by, and there are even simple steps of doing it. And the core reason is there is just machine will be able to read way more transcripts of therapies, and then it should be able to speak simultaneously with many more people, and it should be possible to optimize it all in parallel. And, well, there's.
Speaker A: Now you're touching on something I deeply care about and think is way harder than we imagine. What's the goal of a therapist?
Speaker B: What's the goal of therapist?
Speaker A: So, okay, so one goal. Now, this is terrifying to me, but there's a lot of people that contemplate suicide, suffer from depression, and they could significantly be helped with therapy. And the idea that an AI algorithm might be in charge of that, it's like a life and death task. It's, uh. The stakes are high. So one goal for a therapist, whether human or AI, is to prevent suicide. Ideation to prevent suicide. How do you achieve that?
Speaker B: So, let's see. So, to be clear, I don't think that the current models are good enough for such a task, because it requires insane amount of understanding empathy, and the models are far from displaced. But it's.
Speaker A: But do you think that understanding empathy, that signal is in the data?
Speaker B: I think there is some signal in the data, yes. I mean, there are plenty of transcripts of conversations. And it is possible to. It is possible from it to understand personalities. It is possible from it to understand if conversation is friendly, amicable, antagonistic. I believe that given the fact that the models that we train now, they are chameleons, that they can have any personality, they might turn out to be better in understanding personality of other people than anyone else. And they empathetic to be empathetic.
Speaker A: Yeah. Interesting. But I wonder if there's some level of multiple modalities required to be able to be empathetic of the human experience. Whether language is not enough to understand death, to understand fear, to understand childhood trauma, to understand wit and humor required when you're dancing with a person who might be depressed or suffering both humor and hope and love and all those kinds of things. So there's another underlying question, which is self supervised versus supervised. So can you get that from the data by just reading a huge number of transcripts?
Speaker B: I actually. So I think that reading huge number of transcripts is step one. It's like at the same way as you cannot learn to dance if just from YouTube, by watching it, you have to actually try it out yourself. So I think that here that's a similar situation. I also wouldn't deploy the system in the high stakes situations right away, but kind of see gradually where it goes. And obviously, initially it would have to go hand in hand with humans. But at the moment, we are in the situation that actually, there is many more people who actually would like to have a therapy or speak with someone, then there are therapies out there. So fundamentally, I was thinking, what are the things that can vastly increase people's well being? Therapy is one of them. Being meditation is other one. I guess maybe human connection is the third one. And I guess, pharmacologically it's also possible, maybe direct brain stimulation or something like that. But these are pretty much options out there. Then let's say the way I'm thinking about the AGI endeavor is, by default, that's an endeavor to increase amount of wealth. And I believe that we can vastly increase amount of wealth for everyone and simultaneously. So, I mean, these are like two endeavors that make sense to me. One is like, essentially increase amount of wealth, and second one is increase overall human well being.
Speaker A: And those are coupled together and they.
Speaker B: Can, I would say these are different topics. One can help another.
Speaker A: And, you know, therapist is a funny word, because I see friendship and love as therapy. I mean, so therapist, broadly defined as just friendship as a friend. So, like, therapist has a very kind of clinical sense to it. But what is human connection? You're like, not to get all Camus and Dostoevsky on you, but life is suffering, and we draw, we seek connection with other humans as we desperately try to make sense of this world in the deep, overwhelming loneliness that we feel inside.
Speaker B: So I think connection has to do with understanding. And I think that almost like a lack of understanding causes suffering. If you speak with someone and you do, you feel ignored. That actually causes pain. If you are feeling deeply understood, that actually they might not even tell you what to do in life, but like a pure understanding or just being heard.
Speaker A: Understanding is a kind of. It's a lot, you know, just being heard, feel like you're being heard. Like somehow that's alleviation, temporarily, of the loneliness that if somebody knows you're here with their body language, with the way they are, with the way they look at you, with the way they talk, you feel less alone for a brief moment.
Speaker B: Yeah, very much agree. So I thought in the past about somewhat similar question to yours, which is, what is love? Rather what is connection? And obviously, I think about these things from AI perspective. What would it mean? So I said that intelligence has to do with some compression, which is more or less, like I can say, almost understanding of what is going around. It seems to me that other aspect is there seem to be reward functions. And you can have a, you know, reward for food, for maybe human connection, for, let's say, warmth, sex and so on. And it turns out that the various people might be optimizing slightly different reward functions. They essentially might care about different things. And in case of love, at least the love between two people, you can say that boundary between people dissolves to such extent that they end up optimizing each other reward functions.
Speaker A: Yeah. That's interesting. Celebrate the success of each other.
Speaker B: Yeah. In some sense, I would say love means helping others to optimize their reward functions. Not your reward functions, not the things that you think are important, but the things that the person cares about. You try to help them to optimize it.
Speaker A: So love is, if you think of two reward functions, it's a condition. You combine them together.
Speaker B: Yeah, pretty much.
Speaker A: Maybe like with the weight and it depends, like the dynamic of the relationship.
Speaker B: Yeah, I mean, you could imagine that if you are fully optimizing someone's reward function without yours, then maybe are creating codependency or something like that. I'm not sure what's the appropriate weight. But the interesting thing is I even think that the individual person, we ourselves, we are actually less of a unified insight. So, for instance, if you look at the donut, on the one level, you might think, oh, this looks tasty, I would like to eat it. On other level, you might tell yourself, I shouldn't be doing it because I want to gain muscles. So. And, you know, you might do it regardless, kind of against yourself. So it seems that even within ourselves, they're almost like a kind of intertwined Personas. And I believe that the self love means that the love between all these Personas, which also means being able to love, love yourself when we are angry or stressed or so combining all those.
Speaker A: Reward functions of the different selves you have.
Speaker B: Yeah. And accepting that they are there. Like, you know, often people, they have a negative self talk, or they say, I don't like when I'm angry. And like, I try to imagine, try to imagine if there would be like a small baby lex, like a five years old, who's angry, angry. And then you're like, you shouldn't be angry. Like, stop being angry. Yeah, but like, instead, actually, you want the lex to come over, give him a hug, and he's like, I say, it's fine, okay, you can be angry as long as he want, and then he would stop.
Speaker A: Or maybe not, or maybe not.
Speaker B: But you cannot expect it even.
Speaker A: Yeah, but still, that doesn't explain the why of love. Like, why is love part of the human condition? Why is it useful to combine the reward functions? It seems like that doesn't. I mean, I don't think reinforcement learning frameworks can give us answers to why. Even the hutter framework has an objective function that's static.
Speaker B: So we came to existence as a consequence of evolutionary process, and in some sense, the purpose of evolution is survival. And then this complicated optimization objective baked into us, let's say compression, which might help us operate in the real world and bake into us various reward functions.
Speaker A: Yeah.
Speaker B: Then to be clear, at the moment, we are operating in the regime which is somewhat out of distribution, where they even evolution optimize us.
Speaker A: It's almost like love is a consequence of cooperation that we've discovered is useful.
Speaker B: Correct. In some way. It's even the case if you.
Speaker A: I just love the idea that love is like the outer distribution, or it's.
Speaker B: Not out of distribution. It's like, as you said, it evolved for cooperation.
Speaker A: Yes.
Speaker B: And I believe that in some sense, cooperation ends up helping each of us individually. So it makes sense evolutionary. And there is in some sense, and love means there is this dissolution of boundaries that you have a shared reward function. And we evolve to actually identify ourselves with larger groups. So we can identify ourselves with a family, we can identify ourselves with a country to such extent that people are willing to give away their life for country. So we are wired, actually, even for love. And at the moment, I guess maybe it would be somewhat more beneficial if we would identify ourselves with all the humanity as a whole. So you can clearly see when people travel around the world, when they run into person from the same country, they say, oh, which city you are. And all of a sudden, they find all these similarities. They befriend those folks earlier than others. So there is some sense of the belonging. And I would say, I think it would be overall good thing to the world for people to move towards. I think it's even called open individualism, move toward the mindset of a larger and larger groups.
Speaker A: The challenge there, that's a beautiful vision, and I share it to expand that circle of empathy, that circle of love towards the entirety of humanity. But then you start to ask, well, where do you draw the line? Because why not expand it to other conscious beings? And then finally, for our discussion, something I think about is, why not expand it to AI systems? We start respecting each other when the entity on the other side has the capacity to suffer, because then we develop a capacity to sort of empathize. And so I could see AI systems that are interacting with humans more and more, having conscious, like, displays. So, like, they display consciousness through language and through other means. And so then the question is, like, well, is that consciousness because they're acting conscious? And so, you know, the reason we don't like torturing animals is because they look like they're suffering when they're tortured. And if AI looks like it's suffering when it's tortured, how is that nothing requiring of the same kind of empathy from us and respect and rights than animals do and other humans do?
Speaker B: I think it requires empathy as well. I mean, I would like, I guess, us or humanity or so make a progress in understanding what consciousness is, because I don't want just to be speaking about the philosophy, but rather actually make a scientific. To have a, like, you know, there was a time that people thought that there is a force of life, and the things that have this force, they are alive. And I think that there is actually a path to understand exactly what consciousness is. And in some sense, it might require essentially putting probes inside of a human brain, what neuralink does.
Speaker A: So the goal there, I mean, there's several things with consciousness that make it a real discipline, which is, one is rigorous measurement of consciousness, and then the other is the engineering of consciousness, which may or may not be related. I mean, you could also run into trouble. Like, for example, in the United States, the Department dot Department of Transportation and a lot of different places put a value on human life. I think dots, values, $9 million per person, sort of. In that same way you can get into trouble if you put a number on how conscious of being is, because then you can start making policy. If a cow is 0.1 or like 10% as conscious as a human, then you can start making calculations and it might get you into trouble. But then again, that might be a very good way to do it.
Speaker B: I would like to move to that place that actually we have scientific understanding what consciousness is, and then we'll be able to actually assign value. And I believe that there is even the path for the experimentation in it. So we said that you could put the probes inside of the brain. There is actually a few other things that you could do with devices like neuralinken. So you could imagine that the way even to measure if AI system is conscious is by literally just plugging into the brain. I mean, that assumes that it's kind of easy, but the plug into the brain and asking person if they feel that their consciousness expanded. This direction, of course, has some issues. You can say if someone takes a psychedelic drug, they might feel that their consciousness expanded even though the drug itself is not conscious, right?
Speaker A: So, like, you can't fully trust the self report of a person saying their consciousness is expanded or not. Let me ask you a little bit about psychedelics, because there've been a lot of excellent research on different psychedelics. Psilocybin, MDMA, even DMT drugs in general. Marijuana too. What do you think psychedelics do to the human mind? It seems they take the human mind to some interesting places. Is that just a little hack, a visual hack, or is there some profound expansion of the mind?
Speaker B: So let's see. I don't believe in magic. I believe in that. I believe in science, in causality still, let's say. And then, as I said, I think that brain, that our subjective experience of reality is we live in the simulation run by our brain and the simulation that our brain runs. They can be very pleasant or very hellish drugs. They are changing some hyperparameters of the simulation. It is possible, thanks to change of these hyperparameters, to actually look back on your experience and even see that the given things that we took for granted, they are changeable. So they allow to have an amazing perspective. There is also, for instance, the fact that after DMT, people can see the full movie inside of their head gives me further belief that the brain can generate the full movie, that the brain is actually learning the model of reality to such extent that it tries to predict what's going to happen next.
Speaker A: Yeah. Very high resolution, so it can replay reality.
Speaker B: Is that true? Extremely high resolution, yeah. It's also kind of interesting to me that somehow there seems to be some similarity between these drugs and meditation itself. And I actually started even these days to think about meditation as a psychedelic.
Speaker A: Do you practice meditation?
Speaker B: I practice meditation. I mean, I once, few times on the retreats. And it feels after, like after second or third day of meditation, there is almost like a sense of tripping.
Speaker A: What does a meditation retreat entail?
Speaker B: So you wake up early in the morning and you meditate for extended period of time alone. Sorry. To keep in mind. Yeah. So it's optimized. Even though there are other people, it's optimized for isolation. So you don't speak with anyone, you don't actually look into other people's eyes and you sit on the chair, say Vipassana. Meditation tells you to focus on the breath. So you try to put all attention into breathing and breathing in and breathing out. And crazy thing is that as you focus attention like that, after some time there's starts coming back like some memories that you completely forgotten. It almost feels like you have a mailbox and then you are just archiving email one by one. And at some point, at some point there is a amazing feeling of getting to mailbox. Zero, zero emails. And it's very pleasant. It's kind of, it's, it's, it's crazy to me that, that once you resolve these inner stories or like inner traumas, then once there is nothing left, the default state of human mind is extremely peaceful and happy. Like some sense. It feels that. It feels, at least to me, that way, how when I was a child, that I can look at any object and it's very beautiful. I have a lot of curiosity about the simple things and that's where the usual meditation takes me.
Speaker A: Are you, what are you experiencing? Are you just taking in simple sensory information and they're just enjoying the rawness of that sensory information. So there's no, there's no memories or all that kind of stuff you're just enjoying being.
Speaker B: Yeah, pretty much. I mean, still there is a, it's, it's thoughts are slowing down, sometimes they pop up, but it's also somehow the extended meditation takes you to the space that they are way more friendly, way more positive. There is also this thing that it almost feels that. It almost feels that we are constantly getting a little bit of a reward function and we are just spreading this reward function on various activities. But if you stay still for extended period of time, it kind of accumulates, accumulates, accumulates. And there is a sense. There is a sense that some point it passes some threshold and it feels as drop is falling into kind of ocean of love and bliss. And that's like a. This is like a very pleasant. And that's what I'm saying. Like that corresponds to the subjective experience. Some people, I guess in spiritual community, they describe it that that's the reality. And I would say, I believe that there are all sorts of subjective experience that one can have. And I believe that, for instance, meditation might take you to the subjective experiences, which are very pleasant, collaborative, and I would like a world to move toward a more collaborative place. Yeah, I would say that's very pleasant. And I enjoy doing stuff like that.
Speaker A: I wonder how that maps to your mathematical model of love with the reward function combining a bunch of things. It seems like our life then is we have this reward function and we're accumulating a bunch of stuff in it with weights. It's like multi objective. And what meditation is, is you just remove them. Remove them until the weight on one or just a few is very high. And that's where the pleasure comes from.
Speaker B: Yeah. So something similar how I'm thinking about this. So I told you that there is this like, there is a story of who you are. And I think almost about it as a, you know, text prepended to GPT.
Speaker A: Yeah.
Speaker B: And some people refer to it as ego. Okay. It's like a story who you are.
Speaker A: Okay, so ego is the prompt for GPT-3 or GPT.
Speaker B: Yes. Yes. And that's description of you. And then with meditation, you can get to the point that actually you experience things without the prompt and you experience things like, as they are, you are not biased over the description how they supposed to be. That's very pleasant. And then with respect to the reward function, it's possible to get to the point that there is dissolution of self. And therefore you can say that you are having your brain attempts to simulate the reward function of everyone else or everything. There is this love which feels like a oneness with everything. And that's also, you know, very beautiful, very pleasant. At some point you might have a lot of altruistic thoughts during that moment, and then the self always comes back.
Speaker A: How would you recommend, if somebody is interested in meditation, like a big thing to take on as a project, would you recommend a meditation retreat? How many days? What kind of thing would you recommend?
Speaker B: I think that actually retreat is the way to go. It almost feels that, as I said, like a meditation is a psychedelic, but when you take it in the small dose, you might barely feel it. Once you get the high dose, actually, you're gonna feel it.
Speaker A: So even cold turkey, if you haven't really seriously meditated for a prolonged period of time, just go to a retreat?
Speaker B: Yeah.
Speaker A: How many days start?
Speaker B: Weekend?
Speaker A: One weekend. So like two, three days.
Speaker B: And it's like, it's interesting that first or second day it's hard. And at some point it becomes easy.
Speaker A: There's a lot of seconds in a day. How hard is the meditation retreat? Just sitting there in a chair.
Speaker B: So the thing is, actually, it literally just depends on your. On your own framing. Like, if you are in the mindset that you are waiting for it to be over or you are waiting for nirvana to happen, it will be very unpleasant. And in some sense, even the difficulty, it's not even in the lack of being able to speak with others. Like, you are sitting there, your legs will hurt from sitting.
Speaker A: In terms of the practical things, do you experience kind of discomfort, like physical discomfort of just sitting? Like your butt being numb, your legs being sore, all that kind of stuff?
Speaker B: Yes. You experience it and then the. That they teach you to observe it, rather. And it's like the crazy thing is you at first might have a feeling toward trying to escape it, and that becomes very apparent that that's extremely unpleasant. And then you just observe it, and at some point it just becomes. It just is. It's like, I remember with Ilya told me some time ago that he takes a cold shower, and his mindset of taking a cold shower was to embrace suffering.
Speaker A: Yeah, excellent. I do the same.
Speaker B: This is your style?
Speaker A: Yeah, it's my style. I like this.
Speaker B: So my style is actually. I also sometimes take cold showers. It is purely observing how the water goes through my body. Like, purely being present, not trying to escape from there.
Speaker A: Yeah.
Speaker B: And I would say then it actually becomes pleasant.
Speaker A: It's not like, ah, well, that, that's interesting. Um, I I'm also. That mean, that's. That's the way to deal with anything really difficult, especially in the physical space, is to observe it, to say it's pleasant. Hmm. It's a. I would use a different word. You're, um, you're accepting of the full beauty of reality, I would say. Cause say pleasant, but yeah, in some sense it is pleasant. That's the only way to deal with a cold shower is to become an observer and to find joy in it. Same with, like, really difficult physical exercise or like, running for a really long time. Endurance events, just anytime you're exhausted, any kind of pain. I think the only way to survive it is not to resist. It is to observe it. You mentioned Ilya.
Speaker B: Ilya Setsgever, he's our chief scientist, but also he's a very close friend of mine.
Speaker A: He co founded OpenAI with you. I've spoken with him a few times. He's brilliant. I really enjoy talking to him. His mind, just like yours, works in fascinating ways. Both of you are not able to define deep learning simply, what's it like having him as somebody you have technical discussions with in space of machine learning, deep learning, AI, but also life. What's it like when these two agents get into a self play situation in a room? What's it like collaborating with him?
Speaker B: So I believe that we have extreme respect to each other. So I love Ilya's insight, both, like, I guess, about consciousness, life, AI.
Speaker A: But in terms of the. It's interesting to me because you're a brilliant thinker in the space of machine learning, like intuition, like digging deep in what works, what doesn't, why it works, why it doesn't. And so is Ilya. I'm wondering if there's interesting deep discussions you've had with him in the past, or disagreements that were very productive.
Speaker B: So I can say I also understood over the time, where are my strengths? So, obviously we have plenty of AI discussions and. And I myself have plenty of ideas, but I consider Ilya one of the most prolific AI scientists in the entire world. And I think that I realized that maybe my super skill is being able to bring people to collaborate together, that I have some level of empathy that is unique in AI world, and that might come from either meditation, psychedelics, let's say. I read just hundreds of books on this topic, so. And I also went through a journey of, you know, I develop all sorts of algorithms, so I think that maybe I can. That's my superhuman skill. Ilya is one of the best AI scientists, but then I'm pretty good in assembling teams, and I'm also not holding to people. Like, I'm growing people, and then people become managers at Openaiden that grew, many of them, like a research manager.
Speaker A: So you find places where you're excellent, and he finds, like, his deep scientific insights is where he is, and you find ways you can, the puzzle pieces fit together.
Speaker B: Correct. Ultimately, for instance, let's say, ilya, he doesn't manage people. That's not what he likes, or so I like hanging out with people. By default. I'm an extrovert and I care about people.
Speaker A: Oh, interesting. Okay. Okay, cool. So that fits perfectly together. But I mean, I also just like your intuition about various problems in machine learning. He's definitely one I really enjoy. I remember talking to him about something I was struggling with, which is coming up with a good model for pedestrians, for human beings across the street in the context of autonomous vehicles. And he immediately started to, like, formulate a framework within which you can evolve a model for pedestrians, like, through self play, all that kind of mechanisms. The depth of thought on a particular problem, especially problems he doesn't know anything about, is fascinating to watch. It makes you realize, like, yeah, the limits that the human intellect might be limitless, or it's just impressive to see a descendant of ape come up with clever ideas.
Speaker B: Yeah, I mean, so even in the space of deep learning, when you look at various people, there are people now who invented some breakthroughs once, but there are very few people who did it multiple times. And you can think, if someone invented it once, that might be just a sheer luck. And if someone invented it multiple times, you know, if a probability of inventing it once is one over a million, then probability of inventing it twice or three times would be one over a million square or to the power of three, which would be just impossible. So it literally means that it's given that it's not the luck. And Ilya is one of these few people who, who have a lot of these inventions in his arsenal. It also feels that, for instance, if you think about folks like Gauss or Euler, at first they read a lot of books, and then they did thinking, and then they figure out math, and that's how it feels with Ilya. At first, he read stuff, and then he spent his thinking cycles.
Speaker A: And that's a really good way to put it. When I talk to him, I see thinking. He's actually thinking. He makes me realize that there's deep thinking that the human mind can do. Most of us are not thinking deeply. You really have to put a lot of effort to think deeply. Really put myself in a place where I think deeply about a problem. It takes a lot of effort. It's like a. It's like an airplane taking off or something. You have to achieve deep focus. He's just, he's. What is it? His brain is like a vertical takeoff in terms of airplane analogy. So it's interesting, but it. I mean, Cal Newport talks about this as ideas of deep work. It's, you know, most of us don't work much at all in terms of like, like deeply think about particular problems, whether it's a math, engineering, all that kind of stuff. You want to go to that place often, and that's real hard work. And some of us are better than others at that.
Speaker B: So I think that the big piece has to do with actually even engineering your environment that. Such that it's conducive to that.
Speaker A: Yeah.
Speaker B: So, um, see, both Ilya and I, on the frequent basis we kind of disconnect ourselves from the world in order to be able to do extensive amount of thinking.
Speaker A: Yes.
Speaker B: So Ilya, usually he just leaves iPad at hand. He loves his iPad. And for me, I'm even sometimes just going for a few days to different location to Airbnb. I'm turning off my phone and there is no access to me. And that's extremely important for me to be able to actually just formulate new thoughts, to do deep work rather than to be reactive. And the older I am, the more of these random tasks are at hand.
Speaker A: Before I go on to that thread, let me return to our friend GPT. Let me ask you another ridiculously big question. Can you give an overview of what GPT-3 is? Or like you say in your Twitter bio, GPT n one, how it works and why it works?
Speaker B: So GPT-3 is a humongous neural network. Let's assume that we know what is neural network definition and it is trained on the entire Internet and just to predict next word. So let's say it sees part of the article, and the only task that it has at hand it is to say, what would be the next word? What would be the next word? And it becomes really exceptional at the task of figuring out what's the next word. So you might ask, why would this be an important task? Why would it be important to predict what's the next word of? And it turns out that a lot of problems can be formulated as a text completion problem. So GPT is purely learning to complete the text. But you could imagine, for instance, if you are asking a question, who is president of United States? Then GPT can give you an answer to it. It turns out that many more things can be formulated this way. Format text in the way that you have sentence in English. You make it even look like some content of a website elsewhere, which would be teaching people how to translate things between languages. So it would be en text in English fr colon. And then you ask model to continue. And it turns out that such a model is predicting translation from English to French. And the crazy thing is that this model can be used for way more sophisticated tasks. So you can format text such that it looks like a conversation between two people, and that might be a conversation between you and Elon Musk. And because the model read all the text about Elon Musk, it will be able to predict Elon Musk words as it would be Elon Musk. It will speak about colonization of Mars, about sustainable future, and so on. And it's also possible to even give arbitrary personality to the model. You can say, here is a conversation with a friendly AI bot, and the model will complete the text as a friendly AI bot.
Speaker A: So, I mean, how do I express how amazing this is? So, just to clarify a conversation, generating a conversation between me and Elon Musk, it wouldn't just generate good examples of what Elon would say. It would get the syntax all correct. So, like, interview style, you would say, like, Elon Cohen and Lex Cohen. Like it. It's not just like inklings of semantic correctness. It's like the whole thing. Grammatical, syntactic, semantic. It's just really, really impressive generalization.
Speaker B: Yeah. I mean, I also want to, you know, provide some caveats so it can generate few paragraphs of coherent text, but as you go to longer pieces, it actually goes off the rails. Okay. If you would try to write a book, it won't work out this way.
Speaker A: What way does it go off the rails, by the way? Is there interesting ways in which it goes off the rails? What falls apart first?
Speaker B: So the model is trained on all the existing data that is out there, which means that it is not trained on its own mistakes. So, for instance, if it would make a mistake, then to give you an example. So let's say I have a conversation with a model pretending that is Elon Musk, and then I start putting some. I start actually making up things which are not factual, I would say, sounds.
Speaker A: Like Twitter, but I got you. Sorry. Yeah.
Speaker B: Okay. I don't know. I would say that Elon is my wife, and the model will just keep on carrying it on and.
Speaker A: As if it's true.
Speaker B: Yes. And in some sense, if you would have a normal conversation with Elon, he would be, what the fuck?
Speaker A: Yeah, there would be some feedback. So the model is trained on things that humans have written, but through the generation process, there's no human in the loop feedback.
Speaker B: Correct.
Speaker A: That's fascinating.
Speaker B: Makes sense.
Speaker A: So it's magnifies. Like, the errors get magnified and magnified. And it's also interesting. I mean, first of all, humans have the same problem. It's just that we make fewer errors and magnify the errors slower.
Speaker B: I think that actually what happens with humans is if you have a wrong belief about the world as a kid, then very quickly we'll learn that it's not correct because you are grounded in reality and you are learning from your new experience.
Speaker A: But do you think the model can correct itself too, won't it, through the power of the representation? And so the absence of Elon Musk being your wife, information on the Internet, won't it correct itself?
Speaker B: There won't be examples like that.
Speaker A: So the errors will be subtle at first.
Speaker B: Subtle at first. And in some sense, you can also say that the data that is nothing out there is a data which would represent how the human learns, and maybe model would be trained on such a data, then it would be better off.
Speaker A: How intelligent is GPT-3 do you think? Like, when you think about the nature of intelligence, it seems exceptionally impressive. But then if you think about the big AGI problem, is this footsteps along the way to AGI.
Speaker B: So let's see. It seems that intelligence itself is. There are multiple axes of it, and I would expect that the systems that we are building, they might end up being superhuman on some axis and subhuman on some other axis. It would be surprising to me, on all axis simultaneously, they would become superhuman. Of course, people ask this question, is GPT a spaceship that, that would take us to moon, or are we building a ladder to heaven? That we are just building bigger and bigger ladder? And we don't know in some sense, which one of these two.
Speaker A: Which one is better? I'm trying to. I like stairway to heaven. It's a good song. So I'm not exactly sure which one is better. But you're saying, like, the spaceship to the moon is actually effective.
Speaker B: Correct. So people who criticize GPT, yeah, they say, the other guy is just building a taller ladder and it will never reach the moon. And at the moment, I would say, the way I'm thinking is, this is like a scientific question. And I'm also, in heart, I'm a builder creator. And like, I'm thinking, let's try out. Let's see how far it goes. And so far, we see constantly that there is a progress.
Speaker A: Yeah. So do you think GPT four, GPT five, GPT N plus one, will. There'll be a phase shift, like a transition to a place where we'll be truly surprised? Then again, like, GPT-3 is already very truly surprising. The people that criticize GPT-3 as a stair, as a, what is it? Ladder to heaven? I think too quickly get accustomed to how impressive it is that the prediction of the next word can achieve such depth of semantics, accuracy of syntax, grammar and semantics. Do you think GPT four and five and six will continue to surprise us?
Speaker B: I mean, definitely there will be more impressive models. There is a question, of course, if there will be a phase shift. And also even the way I'm thinking about these models is that when we build these models, we see some level of the capabilities, but we don't even fully understand everything that the model can do. And actually, one of the best things to do is to allow other people to probe the model, to even see what is possible.
Speaker A: Hence using GPT as an API and opening it up to the world.
Speaker B: Yeah, I mean, so when I'm thinking from perspective of obviously various people that have concerns about AGI, including myself, and when I'm thinking from perspective, what's the strategy even to deploy these things to the world? The one strategy that I have seen many times working is the iterative deployment, that you deploy slightly better versions and you allow other people to criticize you. So you actually are tried out. You see where there are fundamental issues, and it's almost, you don't want to be in that situation that you are holding into powerful system, and there's like a huge overhang, then you deploy it and it might have a random, chaotic impact on the world. So you, you actually want to be in the situation that they are gradually deploying systems.
Speaker A: I asked this question of Ilya. Let me ask you this question. I've been reading a lot about Stalin and power. If you're in possession of a system that's like AGI, that's exceptionally powerful, do you think your character and integrity might become corrupted? Like, famously, power corrupts and absolute power corrupts absolutely.
Speaker B: So I believe that you want, at some point, to work toward distributing the power. I think that you want to be in the situation that actually, AGI is not controlled by a small number of people, but essentially by a larger collective.
Speaker A: So the thing is, that requires a George Washington style move in the ascent to power. There's always a moment when somebody gets a lot of power and they have to have the integrity and the moral compass to give away that power, that humans have been good and bad throughout history at this particular step. And I wonder, I wonder, we blind ourselves, for example, between nations, a race towards AI, race between nations. We might blind ourselves and justify to ourselves the development of AI without distributing the power, because we want to defend ourselves. Against China, against Russia, that kind of logic. And I wonder how we design governance mechanisms that prevent us from becoming power hungry and in the process destroying ourselves.
Speaker B: So let's see. I have been thinking about this topic quite a bit, but I also want to admit that once again, I actually want to rely way more on Sam Altman on it herald and excellent block on how even to distribute wealth. And he proposed in his block to tax equity of the companies rather than profit and to distribute it. And this is an example of Washington move. I guess I personally have insane trust in some. He already spent plenty of money running universal basic income project. That gives me, I guess, maybe some level of trust to him. But I also, I guess, love him as a friend.
Speaker A: Yeah. I wonder because we're sort of summoning a new set of technologies. I wonder if we'll be cognizant like you're describing the process of OpenAI, but it could also be at other places, like in the us government. Right. Both China and the US are now full steam ahead on autonomous weapons systems development. And that's really worrying to me because in the framework of something being a national security danger or a military danger, you can do a lot of pretty dark things that blind our moral compass. And I think AI will be one of those things. In some sense, the mission and the work you're doing at OpenAI is like the counterbalance to that. So you want to have more open AI and less autonomous weapon systems.
Speaker B: I like these statements, to be clear, this interesting, and I'm thinking about it myself, but this is a place that I, I put my trust actually in Sam's hands because it's extremely hard for me to reason about it.
Speaker A: Yeah. I mean, one important statement to make is it's good to think about this.
Speaker B: Yeah. No question about it. No question.
Speaker A: Even like low level, quote unquote engineer. Like, there's such a. I remember I programmed a car RC cardinal. They went really fast, like 30, 40 miles an hour. And I remember I was sleep deprived. So I programmed it pretty crappily. And it, like the code froze. So it's doing some basic computer vision and it's going around on track, but it's going full speed. And there was a bug in the code that the car just went. It didn't turn, it went straight full speed and smash into the wall. And I remember thinking the seriousness with which you need to approach the design of artificial intelligence systems and the programming of artificial intelligence systems is high because the consequences are high. Like that little car smashing into the wall for some reason I immediately thought of like an algorithm that controls nuclear weapons having the same kind of bug. And so, like, the lowest level engineer and the CEO of a company all need to have the seriousness in approaching this problem and thinking about the worst case consequences.
Speaker B: So I think that is true. I mean, what I also recognize in myself and others even asking this question, is that it evokes a lot of fear. And fear itself ends up being actually quite debilitating. The place where I arrived at the moment might sound cheesy or so, but it's almost to build things out of love rather than fear. Yeah, I can focus on how I can maximize the value, how the systems that I'm building might be useful. I'm not saying that the fear doesn't exist out there and it totally makes sense to minimize it, but I don't want to be working because I'm scared. I want to be working out of passion, out of curiosity, out of the, you know, looking forward for the positive.
Speaker A: Future with the definition of love arising from rigorous practice of empathy. So not just like your own conception of what is good for the world, but always listening to others.
Speaker B: Correct. Like at the loft where I'm considering reward functions of others, others to limit.
Speaker A: To infinity is like a sum like one to n, where n is 7 billion or whatever.
Speaker B: It is not. Not projecting my reward functions on others.
Speaker A: Yeah, exactly. Okay, can we just take a step back to something else super cool, which is OpenAI codecs. Can you give an overview of what OpenAI codecs and GitHub copilot is, how it works, and why the hell it works so well.
Speaker B: So with GPT-3 we noticed that the system, you know, the system training on all the language out there started having some rudimentary coding capabilities. So we're able to ask it, you know, to implement addition function between two numbers, and indeed it can write Python or JavaScript code for that. Then we thought we might as well just go full steam ahead and try to create a system that is actually good at what we are doing every day ourselves, which is programming. We optimize models for proficiency in coding. We actually even created models that both have a comprehension of language and code, and Codex is API for these models.
Speaker A: So it's first pre trained on language, and then I don't know if you can say fine tuned because there's a lot of code, but it's language and.
Speaker B: Code, it's language and code. It's also optimized for various things like, let's say low latency and so on. Codex is the API that's similar to GPT-3. We expect that there will be proliferation of the potential products that can use coding capabilities. And I can, I can speak about it in a second. Copilot is the first product and developed by GitHub. So as we're building models, we wanted to make sure that these models are useful. And we worked together with GitHub on building the first product. Copilot is actually as you code, it suggests you code completions. And we have seen in the past, there are various tools that can suggest how to few characters of the code or the line of code. The thing about copilot is it can generate ten lines of code. It's often the way how it works is you often write in the comment what you want to happen, because people in comments, they describe what happens next. So these days when I code, instead of going to Google to search for the appropriate code to solve my problem, I say, oh, for this array, could you smooth it? And then it imports some appropriate libraries and say it uses numpy convolution or so that I was not even aware that exists, and it does the appropriate thing.
Speaker A: So you write a comment, maybe the header of a function, and it completes the function. Of course, you don't know what is the space of all the possible small programs it can generate. What are the failure cases, how many edge cases, how many subtle errors there are, how many big errors there are, it's hard to know. But the fact that it works at all in a large number of cases is incredible. It's a kind of search engine into code that's been written on the Internet.
Speaker B: Correct. For instance, when you search things online, then usually you get to some particular case. If you go to stack overflow, people describe the one particular situation and then they seek for a solution. But in case of copilot, it's aware of your entire context. And in context is, oh, these are the libraries that you are using. That's the set of the variables that is initialized and on the spot. It can actually tell you what to do. So the interesting thing is, and we think that the copilot is one possible product using codex, but there is a place for many more. So internally we tried out to create other fun products. It turns out that a lot of tools out there, let's say Google Calendar or Microsoft Word or so they all have internal API to build plugins around them. So there is a way in, the sophisticated way to control calendar or Microsoft Word today, if you want, if you want more complicated behaviors from these programs, you have to add a new button for every behavior. But it is possible to use Codex and tell, for instance, to calendar. Could you schedule an appointment with Lex next week after 02:00 p.m. and either writes corresponding piece of code? And that's the thing that actually you want.
Speaker A: So interesting. So what you figure out is there's a lot of programs with which you can interact through code, and so there you can generate that code from natural language. That's fascinating.
Speaker B: And that's somewhat like also closest to what was the promise of Siri or Alexa. So previously all these behaviors, they were hard coded. And it seems that codex on the fly can pick up the API of, let's say, given software, and then it can turn language into use of this API.
Speaker A: So without hard coding, it can translate to machine language corrected to. So for example, this would be really exciting for me, like for Adobe products like Photoshop, which I think actionscript, I think there's a scripting language that communicates with them, same with premiere.
Speaker B: And you could imagine that that allows even to do coding by voice on your phone. So for instance, in the past, as of today, I'm not editing word documents on my phone, because it's just the keyboard is too small. But if I would be able to tell to my phone, make the header large, then move the paragraphs around, and that's actually what I want. So I can tell you one more cool thing, or even how I'm thinking about Codex. So if you look actually at the evolution of computers, we started with a very primitive interfaces, which is a punch card and punch card, essentially you make a holes in the, in the plastic card to indicate zeros and ones. And during that time there was a small number of specialists who were able to use computers. And by the way, people even suspected that there is no need for many more people to use computers. But then we moved from punch cards to at first assembly and circumental, and these programming languages, they were slightly higher level, they allowed many more people to code, and they also led to more of a proliferation of technology. And further on, there was a jump from c to Java and Python. And every time it has happened, more people are able to code and we build more technology, and it's even hard to imagine now if someone will tell you that you should write code in assembly instead of let's say, Python or Java or JavaScript. And Codex is yet another step toward bringing computers closer to humans, such that you communicate with a computer with your own language, rather than with a specialized language, and I think that it will lead to increase of number of people who can code.
Speaker A: Yeah. And the kind of technologies that those people will create is. It's innumerable. It could, you know, it could be a huge number of technologies we're not predicting at all, because that's less and less requirement of having a technical mind, a programming mind. You're not opening it to the world of other kinds of minds, creative minds, artistic minds, all that kind of stuff.
Speaker B: I would like, for instance, biologists who work on DNA to be able to program and not need to spend a lot of time learning it. And I believe that's a good thing to the world. And I would actually add, I would add, so at the moment, I'm a managing codex team and also language team, and I believe that there is like a plenty of brilliant people out there and they should apply.
Speaker A: Oh, okay. Yeah. Awesome. So what's the language in the codexes? So those are kind of their overlapping teams. It's like GPT, the raw language, and then the codecs is applied to programming.
Speaker B: Correct. And they are quite intertwined. There are many more teams involved, making these models extremely efficient and deployable. For instance, there are people who are working to make our data centers amazing, or there are people who work on putting these models into production, or even pushing it at the very limit of the scale.
Speaker A: So all aspects, from the infrastructure to the actual machine learning.
Speaker B: So I'm just saying there are multiple teams. While the team working on Codex and language, I guess I'm directly managing them. I would love to hire more.
Speaker A: If you're interested in machine learning. This is probably one of the most exciting problems and systems to be working on, because it's actually, it's pretty cool. But the program synthesis, the generating of programs, is very interesting, very interesting problem that has echoes of reasoning and intelligence in it. And I think there's a lot of fundamental questions that you might be able to sneak up to by generating programs.
Speaker B: Yeah, one more exciting thing about the programs is that. So I said that in case of language, one of the troubles is even evaluating language. So when the things are made up, you need somehow either a human to say that this doesn't make sense, or so in case of program, there is this one extra lever that we can actually execute programs and see what they evaluate to. So the process might be somewhat more automated in order to improve the qualities of generations.
Speaker A: That's fascinating. Wow, that's really interesting.
Speaker B: So, for the language, you know, the simulation, to actually execute it, that's the human mind yeah, for programs there is a, there is a computer on which you can evaluate it.
Speaker A: Wow, that's a brilliant little insight that the thing compiles and runs, that's first and second you can evaluate like do automated unit testing.
Speaker B: And in some sense it seems to me that we will be able to make a tremendous progress. You know, we are in the paradigm that there is way more data and there is like a transcription of millions of software engineers.
Speaker A: Yeah, yeah. So I mean, you just mean. Because I was going to ask you about reliability. The thing about programs is you don't know if they're going to like a program that's controlling a nuclear power plant has to be very reliable.
Speaker B: So I wouldn't start with controlling nuclear power plant maybe one day, but that's not actually, that's not on the current roadmap, that's not that step one, you.
Speaker A: Know, it's the russian thing. You just want to go to the most powerful destructive thing right away run by JavaScript. But I got you. So this is a lower impact, but nevertheless, what you make me realize, it is possible to achieve some levels of reliability by doing testing.
Speaker B: And you could imagine that maybe there are ways for model to write event code for testing itself and so on, and there exists ways to create the feedback loops that the model could keep on improving.
Speaker A: By writing programs that generate.
Speaker B: Tests for the instance, for the instance.
Speaker A: And that's how we get consciousness, because it's meta compression. That's what you're going to write. That's the comment, that's the prompt that generates consciousness. Compressor of compressors. You just write that. Do you think the code that generates consciousness would be simple?
Speaker B: So let's see. I mean, ultimately the core idea behind will be simple, but there will be also decent amount of engineering involved. In some sense it seems that spreading these models on many machines, it's not that trivial. And we find all sorts of innovations that make our models more efficient. I believe that first models that I guess are conscious are like a truly intelligent, they will have all sorts of tricks.
Speaker A: But then again there's Richard Sutton argument that maybe the tricks are temporary things.
Speaker B: Yeah, they might be temporary things. And in some sense it's also even important to know that even the cost of a trick. So sometimes people are eager to put the trick while forgetting that there is a cost of maintenance, like long term cost, long term cost or maintenance, or maybe even flexibility of code to actually implement new ideas. So even if you have something that gives you two x, but it requires a 1000 lines of code, I'm not sure if it's actually worth it. So in some sense, if it's five lines of code and two x, I would take it. And we see many of this, but also that requires some level of, I guess, lack of attachment to code that we are willing to remove it.
Speaker A: Yeah. So you led the OpenAI robotics team. Can you give an overview of, of the cool things you're able to accomplish? What are you most proud of?
Speaker B: So when we started robotics, we knew that actually reinforcement learning works, and it is possible to solve fairly complicated problems. Like, for instance, Alphago is an evidence that it is possible to build superhuman go players. Dota two is an evidence that it's possible to build superhuman agents. Playing Delta. So I asked myself a question, you know, what about robots out there? Could we train machines to solve arbitrary tasks in the physical world? Our approach was, I guess, let's pick a complicated problem that if we would solve it, that means that we made some significant progress the domain, and then we went after the problem. So we noticed that actually the robots out there, they are kind of, at the moment, minds per task. So you can have a robot that it's like, if you have a robot opening a battle, it's very likely that the end factor is a battle opener. And in some sense, that's a hack to be able to solve a task, which makes any task easier, and ask myself, so what would be a robot that can actually solve many tasks? And we conclude that a human hands have such a quality that indeed they are. You know, you have five kind of tiny arms attached. Individually, they can manipulate pretty broad spectrum of objects. So we went after a single hand, like trying to solve Rubik's cube single handed. We picked this task because we thought that there is no way to hard code it. And also we picked a robot on which it would be hard to hard code it. And we went after the solution such that it could generalize to other problems.
Speaker A: And just to clarify, it's one robotic hand solving the Rubik's cube. The hard part isn't the solution to the Rubik's cube is the manipulation of the. Of, like, having it not fall out of the hand, having it use the five baby arms to. What is it, like, rotate different parts of the Rubik's cube to achieve the solution?
Speaker B: Correct?
Speaker A: Yeah. So what was the hardest part about that? What was the approach taken there? What are you most proud of?
Speaker B: Obviously, we have, like, a strong belief in reinforcement learning. And one path it is to do reinforcement learning the real world. Other path is to the simulation in some sense. The tricky part about the real world is, at the moment, our models, they require a lot of data. There is essentially no data. And we decided to go through the path of the simulation. And in simulation, you can have infinite amount of data. The tricky part is the fidelity of the simulation, and also, can you, in simulation, represent everything that you represent otherwise in the real world? And it turned out that because there is lack of fidelity, it is possible. What we arrived at is training a model that doesn't solve one simulation, but it actually solves the entire range of simulations, which vary in terms of, like, what's the exactly, the friction of the cube or the weight or so. And the single AI that can solve all of them ends up working well with the reality.
Speaker A: How do you generate the different simulations?
Speaker B: So there's plenty of parameters out there. We just pick them randomly. And in simulation, model just goes for thousands of years and keeps on solving Rubik's cube in each of them. And the thing is, the neural network that we used, it has a memory, and as it presses, for instance, the side of the cube, it can sense. Oh, that's actually, this side was difficult to press. I should press stronger. And throughout this process, kind of learns even how to, how to solve this particular instance of the Rubik's cube by given mass. It's kind of like a, you know, sometimes when you go to a gym and after. After bench press, you try to lift the glass and you kind of forgot, and your hand goes, like, up right away because kind of you got used to. To maybe different weight, and it takes a second to adjust.
Speaker A: Yeah.
Speaker B: And this kind of memory, the model gained through that process of interacting with the cube in the simulation.
Speaker A: I appreciate you speaking to the audience with a bench press. All the bros in the audience probably working out right now, there's probably somebody listening to this actually doing bench press. So maybe put the bar down and pick up the water bottle, and you'll know exactly what Jack is talking about. Okay, so what was the hardest part of getting the whole thing to work?
Speaker B: So, the hardest part is at the moment, when it comes to physical world, when it comes to robots, they require maintenance. It's hard to replicate them million times. It's also. It's hard to replay things exactly. I remember this situation, that one guy at our company, he had a model that performs way better than other models in solving Rubik's cube, and he didn't know what's going on. Why is that? It turned out that he was running it from his laptop. That had better cpu or better maybe local gpu as well. And because of that, there was less of a latency, and the model was the same, and that actually made solving Rubik's cube more reliable. So in some sense, there might be some saddlebags like that when it comes to running things in the real world. Even hinting on that, you could imagine that the initial models, you would like to have models which are insanely huge neural networks, and you would like to give them even more time for thinking. When you have these real time systems, then you might be constrained, actually, by the amount of latency. And ultimately, I would like to build a system that it is worth for you to wait five minutes, because it gives you the answer that you are willing to wait for five minutes.
Speaker A: Latency is a very unpleasant constraint under which to operate.
Speaker B: Correct. Also, there is actually one more thing which is tricky about robots. There is actually not much data. So the data that I'm speaking about would be data of first person experience from the robot. And, like, gigabytes of data like that. If we would have gigabytes of data like that of robots solving various problems, it would be very easy to make a progress on robotics. And you can see that in case of text or code, there is a lot of data, like a first person perspective. Data on writing code.
Speaker A: Yeah. So you had this. You mentioned this really interesting idea that if you were to build, like, a successful robotics company, so OpenAI's mission is much bigger than robotics. This is one of the things you've worked on. But if it was a robotics company, that you wouldn't so quickly dismiss. Supervised learning.
Speaker B: Correct.
Speaker A: That you would build a robot that was perhaps, what, like an empty shell, like dumb. And they would operate under teleoperation. So you would invest. That's just one way to do it. Invest in human supervisor, like direct human control of the robots as it's learning, and over time, add more and more automation.
Speaker B: That's correct. So let's say that's how I would build a robotics company today. If I would be building robotics company, which is spent $10 million or so recording human trajectories, controlling a robot, after.
Speaker A: You find a thing that the robot should be doing that there's a market fit for, you can make a lot of money with that product.
Speaker B: Correct? Correct. Yeah. So I would record data, and then I would essentially train supervised learning model on it. That might be the path today. Long term. I think that actually what is needed is to train powerful models over video. So you have seen maybe models that can generate images like Dali, and people are looking into models generating videos. They're like various algorithmic questions, even how to do it. And it's unclear if there is enough compute for this purpose. But I suspect that the models, which would have a level of understanding of video, same as GPT, has a level of understanding of text, could be used to train robots to solve tasks. They would have a lot of common.
Speaker A: Sense if one day, I'm pretty sure one day, there will be a robotics company. By robotics company, I mean the primary source of income is from robots that is worth over $1 trillion. What do you think that company will do?
Speaker B: I think self driving cars.
Speaker A: No, it's interesting because my mind went to personal robotics robots in the home. It seems like there's much more market opportunity there. I think it's very difficult to achieve. I mean, this might speak to something important, which is, I understand self driving much better than I understand robotics in the home. So I understand how difficult it is to actually solve self driving to a level, not just the actual computer vision and the control problem and just the basic problem, self driving, but creating a product that would undeniably be. That would cost less money. Like, it would save you a lot of money, like orders of magnitude less money that could replace Uber drivers, for example. So car sharing, that's autonomous, that creates a similar or better experience in terms of how quickly you get from a to b, or just whatever, the pleasantness of the experience, the efficiency of the experience, the value of the experience, and at the same time, the car itself costs cheaper, I think that's very difficult to achieve. I think there's a lot more low.
Speaker B: Hanging fruit in the home that could be. I also want to give you perspective on, like, how challenging it would be at home, or like, it maybe kind of depends on the exact problem that you'd be solving. Like, if we're speaking about these robotic arms and hence these things, they cost tens of thousands of dollars or maybe hundred k. And, you know, maybe, obviously, maybe there would be economy of scale, these things would be cheaper. But actually, for any household to buy it, the price would have to go down to maybe thousand bucks.
Speaker A: Yeah, I personally think that. So self driving car provides a clear service. I don't think robots in the home, there'll be a trillion dollar company will just be all about service. Meaning it will not necessarily be about, like, a robotic arm that helps you, I don't know, open a bottle or wash the dishes or any of that kind of stuff. It has to be able to take care of that whole. The therapist thing you mentioned, I think that's, of course, there's a line between what is a robot and what is not. Like, does it really need a body? But, you know, some AI system with some embodiment, I think.
Speaker B: So the tricky part when you think, actually, what's the difficult part is when the robot has, like, when there is a diversity of the environment with which the robot has to interact, that becomes hard. So on one spectrum, you have industrial robots, as they are doing over and over the same thing. It is possible to some extent, to prescribe the movements. And with very small amount of intelligence, the movement can be repeated millions of times. There are also various pieces of industrial robots where it becomes harder and harder. For instance, in case of Tesla, might be a matter of putting a rack inside of a car. And because the rock kind of moves around, it's not that easy. It's not exactly the same every time ends up being the case that you need actually humans to do it while welding cars together. It's a very repetitive process. Then in case of self driving itself, that difficulty has to do with the diversity of the environment, but still the card itself. The problem that you are solving is you try to avoid even interacting with things. You are not touching anything around, because touching itself is hard. And then if you would have in the home robot that has to touch things, and if these things, they change the shape, if there is a huge variety of things to be touched, then that's difficult. If you are speaking about the robot, which there is, you know, head that it's smiling in some way, with cameras that doesn't, you know, touch things, that's relatively simple.
Speaker A: Okay, so to both agree and to push back. So you're referring to touch, like soft robotics, like the actual touch. But I would argue that you could formulate just basic interaction between, like, non contact interaction is also a kind of touch, and that might be very difficult to solve. That's the basic. This not disagreement. But that's the basic open question to me with self driving cars and disagreement with Elon, which is how much interaction is required to solve self driving cars, how much touch is required? You said that in your intuition. Touch is not required. In my intuition, to create a product that's compelling to use, you're going to have to interact with pedestrians, not just avoid pedestrians, but interact with them. When we drive around in major cities, we're constantly threatening everybody's life with our movements, and that's how they respect us. There's a game theory going on with pedestrians, and I am afraid you can't just formulate autonomous driving as a collision avoidance problem.
Speaker B: So I think it goes beyond collision avoidance is the first order approximation. But then, at least in case of Tesla, they are gathering data from people driving their cars. And I believe that's an example of supervised learning data that they can train their models on the. And they are doing it, which can give model this another level of behavior that is needed to actually interact with the real world.
Speaker A: Yeah, it's interesting how much data is required to achieve that. What do you think of the whole Tesla autopilot approach? The computer vision based approach with multiple cameras, and there's a data engine, it's a multitask, multi headed neural network. And it's this fascinating process of similar to what you're talking about with the robotics approach, which is you deploy neural network, and then there's humans that use it, and then it runs into trouble in a bunch of places, and that stuff is sent back. So the deployment discovers a bunch of edge cases, and those edge cases are sent back for supervised annotation, thereby improving the neural network. And that's deployed again, it goes over and over until the network becomes really good at the task of driving, becomes safer and safer. What do you think of that kind of approach to robotics?
Speaker B: I believe that's the way to go. So in some sense, even when I was speaking about collecting trajectories from humans, that's like a first step, and then you deploy the system, and then you have humans revising all the issues. And in some sense, like this approach converges to system that doesn't make mistakes, because for the cases where there are mistakes, you gather data, how to fix them, and the system will keep on improving.
Speaker A: So there's a very, to me, difficult question of how hard that, you know, how long that converging takes, how hard it is. The other aspect of autonomous vehicles, this probably applies to certain robotics applications, is society, right, they put as the quality of the system converges. So one is a human factors perspective of psychology, of humans being able to supervise those, even with teleoperation, those robots. And the other is society willing to accept robots. Currently, society is much harsher on self driving cars than it is on human driven cars in terms of the expectation of safety. So the bar is set much higher than for humans. So if there's a death in an autonomous vehicle, that's seen as much more, much more dramatic than a death in a human driven vehicle. Part of the successful deployment of robots is figuring out how to make robots part of society. Both on the just the human side, on the media journalist side, and also on the policy government side. And that seems to be maybe you can put that into the objective function to optimize, but that is definitely a tricky one. And I wonder if that is actually the trickiest part for self driving cars or any system that's safety critical. It's not the algorithm, it's the society accepting it.
Speaker B: Yeah, I would say I believe that the part of the process of deployment is actually showing people that the given things can be trusted.
Speaker A: Yeah.
Speaker B: And, you know, trust is also like a glass that is actually really easy to crack it and damage it. And I think that's actually very common with innovation, that there is some resistance toward it and it's just a natural progression. So in some sense, people will have to keep on proving that indeed these systems are worth being used. And I would say I also found out that often the best way to convince people is by letting them experience it.
Speaker A: Yeah, absolutely. That's the case with Tesla Autopilot, for example. That's the case with. Yeah, with basically robots in general. It's kind of funny to hear people talk about robots. Like, there's a lot of fear, even with, like legged robots, but when they actually interact with them, there's joy. I love interacting with them. And the same with the car, with the robot. If it starts being useful, I think people immediately understand. And if the product is designed well, they fall in love. You're right.
Speaker B: It's actually even similar. When I'm thinking about copilot, the GitHub copilot, there was a spectrum of responses that people had, and ultimately the important piece was to let people try it out. And then many people just loved it, especially like programmers. Yeah, programmers. But like some of them, you know, they came with a fear.
Speaker A: Yeah.
Speaker B: But then you try it out and you think, actually that's cool. Okay. And, you know, you can try to resist the same way as you could resist moving from punch cards to, let's say, c or so, and it's a little bit futile.
Speaker A: So we talked about generation program, generation of language, even self supervised learning in the visual space for robotics and then reinforcement learning. What to you in like, this whole beautiful spectrum of AI, do you think is a good benchmark, a good test to strive for, to achieve intelligence? That's a strong test of intelligence. You know, it started with Alan Turing and the Turing test. Maybe you think natural language conversation is a good test.
Speaker B: So, you know, it would be nice if, for instance, machine would be able to solve Riemann hypothesis in math, that would be. I think that would be very impressive.
Speaker A: So theorem proving is that to you, proving theorems is a good. Oh, like, one thing that the machine did, you would say, damn it.
Speaker B: Exactly.
Speaker A: Okay.
Speaker B: That would be quite impressive. I mean, the tricky part about the benchmarks is, you know, as we are getting closer with them, we have to invent new benchmarks. There is actually no ultimate benchmark out there.
Speaker A: Yeah, see, my thought with the Riemann hypothesis would be the moment the machine proves it, we would say, okay, well, then the problem was easy.
Speaker B: That's what happens. And I mean, in some sense, that's actually what happens over the years in AI, that, like, we get used to things very quickly.
Speaker A: You know something, I talked to Rodney Brooks. I don't know if you know that is he called alphazero homework problem, because he was saying, like, there's nothing special about it. It's not a big leap. And I didn't. Well, he's coming from, one of the aspects that we referred to is he was part of the founding of iRobot, which deployed now tens of millions of robot in the home. So if you see robots that are actually in the homes of people as the legitimate instantiation of artificial intelligence, then, yes, maybe an AI that plays a silly game like going chess is not a real accomplishment, but to me, it's a fundamental leap. But I think we as humans then say, okay, well, then that game of chess or go wasn't that difficult compared to the thing that's currently unsolved.
Speaker B: So my intuition is that from perspective of the evolution of these AI systems, we'll at first see the tremendous progress in digital space. And the main thing about digital space is also that there is a lot of recorded data, plus you can very rapidly deploy things to billions of people, while in case of physical space, the deployment part takes multiple years. You have to manufacture things, and delivering it to actual people is very hard. So I'm expecting that the first. And the prices in digital space of goods, they would go down to, let's say, marginal cost to zero.
Speaker A: And also the question is how much of our life will be in digital? Because it seems like we're heading towards more and more of our lives being in the digital space. So, like, innovation in the physical space might become less and less significant. Like, why do you need to drive anywhere if most of your life is spent in virtual reality?
Speaker B: I still would like to, at least at the moment. My impression is that I would like to have a physical contact with other people, and that's very important. To me, we don't have a way to replicate it in the computer. It might be the case that over the time it will change, like in.
Speaker A: Ten years from now. Why not have like an arbitrary infinite number of people you can interact with? Some of them are real, some are not, with arbitrary characteristics that you can define based on your own preferences?
Speaker B: I think that's maybe where we are heading and maybe I'm resisting the future.
Speaker A: Yeah. I'm telling you, if I got to choose, if I could live in elder scrolls Skyrim versus the real world, I'm not so sure I would stay with the real world.
Speaker B: Yeah. I mean, the question is, will VR be sufficient to get us there, or do you need to, you know, plug electrodes in the brain? And it would be nice if these electrodes wouldn't be invasive.
Speaker A: Yeah. Or at least like provably non destructive. But in the digital space, do you think we'll be able to solve the touring test, the spirit of the touring test, which is, do you think we'll be able to achieve compelling natural language conversation between people, like have friends that are AI systems on the Internet?
Speaker B: I totally think it's doable.
Speaker A: Do you think the current approach to.
Speaker B: GPT will take us there? So there is the amount there, the part of at first learning all the content out there, and I think that steel system should keep on learning as it speaks with you. Yeah, and I think that should work. The question is how exactly to do it. And, you know, obviously we have people at open air asking these questions and kind of at first pre training on all existing content is like a backbone and is a decent backbone.
Speaker A: Do you think AI needs a body connecting to our robotics question to truly connect with humans? Or can most of the connection be in the digital space?
Speaker B: So let's see. We know that there are people who met each other online and they felt in love.
Speaker A: Yeah.
Speaker B: So it seems that it's conceivable to establish connection, which is purely through Internet. Of course, it might be more compelling the more modalities you add.
Speaker A: So it would be like you're proposing like a tinder, but for AI, you like, swipe right and left and half the systems are AI and the other is humans, and you don't know which is which.
Speaker B: That would be our formulation of Turing test.
Speaker A: The moment AI is able to achieve more, swipe right or left, whatever, the moment it's able to be more attractive than other humans, it passes the Turing test.
Speaker B: Then you would pass the Turing test in attractiveness.
Speaker A: That's right. Well, no, like attractiveness just to clarify, conversation, not just visual. Right, right. It's also attractiveness with wit and humor and whatever. Whatever makes conversations pleasant for humans. Okay. All right, so you're saying it's possible to achieve in the digital space in some sense?
Speaker B: I would almost ask that question, why wouldn't that be possible?
Speaker A: Right. Well, I have this argument with my dad all the time. He thinks that touch and smell are.
Speaker B: Really important, so they can be very important. And I'm saying the initial systems, they won't have it. Still, I wouldn't. There are people being born without these senses, and, you know, I believe that they can still fall in love and have meaningful life.
Speaker A: Yeah. I wonder if it's possible to go close to all the way by just training on transcripts of conversations. Like, I wonder how far that takes us.
Speaker B: So I think that actually still, you want images like I would like. So I don't have kids, but, like, I could imagine having AI tutorial. It has to see, you know, kids drawing some pictures on the paper and.
Speaker A: Also facial expressions and all that kind of stuff. We use dogs and humans use their eyes and to communicate with each other. I think that's a really powerful mechanism of communication. Body language too, that words are much lower bandwidth.
Speaker B: And for body language, we still, we kind of have a system that displays an image of its artificial expression on the computer. It doesn't have to move mechanical pieces or so. So I think that there is like, kind of a progression. You can imagine that text might be the simplest to tackle, but this is not a complete human experience at all. You expand it to, let's say, images, both for input and output. And what you describe is actually the final, I guess, frontier. What makes us human, the fact that we can touch each other or smell or so. And it's the hardest from perspective of data and deployment. And I believe that these things might happen gradually.
Speaker A: Are you excited by that possibility, this particular application of human to AI, friendship and interaction?
Speaker B: So let's see, like, would you, do.
Speaker A: You look forward to a world, you said you're living with a few folks and you're very close friends with them. Do you look forward to a day where one or two of those friends are AI systems?
Speaker B: So if the system would be truly wishing me well, rather than being in the situation that it optimizes for my time to interact with the system, the.
Speaker A: Line between those is, it's a gray. It's a gray area.
Speaker B: I think that's the distinction between love and possession. And these things, they might be often correlated for humans but it's like you, you might find that there are like some friends with whom you haven't spoke for months.
Speaker A: Yeah.
Speaker B: And then, you know, you pick up the phone, it's as the time hasn't passed, they are not holding to you. And I will. I wouldn't like to have AI system that, you know, it's, it's trying to convince me to spend time with it. I would like the system to optimize for what I care about and help me in achieving my own goals.
Speaker A: But there's some, I mean, I don't know, there's some manipulation, there's some possessiveness, there's some insecurities, there's fragility. All those things are necessary to form a close friendship over time, to go through some dark shit together, some bliss and happiness together. I feel like there's a lot of greedy, self centered behavior within that process.
Speaker B: My intuition, but I might be wrong, is that human computer interaction doesn't have to go through computer being greedy, possessive and so on. It is possible to train systems, maybe, that they actually, you know, they are I guess, prompted or fine tuned or so to truly optimize for what you care about. And you could imagine that, you know, the way, how the process would look like is at some point we as a humans, we look at the transcript of the conversation or like an entire interaction and we say, actually here there was more loving way to go about it. And we supervise system toward being more loving. Or maybe we train the system such that it has a reward function toward being more loving.
Speaker A: Yeah. Or maybe the possibility of the system being an asshole and manipulative and possessive every once in a while is a feature, not a bug. Because some of the happiness that we experience when two souls meet each other, when two humans meet each other, is a kind of break from the assholes in the world. And so you need assholes in AI as well because, like, it'll be like a breath of fresh air to discover an AI that the three previous AI's.
Speaker B: You had are too friendly.
Speaker A: No. Or cruel or whatever. It's like some kind of mix. And then this one is just right. But you need to experience the full spectrum. Like, I think you need to be able to engineer assholes. So let's see, because there's some level to us being appreciated. To appreciate the human experience, we need the dark and the light.
Speaker B: So that kind of reminds me, I met a while ago at the meditation retreat one woman, and, you know, beautiful, beautiful woman. And she had, she had a crutch. Okay. She had the trouble walking on one leg. I asked her what has happened? And she said that five years ago she was in Maui, Hawaii, and she was eating a salad and some snail fell into the salad. And apparently there are neurotoxic snails over there. And she got into coma for a year. And apparently there is high chance of even just dying. But she was in the comma. At some point, she regained partially consciousness. She was able to hear people in the room, people behave as she wouldn't be there. At some point, she started being able to speak, but she was mumbling, barely able to express herself. At some point, she got into wheelchair. Then at some point, she actually noticed that she can move her toe. And then she knew that she will be able to walk. And then, you know, that's where she was five years after. And she said that since then, she appreciates the fact that she can move her toe. And I was thinking, hmm, do I need to go through such experience to appreciate that I have. I can move my toe?
Speaker A: Wow, that's really good story. A really deep example. Yeah.
Speaker B: And in some sense, it might be the case that we don't see light if we haven't went through the darkness, but I wouldn't say that we should.
Speaker A: We shouldn't assume that that's the case. We may be able to engineer shortcuts.
Speaker B: Yeah. Ilya had this belief that maybe one has to go for a week or six months to some challenging camp to just experience, you know, a lot of difficulties, and then comes back and actually everything is bright, everything is beautiful.
Speaker A: I'm with Ilya on this. It must be a russian thing. Where are you from originally?
Speaker B: I'm polish.
Speaker A: Polish, okay. I'm tempted to say that explains a lot, but, yeah, there's something about the Russian. The necessity of suffering. I believe. I believe suffering, or rather, struggle is necessary.
Speaker B: I believe that struggle is necessary. I mean, in some sense, you even look at the story of any superhero, that movie. It's not that. It was like everything. Like, it goes easy, easy, easy, easy.
Speaker A: I like how that's your ground. Truth is the story of superheroes. Okay, you mentioned that you used to do research at night and go to bed at like, 06:00 a.m. or 07:00 a.m. i still do that often. What sleep schedules have you tried to make for a productive and happy life? Like, is there. Is there some interesting, wild sleeping patterns that you engage that you found that works really well for you?
Speaker B: I tried at some point, decreasing number of hours of sleep, like a gradually like, half an hour every few days, less. You know, I was hoping to just save time. That clearly didn't work for me. Like, at some point, there's, like, a phase shift, and I felt tired all the time. There was a time that I used to work during the nights. The nice thing about the nights is that no one disturbs you. And even I remember when I was meeting for the first time with Greg Brockman, his CTO and chairman of OpenAI. Our meeting was scheduled to 05:00 p.m. and I overstepped for the meeting. Mm hmm.
Speaker A: Overslept for the meeting.
Speaker B: Yeah.
Speaker A: 05:00 p.m. yeah. Now you sound like me. That's hilarious. Okay. Yeah.
Speaker B: And at the moment, in some sense, my sleeping schedule also has to do with the fact that I'm interacting with people. I sleep without an alarm.
Speaker A: So. So, yeah, the. The team thing, you mentioned extrovert thing. Because most humans operate during a certain set of hours. You're forced to then operate at the same set of hours. But I'm not quite there yet. I found a lot of joy, just like you said, working through the night because it's quiet, because the world doesn't disturb you. And there's some aspect counter to everything you're saying. There's some joyful aspect to sleeping through the mess of the day, because people are having meetings and sending emails and there's drama. Meetings. I can sleep through all the meetings.
Speaker B: You know, I have meetings every day, and they prevent me from having sufficient amount of time for focused work. And then I modified my calendar, and I said that I'm out of office Wednesday, Thursday, and Friday every day, and I'm having meetings only Monday and Tuesday. And that vastly, positively influenced my mood that I have. Literally, I get three days for fully focused work.
Speaker A: Yeah. So there's better solutions to this problem than staying awake all night. Okay. You've been part of development of some of the greatest ideas in artificial intelligence. What would you say is your process for developing good, novel ideas?
Speaker B: You have to be aware that clearly there are many other brilliant people around. So you have to ask yourself a question. Why they give an idea, let's say, wasn't tried by someone else. And in some sense, it has to do with kind of simple. It might sound simple, but like I thinking outside of the box. And what do I mean here? So, for instance, for a while, people in academia, they assumed that you have a fixed data set, and then you optimize the algorithms in order to get the best performance. And that was so in great assumption that no one thought about training models on anti Internet or like that. Maybe some people thought about it, but it felt to many as unfair. And in some sense that's almost like a, it's not my idea or so, but that's an example of breaking a typical assumption. So you want to be in the paradigm that you are breaking a typical.
Speaker A: Assumption in the context of the AI community getting to pick your data set as cheating.
Speaker B: Correct. And in some sense, so that was assumption that many people had out there. And then if you free yourself from assumptions, then they are likely to achieve something that others cannot do. And in some sense, if you are trying to do exactly the same things as others, it's very likely that you're going to have the same results.
Speaker A: Yeah, but there's also that kind of tension which is asking yourself the question, why haven't others done this? Because I mean, I get a lot of good ideas, but I think probably most of them suck when they meet reality.
Speaker B: So actually I think the other big piece is getting into habit of generating ideas, training your brain towards generating ideas, and not even suspending judgment of the ideas. So in some sense I noticed myself that even if I'm in the process of generating ideas, if I tell myself, oh, that was a bad idea, then that actually interrupts the process and I cannot generate more ideas because I'm actually focused on the negative part why it won't work. Yes, but I created also environment in the way that it's very easy for me to store new ideas. So for instance, next to my bed I have a voice recorder and it happens to me often. Like I wake up in the, during the night and I have some idea in the past I was writing them down on my phone, but that means, you know, turning off this, turning on the screen and that wakes me up, or like pulling a paper which requires, you know, turning on the light. These days I just start recording it.
Speaker A: What do you think? I don't know if you know who Jim Keller is.
Speaker B: I know Jim Keller.
Speaker A: He's a big proponent of thinking hard on a problem right before sleep, so that he can sleep through it and solve it in his sleep, or like come up with radical stuff in his sleep. He was trying to get me to do this.
Speaker B: So it happened from my experience perspective, it happened to me many times during the high school days when I was doing mathematics that I had the solution to math problem. As I woke up at the moment regarding thinking hard about the given problem is I'm trying to actually devote substantial amount of time to think about important problems, not just before the sleep, like I'm organizing amount of the huge chunks of time such that I'm not constantly working on the urgent problems, but I actually have time to think about the important one.
Speaker A: So you do it naturally. But his idea is that you kind of prime your brain to make sure that that's the focus. You know, oftentimes people have other worries in their life. That's nothing fundamentally deep problems like, I don't know, just stupid drama in your life. And even at work, all that kind of stuff. He wants to kind of pick the most important problem that you're thinking about and go to bed on that.
Speaker B: I think that's wise. I mean, the other thing that comes to my mind is also I feel the most fresh in the morning. So during the morning I try to work on the most important things rather than just being pulled by urgent things or checking email or.
Speaker A: So what do you do with the cause? I've been doing the voice recorder thing too, but I end up recording so many messages, it's hard to organize.
Speaker B: I have the same problem now. I have heard that Google Pixel is really good in transcribing text. And I might get a Google Pixel just for the sake of transcribing text.
Speaker A: Yeah. People listening to this. If you have a good voice recorder suggestion that transcribes, please let me know. Some of it is this has to do with OpenAI codecs too. Like, some of it is simply like the friction. I need apps that remove that friction between voice and the organization of the resulting transcripts and all that kind of stuff. But yes, you're right. Absolutely. During, for me, it's walking sleep too. But walking and running, especially running, get a lot of thoughts during running. And there's no good mechanism for recording thoughts.
Speaker B: So one more thing that I do, I have a separate phone which has no apps. Maybe it has like audible or let's say Kindle. No one has this phone number. This kind of my meditation phone.
Speaker A: Yeah.
Speaker B: And I try to expand the amount of time that's the phone that I'm having. It has also Google Maps if I need to go somewhere. And I also use this phone to write down ideas.
Speaker A: Ah, that's a really good idea. That's a really good idea.
Speaker B: Often, actually, what I end up doing is even sending a message from that phone to the other phone. So that's actually my way of recording messages. Or I just put them into notes.
Speaker A: I love it. What advice would you give to a young person, high school, college, about how to be successful. You've done a lot of incredible things in the past decade. So maybe. Maybe you have some.
Speaker B: Something. There might be something.
Speaker A: There might be something.
Speaker B: I mean, might sound like a simplistic or so, but I would say, literally, just follow your passion. Double down on it. And if you don't know what's your passion, just figure out what could be a. What could be a passion. So this, that might be an exploration. When I was in elementary school was math and chemistry, and I remember for some time I gave up on math because my school teacher, she told me that I'm dumb. And I guess maybe an advice would be, just ignore people. If they tell you that you're dumb, you're dumb.
Speaker A: You mentioned something offline about chemistry and explosive. What was that about?
Speaker B: So let's see. So a story goes like that. I can. I got into chemistry, maybe I was like a second grade of my elementary school. Third grade, I started going to chemistry classes. I really love building stuff. And I did all the experiments that they describe in the book, how to create oxygen with vinegar and baking soda or so. Okay, so I did all the experiments, and at some point I was. So what's next? What can I do? And explosives, they also. It's like you have a clear reward signal, you know, if the thing worked or not. So I remember at first, I got interested in producing hydrogen. That was kind of funny experiment from school. You can just burn it. And then I moved to nitroglycerin, so that's also relatively easy to synthesize. I started producing essential dynamite. And that one, I think it would be friend. I remember there was a. No, there was at first, like, maybe two attempts that I went with a friend to detonate what we built, and it didn't work out. And, like a third time, he was like, ah, it won't work. Like, let's don't waste time. And we were. I was carrying this, this, you know, that tube with dynamite, I don't know, pound or so dynamite in my backpack. We're, like, riding on the bike to the edges of the city.
Speaker A: Yeah. And attempt number three. This would be attempt number three.
Speaker B: Attempt number three. And now we. We dig a hole to put it inside. It actually had the, you know, electrical detonator. We draw a cable behind the tree. I even. I never had. I haven't ever seen, like, an explosion before. So I thought that there will be a lot of sound and. But, you know, we're, like, laying down and I'm holding the cable and the battery. At some point, you know, it kind of like a three to one. And I just connected it and it felt like at the ground shaked. It was like a. More like a sound. And then the soil started kind of lifting up and started falling on us.
Speaker A: Yeah. Wow.
Speaker B: And then now the friend said, let's make sure next time we have helmets. But also, you know, kind of. I'm happy that nothing happened to me. It could have been the case that I lost the limb or so.
Speaker A: Yeah. But that's childhood of an engineering mind with a strong reward signal of an explosion. I love it. There's some aspect of chemists, the chemist I know, like my dad, with plasma chemistry, plasma physics, he was very much into explosives, too. It's a worrying quality of people that work in chemistry that they love. I think it is. That exactly is the strong signal that the thing worked.
Speaker B: There is no doubt.
Speaker A: There's no doubt there's some magic. It's almost like a reminder that physics works, that chemistry works. It's cool. It's almost like a little glimpse at nature that you yourself engineer. That's why I really like artificial intelligence, especially robotics, is you create a little piece of nature.
Speaker B: And in some sense, even for me with explosives, the motivation was creation rather than distraction.
Speaker A: Yes, exactly. In terms of advice, I forgot to ask about just machine learning and deep learning. For people who are specifically interested in machine learning, how would you recommend they get into the field?
Speaker B: So I would say re implement everything. And also there is plenty of courses, so, like, from scratch, so on different levels of abstraction in some sense, but I would say re implement something from scratch, re implement something from a paper, re implement something, you know, from podcasts that you have heard about. I would say that's a powerful way to understand things. So it's often the case that you read the description and you think you understand, but you truly understand once you build it, then you actually know what really mattered in the description.
Speaker A: Is there particular topics that you find people just fall in love with? I've seen. I tend to really enjoy reinforcement learning because it's much easier to get to a point where you feel like you created something special, like fun games kind.
Speaker B: Of things that are rewarding.
Speaker A: It's rewarding, yeah. As opposed to, like, re implementing from scratch, more like supervised learning kind of things. It's. Yeah.
Speaker B: So, you know, if. If someone would optimize for things to be rewarding, then it feels that the things that are somewhat generative, they have such a property. So, yes, you have, for instance, adversarial networks, or you have just even generated language models. And you can even see internally, we have seen this thing with our releases. So we released recently two models. There is one model called Dali that generates images. And there is other model called clip, that actually you provide various possibilities. What could be the answer to what is on the picture? And it can tell you which one is the most likely. And in some sense, in case of the first one, Dali, it is very easy for you to understand that actually there is magic going on. And in case of the second one, even though it is insanely powerful, and, you know, people from vision community, as they started probing it inside, they actually understood how far it goes. It's difficult for person at first to see how well it works. And that's the same as you said, that in case of supervised learning models, you might not kind of see, or it's not that easy for you to understand the strength, even though you don't.
Speaker A: Believe in magic, to see the magic.
Speaker B: To see the magic.
Speaker A: It's a generative that's really brilliant. So anything that's generative, because then you are at the core of the creation. You get to experience creation without much effort, unless you have to do it from scratch. But.
Speaker B: And it feels that, you know, humans are wired. There is some level of reward for creating stuff.
Speaker A: Yeah.
Speaker B: Like, of course, different people have a different weight on this reward.
Speaker A: Yeah. In the big objective function.
Speaker B: In the big objective function of a person.
Speaker A: Of a person. You wrote that beautiful is what you intensely pay attention to. Even a cockroach is beautiful if you look very closely. Can you expand on this? What is beauty?
Speaker B: So what I wrote here actually corresponds to my subjective experience that I had through extended periods of meditation. It's pretty crazy that at some point, the meditation gets you to the place that you have really increased focus, increased attention. And then you look at the very simple objects that were all the time around. You can look at the table or on a pen or at that nature, and you notice more and more details, and it becomes very pleasant to look at it. And it, once again, it kind of reminds me my childhood, like, just pure joyous of being. It's also. I have seen even the reverse effect that by default, regardless of what we possess, we very quickly get used to it. And, you know, you can have a very beautiful house, and if you don't put sufficient effort, you're just gonna get used to it. And it doesn't bring any more joy regardless of what you have.
Speaker A: Yeah, well, I actually. I find that material possessions get in the way of that experience of pure joy. So I've always. I've been very fortunate to just find joy in simple things. Just. Just like you're saying. Just like, I don't know, objects in my life. Just stupid objects like this cup like thing, you know, just objects sounds okay, I'm not being eloquent, but literally, objects in the world, they're just full of joy. Because it's like, I can't believe, one, I can't believe that I'm fortunate enough to be alive to experience these objects. And then two, I can't believe humans are clever enough to have built these objects. The hierarchy of pleasure that that provides is infinite.
Speaker B: I mean, even if you look at the cup of water. So, you know, you see first, like, a level of, like a reflection of light. But then you think, no, man. There's like a trillions upon trillions of particles bouncing against each other. There is also the tension on the surface that if the back could, like, stand on it and move around. And you think it also has this, like a magical property that as you decrease temperature, it actually expands in volume, which allows for the legs to freeze on the surface and at the bottom to have actually not freeze, which allows for life like crazy. You look in detail at some object, and you think, actually, this table, that was just the figment of someone's imagination at some point. And then there was thousands of people involved to actually manufacture it and put it here. And by default, no one cares.
Speaker A: And then you can start thinking about evolution, how it all started from single cell organisms that led to this table.
Speaker B: And these thoughts, they give me life appreciation.
Speaker A: Yeah, exactly.
Speaker B: And even lack of thoughts, just the pure raw signal also gives the life appreciation.
Speaker A: See, the thing is. And then that's coupled, for me, with the sadness that the whole ride ends. And perhaps is deeply coupled in that the fact that this experience, this moment ends gives it. Gives it an intensity that I'm not sure I would otherwise have. So in that same way, I try to meditate on my own death often. Do you think about your mortality? Are you afraid of death?
Speaker B: So fear of death is like one of the most fundamental fears that each of us has. We might be not even aware of it. It requires to look inside, to even recognize that it's out there. There is still, let's say, this property of nature, that if things would last forever, then they would be also boring to us. The fact that the things change in some way gives any meaning to them. I also found out that it seems to be very healing to people to have these short experiences like I guess psychedelic experiences in which they experience death of self, in which they let go of this fear. And then I maybe can even increase the appreciation of the moment. It seems that many people, they, they, they can easily comprehend fine the fact that the money is finite, while they don't see that time is finite. I have this, like, a discussion with Ilya from time to time. He's saying, you know, man, like, the life will pass. Very passed. At some point I will be 40, 50, 60, 70, and then it's over. This is true. Which also makes me believe that, you know, that every single moment, it is so unique that should be appreciated. And it also makes me think that I should be acting on my life, because otherwise it will pass. I also like this framework of thinking from Jeff Bezos on regret minimization, that, like, I would like, if I will be at that deathbed, to look back on my life and. And not regret that I haven't done something. It's usually you might regret that you haven't tried. I'm fine with failing, but I haven't tried.
Speaker A: What's the nietzsche? Eternal recurrence. Try to live a life that if you had to live it infinitely many times, that would be the. You'd be okay with that kind of life. So try to live it optimally.
Speaker B: I can say that it's almost, like, unbelievable to me where I am in my life. I'm extremely grateful for, actually, people whom I met. I would say, I think that I'm decently smart and so on. But I think that actually, to a great extent, where I am has to do with the people who I met.
Speaker A: Would you be okay if, after this conversation, you died?
Speaker B: So if I'm dead, then it kind of. I don't have a choice anymore. So in some sense, there's, like, a. Plenty of things that I would like to try out in my life. I feel that, you know, I'm gradually going one by one, and I'm just doing them. I think that the list will be always infinite.
Speaker A: Yeah. So might as well go today.
Speaker B: Yeah. I mean, to be clear, I'm not looking forward to die. I would say, if there is no choice, I would accept it. But, like, in some sense, I'm. If there would be a choice, if there would be possibility to leave, I would fight for living.
Speaker A: I find it's more honest and real to think about, you know, dying today. At the end of the day, that seems to me, at least to my brain, more honest slap in the face, as opposed to, I still have ten years, like, today, then I'm much more about appreciating the cup and the table and so on and less about, like, silly worldly accomplishments and all those kinds of things.
Speaker B: We have in the company. A person who say at some point found out that they have cancer. And that also gives, you know, huge perspective with respect to what matters now.
Speaker A: Yeah.
Speaker B: And, you know, often people in situations like that, they conclude that actually what matters is human connection and love, and.
Speaker A: That'S people conclude, also, if you have kids, it's kids as family. You, I think, tweeted, we don't assign the minus infinity reward to our death. Such a reward would prevent us from taking any risk. We wouldn't be able to cross the road in fear of being hit by a car. So in the objective function you mentioned, fear of death might be fundamental to the human condition.
Speaker B: So, as I said, let's assume that there are, like, reward functions in our brain, and I. And the interesting thing is even realization how different reward functions can play with your behavior. As a matter of fact, I wouldn't say that you should assign infinite negative reward to anything because that messes up the math.
Speaker A: The math doesn't work out.
Speaker B: It doesn't work out. And as you said, even government or some insurance companies said they assign $9 million to human life.
Speaker A: Yeah.
Speaker B: And I'm just saying it with respect to. That might be a hard statement to ourselves, but in some sense that there is a finite value of our own life. I'm trying to put it from perspective of being less, of being more ego less, and realizing fragility of my own life. And in some sense, the fear of death might prevent you from acting, because anything can cause death.
Speaker A: Yeah. And I'm sure, actually, if you were to put death in the objective function, there's probably so many aspects to death and fear of death and realization of death and mortality. There's just whole components of finiteness of not just your life, but every experience and so on that you're going to have to formalize mathematically.
Speaker B: And also, you know, that might lead to you spending a lot of compute cycles on this, like, deliberating this terrible future instead of experiencing now. And in some sense, it's also kind of unpleasant simulation to run in your head.
Speaker A: Yeah. Do you. Do you think there's an objective function that describes the entirety of human life? So, you know, usually the way you ask that is, what is the meaning of life? Is there a universal objective functions that captures the why of life?
Speaker B: So, yeah, I mean, I suspect that they will ask this question, but it's also a question that I asked myself many, many times. See, I can tell you a framework that I have these days to think about these questions. So I think that fundamentally, meaning of life has to do with some of our reward functions that we have in brain. And they might have to do with, let's say, for instance, curiosity or human connection, which might mean understanding others. It's also possible for a person to slightly modify their reward function. Usually they mostly stay fixed, but it's possible to modify a reward function. And you can pretty much choose. So in some sense, the reward functions, optimizing reward functions, they will give you life satisfaction.
Speaker A: Is there some randomness in the function?
Speaker B: I think when you are born, there is some randomness. Like, you can see that some people, for instance, they care more about building stuff. Some people care more about caring for others. Some people, there are all sorts of default reward functions. And then in some sense, you can ask yourself, what is the satisfying way for you to go after this reward function? And you just go after this reward function? And some people also ask, are these reward functions real? I almost think about it as, let's say, if you would have to discover mathematics. In mathematics, you are likely to run into various objects, like a complex numbers or differentiation. Some other objects, and these are very natural objects that arise. And similarly, the reward functions that we are having in our brain, they are somewhat very natural that there is a reward function for understanding, like a comprehension, curiosity and so on. And so in some sense, they are in same way natural as they're natural objects in mathematics.
Speaker A: Interesting. So, you know, there's the old sort of debate. Is mathematics invented or discovered? You're saying reward functions are discovered. So nature.
Speaker B: So nature provided some, you can still, let's say, expand it throughout the life. Some of the reward functions, they might be futile. Like, for instance, there might be a reward function. Maximize amount of wealth.
Speaker A: Yeah.
Speaker B: And this is more like a learning reward function. But we know also that some reward functions, if you optimize them, you won't be quite satisfied.
Speaker A: Well, I don't know which part of your reward function resulted in you coming today, but I am deeply appreciative that you did spend your valuable time with me. Wojciech is really fun talking to you. You're brilliant. You're a good human being. An honor to meet you and an honor to talk to you. Thanks for talking today, brother.
Speaker B: Thank you, Lex, a lot. I appreciate your questions. Curiosity. I had a lot of time being here.
Speaker A: Thanks for listening to this conversation with Wojcieg. Zaremba to support this podcast. Please check out our sponsors in the description. And now let me leave you with some words from Arthur C. Clarke, who is the author of 2001 A Space Odyssey. It may be that our role on this planet is not to worship God, but to create him. Thank you for listening and I hope to see you next time.
