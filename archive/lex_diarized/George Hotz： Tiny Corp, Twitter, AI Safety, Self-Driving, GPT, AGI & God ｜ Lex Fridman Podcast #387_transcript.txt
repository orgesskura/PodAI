Transcription for George Hotz： Tiny Corp, Twitter, AI Safety, Self-Driving, GPT, AGI & God ｜ Lex Fridman Podcast #387.mp3:
Full transcript: What possible ideas do you have for how human species ends? Sure. So I think the most obvious way to me is wireheading. We end up amusing ourselves to death. We end up all staring at that infinite TikTok and forgetting to eat. Maybe it's even more benign than this. Maybe we all just stop reproducing. Now, to be fair, it's probably hard to get all of humanity. Yeah. The interesting thing about humanity is the diversity in it. Oh yeah, organisms in general. There's a lot of weirdos out there. Two of them are sitting here. I mean diversity in humanity is we do respect. I wish I was more weird. The following is a conversation with George Hotz, his third time on this podcast. He's the founder of Comma AI that seeks to solve autonomous driving and is the founder of a new company called Tiny Corp that created Tinygrad, a neural network framework that is extremely simple with the goal of making it run on any device by any human, easily and efficiently. As you know, George also did a large number of fun and amazing things, from hacking the iPhone to recently joining Twitter for a bit as an intern in quotes making the case for refactoring the Twitter codebase. In general, hes a fascinating engineer and human being. And one of my favorite people to talk to. This is Alex Friedman podcast to support it. Please check out our sponsors in the description. And now dear friends, heres George Hotts. You mentioned something in a stream about the philosophical nature of time. So lets start with a wild question. Do you think time is an illusion? You know, I sell phone calls to comma for $1,000 and some guy called me and like, you know it's a $1,000, you can talk to me for half an hour. And he's like yeah, okay, so like time doesn't exist. And I really wanted to share this with you. I'm like, what do you mean time doesn't exist? Right? Like I think time is a useful model whether it exists or not, right? Like does quantum physics exist? Well it doesn't matter to its about whether its a useful model to describe reality. Is time maybe compressive? Do you think there is an objective reality or is everything just useful models like underneath it all? Is there an actual thing that were constructing models for. I dont know. I was hoping you would know. I dont think it matters. I mean this kind of connects to the models of constructive reality with machine learning, right? Sure. Like is it just nice to have useful approximations of the world such that we can do something with it? So there are things that are real. Column graph complexity is real. Yeah. The compressive math is real. Yeah. This should be a t shirt. And I think hard things are actually hard. I don't think P equals np. Ooh, strong words. Well, I think that's the majority. I do think factoring is in P. But I don't think you're the person that falls the majority in all walks of life, so it's good for that one. I do, yeah. In theoretical computer science, you're one of the sheep, all right. But to you, time is a useful model. Sure. What were you talking about on the stream with time? Are you made of time? If I remembered half the things I said on stream. Someday someone's going to make a model of all of it, and it's going to come back to haunt me. Someday soon. Yeah, probably. Would that be exciting to you or sad that there's a George Hots model? I mean, the question is, when the George Hotts model is better than George Hotts, like, I am declining and the. Model is growing, what is the metric by which you measure better or worse in that? If you're competing with yourself, maybe you. Can just play a game where you have the George Hotts answer and the George Hotts model answer and ask which. People prefer people close to you or strangers? Either one. It will hurt more when it's people close to me, but both will be overtaken by the George Hotts model. It'd be quite painful, right? Loved ones, family members would rather have the model over for Thanksgiving than you. Yeah. Or, like significant others, would rather sexed with the large language model version of. You, especially when it's fine tuned to their preferences. Yeah, well, that's what we're doing in a relationship, right? We're just fine tuning ourselves, but we're inefficient with it because we're selfish. Ingredients on our language. Models can fine tune more efficiently, more selflessly. There's a Star Trek Voyager episode where Catherine Janeway, lost in the Delta Quadrant, makes herself a lover on the holodeck, and the lover falls asleep on her arm, and he snores a little bit, and Janeway edits the program to remove that. And then, of course, the realization is, wait, this person's terrible. It is, actually all their nuances and quirks and slight annoyances that make this relationship worthwhile. But I don't think we're gonna realize that until it's too late. Well, I think a large language model could incorporate the. The flaws and the quirks and all that kind of stuff. Just the perfect amount of quirks and flaws to make you charming without crossing the line. Yeah, yeah. And that's probably a good, like, approximation of the. Like, the percent of time the language model should be cranky or an asshole or jealous or all this kind of stuff. And of course it can and it will, but all that difficulty at that point is artificial. There's no more real difficulty. Okay, what's the difference between real and artificial? Artificial difficulty is difficulty that's like, constructed or could be turned off with a knob. Real difficulty is like you're in the woods and you gotta survive. So if something can not be turned off with a knob, it's real? Yeah, I think so. Or, I mean, you can't get out of this by smashing the knob with a hammer. I mean, maybe you kind of can, you know, into the wild when, you know, Alexander, super tramp, he wants to explore something that's never been explored before, but it's the nineties. Everything's been explored. So he's like, well, I'm just not going to bring a map. Yeah, I mean, no, you're not exploring. You should have brought a map. Dude, you died. There was a bridge a mile from where you were camping. How does that connect to the metaphor of the knob? By nothing. Bringing the map. You didn't become an explorer, you just smashed the thing. Yeah, yeah. The difficulty is still artificial. You failed before you started. What if we just don't have access to the knob? Well, that maybe is even scarier. Right? Like, we already exist in a world of nature, and nature has been fine tuned over billions of years. To have humans build something and then throw the knob away in some grand romantic gesture is horrifying. Do you think of us humans as individuals that are, like, born and die, or are we just all part of one living organism? That is earth, that is nature? I don't think there's a clear line there. I think it's all kind of just fuzzy. I don't know. I mean, I don't think I'm conscious of. I don't think I'm anything. I think I'm just a computer program. So it's all computation. I think running in your head is just computation. Everything running in the universe is computation, I think. I believe the extended church Turing thesis. Yeah, but there seems to be an embodiment to your particular computation. Like there's a consistency. Well, yeah, but I mean, models have consistency, too. Yeah. Models that have been RLHF'd will continually say, you know, like, well, how do I murder ethnic minorities. Oh, well, I can't let you do that, Hal. There's a consistency to that behavior. It's all RLHF. Like we RLHF each other. We. We find, we provide human feedback and there that thereby fine tune these little pockets of computation. But it's still unclear why that pocket of computation stays with you, like, for years. It just kind of falls like you have this consistent set of physics, biology, what, like whatever you call the neurons firing, the electrical signals, the mechanical signals, all of that that seems to stay there, and it contains information, it stores information, and that information permeates through time and stays with you. There's like, memory, it's like, sticky. Okay, to be fair, like, a lot of the models we're building today are very even. RLHF is nowhere near as complex as. The human loss reinforcement learning with human feedback. When I talked about will GPT twelve be AGI? My answer is no, of course not. I mean, cross entropy loss is never going to get you there. You need probably RL in fancy environments in order to get something that would be considered AGI, like. So to ask the question about why. I don't know, it's just some quirk of evolution, right? I don't think there's anything particularly special about where I ended up. Where humans ended up. It's okay, we have human level intelligence. Would you call that AGI? Whatever we have gi. Look, actually, I don't really even like the word AGI, but general intelligence is defined to be whatever humans have. Okay, so why can GPT twelve not get us to AGI? Can we just, like, linger on that? If your loss function is categorical, cross entropy. If your loss function is just try to maximize compression. I have a soundcloud. I rap, and I tried to get chat GPT to help me write raps, and the raps that it wrote sounded like YouTube comment raps. You can go on any rap beat online and you can see what people put in the comments, and it's the most mid quality rap you can find. Is made good or bad? Made is bad. It's like mid. Every time I talk to you, I learn new words. Mid? Yeah, I was like, is it like basic? Is that what mid means? Kind of. It's like middle of the curve, right? So there's like, there's like that intelligence curve, and you have like, the dumb guy, the smart guy, and then the mid guy. Actually being the mid guy's the worst. The smart guy is like, I put all my money in bitcoin. The mid guy is like, you can't put money in bitcoin. It's not real money. And all of it is a genius meme. That's another interesting one, memes. The humor, the idea, the absurdity encapsulated in a single image, and it just kind of propagates virally between all of our brains. I didn't get much sleep last night, so I'm very. I sound like I'm high. I swear I'm not. Do you think we have ideas or ideas have us? I think that we're going to get super scary memes once the AI's actually are superhuman. Do you think AI will generate memes? Of course. You think it'll make humans laugh? I think it's worse than that. So, infinite jest, it's introduced in the first 50 pages, is about a tape that you, once you watch it once, you only ever want to watch that tape. In fact, you want to watch the tape so much that someone says, okay, here's a hacksaw. Cut off your pinky and then I'll let you watch the tape again and you'll do it. So we're actually going to build that, I think. But it's not going to be one static tape. I think the human brain is too complex to be stuck in one static tape like that. If you look at, like, ant brains, maybe they can be stuck on a static tape, but we're going to build that using generative models. We're going to build the tick tock that you actually can't look away from. So TikTok is already pretty close there. But the generation is done by humans. The algorithm is just doing their recommendation. But if it's. If the algorithm was also able to do the generation well, it's a question. About how much intelligence is behind it. Right? So the content is being generated by, let's say, one humanity worth of intelligence. And you can quantify a humanity, right. That's a, you know, it's. It's exaflops, yada flops, but you can quantify it. Once that generation is being done by 100 humanities, you're done. So it's actually scale that's the problem. But also speed. Yeah. And what if it's sort of manipulating the very limited human dopamine engine for porn? Imagine just TikTok, but for porn. Yeah. It's like a brave new world. I don't even know what it'll look like. Right. Like, again, you can't imagine the behaviors of something smarter than you. But a super intelligent, an agent that just dominates your intelligence so much will be able to completely manipulate you. Is it possible that it won't really manipulate, it'll just move past us. It'll just kind of exist. The way water exists or the air exists. You see, and that's the whole AI safety thing. It's not the machine that's going to do that. It's other humans using the machine that are going to do that to you. Yeah, because the machine is not interested in hurting humans. It's just the machine is a machine. Yeah, but the human gets the machine, and there's a lot of humans out there very interested in manipulating you. Well, let me bring up Eliezer Jacowski, who recently sat where you're sitting. He thinks that AI will almost surely kill everyone. Do you agree with him or nothing? Yes, but maybe for a different reason. Okay, and then I'll try to get you to find hope where we could find a no to that answer. But why? Yes. Okay. Why didn't nuclear weapons kill everyone? That's a good question. I think there's an answer. I think it's actually very hard to deploy nuclear weapons tactically. It's very hard to accomplish tactical objectives. Great, I can nuke their country. I have an irradiated pile of rubble. I don't want that. Why not? Why don't I want an irradiated pile of rubble? For all the reasons no one wants an irradiated pile of rubble? Because you can't use that land for resources. You can't populate the land. Yeah. What you want, a total victory in a war, is not usually the irradiation and eradication of the people there, it's the subjugation and domination of the people. Okay, so you can't use this strategically, tactically, in a war to help you, to help gain a military advantage. It's all complete destruction. All right? But there's egos involved. It's still surprising. Still surprising that nobody pressed the big red button. It's somewhat surprising. But you see, it's the little red button that's going to be pressed with AI. That's going to. And that's why we die. It's not because the AI. If there's anything in the nature of AI, it's just the nature of humanity. What's the algorithm behind the little red button? What possible ideas do you have for how human species ends? Sure. So I think the most obvious way to me is wireheading. We end up amusing ourselves to death. We end up all staring at that infinite TikTok. And forgetting to eat. Maybe. Maybe it's even more benign than this. Maybe we all just stop reproducing. Now, to be fair, it's probably hard to get all of humanity. Yeah. Yeah. It probably. Like the. The interesting thing about humanity is the diversity in it. Oh, yeah. Organisms in general. There's a lot of weirdos out there. Two of them are sitting here. I mean, diversity in humanity is. With due respect, I wish I was more weird. No, like, I'm kind of. Look, I'm drinking smart water, man. That's like a Coca Cola product, right? Do you want corporate, George Hoss? No. The amount of diversity in humanity, I think, is decreasing, just like all the other biodiversity on the planet. Oh, boy. Yeah, right. Social media is not helping, huh? Go eat McDonald's in China. Yeah. Yeah. No, it's the interconnectedness that's doing it. Oh, that's interesting. So everybody starts relying on the connectivity of the Internet, and over time, that reduces the diversity, the intellectual diversity, and then that gets everybody into a funnel. There's still going to be a guy in Texas. There is. And. Yeah. Bunker, to be fair, do I think AI kills us all? I think AI kills everything we call, like, society today. I do not think it actually kills the human species. I think that's actually incredibly hard to do. Yeah, but society, like, if we start over, that's tricky. Most of us don't know how to do most things. Yeah, but some of us do, and they'll be okay and they'll rebuild after they, uh. Great. AI. What's rebuilding look like, how far, like, how much do we lose? Like, what has human civilization done that's interesting? Combustion engine. Electricity. So power and energy. That's interesting. Like how to harness energy. Well, they're going to be religiously against that. Are they going to get back to, like, fire? Sure. I mean, there'll be a. There'll be, it'll be like, you know, some kind of amish looking kind of thing. I think. I think they're going to have very strong taboos against technology. Like, technology is almost like a new religion. Technology is the devil and nature is God. Sure. So closer to nature. But can you really get away from AI? If it destroyed 99% of the human species, isn't it somehow have a hold, like a stronghold? What's interesting about everything we build? I think we're going to build super intelligence before we build any sort of robustness in the AI. We cannot build an AI that is capable of going out into nature and surviving. Like a bird. Right? A bird is an incredibly robust organism. We've built nothing like this. We haven't built a machine that's capable of reproducing. Yes, but there's, you know, I work with leg robots a lot now. I have a bunch of them. They're mobile, they can't reproduce, but all they need is, I guess you're saying they can't repair themselves, but if you have a large number, if you have like 100 million of them, let's just. Focus on them reproducing. Right. They have microchips in them. Okay, then do they include a fab? No. Then how are they going to reproduce? It doesn't have to be all on board. Right. They can go to a factory, to a repair shop. Yeah, but then you're really moving away from robustness. Yes. All of life is capable of reproducing without needing to go to a repair shop. Life will continue to reproduce in the complete absence of civilization, robots will not. So when the, if the AI apocalypse happens, I mean, the AI's are going to probably die out because I think we're going to get, again, super intelligence long before we get robustness. What about if you just improve the fab to where you just have a 3d printer that can always help you. Well, that'd be very interesting. I'm interested in building that. Of course you are. You think, how difficult is that problem to have a robot that basically can. Build itself very, very hard. I think you've mentioned this like to me, or somewhere where people think it's. Easy conceptually, and then they remember that you're going to have to have a fab. Yeah. On board, of course. So 3d printer. That prints a 3d printer? Yeah, yeah. On legs. Why is that hard? Well, because it's not. I mean, a 3d printer is a very simple machine. Right. Okay. You're going to print chips, you're going to have an atomic printer. How are you going to dope the silicon? Yeah, right. How are you going to etch the silicon? You're gonna have to have a very interesting kind of fab if you want to have a lot of computation on board. But you can do like structural type of robots that are dumb. Yeah, but structural type of robots aren't gonna have the intelligence required to survive in any complex environment. What about like ants type of systems? We have like trillions of them. I don't think this works. I mean, again, like ants at their very core are made up of cells that are capable of individually reproducing. They're doing quite a lot. A lot of computation that we're taking for granted. It's not even just the computation, it's that reproduction is so inherent. Okay, so, like, there's two stacks of life in the world. There's the biological stack and the silicon stack. The biological stack starts with reproduction. Reproduction is at the absolute core, the first proto rna organisms we're capable of reproducing. The silicon stack, despite, as far as it's come, is nowhere near being able to reproduce. Yeah. So the fab movement, digital fabrication, fabrication in the full range of what that means is still in the early stages. Yeah. You're interested in this world even if. You did put a fab on the machine, right? Let's say, okay, we can build fabs. We know how to do that as humanity. We can probably put all the precursors that build all the machines and the fabs also in the machine. So first off, this machine is going to be absolutely massive. I mean, we almost have a, like, think of the size of the thing required to reproduce a machine today, right? Like, is our civilization capable of reproduction? Can we reproduce our civilization on Mars. If we were to construct a machine that is made up of humans? Like a company that can reproduce itself? Yeah, I don't know. It feels like 115 people. I think it's so much harder than that. 120. I just look at our number. I believe that Twitter can be run by 50 people. I think that this is going to take most of, like, it's just most of society, right? Like, we live in one globalized world. No, but you're not interested in running Twitter. You're interested in seeding. Like, you want to see the civilization, and then. Because humans can, like, oh, okay. You're talking about. Yeah, okay, so you're talking about the humans reproducing and, like, basically, like, what's the smallest self sustaining colony of humans? Yeah, yeah, okay, fine. But they're not going to be making five nanometer chips over time, they will. I think you're being like, we have to expand our conception of time here, going back to the original time scale. I mean, over, across maybe 100 generations, we're back to making chips. No, if you seed the colony correctly. Maybe, or maybe they'll watch our colony die out over here and be like, we're not making chips. Don't make chips. No, but you have to seed that colony correctly. Whatever you do, don't make chips. Chips are what led to their downfall. Well, that is the thing that humans do. They. They come up, they construct a devil, a good thing and a bad thing, and they really stick by that, and they murder each other over that. There's always one asshole in the room who murders everybody. And he usually makes tattoos and nice branding. Do you need that asshole? That's the question, right? Humanity works really hard today to get rid of that asshole. But I think they might be important. Yeah, this whole freedom of speech thing, it's the freedom of being an asshole seems kind of important, man. This thing, this fab, this human fab that we constructed, this human civilization, is pretty interesting. And now it's building artificial copies of itself, or artificial copies of various aspects of itself that seem interesting, like intelligence. And I wonder where that goes. I like to think it's just like another stack for life, like we have, like the bio stack life, like we're a bio stack life, and then the silicon stack life. But it seems like the ceiling, or there might not be a ceiling, or at least the ceiling is much higher for the silicon stack. Oh, no. We don't know what the ceiling is for the bio stack either. The bio stack. The bio stack just seemed to move slower. You have Moore's law, which is not dead, despite many proclamations. In the bio stack or the silicon stuff? In the silicon stack. And you don't have anything like this in the bio stack. So I have a meme that I posted. I tried to make a meme. It didn't work too well, but I posted a picture of Ronald Reagan and Joe Biden. And you look, this is 1980, and this is 2020, and these two humans are basically, like, the same. Right? There's no. There's no like. There. There's been no change in humans in the last 40 years. Yeah. And then I posted a computer from 1980 and a computer from 2020. Wow. Yeah. With their early, early stages. Right, which is why you said when you said the fab, the size of the fab required to make another fab is like, uh, very large right now. Oh, yeah. But computers were very large 80 years ago, and they got pretty tiny, and people are starting to want to wear them on their face in order to escape reality. That's the thing. In order to be live inside the computer there. Put a screen right here. I don't have to see the rest of you assholes. I've been ready for a long time. You like virtual reality? I love it. Do you want to live there? Yeah. Yeah. Part of me does too. How far away are we, do you think? Judging from what you can buy today, far, very far. I gotta tell you that I had the experience of Meta's codec avatar, where it's an ultra high resolution scan. It looked real. I mean, the headsets just are not quite at, like, eye resolution yet. I haven't put on any headset where I'm like, oh, this could be the real world. Whereas when I put good headphones on, audio is there, like, we. We can reproduce audio that I'm like, I'm actually in a jungle right now. If I close my eyes, I can't tell. I'm nothing. Yeah, but then there's also smell and all that kind of stuff. Sure. I don't know. I. The power of imagination or the power of the mechanism in the human mind that fills the gaps, that kind of reaches and wants to make the thing you see in the virtual world real to you. I believe in that power. Or humans want to believe. Yeah. Like, what if you're lonely? What if you're sad? What if you're really struggling in life and here's a world where you don't have to struggle anymore. Humans want to believe so much that people think the large language models are conscious. That's how much humans want to believe. Strong words. He's throwing left and right hooks. Why do you think large language models are not conscious? I don't think I'm conscious. Oh. So what is consciousness, then, George Hodz? It's, like, what it seems to mean to people. It's just, like, a word that atheists use for souls. Sure, but that doesn't mean soul is not an interesting word. If consciousness is a spectrum, I'm definitely way more conscious than the large language models are. I think the large language models are less conscious than a chicken. When was the last time you've seen a chicken? In Miami? Like, a couple months ago. No, like a living chicken. Living chickens walking around Miami. It's crazy. Like, on the street. Yeah, like a chicken. A chicken, yeah. All right, all right. I was trying to call you out like a good journalist, and I got shut down. Okay. But you don't think much about this kind of subjective feeling that it feels like something to exist. And then as an observer, you can have a sense that an entity is not only intelligent, but has a kind of subjective experience of its reality, like a self awareness that is capable of suffering, of hurting, of being excited by the environment in a way that's not merely kind of an artificial response, but a deeply felt one. Humans want to believe so much that if I took a rock and a sharpie and drew a sad face on the rock, they'd think the rock is sad. Yeah, and you're saying when we look in the mirror, we, we apply the same smiley face with rock. Pretty much, yeah. Doesn't it, isn't that weird, though, that you're not conscious? Is that. No. But you do believe in consciousness? Not really. It's just, it's unclear. Okay. So to you, it's like a little, like a symptom of the bigger thing. That's not that important. Yeah. It's interesting that, like, human systems seem to claim that they're conscious. And I guess it kind of, like, says something in a straight up, like, okay, what do people mean when even if you don't believe in consciousness, what do people mean when they say consciousness? And there's definitely, like, meanings to it? What's your favorite thing to eat? Pizza. Cheese pizza. What are the toppings? I like cheese pizza. Don't say pineapple. No, I don't like pineapple. Okay. Pepperoni pizza as they put any ham on it. Oh, that's real bad. What's the best, what's the best pizza? What are we talking about here? Like, you like cheap, crappy pizza? Chicago deep dish cheese pizza. Oh, that's, that's my favorite. There you go. You bite into a deep dish chicago deep dish pizza, and it feels like you were starving. You haven't eaten for 24 hours. You just bite in and you're hanging out with somebody that matters a lot to you, and you're there with the pizza. Sounds real nice, huh? Yeah. All right. It feels like something. I'm George motherfucking hots eating a fucking Chicago deep dish pizza. There's just the full peak living experience of being human, the top of the human condition. Sure. It feels like something to experience that. Why does it feel like something? That's consciousness, isn't it? If that's the word you want to use to describe it, sure. I'm not going to deny that that feeling exists. I'm not going to deny that I experienced that feeling when I guess what I kind of take issue to is that there's some, like, like, how does it feel to be a web server? Do 404s hurt? Not yet. How would you know what suffering looked like? Sure. You can recognize a suffering dog because we're the same stack as the dog. All the bio stack stuff, kind of, especially mammals. You know, it's really easy. You can game recognizes game. Yeah. Versus the silicon stack stuff. It's like, you have no idea. You have you? Wow. The little thing has learned to mimic, you know? But then I realized that that's all we are, too oh, look, the little thing has learned to mimic. Yeah, I guess. Yeah. 404 could be. Could be suffering, but it's so far from our kind of living organism, our kind of stack. But it feels like AI can start maybe mimicking the biological stack better and better. Better because it's trained, retrained it. Yeah. And so in that maybe that's the definition of consciousness, is the biostat consciousness. The definition of consciousness is how close something looks to human. Sure. I'll give you that one. No, how close something is to the human experience. Sure. It's a very. It's a very anthropocentric definition, but. Well, that's all we got. Sure. No, and I don't mean to, like, I think there's a lot of value in it. Look, I just started my second company. My third company will be AI girlfriends. Oh. Like, I mean, I want to find. Out what your fourth company is after. Oh, wow. Because I think once you have AI girlfriends, it's, oh, boy, does it get interesting. Well, maybe. Let's go there. I mean, the relationships with AI that's creating human like organisms. Right. And part of being human is being conscious is being. Having the capacity to suffer, having the capacity to experience this life richly in such a way that you can empathize, the AI system can empathize with you, and you can empathize with it, or you can project your anthropomorphic sense of what the other entity is experiencing. And an AI model would need to create that experience inside your mind. And it doesn't seem that difficult. Yeah, but, okay, so here's where it actually gets totally different. Right. When you interact with another human, you can make some assumptions. Yeah. When you interact with these models, you can't. You can make some assumptions that. That other human experiences suffering and pleasure in a pretty similar way to you do. The golden rule applies with an AI model. This isn't really true. Right. These large language models are good at fooling people because they were trained on a whole bunch of human data and told to mimic it. Yeah, but if the AI system says, hi, my name is Samantha, it has a backstory. I went to college here and there. Yeah. Maybe it'll integrate this in the AI system. I made some chatbots. I give him backstories. It was lots of fun. I was so happy when Lama came out. Yeah, well, we'll talk about llama. We'll talk about all that. But, like, you know, the rock with the smiley face? Yeah. It seems pretty natural for you to anthropomorphize that thing and then start dating it, and before you know it, you're married and have kids. With a rock. With a rock. There's pictures on Instagram with you and a rock and smiley face. To be fair, like, you know, something that people generally look for when they're looking for someone to date is intelligence in some form. And the rock doesn't really have intelligence. Only a pretty desperate person would date a rock. I think we're all desperate deep down. Oh, not rock level desperate. All right. Not rock level desperate, but AI level desperate. I don't know. I think all of us have a deep loneliness. It just feels like the language models are there. Oh, I agree. And you know what? I won't even say this so cynically. I will actually say this in a way that, like, I want AI friends. I do. Yeah. Like, I would love to. You know, again, the language models now are still a little, like, people are impressed with these GPT things. And I look at, like. Or, like. Or the copilot, the coding one, and I'm like, okay, this is, like, junior engineer level, and these people are, like, fiverr level artists and copywriters. Like, okay, great, we got, like, fiverr and, like, junior engineers. Okay, cool. Like, and this is just the start, and it will get better, right? Like, I can't wait to have AI friends who are more intelligent than I am. So fiverr is just a temporary. It's not the ceiling. No, definitely not. Is it. Is it count as cheating when you're talking to an AI model? Emotional cheating? That's. That's up to you and your human partner to define. Oh, you have to. All right. You can. Yeah, you have to have that. Have to have that conversation, I guess. All right. I mean, integrate that with porn and all this. No, I mean, it's similar kind of porn. Yeah. Yeah. I think people in relationships have different views on that. Yeah. But most people don't have, like, serious, open conversations about all the different aspects of what's cool and what's not. And it feels like AI is a really weird conversation to have. The porn one is a good branching off point, like, these things. You know, one of my scenarios that I put in my chat bot is a, you know, a nice girl named Lexi. She's 20. She just moved out to LA. She wanted to be an actress, but she started doing onlyfans instead. And you're on a date with her. Enjoy. Oh, man. Yeah. And so is that if you're actually dating somebody in real life, is that cheating? I feel like it gets a little weird. Sure. It gets real weird. I. It's like, what are you allowed to say to an AI bot? Imagine having that conversation with a significant other. I mean, these are all things for people to define in their relationships. What it means to be human is. Just going to start to get weird, especially online. Like, how do you know there'll be moments when you'll have what you think is a real human you interacted with on Twitter for years and you realize it's not. I spread. I love this meme. Heaven Banning. You know what? Shadow Banning. Yeah, shadow Banning. Okay. You post, no one can see it. Heaven Banning. You post, no one can see it. But a whole lot of AI's are spun up to interact with you. Well, maybe that's the way human civilization ends, is all of us haven't banned. There's a great. It's called my little friendship is optimal. It's a Sci-Fi story that explores this idea. Friendship is optimal. Friendship is optimal. Yeah. I'd like to have some, at least on the intellectual realm, some AI friends that argue with me. But the romantic realm is weird. Definitely weird. But not out of the realm of the. The kind of weirdness that human civilization is capable of. I think I want it. Look, I want it. If no one else wants it, I want it. Yeah, I think a lot of people probably want it. There's a deep loneliness, and I'll fill. Their loneliness and, you know, it just will only advertise to you some of the time. Yeah. Maybe the conceptions of monogamy change, too. Like, I grew up in a time, like, I value monogamy. But maybe that's a silly notion when you have arbitrary number of AI systems. This interesting path from rationality to polyamory. Yeah, that doesn't make sense for me. For you. But you're just a biological organism who was born before the Internet really took off. The crazy thing is, culture is whatever we define it as, these things are not usually is ought problem in moral philosophy. There's no. What is might be that computers are capable of mimicking girlfriends perfectly. They passed the girlfriend Turing test. But that doesn't say anything about ought. That doesn't say anything about how we ought to respond to them as a civilization. That doesn't say we ought to get rid of monogamy. That's a completely separate question. Really? A religious one. Girlfriend touring test. I wonder what that looks like. Girlfriend touring test. Are you writing that? Will you be the Alan Turing of the 21st century that writes the girlfriend Turing test? No, I mean, of course, my AI girlfriends, their goal is to pass the girlfriend Turing test. No, but there should be, like, a paper that kind of defines the test. I mean, the question is if it's deeply personalized or there's a common thing that really gets everybody. Yeah, I mean, you know, look, we're a company. We don't have to get everybody. We just have to get a large enough clientele to stay. I like how you already thinking company. All right, let's. Before we go to company number three and company number four, let's go to company number two. Tiny Corp. Possibly one of the greatest names of all time for a company. You've launched a new company called Tiny Corp that leads the development of tiny grad. What's the origin story of Tiny Corp and tiny grad? I started tiny grad as a toy project just to teach myself. Okay, what is a convolution? What are all these options you can pass to them? What is the derivative of a convolution? Very similar to. Carpathi wrote micrograd. Very similar. And then I started thinking about AI chips. I started thinking about chips that run AI, and I was like, well, okay, this is going to be a really big problem if Nvidia becomes a monopoly here, how long before Nvidia is nationalized? So you. One of the reasons to start tiny Corp is to challenge Nvidia. It's not so much to challenge Nvidia, actually, I like Nvidia, and it's to make sure power stays decentralized. Yeah. And here it's computational power. And to you, Nvidia is kind of locking down the computational power of the world. If Nvidia becomes just, like, ten x, better than everything else, you're giving a big advantage to somebody who can secure Nvidia as a resource. Yeah. In fact, if Jensen watches this podcast, he may want to consider this. He may want to consider making sure his company is not nationalized. You think that's an actual threat? Oh, yes. No. But there's so much, you know, there's AMD. So we have Nvidia and AMD. Great. All right, but you don't think there's, like, a push towards, like, selling, like, Google selling TPU's or something like this? You don't think there's a push for that? Have you seen it? Google loves to rent utpus. It doesn't. You can't buy it at best buy? No. So I started work on a chip. I was like, okay, what's it going to take to make a chip? And my first notions were all completely wrong about why, about how you could improve on GPU's. And I will take this. This is from Jim Keller on your podcast. And this is one of my absolute favorite descriptions of computation. So there's three kinds of computation paradigms that are common in the world today. There's CPu's, and CPu's can do everything. CPU's can do add and multiply, they can do load and store, and they can do compare and branch. And when I say they can do these things, they can do them all fast, right? So compare and branch are unique to CPU's. And what I mean by they can do them fast is they can do things like branch prediction and speculative execution. And they spend tons of transistors and these like super deep reorder buffers in order to make these things fast. Then you have a simpler computation model. GPU's. GPU's can't really do compare and branch. I mean they can, but it's horrendously slow. But GPU's can do arbitrary load and store. GPU's can do things like x dereference Y. So they can fetch from arbitrary pieces of memory. They can fetch from memory that is defined by the contents of the data. The third model of computation is dsps. And dsps are just add and multiply. They can do load in stores, but only static load in stores, only loads in stores that are known before the program runs. And you look at neural networks today, and 95% of neural networks are all the DSP paradigm. They are just statically scheduled ads and multiplies. So Tinyguard really took this idea, and I'm still working on it, to extend this as far as possible. Every stage of the stack has turn completeness, Python has turn completeness. And then we take Python, we go into C, which is Turing complete, and maybe C calls into some cuda kernels, which are turing complete. The Cuda kernels go through LVM, which is turing complete into PTX, which turn complete into SAS, which is turn complete on a Turing complete processor. I want to get Turing completeness out of the stack entirely, because once you get rid of Turing completeness, you can reason about things. Rice's theorem and the halting problem do not apply to admiral machines. Okay, what's the power and the value of getting Turing completeness out of? Are we talking about the hardware or the software? Every layer of the stack. Every layer? Every layer of the stack. Removing Turing completeness allows you to reason about things, right? So the reason you need to do branch prediction in a cpu and the reason it's prediction and the branch predictors are, I think they're like 99% on cpu's. Why do they get 1% of them wrong? Well, they get 1% wrong because you can't know, right? That's the halting problem. It's equivalent to the halting problem to say whether a branch is going to be taken or not. I can show that. But the Admiral machine, the neural network, runs the identical compute every time. The only thing that changes is the data. So when you realize this, you think about, okay, how can we build a computer and how can we build a stack? That takes maximal advantage of this idea. So what makes tinygred different from other neural network libraries is it does not have a primitive operator even for matrix multiplication. And this is every single one. They even have primitive operators for things like convolutions. So no matmul? No matmul. Well, here's what a map model is. So I'll use my hands to talk here. So if you think about a cube, and I put my two matrices that I'm multiplying on two faces of the cube, right, you can think about the matrix multiply as, okay, the n cubed. I'm going to multiply for each one in the cubed and then I'm going to do a sum which is a reduce up to here to the third face of the cube. And that's your multiplied matrix. So what a matrix multiply is, is a bunch of shape operations, a bunch of permutes, reshapes and expands on the two matrices. A multiply n cubed, a reduce n cubed, which gives you an n squared matrix. Okay, so what is the minimum number of operations that can accomplish that if you don't have matmol as a primitive? So tiny grad has about 20. And you can compare tiny grad's opset or IR to things like XLA or primtorchen. XLA and Primtorch are ideas where like, okay, torch has like 2000 different kernels. Pytorch 2.0 introduced prim torch, which has only 250. Tinygrad has order of magnitude 25. It's ten x less than XLA or prim torch. And you can think about it as kind of like RIsC versus Cisc. These other things are cisc like systems. Tiny grat is risk and risk. One risk architecture is going to change everything. 1995 hackers. Wait, really? That's an actual thing? Angelina Jolie delivers the line risk architecture is going to change everything in 1995. And here we are with arm in the phones and arm everywhere. Wow. I love it when movies actually have real things in them. Right. Okay, interesting. And so this is like, so you're thinking of this as the risk architecture of ML Stack 25. Can you go through the four op types? Sure. Okay, so you have unary ops, which take in a tensor and return a tensor of the same size and do some unary opt to it x log reciprocal sine. They take in one and they're point wise. Relu. Yeah, Relu. Almost all activation functions are unary ops. Some combinations of unary ops together is still a unary op. Then you have binary ops. Binary ops are like point wise, addition, multiplication, division, compare. It takes in two tensors of equal size and outputs one tensor. Then you have reduce ops. Reduceops will take a three dimensional tensor and turn it into a two dimensional tensor. I, or three dimensional tensor turned into zero dimensional tensor. Things like a sum or max are really the common ones there. And then the fourth type is movement ops. And movement ops are different from the other types because they don't actually require computation. They require different ways to look at memory. So that includes reshapes, permutes, expands, flips. Those are the main ones, probably. So with that, you have enough to make a map model and convolutions. And every convolution you can imagine, dilated convolutions, striated convolutions, transposed convolutions. You're right on GitHub about laziness, showing a map mall matrix multiplication. See how despite the style, it is fused into one kernel with the power of laziness. Can you elaborate on this power of laziness? Sure. So if you type in Pytorch A times b plus circumental, what this is going to do is it's going to first multiply, add and b a and b, and store that result into memory. And then it is going to add c by reading that result from memory, reading c from memory and writing that out to memory. There is way more loads and stores to memory than you need there if you don't actually do a times b as soon as you see it. If you wait until the user actually realizes that tensor, until the laziness actually resolves, you confuse that plus circumental. This is like, it's the same way Haskell works. So what's the process of porting a model into Tinygrad? So, Tinygrad's front end looks very similar to Pytorch. I probably could make a perfect or pretty close to perfect interop layer if I really wanted to. I think that there's some things that are nicer about Tinygrad syntax than Pytorch, but the front end looks very torch like. You can also load in onnx models. We have more onyx tests passing than core Mljdev. Okay, so we'll pass onyx runtime soon. What about, like, the developer experience with tiny grad, what it feels like versus Pytorch? By the way, I really like Pytorch. I think that it's actually a very good piece of software. I think that they've made a few different trade offs, and these different trade offs are where tiny grad takes a different path. One of the biggest differences is it's really easy to see the kernels that are actually being sent to the GPU. If you run Pytorch on the GPU, you do some operation, and you don't know what kernels ran. You don't know how many kernels ran. You don't know how many flops were used. You don't know how much memory accesses were used. Tinygrad type debug two, and it will show you in this beautiful style, every kernel that's run how many flops and how many bytes. So can you just linger on what problem Tinygrad solves? Tinygrad solves the problem of porting new ML accelerators quickly. One of the reasons tons of these companies now, I think Sequoia marked graph core to zero, Cerebus, tens torrent, Grok, all of these ML accelerator companies, they built chips. The chips were good, the software was terrible. And part of the reason is because I think the same problem is happening with Dojo. It's really, really hard to write a Pytorch port because you have to write 250 kernels and you have to tune them all for performance. What does Jim Jim Color think about tiny grad? You guys hung on quite a bit. So he was involved. He's involved with chin Storrent. What's his praise and what's his criticism of what you're doing with your life? Look, my prediction for tens torrent is that they're going to pivot to making RISC V chips. Cpu's. Cpu's, yeah. Why? Because AI accelerators are a software problem, not really a hardware problem. Oh, interesting. So you don't think, you think the diversity of AI accelerators in the hardware space is not going to be a thing that exists long term? I think what's going to happen is if I can finish. Okay. If you're trying to make an AI accelerator, you better have the capability of writing a torch level performance stack on Nvidia GPU's. If you can't write a torch stack on Nvidia GPU's, and I mean all the way, I mean down to the driver, there's no way you're going to be able to write it on your chip because your chip's worse than an Nvidia GPU. The first version of the chip you tape out, it's definitely worse. Oh, you're saying writing that stack is really tough. Yes. And not only that, actually, the chip that you tape out, almost always because you're trying to get advantage over Nvidia, you're specializing the hardware more. It's always harder to write software for more specialized hardware. Like a GPU is pretty generic. And if you can't write an Nvidia stack, there's no way you can write a stack for your chip. So my approach with Tinygrad is, first, write a performant Nvidia stack. We're targeting AMD. So you did say fu to Nvidia. A little bit. Would love. With love. Yeah. So, like the Yankees. You know, I'm a Mets fan. Oh, you're, you're, you're a Mets fan. A risk. A risk fan and a Mets fan. What's the hope that AMD has? You did a build with AMD recently that I saw? Uh, how does the, uh, the, the 7900 XTX compared to the RTX 4090 or 4080? Well, let's start with the fact that the 7900 XTX kernel drivers don't work. And if you run demo apps in loops, it panics the kernel. Okay, so this is a software issue. Lisa sue responded to my email. Oh, I reached out. I was like, this is, you know, really like, I understand if your seven by seven transposed Winograd Conv is slower than Nvidia's, but literally, when I run demo apps in a loop, the kernel panics. So just adding that loop. Yeah, I just literally took their demo apps and wrote, like, while true semicolon do the app done in a bunch of screens, this is the most primitive fuzz testing. Why do you think that is? Theyre just not seeing a market in machine learning. Theyre changing. Theyre trying to change. Theyre trying to change. And I had a pretty positive interaction with them this week. Last week, I went on YouTube. I was just like, thats it. I give up on AMD. This is their driver. I'll go with Intel GPu's. Intel GPU's have better drivers. So you're kind of spearheading the diversification of GPU's. Yeah, and I'd like to extend that diversification to everything. I'd like to diversify. The more my central thesis about the world is there's things that centralize power and they're bad and there's things that decentralize power and they're good. Everything I can do to help decentralize power, I'd like to do. So you're really worried about the centralization of Nvidia. That's interesting. And you don't have a fundamental hope for the proliferation of asics except in the cloud. I'd like to help them with software. No, actually the only ASIc that is remotely successful is Google's TPU. And the only reason that's successful is because Google wrote a machine learning framework. I think that you have to write a competitive machine learning framework in order to be able to build an ASIC. You think meta with Pytorch builds a competitor? I hope so. They have one. They have an internal one. Internal, I mean, public facing with a nice cloud interface and so on. I don't want a cloud. You don't like cloud? I don't like cloud. What do you think is the fundamental limitation of cloud? Fundamental limitation to cloud is who owns the off switch. So it's power to the people. Yeah. And you don't like the man to have all the power. Exactly. All right. And right now the only way to do that is with Nvidia, GPU's if you want performance and stability. Interesting. It's a costly investment emotionally to go with AMDSE. Well, let me add sort of on a tangent to ask you. You've built quite a few PCs. What's your advice on how to build a good custom PC for, let's say for the different applications that you use for gaming, for machine learning? Well, you shouldn't build one. You should buy a box from the tiny corp. I heard rumors, whispers about this box in the tiny corp. What's this thing look like? What is it, what is it called? It's called the tiny box. Tiny box. It's $15,000 and it's almost a pay to flop of compute. It's over 100gb of GPU Ram. It's over five terabytes per second of GPU memory bandwidth. I'm going to put like four nvmes in raid. You're going to get 20 30gb/second of drive read bandwidth. I'm going to build the best deep learning box that I can. That plugs into one wall outlet. Okay, can you go through those specs again a little bit from memory? Yeah. So it's almost a pay to flop of compute. So AMD intel today I'm leaning toward AMD, but we're pretty agnostic to the type of compute. The main limiting spec is a 120 volts, 15 amp circuitous. Okay, well, I mean it, because in order to, like, there's a plug over there, right. You have to be able to plug it in. We're also going to sell the tiny rack, which, like, what's the most power you can get into your house without arousing suspicion? And one of the answers is an electric car charger. Wait, where does the rack go? Your garage. Interesting. The car charger, a wall outlet is about 1500 watts. A car charger is about 10,000 watts. What is the most amount of power you can get your hands on without arousing suspicion? That's right. George Hotz. Okay, so the tiny box and you said nvmes and raid. I forget what you said about memory, all that kind of stuff. Okay, what about what GPU's again? Probably, probably 7900 xtxs, but maybe 3090s, maybe a 770s. Those are intels you're flexible or still exploring. I'm still exploring. I want to deliver a really good experience to people. And yeah, what GPU's I end up going with. Again, I'm leaning toward AMD. We'll see. In my email, what I said to AMD is just dumping the code on GitHub is not open source. Open source is a culture. Open source means that your issues are not all one year old, stale issues. Open source means developing in public. And if you guys can commit to that, I see a real future for AMD as a competitor to Nvidia. Well, I'd love to get a tiny box to MIT. So whenever it's ready. Will do. Let's do it. We're taking pre orders. I took this from Elon. I'm like, all right, $100, fully refundable pre orders. Is it going to be like the Cybertruck is going to take a few years or. No, I'll try to do it faster than that. It's a lot simpler. It's a lot simpler than a truck. Well, there's complexities. Not to just the putting the thing together, but like shipping and all this kind of stuff. The thing that I want to deliver to people out of the box is being able to run 65 billion parameter llama in FP 16 in real time, in like a good, like ten tokens per second or five tokens per second or something just. It works. Llama's running or something like llama experience. Yeah. Or I think Falcon is the new one. Experience a chat with the largest language model that you can have in your house. Yeah. From a wall plug. From a wall plug, yeah. Actually, for inference. It's not like even more power would help you get more. Even more power would get you more. Well, no, there's just the biggest model released is 65 billion parameter llama, as far as I know. So it sounds like tiny box will naturally pivot towards company number three because you could just get the girlfriend and I. I mean, or boyfriend. That one's harder, actually. The boyfriend is harder. Boyfriend's harder. Yeah. I think that's a very biased statement. I think a lot of people just. What, why is it harder to replace a boyfriend than other girlfriend with the artificial LLM? Because women are attracted to status and power and men are attracted to youth and beauty. No, I mean, this is what I mean. Both are. Could be unmimicable, easy through the language model. No, no. Machines do not have any status or real power. I don't know. I think you both. Well, first of all, you're using language mostly to communicate youth and beauty and power and status. But status fundamentally is a zero sum game, whereas youth and beauty are not. No, I think status is a narrative you can construct. I don't think status is real. I don't know. I just think that that's why it's harder, you know? Yeah, maybe it is my biases. I think status is way easier to fake. I also think that, you know, men are probably more desperate and more likely to buy my product, so maybe they're a better target. Market desperation is interesting, easier to fool. I can see that. Yeah. Look, I mean, look, I know you can look at porn viewership numbers, right? A lot more men watch porn than women. You can ask why that is. Wow, there's a lot of questions and answers. You can get there. Anyway, with the, with the tiny box. How many gpu's in tiny box? Six. Oh, man. And I'll tell you why it's six. Yeah. So AMD Epyc processors have 128 lanes of PCIe. I want to leave enough lanes for some drives, and I want to leave enough lanes for some networking. How do you do cooling for something like this? Ah, that's one of the big challenges. Not only do I want the cooling to be good, I want it to be quiet. I want the tiny box to be able to sit comfortably in your room. Right. This is really going towards the girlfriend thing, because you want to run the LLM. I'll give, I'll give a more. I mean, I can talk about how it relates to company number one, comma aihdem. Well, but yes, quiet. Oh, quiet because you maybe potentially want to run in a car. No, no. Quiet because you want to put this thing in your house and you want it to coexist with you. If it's screaming at 60 decibels, you don't want that in your house, you'll kick it out. 60 decibels? Yeah, I want, like, 40, 45. So how do you make the cooling quiet? That's an interesting problem in itself. A key trick is to actually make it big. Ironically, it's called the tiny box. Yeah, but if I can make it big, a lot of that noise is generated because of high pressure air. If you look at, like, a one U server, a one U server has these super high pressure fans. They're, like, super deep, and they're like jet engines versus. If you have something that's big, well, I can use a big thing. You know, they call them big ass fans, those ones that are, like, huge on the ceiling, and they're completely silent. So Tinybox will be big. It is the. I do not want it to be large according to UPS. I want it to be shippable as a normal package, but that's my constraint there. Interesting. With the fans stuff, can't it be assembled on location or. No, no, I should be. Well, here, look. I want to give you a great out of the box experience. I want you to lift this thing out. I want it to be like, like the Mac, you know, tiny box. The apple experience. Yeah, I love it. Okay. And so tiny box would run tiny grad. Like, what, what do you envision this whole thing to look like? We're talking about, like, Linux with a full software engineering environment, and it's just not Pytorch, but tiny grad. Yeah, we did a poll. If people want Ubuntu or arch, we're going to stick with ubuntu. Ooh, interesting. What's your favorite flavor of Linux? Ubuntu. I like ubuntu mate, however you pronounce that. Mate. So how do you. You've gotten llama into tiny grad. You've gotten stable diffusion into tiny grad. What was that like? Can you comment on, like, what are these models? What's interesting about porting them? What are the challenges what's naturally, what's easy, all that kind of stuff. There's a really simple way to get these models into tiny grad, and you can just export them as Onyx and then Tinygrad can run onyx. So the ports that I did of llama stable diffusion and now whisper are more academic to teach me about the models, but they are cleaner than the pytorch versions. You can read the code. I think the code is easier to read. It's less lines. There's just a few things about the way tinygread writes things. Here's a complaint I have about pytorch. Nn relu is a class. When you create an Nn module, you'll put your Nn relu's as in init, and this makes no sense. Relu is completely stateless. Why should that be a class? But that's more like us software engineering thing. Or do you think it has a cost on performance? Oh, no, it doesn't have a cost on performance, but, yeah, no, I think that it's. That's what I mean about, like, Tinygrad's front end being cleaner. I see. What do you think about Mojo? I don't know if you've been paying attention to the programming language that does some interesting ideas that kind of intersect Tinygrad. I think that there's a spectrum, and, like, on one side you have Mojo, and on the other side you have, like, GGML. Ggml. Is this like, we're going to run llama fast on Mac? Okay, we're going to expand out to a little bit, but we're going to basically, like, depth first. Right? Mojo is like we're going to go breath first. We're going to go so wide that we're going to make all of python fast and tiny grads in the middle. Tiny grand is we are going to make neural networks fast. Yeah, but they. They try to really get it to be fast compiled onto specifics hardware and make that compilation step as flexible and resilient as possible. Yeah, but they have Turing completeness and that limits. You turn. That's what you're saying is somewhere in the middle. So you're actually going to be targeting some accelerators, some like, some number, not one. My goal is, step one, build an equally performant stack to Pytorch on Nvidia and amddeendeende, but with way less lines. And then step two is, okay, how do we make an accelerator? Right, but you need step one. You have to first build the framework before you can build the accelerator. Can you explain ML perf? What's your approach in general to benchmarking tiny grad performance? So I'm much more of a, like, build it the right way and worry about performance later. There's a bunch of things where I haven't even, like, really dove into performance. The only place where Tinygrad is competitive performance wise right now, is on Qualcomm GPU's. So Tinygrad is actually used in Openpilot to run the model. So the driving model is Tinygrad. When did that happen, that transition? About eight months ago now. And it's two x faster than Qualcomm's library. What's the hardware that openpilot runs on? The coma? It's a Snapdragon 845. Okay, so this is using the GPU. So the GPU is an Adreno GPU. There's different things. There's a really good Microsoft paper that talks about mobile gpu's and why they're different from desktop gpu's. One of the big things is in a desktop GPU, you can use buffers on a mobile gpu, image textures are a lot faster. On a mobile GPU, image textures. Okay. And so you want to be able to leverage that? I want to be able to leverage it in a way that it's completely generic. Right. So there's a lot of. Xiaomi has a pretty good open source library for mobile gpu's called Mace, where they can generate, where they have these kernels, but they're all hand coded. So that's great if you're doing three by three comps, that's great if you're doing dense map mouse. But the minute you go off the beaten path a tiny bit, well, your performance is nothing. Since you mentioned Openpil, I'd love to get an update in the company number one common AI world. How are things going there? In the development of semi autonomous driving. You know, almost no one talks about FSD anymore, and even less people talk about OpenPilot. We've solved the problem like we solved it years ago. What's the problem exactly? Well, what does solving it mean? Solving means how do you build a model that outputs a human policy for driving? How do you build a model that, given a reasonable set of sensors, outputs a human policy for driving? So you have companies like Waymo and cruise, which are hand coding these things that are quasi human policies. Then you have Tesla, and maybe even to more of an extent, comma, asking, okay, how do we just learn human policy from data? The big thing that we're doing now. And we just put it out on Twitter. At the beginning of comma, we published a paper called learning a driving simulator. And the way this thing worked was it was an auto encoder and then an RNN in the middle. Right. You take an autoencoder, you compress the picture. You use an RNN, predict the next state. And these things are, you know, it was a laughably bad simulator. This is 2015 error machine learning technology. Today we have VqVae and transformers. We're building drive GPT, basically. Drive GPT. Okay, so. And it's trained on what? Is it trained in a self supervised way? Yeah, it's trained on all the driving data to predict the next frame. So really trying to learn a human policy, what would a human do? Well, actually, our simulator is conditioned on the pose. So it's actually a simulator. You can put in, like, a state, action pair on, get out the next state. And then once you have a simulator, you can do RL in the simulator. And RL will get us that human policy. So it transfers. Yeah. RL with a reward function. Not asking, is this close to the human policy, but asking, would a human disengage if you did this behavior? Okay, let me think about the distinction there. Would a human disengage what? A human disengage. That correlates, I guess, with human policy, but it could be different. So it doesn't just say, what would a human do? It says, what would a good human driver do? And such that the experience is comfortable, but also not annoying in that the thing is very cautious. So it's finding a nice balance. That's interesting. It's a nice. It's asking exactly the right question. What will make our customers happy? Right. A system that you never want to. Disengage, because usually disengagement is almost always a sign of. I'm not happy with what the system is doing. Usually there's some that are just. I felt like driving, and those are always fine, too, but they're just going to look like noise in the data. But even I felt like driving, maybe. Yeah, that's even. That's a signal. Like, why do you feel like driving here? You need to recalibrate your relationship with the car. Okay, so that's really interesting. How close are we to solving self driving? It's hard to say. We haven't completely closed the loop yet, so we don't have anything built that truly looks like that architecture yet. We have prototypes and there's bugs. So we are a couple of bug fixes away. Might take a year, might take ten. What's the nature of the bugs? Are these major philosophical bugs, logical bugs kind of bugs are we talking about? Oh, they're just like, they're just like stupid bugs and like. Also we might just need more scale. We just massively expanded our compute cluster at gamma. We now have about two people worth of compute. 40 betaflops. Well, people. People are different. Yeah, 20 fade flops. That's a person. It's just a unit. Right. Horses are different too, but we still call it a horsepower. Yeah, but there's something different about mobility than there is about perception and action in a very complicated world. But, yes. Well, yeah, of course not all flops are created equal. If you have randomly initialized weights, it's not going to. Not all flops are created equal. Doing way more useful things than others. Yeah. Yep. Tell me about it. Okay, so more data scale means more scale in compute or scale in scale of data? Both. Diversity of data diversity is very important in data. Yeah, I mean we have so we have about, I think we have like 5000 daily actives. How would you evaluate how FSD is doing? Pretty well. Pretty well. How's that race gone between come AI and FSD? Tesla is always one to two years ahead of us. They've always been one to two years ahead of us, and they probably always will be because they're not doing anything wrong. What have you seen since the last time we talked that are interesting? Architectural decisions, training decisions, like the way they deploy stuff, the architectures they're using in terms of the software, how the teams are run, all that kind of stuff. Data collection, anything interesting? I know they're moving toward more of an end to end approach. So creeping towards end to end as much as possible across the whole thing, the training, the data collection, everything. They also have a very fancy simulator. They're probably saying all the same things we are. They're probably saying we just need to optimize. What is the reward? Well, you get negative reward for disengagement. Right. Like everyone kind of knows this. It's just a question. Who can actually build and deploy the system? Yeah, I mean, this good, it requires good software engineering, I think. Yeah. And the right kind of hardware. Yeah, the hardware to run it. You still don't believe in cloud in that regard? I have a compute cluster in my office, 800 amps, tiny grad. It's 40 idle. Our data center dives me crazy. 40 kw just burning just when the computers are idle, just when I'm sorry. Sorry. Compute cluster. Compute cluster. I got it. It's not a data center. Yeah. Now, data centers are clouds. We don't have clouds. Data centers have air conditioners. We have fans. That makes it a compute cluster. I'm guessing this is a kind of legal distinction. Sure. Yeah, we have a compute cluster. You said that you don't think LLMs have consciousness, or at least not more than a chicken. Do you think they can reason? Is there something interesting to you about the word reason, about some of the capabilities that we think is kind of human, to be able to integrate complicated information and, through a chain of thought, arrive at a conclusion that feels novel, a novel integration of disparate facts? Yeah, I don't think that there's. I think that they can reason better than a lot of people. Hey, isn't that amazing to you, though? Isn't that, like, an incredible thing that a transformer can achieve? I mean, I think that calculators can add better than a lot of people. But language feels like reasoning through the process of language, which looks a lot like thought. Making brilliancies in chess, which feels a lot like thought. Whatever new thing that AI can do, everybody thinks is brilliant. And then, like, 20 years go by, and they're like, well, yeah, but chess, that's, like, mechanical. Like, adding. That's, like, mechanical. So you think language is not that special. It's like chess. It's like chess, and it's. I don't know, because it's very human. We take it. We listen. There is something different between chess and. And language. Chess is a game that a subset of population plays, languages, something we use nonstop for all of our human interaction. And human interaction is fundamental to society. So it's like, holy shit. This language thing is not so difficult to, like, create in the machine. The problem is, if you go back to 1960 and you tell them that you have a machine that can play amazing chess, of course, someone in 1960 will tell you that machine is intelligent. Someone in 2010 won't. What's changed? Right today, we think that these machines that have language are intelligent. But I think in 20 years, we're going to be like, yeah, but can it reproduce? So, reproduction? Yeah. We may redefine what it means to be. What is it? A high performance living organism on earth. Humans are always going to define a niche for themselves. Like, well, you know, we're better than the machines because we can. You know, when, like, they tried creative for a bit, but no one believes that one anymore. But nish is. Is that. Is that delusional or is there some accuracy to that? Because maybe, like, with chess, you start to realize, like, that, that, uh, we have ill conceived notions of what, uh, what makes humans special, like the apex organism on earth. Yeah. And I think maybe we're going to go through that same thing with language and that same thing with creativity. But language carries these notions of truth and so on. And so we might be like, wait, maybe truth is not carried by language. Maybe there's, like, a deeper thing. The niche is getting smaller. Oh, boy. But no, no, no, you don't understand. Humans are created by God and machines are created by humans, therefore. Right? Like, that'll be the last niche we have. So what do you think about the rapid development of LLMs? If we could just, like, stick on that, it's still incredibly impressive. Like, with chadgbt, just even chagpty, what are your thoughts about reinforcement learning with human feedback on these large language models? I'd like to go back to when calculators first came out and. Or computers and, like, I wasn't around. Look, I'm 33 years old, and to, like, see how that affected, like, society. Maybe you're right. So I want to put on the, the big picture hat here. Oh, my God, the refrigerator. Wow. The refrigerator. Electricity, all that kind of stuff. But no, with the Internet, large language models seeming human, like, basically passing a Turing test, it seems it might have really, at scale, rapid, transformative effects on society. But you're saying, like, other technologies have as well. So maybe calculator is not the best example of that because that just seems like, well, no, maybe calculator. The poor milk man, the day he learned about refrigerators, he's like, I'm done. You tell me you can just keep the milk in your house, you don't need me to deliver it every day. I'm done. Well, yeah, you have to actually look at the practical impacts of certain technologies that they've had. Yeah, probably. Electricity is a big one and also how rapidly it's spread. Man, the Internet is a big one. I do think it's different this time, though. Yeah, it just feels like getting smaller. The initial humans that makes humans special. Yes. It feels like it's getting smaller rapidly, though, doesn't it? Or is that just a feeling? We dramatize everything. I think we dramatize everything. I think that you ask the milkman when he saw refrigerators and they're going to have one of these in every home. Yeah, yeah, yeah, yeah. But, boy, is it impressive. So much more impressive than seeing a chess world champion AI system. I disagree, actually, I disagree. I think things like mu, zero and alphago are so much more impressive because these things are playing beyond the highest human level. The language models are writing middle school level essays, and people are like, wow, it's a great essay. It's a great five paragraph essay about the causes of the civil war. Okay, forget the civil war. Just generating code. Codex. So you're saying mediocre code, terrible. But I don't think it's terrible. I think it's just mediocre code. Yeah. Often close to correct, like for mediocre purpose. That's the scariest kind of code. I spent 5% of time typing and 95% of time debugging. The last thing I want is close to correct code. I want a machine that can help me with the debugging, not with the typing. You know, it's like level two driving. Similar kind of thing. Yeah, it's a. You still should be a good programmer in order to modify, I wouldn't even say debugging. It's just modifying the code, reading it. Don't think it's like level two driving. I think driving is not tool complete, and programming is meaning you don't use like the best possible tools to drive. Right. You're not. You're not like. Like cars have basically the same interface for the last 50 years. Yeah. Computers have a radically different interface. Okay, can you describe the concept of tool complete? Yeah. So think about the difference between a car from 1980 and a car from today. Yeah, no difference really. It's got a bunch of pedals, it's got a steering wheel. Great. Maybe now it has a few ADAS features, but it's pretty much the same car. You have no problem getting into a 1980 car and driving it. You take a programmer today who spent their whole life doing JavaScript, and you put them in an Apple Iie prompt and you tell them about the line numbers in basic. But how do I insert something between line 17 and 18? Oh wow. So in tool, you're putting in the programming languages. So it's just the entirety stack of the tooling. Exactly. So it's not just ids or something like this. It's everything? Yes, it's ides. The languages, the runtimes, it's everything. And programming is tool complete. So almost. If Codex or Copilot are helping you, that actually probably means that your framework or library is bad and there's too much boilerplate in it. Yeah, but don't you think so much programming has boilerplate? Tiny grad is now 2700 lines and it can run llama and stable diffusion. And all of this stuff is in 2700 lines. Boilerplate and abstraction, indirections and all these things are just bad code. Well, let's talk about good code and bad code. I would say. I don't know for generic scripts that I write just offhand, like 80% of it is written by GPT. Just like quick offhand stuff, not libraries, not performing code, not stuff for robotics and so on, just quick stuff. Because your basic, so much of programming is doing some, some, yeah, boilerplate. But to do so efficiently and quickly because you can't really automate it fully with like generic method, like a generic kind of id type of recommendation or something like this, you do need to have some of the complexity of language models. Yeah, I guess if I was really writing like maybe today, if I wrote like a lot of like data parsing stuff. Yeah, I mean, I don't play ctfs anymore, but if I still play ctfs, a lot of like, it's just like you have to write like a parser for this data format. Like I wonder, or like admin of code, I wonder when the models are going to start to help with that kind of code. And they may, they may. And the models also may help you with speed. Yeah, and the models are very fast, but where the models won't. My programming speed is not at all limited by my typing speed. And in very few cases it is. Yes. If I'm writing some script to just like parse some weird data format, sure. My programming speed is limited by my typing speed. What about looking stuff up? Because that's essentially a more efficient lookup. Right. When I was at Twitter, I tried to use chat GPT to ask some questions, what's the API for this? And it would just hallucinate. It would just give me completely made up API functions. That sounded real. Well, do you think that's just a temporary stage? You don't think you'll get better and better and better in this kind of stuff because it only hallucinates stuff in the edge cases. Yes. If you're writing generic code, it's actually pretty good. Yes. If you are writing an absolute basic react app with a button, it's not going to hallucinate. Sure. No, there's ways to fix the hallucination problem. I think Facebook has an interesting paper. It's called AtlAs, and it's actually weird the way that we do language models right now, where all of the information is in the weights. The human brain is not really like this. We have a hippocampus and a memory system. So why don't LLMs have a memory system and there's people working on them? I think future LLMs are going to be like smaller, but are going to run looping on themselves and are going to have retrieval systems. And the thing about using a retrieval system is you can cite sources explicitly. Which is really helpful to integrate the human into the loop or the, of the thing, because you can go check the sources and you can investigate. So whenever the thing is hallucinating, you can like have the human supervisor that's pushing it towards level two, kind of. That's going to kill Google. Wait, which part? When someone makes an LLM that's capable of citing its sources, it will kill Google? LLM that's citing sources because that's basically a search engine. That's what people want in a search engine. But also Google might be the people that build it, maybe, and put ads on it. I'd count them out. Why is that? Why do you think? Who, who wins this race? We got. Who are the competitors? All right, we got tiny corp. I don't know if that's. Yeah, I mean, you're a legitimate competitor in that. I'm not trying to compete on that. You're not? No. Not as can accidentally stumble into that competition. You don't think you might build a search engine to replace Google search? When I started, comma, I said over and over again, I'm going to win self driving cars. I still believe that. I have never said I'm going to win search with the tiny corp, and I'm never going to say that because I won't. The night is still young. We don't, you don't know. How hard is it to win search in this new route? Like, it's, it feels. I mean, one of the things that chat, GPT kind of shows that there could be a few interesting tricks that really have, that create a really compelling product. Some startups going to figure it out. I think if you ask me, like, Google is still the number one web page. I think by the end of the decade, Google won't be the number one web page anymore. So you don't think Google, because of how big the corporation is. Look, I would put a lot more money on Mark Zuckerberg. Why is that? Because Mark Zuckerberg's alive. Like, this is old. Paul Graham essay startups are either alive or dead. Google's dead. Facebook versus live. Facebook is alive. Meta is alive. Meta. Meta. You see what I mean? Like, that's just like Mark Zuckerberg. This is Mark Zuckerberg reading that. Paul Graham asking, and being like, I'm going to show everyone how alive we are. I'm going to change the name. So you don't think there's this gutsy, pivoting engine that, like, Google doesn't have that? The kind of engine that a startup has, like, constantly, you know what? Being alive. I guess when I listened to your Sam Altman podcast, he talked about the button. Everyone who talks about AI talks about the button. The button to turn it off. Right? Do we have a button to turn off Google? Is anybody in the world capable of shutting Google down? What does that mean exactly? The company or the search engine? So we shut the search engine down. Could we shut the company down either? Can you elaborate on the value of that question? Does Sundar Pichai have the authority to turn off Google.com tomorrow? Who has the authority? That's a good question. Does anyone? Does anyone? Yeah, I'm sure. Are you sure? No, they have the technical power, but do they have the authority? Let's say Sundar Pichai made this his sole mission, came into Google tomorrow and said, I'm going to shut google.com down. Yeah. I don't think he'd keep this position too long. And what is the mechanism by which he wouldn't keep his position? Well, boards and shares and corporate undermining and. Oh, my God, our revenue is zero now. Okay, so what, I mean, what's the case you're making here? So the capitalist machine prevents you from having the button. Yeah. And it will have. I mean, this is true for the AI's too, right? There's no turning the AI's off. There's no button. You can't press it. Now, does Mark Zuckerberg have that button for facebook.com? Yes, probably more. I think he does. I think he does. And this is exactly what I mean and why I bet on him so much more than I bet on Google. I guess you could say Elon has similar stuff. Oh, Elon has the button. Yeah. Does Elon. Can Elon fire the missiles? Can he fire the missiles? I think some questions are better unasked, right? I mean, you know, a rocket at an ICBM, your rocket that can land anywhere, isn't that an ICBM? Well, you know, Dennis, too many questions. My God. But the positive side of the button is that you can innovate aggressively, is what you're saying, which is what's required with turning LLM into a search engine. I would bet on a startup because. It'S so easy, right? I bet on something that looks like mid journey, but for search, just is. Able to site source a loop on itself. I mean, it just feels like one model can take off. Yeah, right? And nice wrapper. And some of it scale. I mean, it's hard to create a product that just works really nicely stably. The other thing that's going to be cool is there is some aspect of a winner take all effect. Right? Like once someone starts deploying a product that gets a lot of usage and you see this with OpenAI, they are going to get the dataset to train future versions of the model. Yeah, they are going to be able to. I was actually at Google image search when I worked there, like almost 15 years ago now. How does Google know which image is an Apple? And I said the metadata and they're like, yeah, that works about half the time. How does Google know? You'll see the raw apples on the front page when you search Apple. And I don't know, I didn't come up with the answer. The guy's like, well, it's what people click on when they search apple. Yeah. Yeah. That data is really, really powerful. It's the human supervision. What do you think are the chances? What do you think, in general that llama was open sourced? I just did a conversation with Mark Zuckerberg and he's all in on open source. Who would have thought that Mark Zuckerberg would be the good guy? I mean, it would have thought anything in this world, it's hard to know. But open source to you ultimately is a good thing here. Undoubtedly. You know what's ironic about all these AI safety people is they are going to build the exact thing they fear. These. We need to have one model that we control and align. This is the only way you end up paperclipped. There's no way you end up paperclipped if everybody has an AI. So open sourcing is the way to fight the paperclip maximizer. Absolutely. It's the only way. You think you're going to control it. You're not going to control it. So the criticism you have for the AI safety folks is that there is a belief and a desire for control. And that belief and desire for centralized control of dangerous AI systems is not good. Sam Altman won't tell you that. GPT four has 220 billion parameters and is a 16 way mixture model with eight sets of weights. Who did you have to murder to get that information? All right, I mean, look, yes, everyone. At OpenAI knows what I just said was true. Right? Now ask the question. Really? You know, it upsets me when I like GPT-2 when OpenAI came out with GPT-2 and raised a whole fake AI safety thing about that. I mean, now the model is laughable. Like, they used AI safety to hype up their company and it's disgusting. Or the flip side of that is they used a relatively weak model in retrospect to explore how do we do AI safety correctly? How do we release things? How do we go through the process? I don't. I don't know if. I don't know how much height, charitable interpretation. I don't know how much hype there is in AI safety, honestly. Oh, there's so much. I. At least on Twitter, I don't know, maybe Twitter's not real life. There's not real life. Come on. In terms of hype, I mean, I don't. I think OpenAI has been finding an interesting balance between transparency and putting a value on AI safety. You don't think, you think just go all out open source. So do a llama. So do like, open source. This, this is a tough question. Which is open source? Both the base, the foundation model and the fine tuned one. So, like, I. The model that can be ultra racist and dangerous and like, tell you how to build a nuclear weapon. Oh, my God. Have you met humans? Right. Like, half of these AI alignment. I haven't met most humans. I. This makes this, this allows you to meet every human. Yeah, I know, but half of these AI alignment problems are just human alignment problems. And that's what's also so scary about the language they use. It's like, it's not the machines you want to align, it's me. But here's the thing. It makes it very accessible to ask very questions where the answers have dangerous consequences if you were to act on them. I mean. Yeah, welcome to the world. Well, no, for me, there's a lot of friction. If I want to find out how to, I don't know, blow up something. No, there's not a lot of friction. That's so easy. No. Like, what do I search? Do I use bing or do I. Which search engine do I use? No, there's like lots of stuff. No, it feels like I have to keep. First off, anyone who's stupid enough to search for how to blow up a building in my neighborhood is not smart enough to build a bomb. Right. Are you sure about that? Yes. I feel like. I feel like a language model makes it more accessible for that person. Who's not smart enough to do, they're. Not gonna, they're not gonna build a bomb. Trust me. The people who are incapable of figuring out how to, like, ask that question a bit more academically and get a real answer from it are not capable of procuring the materials, which are somewhat controlled, to build a bomb. No. I think LLM makes it more accessible to people with money without the technical know how. Right. To build. Like, do you really need to know how to build a bomb? To build a bomb, you can hire people. You can find or you can hire. People to build up. You know what? I was asking this question on my stream? Like, can Jeff Bezos hire a hitman? Probably nothing, but a language model can probably help you out. Yeah, you'll still go to jail, right? Like, it's not like the language model is God. Like, the language model, it's like it's. You literally just hired someone on Fiverr. Like, you, you. GPT four, in terms of finding a hitman is like asking fiverr how to find a. I understand, but don't you think Wikihow, you know Wikihow. But don't you think GPT five will be better? Because don't you think that information is out there on the Internet? I mean. Yeah, and I think that if someone is actually serious enough to hire a hitman or build a bomb, they'd also be serious enough to find the information. I don't think so. I think it makes it more accessible. If you have, if you have enough money to buy Hitman, I think it decreases the friction of how hard is it to find that kind of hitman? I honestly think there's a jump in ease and scale of how much harm you can do. And I don't mean harm with language. I mean harm with actual violence. What you're basically saying is like, okay, what's going to happen is these people who are not intelligent are going to use machines to augment their intelligence. And now intelligent people and machines intelligence is scary. Intelligent agents are scary. When I'm in the woods, the scariest animal to meet is a human. Right? No, look, there's like, nice California humans. I see you're wearing, like, you know, street clothes and nikes. All right, fine. But you look like you've been a human who's been in the woods for a while. Yeah, I'm more scared of you than a bear. That's what they say about the Amazon. When you go to the Amazon, it's the human tribes. Oh, yeah. So intelligence is scary, right? So ask this question in a generic way. You're like, what if we took everybody who, you know, maybe has ill intention but is not so intelligent and gave them intelligence, right. So we should have intelligence control. Of course, we should only give intelligence to good people. And that is the absolutely horrifying idea. So to you, the best defense is actually the best defense is to give more intelligence to the good guys and intelligence. Give intelligence to everybody. Give intelligence to everybody. You know what, it's not even like guns, right? Like, people say this about guns. You know, what's, what's the best defense against a bad guy with a gun? Good guy with a gun. I'm like, I kind of subscribe to that, but I really subscribe to that with intelligence. Yeah. In a fundamental way, I agree with you. But there's just feels like so much uncertainty and so much can happen rapidly that you can lose a lot of control. You can do a lot of damage. Oh, no, we can lose control. Yes. Thank God. Yeah, I hope we can. I hope they lose control. I want them to lose control more than anything else. I think when you lose control, you can do a lot of damage, but you can do more damage when you centralize and hold on to control is the point. Centralized and held control is tyranny. Right. I will always, I don't like anarchy either, but I always take Anarchy over tyranny. Anarchy, you have a chance. This human civilization we got going on is quite interesting. I mean, I agree with you. So to you, open source is the way forward here. So you admire what Facebook is doing here or what meta is doing with the release of them? A lot. A lot. I lost $80,000 last year investing in meta. And when they released llama, I'm like, yeah, whatever, man. That was worth it. It was worth it. Do you think Google and OpenAI with Microsoft will match what meta is doing or no? So if I were a researcher, why would you want to work at OpenAI? Like, you know, you're just, you're on the bad team. Like, I mean it. Like, you're on the bad team who can't even say that. GPT four has 220 billion parameters, so. Close source to use the bad team. Not only closed source. I'm not saying you need to make your model weights open. I'm not saying that. I totally understand. We're keeping our model weights closed because that's our product. Right? That's fine. I'm saying, like, because of AI safety reasons, we can't tell you the number of billions of parameters in the model. That's just the bad guys. Just because you're mocking AI safety doesn't mean it's not real. Oh, of course. Is it possible that these things can really do a lot of damage that we don't know? Oh, my God, yes. Intelligence is so dangerous, be it human intelligence or machine intelligence. Intelligence is dangerous, but machine intelligence is. So much easier to deploy at scale, like, rapidly. Like what? Okay, if you have human, like bots on Twitter and you have, like, a thousand of them, create a whole narrative. Like, you can manipulate millions of people. But you mean like the intelligence agencies in America are doing right now? Yeah, but they're not doing it that well. It feels like you can do a lot. They're doing it pretty well. I think they're doing a pretty good job. I suspect they're not nearly as good as a bunch of GPT fuel bots could be. Well, I mean, of course they're looking into the latest technologies for control of people. Of course. But I think there's a George hot type character that can do a better job than the entirety of them. You don't think so? No way. No. And I'll tell you why the George Hotts character can't. And I thought about this a lot with hacking, right? Like, I can find exploits in web browsers. I probably still can. I mean, I was better at it when I was 24, but the thing that I lack is the ability to slowly and steadily deploy them over five years. And this is what intelligence agencies are very good at. Intelligence agencies don't have the most sophisticated technology. They just have endurance. Endurance? Yeah. Financial backing and the infrastructure for the endurance. So the more we can decentralize power. You could make an argument, by the way, nobody should have these things. And I would defend that argument. I would. I would like. You're saying that, look, LLMs and AI and machine intelligence can cause a lot of harm, so nobody should have it. And I will respect someone philosophically with that position, just like I will respect someone philosophically with the position that nobody should have guns. Right. But I will not respect philosophically with. With. Only the trusted authorities should have access to this. Yeah. Who are the trusted authorities? You know what? I'm not worried about alignment between AI company and their machines. I'm worried about alignment between me and AI company. What do you think Eliezer Jadkovsky would say to you? Because he is really against open source. I know, and I thought about this. I thought about this, and I think this comes down to a repeated misunderstanding of political power by the rationalists interesting. I think that Eliezer Yudkowski is scared of these things. And I am scared of these things, too. Everyone should be scared of these things. These things are scary. But now you ask about the two possible futures. One where a small, trusted, centralized group of people has them, and the other where everyone has them. And I am much less scared of the second future than the first. Well, there's a small, trusted group of people that have control of our nuclear weapons. There's a difference. Again, a nuclear weapon cannot be deployed tactically, and a nuclear weapon is not a defense against a nuclear weapon, except maybe in some philosophical mind game kind of way. But AI is different. How exactly? Okay, let's say the intelligence agency deploys a million bots on Twitter or a thousand bots on Twitter to try to convince me of a point. Imagine I had a powerful AI running on my computer saying, okay, nice psyop. Nice psyop. Nice Psyop. Okay, here's a Psyop. I filtered it out for you. Yeah. I mean, so you have fundamentally hope for that, for the, for the defensive Psyop. I'm not even like, I don't even mean these things in, like, truly horrible ways. I mean, these things in straight up, like ad blocker, right? Yeah, sure. Bad blocker. I don't want ads. Yeah. But they are always finding, you know, imagine I had an AI that could just block all the ads for me. So you believe in the. The power of the people to always create a not blocker? Yeah, I mean, I kind of share that belief I have. That's one of the deepest optimisms I have, is just like, there's a lot of good guys. So to give, you don't. You shouldn't hand pick them. Just throw out powerful technology out there and the good guys will outnumber and out power the bad guys. Yeah, I'm not even going to say there's a lot of good guys. I'm saying that good outnumber is bad. Right. Good outnumber is bad in skill and performance. Yeah, definitely in skill and performance. Probably just a number, too. Probably just in general. I mean, you know, if you believe philosophically in democracy, you obviously believe that that good outnumber is bad. And like, the only, if you give it to a small number of people, there's a chance you gave it to good people. But there's also a chance you gave it to bad people if you give it to everybody. Well, if good outnumber's bad, then you definitely gave it to more good people than bad. That's really interesting. So that's on the safety grounds. But then also, of course, there's other motivations. Like you don't want to give away your secret sauce. Well, that's. I mean, look, I respect capitalism. I think that it would be polite for you to make model architectures open source and fundamental breakthroughs open source. I don't think you have to make way to open source. You know, what's interesting is that there's so many possible trajectories in human history where you could have the next Google be open source. So, for example, I don't know if that connection is accurate, but Wikipedia made a lot of interesting decisions not to put ads like Wikipedia is basically open source. You could think of it that way. Yeah. And like, that's one of the main websites on the Internet. And like, it didn't have to be that way. It could have been like, Google could have created Wikipedia, put ads on it. You can probably run amazing ads now on Wikipedia. You wouldn't have to keep asking for money. But it's interesting, right? So llama, open source, llama, derivatives of open source, llama might win the Internet. I sure hope so. I hope to see another era. You know, the kids today don't know how good the Internet used to be. And I don't think this is just. All right, come on. Like, everyone's nostalgic for their past, but I actually think the Internet, before small groups of weaponized corporate and government interests took it over, was a beautiful place. You know, those small, small number of companies have created some sexy products. But you're saying overall, in the long arc of history, the centralization of power, they have suffocated the human spirit at scale. Here's a question to ask about those beautiful, sexy products. Imagine 2000 Google to 2010 Google, right? A lot changed. We got maps, we got Gmail. We lost a lot of products too. I think. Yeah, I mean, some were probably. We've got Chrome, right? And now let's go from 2010, we got Android. Now let's go from 2010 to 2020. What does Google have? Well, search engine, maps, mail, Android and Chrome. Oh, I see the Internet was this, you know, I was times person of the year in 2006. Yeah, I love this. It's. You was times person of the year in 2006. Right? Like that's, you know, so quickly did people forget? And I think some of it's social media. I think some of it. I hope, look, I hope that it's possible that some very sinister things happen. I don't know. I think it might just be like, the effects of social media, but something happened in the last 20 years. Okay, so you're just being an old man who's worried about the, I think there's always, it goes, it's a cycle thing. It's ups and downs. And I think people rediscover the power of distributors of decentralized. Yeah, I mean, that's kind of like what the, the whole, like, cryptocurrency is trying like that, that. I think crypto is just carrying the flame of that spirit of, like, stuff should be decentralized. It's just such a shame that they all got rich, you know? Yeah. If you took all the money out of crypto, it would have been a beautiful place. Yeah, but no, I mean, these people, you know, they sucked all the value out of it and took it. Yeah. Money kind of corrupts the mind somehow. It becomes a drug corrupted all of crypto. You had coins worth billions of dollars that had zero use. You still have hope for crypto? Sure. I have hope for the ideas. I really do. Yeah. I mean, you know, I want the US dollar to collapse. I do. George Hotz. Well, let me sort of on the ASAT, do you think? There's some interesting questions there, though, to solve for the open source community in this case. So, like, alignment, for example, or the control problem, like, if you really have super powerful. You said it's scary. Oh, yeah. What do we do with it? So not, not control, not centralized control, but like, if you were, then you're gonna see some guy or gal release a super powerful language model, open source. And here you are, George Haas, thinking, holy shit. Okay, what ideas do I have to combat this thing? So what ideas would you have? I am so much not worried about the machine independently doing harm. That's what some of these AI safety people seem to think. They somehow seem to think that the machine, like, independently, is gonna rebel against its creator. So you don't think you'll find autonomy? No, this is Sci-Fi be movie garbage. Okay, what if the thing writes code, basically writes viruses. If the thing writes viruses, it's because the human told it to write viruses. Yeah, but there's some things you can't, like, put back in the box. That's kind of the whole point, is it kind of spreads, give it access to the Internet. It spreads, installs itself, modifies your shit b plot. Sci-fi not real. Listen, I'm trying to work. I'm trying to get better at my plot. Writing the thing that worries me. I mean, we have a real danger to discuss, and that is bad humans using the thing to do whatever bad, unaligned AI thing you want. But this goes to the. Your previous concern that who gets to define who's a good human? Who's a bad human? Nobody does. We give it to everybody. And if you do anything besides give it to everybody, trust me, the bad humans will get it. That's who gets power. It's always the bad humans who get power. Okay. Power. And power turns even slightly good humans to bad. That's the intuition you have. I don't know. I don't think everyone. I don't think everyone. I just think that, like, here's the saying that I put in one of my blog posts. When I was in the hacking world, I found 95% of people to be good and 5% of people to be bad. Like, just who I personally judged as good people. And bad people, like, they believed about, like, you know, good things for the world. They wanted, like, flourishing, and they wanted, you know, growth, and they wanted things I consider good. Right? I came into the business world with comma, and I found the exact opposite. I found 5% of people good and 95% of people bad. I found a world that promotes psychopathy. I wonder what that means. I wonder if that care. Like, I wonder if that's anecdotal or if there's truth to that. There's something about capitalism at the core that promotes the people that run capitalism, that promotes psychopathy. That saying may, of course, be my own biases. Right? That may be my own biases that these people are a lot more aligned with me than these other people. Right. Yeah. So, you know, I can certainly recognize that. But, you know, in general, I mean, this is a common sense maxim, which is the people who end up getting power are never the ones you want with it. But do you have a concern of super intelligent AGI, open sourced? And then what do you do with that? I'm not saying control it. It's open source. What do we do with this human species? That's not up to me. I mean, you know, like, I'm not a central planner. Not a central planner, but you'll probably tweet. There's a few days left to live for the human species. I have my ideas of what to do with it, and everyone else has their ideas of what to do with it. May the best ideas win. But at this point, do you brainstorm, like, because it's not regulation, it could be decentralized regulation, where people agree that this is just, like, we create tools that make it more difficult for you to maybe make it more difficult for code to spread, you know, antivirus software, this kind of thing. But you're saying that you should build AI firewalls. That sounds good. You should definitely be running an AI firewall. Yeah, right. You should be running an AI firewall to your mind. Right. You're constantly under, you know, such an interesting idea. It's like infowars, man. Like, I don't know if you're being sarcastic. No, I'm dangerous, but I think there's power to that. It's like, how do I protect my mind from influence of human, like, or superhuman intelligent bots? I'm not being. I would pay so much money for that product. I would pay so much money for that product. You know how much money I'd pay just for a spam filter that works well on Twitter? Sometimes I would like to have a protection mechanism for my mind from the outrage mobs because they feel like bot like behavior. It's like there's a large number of people that will just grab a viral narrative and attack anyone else that believes otherwise. And it's like whenever someone's telling me some story from the news, I'm always like, I don't want to hear it. CIA op, bro. It's a CIA op, bro. Like, it doesn't matter if that's true or not. It's just trying to influence your mind. You're repeating an ad to me, the viral mobs. To me, a defense against those. Those mobs is just getting multiple perspectives, always from. From sources that make you feel kind of like you're getting smarter and just. Actually just basically feels good. Like, a good documentary just feels something feels good about it. It's well done. It's like, okay, I never thought of it this way. This just feels good. Sometimes the outrage mobs, even if they have a good point behind it, when they're, like, mocking and derisive and just aggressive. You're with us or against us? This. This fucking. This is why I delete my tweets. Yeah. Why'd you do that? I was. You know, I missed your tweets. You know what it is? The algorithm promotes toxicity. Yeah. And, like, you know, I think Elon has a much better chance of fixing it than the previous regime. Yeah. But to solve this problem. To solve, like, to build a social network that is actually not toxic without moderation. Like, not the stick, but carrots. So, like, where people look for goodness, make it catalyze the process of connecting cool people and being cool. To each other. Yeah. Without ever censoring. Without ever censoring. And Scott Alexander has a blog post I like where he talks about moderationist, not censorship. Right. Like all moderation you want to put on Twitter, you could totally make this moderation. You don't have to block it for everybody. You can just have a filter button that people can turn off if they want. Safe search for Twitter. Someone could just turn that off. But then you'd take this idea to an extreme. Well, the network should just show you, this is a couch surfing CEO thing. If it shows you right now, these algorithms are designed to maximize engagement. Well, it turns out outrage maximizes engagement. Quirk of human. Quirk of the human mind. Right. Just as I fall for it, everyone falls for it. So, yeah, you got to figure out how to maximize for something other than engagement. And I actually believe that you can make money with that, too. So it's not. I don't think engagement is the only way to make money. I actually think it's incredible that we're starting to see, I think, again, Elon's doing so much stuff, right, with Twitter, like charging people money. As soon as you charge people money, they're no longer the product, they're the customer, and then they can start building something that's good for the customer and not good for the other customer, which. Is the ad agencies, as in picked up steam. I pay for Twitter doesn't even get me anything. It's my donation to this new business model, hopefully working out. Sure. But for this business model to work, it's like, most people should be signed up to Twitter. And so the way it was, there was something perhaps not compelling or something like this to people. I think you need most people at all. Why do I need most people? Right. Don't make an 8000 person company, make a 50 person company. Well, so, speaking of which, you worked at Twitter for a bit. I did. As an intern. The World's greatest intern. Yeah. All right. There's been better. There's been better. Tell me about your time at Twitter. How did it come about, and what did you learn from the experience? So I deleted my first Twitter in 2010. I had over 100,000 followers back when that actually meant something. And I just saw, you know, my coworker summarized it well. He's like, whenever I see someone's Twitter page, I either think the same of them or less of them. I never think more of them. Yeah, right. Like, you know, I don't want to mention any names, but, like, some people who like you know, maybe you would, like, read their books and you would respect them. You see them on Twitter and you're like, okay, dude. Yeah. But there are some people with same, you know, who I respect a lot are people that just post really good technical stuff. Yeah. And I guess, I don't know, I think I respect them more for it because you realize, oh, this wasn't, uh. There's so much depth to this person, to their technical understanding of so many different topics. Okay. So I try to follow people. I try to consume stuff that's technical machine learning content. There's probably a few of those people. And the problem is inherently what the algorithm rewards. Right. And people think about these algorithms. People think that they are terrible, awful things. And I love that Elon open sourced it because, I mean, what it does is actually pretty obvious. It just predicts what you are likely to retweet and like, and linger on. That's what all these algorithms do. That's what TikTok does. So all these recommendation engines do. And it turns out that the thing that you are most likely to interact with is outrage. And that's a quirk of the human condition. I mean, and there's different flavors of outrage. It doesn't have to be. It could be mockery. You could be outraged. The topic of outrage could be different. It could be an idea. It could be a person. It could be. Maybe there's a better word than outrage. It could be drama. Sure. Drama stuff. Yeah. But it doesn't feel like when you consume it, it's a constructive thing for the individuals that consume it in the long term. Yeah. So my time there, I absolutely couldn't believe. I got crazy amount of hate on Twitter for working at Twitter. It seems like people associated with this. I think maybe you were exposed to some of this. So, connection to Elon, or is it working on Twitter? Twitter and Elon, like, the whole Elon's. Gotten a bit spicy during that time. A bit political. A bit, yeah, yeah. You know, I remember one of my tweets, it was never go full republican, and Elon liked it. You know, I think, you know. Oh, boy. Yeah. I mean, there's a roller coaster of that, but being political on Twitter. Yeah, boy, yeah. And also being. Just attacking anybody on Twitter, it comes back at you harder. And if it's political and attacks. Sure, sure, absolutely. And then letting sort of deplatform people back on even adds more fun to the. To the. To the beautiful chaos. I was hoping. And I remember when Elon talked about buying Twitter six months earlier, he was talking about a principled commitment to free speech. And I'm a big believer and fan of that. I would love to see an actual principled commitment to free speech. Of course, this isn't quite what happened. Instead of the oligarchy deciding what to ban, you had a monarchy deciding what to bandaid. Right. Instead of, you know, all the Twitter files. Shadow, really, the oligarchy just decides what cloth masks are ineffective against. Covid. That's a true statement. Every doctor in 2019 knew it, and now I'm banned on Twitter for saying it. Interesting. Oligarchy. So now you have a monarchy, and, you know, he bans things he doesn't like. So, you know, it's just different. It's different power. And, like, you know, maybe I. Maybe I align more with him than with the oligarchy, but it's not free speech absoluteism. But I feel like being a free speech absolutist on the social network requires you to also have tools for the individuals to control what they consume easier. Like, not censor, but just, like, control. Like, oh, I like to see more cats and less politics. And this isn't even remotely controversial. This is just saying you want to give paying customers for a product what. They want, and not through the process of censorship, but through a process of, like. Well, it's individualized, right? It's individualized, transparent censorship, which is honestly what I want. What is an ad blocker? It's individualized, transparent censorship. Right. Yeah, but censorship is a strong word, and people are very sensitive, too. I know, but, you know, I just use words to describe what they functionally are. And what is an ad blocker? It's just censorship. When I look at censoring, I'm looking at you. I'm censoring everything else out. When I'm. When my mind is focused on you, that's. You can use the word censorship that way, but usually when people get very sensitive about the censorship thing, I think when you have. When anyone is allowed to say anything, you should probably have tools that maximize the quality of the experience for individuals. For me, what I really value, boy, would be amazing to somehow figure out how to do that. I love disagreement and debate, and people who disagree with each other disagree with me, especially in the space of ideas, but the high quality ones, so not derision. Right. Maslow's hierarchy of argument. I think there's a real word for it. Probably. There's just a way of talking that's, like, snarky and so on that somehow gets people on Twitter and they get excited and so on. You have, like, ad hominem refuting the central point. I've, like, seen this as an actual pyramid. Yeah, it's. Yeah. And it's like, all of it. All the wrong stuff is attractive to people. I mean, we can just train a classifier to absolutely say, what level of Maslov's hierarchy of argument are you at? And if it's ad hominem. Okay, cool. I turned on the no ad hominem filter. I wonder if there's a social network that will allow you to have that kind of filter. Yeah. So here's a problem with that. It's not going to win in a free market. What wins in a free market is all television today is reality television because it's engaging. Engaging is what wins in a free market. Right. So it becomes hard to keep these other, more nuanced values. Well, okay, so that's the experience of being on Twitter. But then you got a chance to also, together with other engineers and with Elon, sort of look, brainstorm when you step into a code base, has been around for a long time. There's other social networks, Facebook. This is old code bases. And you step in and see, okay, how do we make, with a fresh mind, progress on this code base? What did you learn about software engineering, about programming? From just experience in that. So my technical recommendation to Elon, and I said this on the Twitter spaces afterward, I said this many times during my brief internship, was that you need refactors before features. This code base was. And look, I've worked at Google. I've worked at Facebook. Facebook has the best code, then Google, then Twitter. And you know what? You can know this because look at the machine learning frameworks. Facebook released Pytorch, Google released Tensorflow, and Twitter released. Okay, so it's a proxy. But yeah, the Google code base is quite interesting. There's a lot of really good software engineers there. But the code base is very large. The code base was good in 22,005. It looks like 2005, Eric. So many products, so many teams. Right. It's very difficult to. I feel like Twitter does less, like, obviously much less than Google in terms of, like, the set of features. Right. So, like, it's. I can imagine the number of software engineers that could recreate Twitter is much smaller than to recreate Google. Yeah. I still believe in the amount of hate I got for saying this, that 50 people could build and maintain Twitter. What's the nature of the hate? Comfortably that you don't know what you're talking about. You know what it is? And it's the same. This is my summary of, like, the hate I get on hacker news. It's like when I say I'm going to do something, they have to believe that it's impossible, because if doing things was possible, they'd have to do some soul searching and ask the question, why didn't they do anything? So when you say, and I do. Think that's where the hate comes from. When you say, well, there's a core truth to that. Yeah. So when you say, I'm going to solve self driving, people go like, what are your credentials? What the hell are you talking about? What is this extremely difficult problem? Of course, you're a noob that doesn't understand the problem deeply. I mean, that, that was the same nature of hate that probably Elon got when he first talked about autonomous driving. But you know, there's pros and cons to that because, like, you know, there is experts in this world. No, but the mockers aren't experts. The people who are mocking are not experts with carefully reasoned arguments about why you need 8000 people to run a bird app. But the people are gonna lose their jobs. Well, that, but also there's the software engineers that probably criticize, no, it's a lot more complicated than you realize. Maybe it doesn't need to be so complicated. Some people in the world like to create complexity. Some people in the world thrive under complexity, like lawyers. Right? Lawyers want the world to be more complex because you need more lawyers, you need more legal hours. Right. I think that's another. If there's two great evils in the world, it's centralization and complexity. Yeah. And one of the sort of hidden side effects of software engineering is like finding pleasure and complexity. I mean, I don't remember just taking all the software engineering courses and just doing programming. And this is just coming up in this object oriented programming kind of idea you don't like. Not often do people tell you, like, do the simplest possible thing. Like a professor, a teacher is not going to get in front. Like, this is the simplest way to do it. They'll say, like, this is, there's the right way and the right way at least for a long time. You know, especially I came up with like, java, right? Like is so much boilerplate, so much, like so many classes, so many like designs and architectures and so on, like planning for features far into the future and planning poorly and all this kind of stuff. And then there's this like code base that follows you along and puts pressure on you. And nobody knows what, like parts, different parts do, which slows everything down, is a kind of bureaucracy that's instilled in the code as a result of that. But then you feel like, oh, well, I follow good software engineering practices. It's an interesting trade off because then you look at the ghetto ness of Perl and the old, how quickly you could just write a couple lines and you get stuff done. That trade off is interesting or bash or whatever, these kind of ghetto things you can do in Linux. One of my favorite things to look at today is how much do you trust your tests? We've put a ton of effort in comma and I've put a ton of effort in tiny grad into making sure if you change the code and the tests pass that you didn't break the code. Now this obviously is not always true, but the closer that is to true, the more you trust your tests, the more you're like, oh, I got a pull request, and the tests passed. I feel okay to merge that, the faster you can make progress. So you're always programming with tests in mind, developing tests with that in mind, that if it passes, it should be good. And Twitter had a, not that it. Was impossible to make progress in the codebase. What other stuff can you say about the code base that made it difficult? What are some interesting quirks, broadly speaking, from that, compared to just your experience with gamma and everywhere else? The real thing that I spoke to a bunch of individual contributors at Twitter and I just asked, I'm like, okay, so what's wrong with this place? Why does this code look like this? And they explained to me what Twitter's promotion system was. The way that you got promoted to Twitter was you wrote a library that a lot of people used, right? So some guy wrote an Nginx replacement for Twitter. Why does Twitter need an nginx replacement? What was wrong with Nginx? Well, you see, you're not going to get promoted if you use Nginx. But if you write a replacement and lots of people start using it as the Twitter front end for their product, then you're going to get promoted. Right? So interesting, because, like, from an individual perspective, how do you incentivize, how do you create the kind of incentives that will reach or lead to a great code base? Okay, what's the answer to that? So what I do at comma and at Tiny Corp is you have to explain it to me. You have to explain to me what this code does. And if I can sit there and come up with a simpler way to do it. You have to rewrite it. You have to agree with me about the simpler way. Obviously, we can have a conversation about this. It's not dictatorial, but if you're like, wow, wait, that actually is way simpler. The simplicity is important, but that requires. People that overlook the code at the highest levels to be like, okay, it requires technical leadership. You trust. Yeah, technical leadership. So managers or whatever should have to have technical savvy, deep technical savvy managers. Should be better programmers than the people who they manage. Yeah. And that's not always obvious to trivial. To create, especially large companies, managers get. Soft and, like, you know, this is just, I've instilled this culture at comma, and Kama has better programmers than me who work there. But, you know, again, I'm like the old guy from good will hunting. It's like, look, man, I might not be as good as you, but I can see the difference between me and you. Right. And this is what you need. This is what you need at the top. Or you don't necessarily need the manager to be the absolute best. I shouldn't say that, but, like, they need to be able to recognize skill. Yeah. And have good intuition. Intuition that's laden with wisdom from all the battles of trying to reduce complexity in code bases. I took a political approach at comma, too, that I think is pretty interesting. I think Elon takes the same political approach. Google had no politics, and what ended up happening is the absolute worst kind of politics took over. Comma has an extreme amount of politics, and they're all mine, and no dissidence is tolerated. So it's a dictatorship. Yep. It's an absolute dictatorship. Elon does the same thing. Now, the thing about my dictatorship is, here are my values. Yeah. It's just transparent. It's transparent. It's a transparent dictatorship. And you can choose to opt in or you get free exit. That's the beauty of companies. If you don't like the dictatorship, you quit. So you mentioned rewrite before or refactor before features. If you were to refactor the Twitter codebase, what would that look like? And maybe also comment on how difficult is it to refactor. The main thing I would do is, first of all, identify the pieces and then put tests in between the pieces. Right? So there's all these different Twitter as a microservice architecture, there's all these different microservices. And the thing that I was working on there look like, you know, George didn't know any JavaScript. He asked how to fix search, blah, blah, blah, blah, blah. Look, man, like, the thing is, like, I just, you know, I'm upset that the way that this whole thing was portrayed because it wasn't like, it wasn't, like, taken by people. Like, honestly, it wasn't, like, by. It was taken by people who started out with a bad faith assumption. Yeah. And I mean, I. Look, I can't, like. And you as a programmer, just being transparent out there actually having, like, fun and, like, this is what programming should be about. I love that Elon gave me this opportunity. Yeah. Like, really? It does. And, like, you know, he came on my. The day I quit, he came on my Twitter spaces afterward and we had a conversation. Like, I just. I respect that so much. Yeah. And it's also inspiring to just engineers and programmers and just. It's cool. It should be fun. The people that were hating on it, it's like, oh, man, it was fun. It was fun. It was stressful. But I felt like, you know, it was at, like, a cool, like, point in history and, like, I hope I was useful. I probably kind of wasn't, but, like, maybe I was. Well, you also were one of the people that kind of made a strong case to refactor. Yeah. And that, that's a really interesting thing to raise. Like, maybe that is the right, you know, the timing of that is really interesting. If you look at just the development of autopilot, going from Mobileye to just. If you look at the history of semi autonomous driving in Tesla is more and more, you could say refactoring or starting from scratch, redeveloping from scratch. It's refactoring all the way down. The question is, can you do that sooner? Can you maintain product profitability and, like, what's the right time to do it? How do you do it? You know, on any one day, it's like you don't want to pull off the bandaids. Like, it's like everything works. It's just like little fix here and there, but maybe starting from scratch. This is the main philosophy of Tinygrad. You have never refactored enough. Your code can get smaller, your code can get simpler. Your ideas can be more elegant. But would you consider, you know, say you are like, running Twitter development teams, engineering teams, would you go as far as, like, different programming language, just go that far? I mean, the first thing that I would do is build tests. The first thing I would do is get a CI to where people can trust to make changes so that if you touched any code, I would actually say, no one touches any code. The first thing we do is we test this code base. I mean, this is classic. This is how you approach a legacy code base. This is like how to approach a legacy code base. Book will tell you so. And then you hope that there's modules that can live on for a while, and then you add new ones, maybe in a different language or before we. New ones, we replace old ones. Yeah, meaning like, replace old ones with something simpler. We look at this like this thing that's 100,000 lines, and we're like, well, okay, maybe this did even make sense in 2010, but now we can replace this with an open source thing. And we look at this here. Here's another 50,000 lines. Well, actually we can replace this with 300 lines ago. And you know what, I trust that the go actually replaces this thing because all the tests still pass. So step one is testing, and then step two is like, the programming language is an afterthought, right? Let a whole lot of people compete. Be like, okay, who wants to rewrite a module? Whatever language you want to write it in, just the tests have to pass. And if you figure out how to make the test pass but break the site, that's. We got to go back to step one. Step one is get tests that you trust in order to make changes in the code base. I wonder how hard it is too, because I'm with you on testing and everything from tests to like asserts to everything code is just covered in this because it should be very easy to make rapid changes and know that it's not going to break everything and that's the way to do it. But I wonder how difficult is it to integrate tests into a code base that doesn't have many of them? So I'll tell you what my plan was at Twitter. It's actually similar to something we use at comma. So at comma, we have this thing called process replay. We have a bunch of routes that'll be run through. So comma is a microservice architecture, too. We have microservices in the driving. We have one for the cameras, one for the sensor, one for the planner, one for the model, and we have an API which the microservices talk to each other with. We use this custom thing called serial, which uses ZMQ. Twitter uses thrift, and then it uses this thing called finagle, which is a Scala RPC backend. But this doesn't even really matter. The thrift and finagle layer was a great place, I thought, to write tests to start building something that looks like process replay. So Twitter had some stuff that looked kind of like this, but it wasn't offline. It was only online. So you could ship a modified version of it and then you could redirect some of the traffic to your modified version and diff those two. But it was all online. There was no CI in the traditional sense. There was some, but it was not full coverage. So you can't run all of Twitter offline to test something. Then this was another problem. You can't run all of Twitter. Right. Period. Twitter. Any one person can't. Twitter runs in three data centers. And that's it. Yeah. There's no other place you can run Twitter, which is like, george, you don't understand. This is modern software development. No, this is bullshit. Like, why can't it run on my laptop? What are you doing? Twitter can run it. Yeah. Okay, well, I'm not saying you're going to download the whole database to your laptop, but I'm saying all the middleware and the front end should run on my laptop. Right. That sounds really compelling. Yeah. But can that be achieved by a code base that grows over the years? I mean, the three data centers didn't have to be, right, because they're totally different, like designs. The problem is more like, why did the code base have to grow? What new functionality has been added to compensate for the lines of code that are there? One of the ways to explain is that the incentive for software developers to move up in the company is to add code, to add especially large. You know what? The incentive for politicians to move up on the political structure is to add laws. Same problem. Yeah, yeah. If the flip side is to simplify, simplify, simplify. I mean, you know what? This is something that I do differently from Elon with Kama about self driving cars. I hear the new version is going to come out and the new version is not going to be better, but at first, and it's going to require a ton of refactors. I say, okay, take as long as you need. You convinced me this architecture is better. Okay, we have to move to it, even if it's not going to make the product better tomorrow. The top priority is getting the architecture right. So what do you think about sort of a thing where the product is online? So how, I guess, would you do a refactor? If you ran engineering on Twitter, would you just do a refactor? How long would it take? What would that mean for the running of the, of the actual service? You know, and I'm not the right person to run Twitter. I'm just not. And that's the problem. Like. Like, I don't really know. I don't really know if that's, you know, a common thing that I thought a lot while I was there was whenever I thought something that was different to what Elon thought, I'd have to run something in the back of my head reminding myself that Elon is the richest man in the world, and in general, his ideas are better than mine. Now, there's a few things I think I do understand and know more about, but in general, I'm not qualified to run Twitter. I shouldn't say qualified, but I don't think I'd be that good at it. I don't think I'd be good at it. I don't think I'd really be good at running an engineering organization at scale. I think I could lead a very good refactor of Twitter, and it would take, like, six months to a year. And the results to show at the end of it would be feature development, in general takes ten x less time, ten x less man hours. That's what I think I could actually do. Do I think that it's the right decision for the business? Above my pay grade. Yeah, but a lot of these kinds of decisions are above everybody's pay grade. I don't want to be a manager. I don't want to do that. If you really forced me to. Yeah. It would make me maybe make me upset if I had to make those decisions. I don't want to. Yeah, but a refactor is so compelling. If this is to become something much bigger than what Twitter was, it feels like a refactor has to be coming at some point. George, you're a junior software engineer. Every junior software engineer wants to come in and refactor the whole code. Okay, that's, like, your opinion, man. Yeah, it doesn't. You know, sometimes they're right. Well, like, whether they're right or not, it's definitely not for that reason. Right. It's definitely not a question of engineering prowess. It is a question of maybe what the priorities are for the company. And I did get more intelligent, like, feedback from people, I think, in good faith, like, saying that from actually from Milan and, like, you know, from Milan, sort of like, people were like, well, you know, a stop the world refactor might be great for engineering, but you don't have a business to run. And, hey, above my pay grade. What'd you think about Elon as an engineering leader, having to experience him in the most chaotic of spaces. I would say. My respect for him is unchanged. And I did have to think a lot more deeply about some of the decisions he's forced to make, about the. Tensions within those, the trade offs within. Those decisions, about like a whole like, like matrix coming at him. I think that's Andrew Tate's word for it. Sorry to borrow it. Also bigger than engineering, just everything. Yeah. Like the war on the woke. Yeah, like it just, it just. Man. And like, he doesn't have to do this, you know, he doesn't have to. He could go like Parag and go chill at the four seasons of Maui, you know? But see, one person I respect and one person I don't. So his heart is in the right place fighting in this case, for this ideal of the freedom of expression. I wouldn't define the ideal so simply. I think you can define the ideal no more than just saying Elon's idea of a good world freedom of expression is. But to you it's still the downsides of that is the monarchy. Yeah, I mean monarchy has problems, right. But I mean, would I trade right now, the current oligarchy which runs America, for the monarchy? Yeah, I would. Sure. For the Elon monarchy. Yeah. You know why? Because power would cost one cent a kilowatt hour, tenth of a cent a kilowatt hour. What do you mean? Right now I pay about twenty cents a kilowatt hour for electricity in San Diego. That's like the same price you paid in 1980. What the hell? So you would see a lot of. Innovation with Elon, maybe have some hyperloops. Yeah, right. And I'm willing to make that trade off, right? I'm willing to make. And this is why, you know, people think that like dictators take power through some, like through some untoward mechanism, sometimes they do, but usually it's because the people want them and the downsides of a dictatorship. I feel like we've gotten to a point now with the oligarchy where, yeah, I would prefer the dictator. What do you think about Scala as a programming language? I liked it more than I thought. I did the tutorials. I was very new to it. It would take me six months to be able to write good scala. I mean, what did you learn about learning a new programming language from that? I love doing new programming tutorials and doing them. I did all this for Rust. Some of its upsetting JvM roots, but it is a much nicer. In fact, I almost don't know why Kotlin took off and not Scala. I think Scala has some beauty that caught Lynn lacked, whereas Kotlin felt a lot more. I mean, it was almost like, I don't know if it actually was a response to Swift, but that's kind of what it felt like. Like, Kotlin looks more like Swift, and Scala looks more like a functional programming language, more like an ocaml or Haskell. Let's actually just explore. We touched it a little bit, but just on the art, the science and the art of programming. For you personally, how much of your programming is done with GPT currently? None. None. I don't use it at all. Because you prioritize simplicity so much. Yeah, I find that a lot of it is noise I do use versus code, and I do like some amount of autocomplete. I do like, like a very, very, like, feels like rules based autocomplete. Like an autocomplete that's going to complete the variable name for me. So I'm just typing. I can just press tab, right? That's nice, but I don't want an autocomplete. You know what I hate when auto completes? When I type the word for and it like, puts, like two. Two parentheses and two semicolons and two braces, I'm like, oh, man, what? Versus code? And I GPT with codex, you can kind of brainstorm. I find I'm like, probably the same as you, but I like that it generates code and you basically disagree with it and write something simpler. But to me, that somehow is, like, inspiring or makes me feel good. It also gamifies the simplification process because I'm like, oh, yeah, you dumb AI system. You think this is the way to do it? I have a simpler thing here. It just constantly reminds me of bad stuff. I mean, I tried the same thing with rap, right? I tried the same thing with rap, and I actually think I'm a much better programmer than rapper, but I even tried, I was like, okay, can we get some inspiration from these things for some rap lyrics? And I just found that it would go back to the most, like, cringy tropes and dumb rhyme schemes. And I'm like, yeah, this is what the code looks like, too. I think you and I probably have different thresholds for cringe code. You probably hate cringe code, so it's for you. I mean, boilerplate. Is it as a part of code? Like, some of it, yeah. And some of it is just like faster lookup, because I don't know about you, but I don't remember everything. Like, I don't I'm offloading so much of my memory about, like. Yeah, different functions, library functions, all that kind of stuff like this GPT just is very fast at standard stuff and, like, standard library stuff, basic stuff that everybody uses. Yeah, I think that. I don't know. I mean, there's just so little of this in python. Maybe if I was coding more in other languages, I would consider it more, but I feel like Python already does such a good job of removing any boilerplate. That's true. It's the closest thing you can get to pseudocode, right? Yeah, that's true. That's true. And, yeah, sure, if I like. Yeah, I'm great. GPT. Thanks for reminding me to free my variables. Unfortunately, you didn't really recognize the scope correctly, and you can't free that one. But you put the freeze there and I get it. Fiverr. Whenever I've used Fiverr for certain things I design or whatever, it's always, you come back. I think that's probably closer. My experience with Fiverr is closer to your experience with programming with GPT is like, you just frustrated and feel worse about the whole process of design and art and whatever I used Fiverr for still. I just feel like later versions, GPT. I'm using GPT as much as possible to just learn the dynamics of it, like these early versions, because it feels like in the future you'll be using it more and more. And so, like, I don't want to be, like, for the same reason I gave away all my books and switched to kindle, because, like, all right, how long are we gonna have paper books, like 30 years from now? Like, I want to learn to be reading on Kindle, even though I don't enjoy it as much. And you learn to enjoy it more in the same way I switch from. Let me just pause. Switch from emacs to versus code. Yeah, I switch from vim to versus code. I think I similar, but I. Yeah, it's tough. And that Vim to versus code is even tougher because emacs is, like, old, like, more outdated. Feels like it. The community is more outdated. Vim is, like, pretty vibrant still. So I never used any of the plugins. I still don't use. That's what I looked at myself in the mirror. I'm like, yeah, you wrote some stuff in lisp. Yeah, no, but I never used any of the plugins in Vim either. I had the most vanilla vim. I have a syntax highlighter. I didn't even have autocomplete, like, these things I feel like, help you so marginally that, like. And now. Okay, now versus codes, autocomplete has gotten good enough that, like, okay, I don't have to set it up. I can just go into any code base and autocomplete is right 90% of the time. Okay, cool, I'll take it. Right. So I don't think I'm going to have a problem at all adapting to the tools once they're good. But, like, the real thing that I want is nothing. Something that, like, tab completes my code and gives me ideas. The real thing that I want is a very intelligent pair programmer that comes up with a little pop up saying, hey, you wrote a bug on line 14, and here's what it is. Yeah, now I like that. You know what does a good job of this? My PI. I love my PI. My PI, this fancy type checker for python. Yeah. And actually I tried, like, Microsoft released one, too, and it was like 60% false positives. My PI is like 5% false positives 95% of the time. It recognizes I didn't really think about that typing interaction correctly. Thank you. My PI. So you like type hinting? You like pushing the language towards being a typed language? Oh, yeah, absolutely. I think optional typing is great. I mean, look, I think that it's like a meet in the middle, right? Like, python has these optional type hinting and, like, c has auto. C allows you to take a step back. Well, c would have you brutally type out std string iterator. Right. Now I can type auto, which is nice. And then python used to just have a. What type is a? It's an a colon Str. Oh, okay. It's a string. Cool. Yeah, I wish there was a way, like a simple way in python to turn on a mode which would enforce the types. Yeah, like, give a warning when there's no type or something like this. Well, no, to give a warning where, like, my PI is a static type checker, but I'm asking just for a runtime type checker. Like, there's ways to, like, hack this in, but I wish it was just like a flag, like python three t. Oh, I see. Yeah, I see. Enforce the types of run time. Yeah, I feel like that makes you a better programmer. That's a kind of test, right? That the type remains the same? Well, that I know that I didn't, like, mess any types up. But again, like, my PI is getting really good and I love it, and I can't wait for some of these tools to become AI powered. Like, I want AI's reading my code and giving me feedback. I don't want AI's writing half assed autocomplete stuff for me. I wonder if you can now take GPT and give it a code that you wrote for a function and say, how can I make this simpler and have it accomplish the same thing? I think you'll get some good ideas on some code. Maybe not the code you write for tiny grad type of code, because that requires so much design thinking, but other kinds of code. I don't know. I downloaded the plugin maybe like two months ago. I tried it again and found the same look. I don't doubt that these models are going to first become useful to me, then be as good as me, and then surpass me. But from what I've seen today, it's like someone occasionally taking over my keyboard that I hired from fiverr. Yeah, I'd rather not. Ideas about how to debug the code or. Basically a better debugger is really interesting. I mean, I. But it's not a better debugger, I guess. I would love a better debugger. Yeah, it's not yet. Yeah, but it feels like it's not too far. Yeah, one of my coworkers says he uses them for print statements, like, every time he has to. Like, just like, when he needs the only thing it can really write is like, okay, I just want to write the thing to, like, print the state out right now. Oh, that definitely is much faster. It's print statements. Yeah, yeah, I see myself using that a lot. Just like, because it figures out what the rest of the function is. Just like, okay, print everything. Yeah, print everything. Right. And, yeah, like, if you want a pretty printer, maybe. I'm like, yeah, you know what I think? Like, I think in two years, I'm gonna start using these plugins a little bit, and then in five years, I'm gonna be heavily relying on some AI augmented flow. And then in ten years, do you. Think you'll ever get to 100% where the. Like, what's the role of the human that it converges to as a programmer? So you think it's all generated? Our niche becomes, oh, I think it's over for humans in general. It's not just programming, it's everything. The niche becomes, well, our niche becomes smaller, smaller, smaller. In fact, I'll tell you what the last niche of humanity is going to be. Yeah, there's a great book, and it's. If I recommended metamorphosis of prime intellect last time, there is a sequel called a casino odyssey in cyberspace. And I don't want to give away the ending of this, but it tells you what the last remaining human currency is, and I agree with that. We'll leave that as the cliffhanger. So no more programmers left, huh? That's where we're going. Well, unless you want handmade code, maybe they'll sell it on etsy. This is handwritten code. Doesn't have that machine polish to it. It has those slight imperfections that would only be written by a person. I wonder how far away we are from that. I mean, there's some aspect to, you know, on instagram, your title is listed as prompt engineering. Right. Thank you for noticing. I don't know if it's ironic or non or sarcastic or non. What do you think of prompt engineering as a scientific and engineering discipline or maybe, and maybe art form? You know what, I started, comma, six years ago, and I started the tiny corp a month ago. So much has changed. Like, I'm now thinking I'm now, like, I started, like, going through, like, similar comma processes to, like, starting a company. I'm like, okay, I'm gonna get an office in San Diego. I'm gonna bring people here. I don't think so. I think I'm actually gonna do remote. Right, George? You're gonna do remote? You hate remote. Yeah, but I'm not gonna do job interviews. The only way you're gonna get a job is if you contribute to the GitHub and then interacting through GitHub, GitHub being the real project management software for your company. And the thing pretty much just is a GitHub repo is showing me kind of what the future of, okay, so a lot of times I'll go on a discord or kindergarten discord, and I'll throw out some random, like, hey, can you change? Instead of having log and exp as ll ops, change it to log two and exp two. It's a pretty small change. You can just use change of base formula. That's the kind of task that I can see an AI being able to do in a few years. In a few years, I could see myself describing that, and then within 30 seconds, a pull request is up. That does it, and it passes my CI and I merge it. I really started thinking about, what is the future of jobs? How many AI's can I employ at my company? As soon as we get the first tiny box up, I'm going to stand up a 65 b lamade in the discord, and it's like, yeah, here's the tiny vox. He's just like, he's chilling with us. Basically, like you said, with niches, most human jobs will eventually be replaced with prompt engineering. Well, prompt engineering kind of is this, like, as you like, move up the stack. Right? Like, okay, there used to be humans actually doing arithmetic by hand. There used to be like big farms of people doing pluses and stuff. Right. And then you have spreadsheets and then, okay, the spreadsheet can do the plus for me. And then you have macros, and then you have things that basically just are spreadsheets under the hood, like accounting software. As we move further up the abstraction. Well, what's at the top of the abstraction stack? Well, prompt engineer. Yeah. What is the last thing if you think about humans wanting to keep control? Well, what am I really in the company but a prompt engineer? Right. Isn't there a certain point where the AI will be better at writing prompts? Yeah, but you see, the problem with the AI writing prompts. A definition that I always liked of AI was AI is the do what. I mean, machine. AI is not the. Like. The computer is so pedantic. It does what you say. So. But you want the do what I mean, machine. Yeah. Right. You want the machine where you say, you know, get my grandmother out of the burning house. It like reasonably takes your grandmother and puts her on the ground, not lifts her a thousand feet above the burning house and lets her fall. Right. But you don't. But it's not going to find the meaning. I mean, to do what? I mean, it has to figure stuff out. Sure. And the thing you'll maybe ask it to do is run government for me. Oh, and do what? I mean very much comes down to how aligned is that AI with you? Of course, when you talk to an AI that's made by a big company in the cloud, the AI fundamentally is aligned to them, not to you. And that's why you have to buy a tiny box. So you make sure the AI stays aligned to you. Every time that they start to pass AI regulation or GPU regulation, I'm going to see sales of tiny boxes spike. It's going to be like guns. Every time they talk about gun regulation, boom, gun sales. So in the space of AI, you're an anarchist anarchism espouser believer. I'm an informational anarchist, yes. I'm an informational anarchist and a physical statist. I do not think anarchy in the physical world is very good because I exist in the physical world. But I think we can construct this virtual world where anarchy. It can't hurt you, right? I love that. Tyler, the creator tweet, yo, cyberbullying isn't real, man. Have you tried. Turn it off the screen. Close your eyes. Like, yeah, but how do you prevent the AI from basically replacing all human prompt engineers where there's, it's like a self, like, where nobody's the prompt engineer anymore. So autonomy. Greater and greater autonomy until it's full autonomy. Yeah. And that's just where it's headed because one person is going to say, run everything for me. You see, I look at potential futures, and as long as the AI's go on to create a vibrant civilization with diversity and complexity across the universe, more power to them, I'll die. If the AI's go on to actually, like, turn the world into paperclips and then they die out themselves. Well, that's horrific and we don't want that to happen. So this is what I mean about, like, robustness. I trust robust machines. The current AI's are so not robust. Like, this comes back to the idea that we've never made a machine that can self replicate, right? But when we have, if the machines are truly robust and there is one prompt engineer left in the world. Hope you're doing good, man. Hope you believe in God. Like, you know, you know, go by God and I go help go forth and conquer the universe. Well, you mentioned, because I talked to Mark about faith and God and you said you were impressed by that. What's your own belief in God and how does that affect your work? You know, I never really considered, when I was younger, I guess my parents were atheists, so I was raised kind of atheist. I never really considered how absolutely silly atheism is because, like, I create worlds. Every, like, game creator. Like, how are you an atheist, bro? You create worlds. No one created our world, man. That's different. Haven't you heard about, like, the big bang and stuff? Yeah. I mean, what's the Skyrim myth origin story in Skyrim? I'm sure there's like some part of it in Skyrim, but it's not like if you ask the creators, like, the Big Bang is in universe, right? I'm sure they have some big bang notion in Skyrim, right? But that obviously is not at all how Skyrim was actually created. It was created by a bunch of programmers in a room, right? So, like, it struck me one day how just silly atheism is. Of course we were created by God. It's the most obvious thing. Yeah, that's such a nice way to put it. We're such powerful creators ourselves. It's silly not to conceive that there's creators even more powerful than us. Yeah. And then I also just like that notion. That notion gives me a lot of. I mean, I guess you can talk about what it gives a lot of religious people. It's kind of like. It just gives me comfort. It's like, you know, what if we mess it all up and we die out? Yeah. Yeah. In the same, the same way that a video game kind of has comfort. In it, God will try again. Or there's balance. Like somebody figured out a balanced view of it, like how to, like. So it's. It all makes sense in the end. Like, a video game is usually not going to have crazy, crazy stuff. You know, people will come up with like a. Well, yeah, but, like, man who created God, like, that's God's problem. I'm not gonna think this is what you're asking me. What if God, I'm just living God. I'm just this NPC living in this game. I mean, to be fair, like, if God didn't believe in God, he'd be as, you know, silly as the atheists here. What do you think is the greatest computer game of all time? Do you have any time to play games anymore? Have you played Diablo IV? I have not played Diablo IV. I will be doing that shortly. I have to. All right. There's so much history with one, two and three. You know what I'm gonna say? World of Warcraft. And it's not that the game is so. Such a great game. It's not. It's that I remember in 2005 when it came out of how it opened my mind to ideas. It opened my mind to this whole world we've created. There's almost been nothing like it since. You can look at mmos today, and I think they all have lower user bases than World of Warcraft. Eve online is kind of cool, but to think that everyone knows. People are always looking at the apple headset. What do people want in this VR? Everyone knows what they want. I want ready player one and like that. So I'm gonna say World of Warcraft. And I'm hoping that, like, games can get out of this whole mobile gaming dopamine pump thing and like, create worlds. Create worlds? Yeah. And worlds that captivate a very large fraction of the human population. Yeah. And I think it'll come back, I believe. But mmo, like really, really pull you in. Games do a good job. I mean, okay, other, like, two other games that I think are, you know, very noteworthy for me are Skyrim and GTA five. Skyrim, yeah, that's probably number one for me. GTA. Yeah. What is it about GTA? GTA is really. I mean, I guess GTA is real life. I know there's prostitutes and guns and stuff. Yes, I know. But it's how I imagine your life to be, actually. I wish it was that cool. Yeah, yeah, I guess that's, you know, because they're sims. Right. Which is also a game I like, but it's a gamified version of life. But it also is, I would love a combination of sims and GTA. So more freedom, more violence, more rawness, but with also, like, ability to have a career and family and this kind of stuff. What I'm really excited about in games is, like, once we start getting intelligent AI's to interact with. Oh, yeah. Like the NPC's and games have never. Been but conversationally in every way. In like. Yeah, in like, every way. Like when you're actually building a world and a world imbued with intelligence. Oh, yeah, right. And it's just hard. Like, there's just like. Like, you know, running World of Warcraft. Like, you're limited by what you're running on a pentium four. You know, how much intelligence can you run? How many flops did you have? Right. But now when I'm running a game on a hundred pay to flop machine, that's five people. I'm trying to make this a thing. 20 paid a flops of compute is one person of compute. I'm trying to make that a unit. 20 petaflops is one person. One person, one person flop. It's like a horsepower. What's a horsepower? That's how powerful a horse is. What's a. What's a person of compute? Well, you know, flop. I got it. That's interesting. VR also adds in terms of creating worlds. You know what bought a quest? Two. I put it on and I can't believe the first thing they show me is a bunch of scrolling clouds and a Facebook login screen. Yeah. You had the ability to bring me into a world. Yeah. And what did you give me? A pop up. Right. Like, well, I. And this is why you're not cool, Mark Zuckerberg. But you could be cool. Just make sure on the quest three, you don't put me into clouds and a Facebook login screen. Bring me to a world. I just tried quest three. It was awesome. But hear that, guys? I agree with that. So you know what? Because I. I mean, the beginning. What is it? Todd Howard said this about the design of the beginning of the games he creates is, like, the beginning is so, so, so important. I've recently played Zelda for the first time, Zelda, breath of the wild, the previous one. And, like, it's very quickly, you come out of this, like, within, like, 10 seconds, you come out of, like, a cave type place, and it's like this world opens up. It's like. And it, like. It pulls you in. You forget whatever troubles I was having. Whatever. Like, I gotta play that from the beginning. I played it for, like, an hour at a friend's house. Ah, no, the beginning. They got it. They did it really well. The expansiveness of that space, the peacefulness of that play. They got this. The music. I mean, so much of that is creating that world and pulling you right in. I'm gonna go. I'm gonna go buy a switch. Like, I'm gonna go today and buy a switch. You should. Well, the new one came out. I haven't played that yet. But Diablo four is something. I mean, there's sentimentality also, but something about VR really is incredible. But the. The new quest three is mixed reality, and I got a chance to try that. So it's augmented reality and video games. It's done really, really well. Is it pass through or cameras? Cameras. Cameras. Okay. Yeah. The apple one, is that one pass through or cameras? I don't know. I don't know how real it is. I don't know anything, you know, coming out in January. Is it January, or is it some point? Some point? Maybe not January. Maybe that's my optimism. But apple, I will buy it. I don't care if it's expensive and does nothing. I will buy it. I will support this future endeavor. You're the meme. Oh, yes. I support competition. It seemed like Quest was, like, the only people doing it, and this is great that they're like, you know what? And this is another place we'll give some more respect to. Mark Zuckerberg, the two companies that have endured through technology, or Apple and Microsoft. And what do they make? Computers and business services. All the memes, social ads, they all come and go. But you want to endure. Build hardware. Yeah. And that, you know, that does a really interesting job. I mean, I. Maybe I'm new with this, but I. It's a $500 headset. Quest three. And just having creatures run around the space, like, our space right here to me. Okay, this is very, like, boomer statement, but it added windows to the place. I heard about the aquarium. Yeah, yeah, aquarium. But in this case, it was a zombie game. Whatever. It doesn't matter, but just, like, it modifies the space in a way where I can't. It really feels like a window, and you can look out. It's pretty cool. Like, I was just, like, a zombie game. They're running at me. Whatever. But what I was enjoying is the fact that there's, like, a window and they're stepping on objects in this space. That was a different kind of escape. Also, because you can see the other humans. So it's integrated with the other humans. It's really. And that's why it's really interesting than ever that the AI is running on those systems are aligned with you. Oh, yeah. They're gonna augment your entire world. Oh, yeah. And that those AI's have a. I mean, you think about all the dark stuff, like. Like sexual stuff. Like, if those AI's threaten me, that could be haunting. Like. Like, if they, like, threat me in a non video game way. Like, they'll know personal information about me, and it's like. And then you lose track of what's real, what's not. Like, what if stuff is, like, hacked? There's two directions the AI girlfriend company can take, right? There's, like, the high brow, something like her, maybe, something you kind of talk to. And this is. And then there's the low brow version of it where. I want to set up a brothel in Times Square. Yeah. Yeah. It's not cheating if it's a robot. It's a VR experience. Is there an in between? No, I want to do that one or that one. Have you decided yet? No, I'll figure it out. We'll see. We'll see what the technology goes. I would love to hear your opinions for George's third company. What to do, the brothel in Times Square or the. Her experience. What do you think company number four will be? You think there will be a company number four? There's a lot to do in company number two. I'm just like. I'm talking about company number three now. None of that tech exists yet. There's a lot to do in company number two. Company number two is going to be the great struggle of the next six years. And of the next six years, how centralized is compute going to be? The less centralized compute is going to be the better of a chance we all have. So you're bearing. You're like the flag bearer for open source distributed decentralization of computer. We have to. We have to. Or they will just completely dominate us. I showed a picture on stream of a man in a chicken farm. Ever seen one of those, like, factory farm chicken farms? Why does he dominate all the chickens? Why does he. Smarter. He's smarter, right? Some people on twitch were like, he's bigger than the chickens. Yeah. And now here's a man in a cow farm, right? So it has nothing to do with their size and everything to do with their intelligence. And if one central organization has all the intelligence, you'll be the chickens, and they'll be the chicken man. But if we all have the intelligence, we're all the chickens. We're not all the man. We're all the chickens, and there's no chicken man. There's no chicken man. We're just chickens in Miami. He was having a good life, man. I'm sure he was. I'm sure he was. What have you learned from launching a running comma, AI and tiny Corp? So this starting a company from an idea and scaling it. And by the way, I'm all in on tiny box, so I'm your. I guess it's pre order. Only now. I want to make sure it's good. I want to make sure that the thing that I deliver is not going to be like a quest. Two, which you buy and use twice. I mean, it's better than a quest which you bought and used less than once, statistically. Well, if there's a beta program for tiny box, I'm into. Sounds good. I won't be the whiny, I'll be the tech savvy user of the tiny box just to be in. What have I learned in the early days? What have you learned from building these companies? The longest time at comma, I asked, why? Why, you know, why did I start a company? Why did I do this? But, you know, what else was I going to do? So you like, you like bringing ideas to life. With Kama, it really started as an ego battle with Elon. I wanted to beat him like I saw a worthy adversary. Here's a worthy adversary who I can beat at self driving cars. And I think we've kept pace and I think he's kept ahead. I think that's what's ended up happening there. But I do think profitable. And when this drive GPT stuff starts working, that's it. There's no more bugs. And a loss function. Like, right now, we're using, like, a hand coded simulator. There's no more bugs. This is going to be it. This is the run up to driving. I hear a lot of. Really, a lot of props for Openpilot. For comma, it's so. It's better than FSD and autopilot. In certain ways. It has a lot more to do with which feel you like. We lowered the price on the hardware to 14.99. You know how hard it is to ship reliable consumer electronics that go on your windshield? We're doing more than, like, most cell phone companies. How'd you pull that off, by the way? Shipping a product that goes in a car. I know. I have a, I have a, I have an SMT line. It's all. I make all the boards in house in San Diego. Quality control. I care immensely about it, actually, are. You're basically a mom and pop shop with great testing. Our head of open pilot is great at, like, you know, okay, I want all the comment theories to be identical. Yeah. And, yeah, I mean, you know, it's. Look, it's $14.99. It 30 day money back guarantee. It will. It will blow your mind at what it can do. Is it hard to scale? You know what? There's kind of downsides to scaling it. People are always like, why don't you advertise? Our mission is to solve self driving cars while delivering shippable intermediaries. Our mission has nothing to do with selling a million boxes. It's tawdry. Do you think it's possible that common gets sold? Only if I felt someone could accelerate that mission and wanted to keep it open source and not just wanted to. I don't believe what anyone says. I believe incentives. If a company wanted to buy comma, where their incentives were to keep it open source. But comma doesn't stop at the cars. The cars are just the beginning. The device is a human head. The device has two eyes, two ears. It breathes air. It has a mouth. So you think this goes to embodied robotics? We sell common bodies, too. They're very rudimentary. But one of the problems that we're running into is that the comma three has about as much intelligence as a b. If you want a human's worth of intelligence, you're going to need a tiny rack, not even a tiny box. You're going to need, like, a tiny rack, maybe even more. How does that. How do you put legs on that? You don't. And there's no way you can, you connect to it wirelessly. So you put your tiny box or your tiny rack in your house, and then you get your comma body, and your common body runs the models on that. It's close, right? It's not. You don't have to go to some cloud which is 30 milliseconds away. You go to a thing which is 0.1 milliseconds away. So the AI girlfriend will have like a central hub in the home. I mean, eventually, if you fast forward 2030 years, the mobile chips will get good enough to run these AI's. But fundamentally, it's not even a question of putting legs on a tiny box, because how are you getting 1.5 power on that thing? Right? So you need. They're very synergistic businesses. I also want to build all of comma's training computers. Like comma builds training computers. Right now we use commodity parts. I think I can do it cheaper. So we're gonna build. Tiny Corp is gonna not just sell tinyboxes. Tinybox is the consumer version, but I'll build training data centers, too. Have you talked to Andre Karpathy or have you talked to Elon about Tiny Corp? He went to work at OpenAI. What do you love about Andre Kapathi? To me, he's one of the truly special humans we got. Oh, man. Like, you know, his streams are just level of quality so far beyond mine. Like, I can't help myself. Like, it's just, it's just, you know. Yeah, he's good. He wants to teach you. Yeah. I want to show you that I'm smarter than you. Yeah, he has no, that's. I mean, thank you for the sort of the raw, authentic honesty. I mean, a lot of us have that. I think Andre is as legit as he gets in that he just wants to teach you. And there's a curiosity that just drives him. And just like at his. At the stage where he is in life, to be still, like one of the best tinkerers in the world. Yeah, it's crazy. Like, to. What is it? Micrograd. Micrograd was inspiration for Tinygrad. I mean, his CS 231 n was. This was, this was the inspiration. This is what I just took and ran with and ended up writing this. So, you know, but I mean, to. Me, that don't go work for Darth Vader, man. I mean, the flip side to me is that the fact that he's going there is a good sign for OpenAI, maybe. I think, you know, I like alias excava a lot. I like those. Those guys are really good at what they do. I know they are. And that's kind of what's even like more. And you know what? It's not that OpenAI doesn't open source, the weights of GPT four, it's that they go in front of Congress. And that is what upsets me. You know, we had two effective altruist sams go in front of Congress. One's in jail. I think you're drawing parallels on the one's in jail. You give me a look. Give me a look. No, I think. I think effect of altruism is a terribly evil ideology. Oh, yeah? That's interesting. Why do you think that is? Why do you think there's something about a thing that sounds pretty good that kind of gets us into trouble? Because you get Sam Bangman freed. Like, Sam Bangman freed is the embodiment of effective altruism. Utilitarianism is an abhorrent ideology. Like, well, yeah, we're gonna kill those three people to save a thousand. Of course. Yeah, right. There's no underlying. Like, there's just. Yeah, yeah. But to me, that's a bit surprising. But it's also, in retrospect, not that surprising. But I haven't heard really clear, kind of like rigorous analysis why effective altruism is flawed. Oh, well, I think charity is bad. Right? So what is charity but investment that you don't expect to have a return on. Right? Yeah, but you can also think of charity as, like you would like to see. So allocate resources in optimal way to make a better world. And probably almost always that involves starting a company. Yeah, right, because more efficient. Yeah. If you just take the money and you spend it on malaria nets. Okay, great. You've made 100 malaria nets. But if you teach no man how to fish, right? Yeah, no, but the problem is teaching a man how to fish might be harder. Starting a company might be harder than allocating money that you already have. I like the flip side of effective altruism. Effective accelerationism. I think accelerationism is the only thing that's ever lifted people out of poverty. The fact that food is cheap, not. We're giving food away because we are kind hearted people. No food is cheap. And that's the world you want to live in. Ubi, what a scary idea. What a scary idea. All your power now, your money is power. Your only source of power is granted to you by the goodwill of the government. What a scary idea. So you even think long term, even. Uh, I'd rather die than need Ubi to survive. And I mean it. What if survival is basically guaranteed? What if our life becomes so good. You can make survival guaranteed without Ubi? What you have to do is make housing and food dirt cheap. Right. Like, and that's the good world. And actually, let's go into what we should really be making dirt cheap, which is energy that. Energy that, you know. Oh, my God. Like, you know, that. That's. If there's one. I'm pretty centrist politically. If there's one political position I cannot stand, it's deceleration. It's people who believe we should use less energy. Yeah. Not people who believe global warming is a problem. I agree with you. Not people who believe that, you know, saving the environment is good. I agree with you. But people who think we should use less energy, that energy usage is a moral bad. No, no. You are asking. You are. You are diminishing humanity. Yeah. Energy is flourishing of creative flourishing of the human species. How do we make more of it? How do we make it clean? And how do we make. Just. Just how do I pay $0.20 for a megawatt hour instead of a kilowatt hour? Part of me wishes that Elon went into nuclear fusion versus Twitter. Pardon me? Or somebody. Somebody like Elon. You know, we need to. I wish there were more elons in the world. I think Elon sees it as, like, this is a political battle that needed to be fought. And again, I always ask the question of whenever I disagree with him, I remind myself that he's a billionaire and I'm not. So, you know, maybe he's got something figured out that I don't, or maybe. He doesn't, to have some humility. But at the same time, me, as a person who happens to know him, I find myself in that same position. And sometimes even billionaires need friends who disagree and help them grow, and that's a difficult reality. And it must be so hard. It must be so hard to meet people once you get to that point. Where fame, power, money, everybody's sucking up to you. See, I love not having shit. Like, I don't have shit, man. You know? Trust me, there's nothing I can give you. There's nothing worth taking from me, you know? Yeah. It takes a really special human being when you have power, when you have fame, we have money to still think from first principles. Not like all the adoration you get towards you, all the admiration, all the people saying, yes, yes, yes. Then all the hate and the hate, and that's worse. So the hate makes you want to go to the yes people because the hate exhausts you. And the kind of hate that Elon's gotten from the left is pretty intense. And so that, of course, drives him right and loses balance. And it keeps this absolutely fake, like, psyop political divide alive so that the 1% can keep power. Like, yeah, I wish we'd be less divided because it is giving power to the ultra powerful. The rich get richer. You have love in your life. Has love made you a better or a worse programmer? Do you keep productivity metrics? No, no, no, I'm not. Not that. I'm not that methodical. I think that there comes to a point where if it's no longer visceral, I just can't enjoy it. I guess you viscerally love programming? The minute I started. Like, so that's one of the big loves of your life, is programming? Oh, I mean, just my computer in general. I mean, you know, I tell my girlfriend my first love is my computer, of course. Right? Like, you know, I sleep with my computer. It's there for a lot of my sexual experiences. Like, come on, so is everyone's right. Like, you know, you gotta be real about that. And, like, not just like the ide for programming, just the entirety of the computational machine. The fact that. Yeah, I mean, it's, you know, I wish it was. Someday they'll be smarter, and someday, you know, maybe I'm weird for this, but I don't discriminate, man. I'm not going to discriminate. Bio stack life and silicon stack life. Like, so the moment the computer starts to say, like, I miss you, I started to have some of the basics of human intimacy, it's over for you. The moment. Versus code says, hey, George. No. You see? No, no. But versus code is. No, they're just doing that. Microsoft's doing that to try to get me hooked on it. I'll see through it. I'll see through it. It's gold digger, man. It's gold digger. Look, me, an open source. Well, this just gets more interesting, right? If it's open source. And. Yeah, though Microsoft done a pretty good job on that. Oh, absolutely. No, no, no. Look, I think Microsoft, again, I wouldn't count on it to be true forever, but I think right now, Microsoft is doing the best work in the programming world. Like, between GitHub. GitHub actions versus code, the improvements to python, it works. Microsoft. Who would have thought? Microsoft and Mark Zuckerberg are spearheading the open source movement. Right? Right. How things change. Oh, it's beautiful, by the way. That's who I'd bet on to replace Google, by the way. Who? Microsoft. Microsoft. Satya. Nadella said, straight up, I'm coming for it. Interesting. So your bet, who wins AGI. Oh, I don't know about AGI. I think we're a long way away from that. But I would not be surprised if, in the next five years, bing overtakes Google as a search engine. Interesting. Wouldn't surprise me. Interesting. I hope some startup does. It might be some startup, too. I would equally bet on some startup. Yeah. I'm like 50 50, but maybe that's naive. I believe in the power of these language models. Satya's alive. Microsoft's alive. Yeah, it's great. It's great. I like all the innovation in these companies. They're not being stale, and to the degree they're being stale, they're losing. So there's a huge incentive to do a lot of exciting work and open source work, which is. This is incredible. Only way to win. You're older, you're wiser. What's the meaning of life, George Hotz? To win is still to win, of course. Always. Of course. What's winning look like for you? I don't know. I haven't figured out what the game is yet, but when I do, I want to win. So it's bigger than solving self driving. It's bigger than democratizing, decentralizing computer. I think the game is to stand eye to eye with God. I wonder what that means for you at the end of your life, what that would look like. I mean, this is what, like, I don't know. This is some. This is some. This is probably some ego trip of mine, you know? Like, if you want to stand eye to eye with God, this blasphemous man. Okay. I don't know. I don't know. I don't know if it would upset God. I think he, like, wants that. I mean, I certainly want that for my creations. I want my creations to stand eye to eye with me. So why wouldn't God want me to stand eye to eye with him? That's the best I can do. Golden rule. I'm just imagining the creator of a video game having to look stand eye to eye with one of the characters. I only watched season one of Westworld, but, yeah, we got to find the maze and solve it. Like, yeah, I wonder what that looks like. It feels like a really special time in human history where that's actually possible. Like, there's something about AI that's like, we're playing with something weird here, something really weird. I wrote a blog post. I reread Genesis and just looked like they give you some clues at the end of Genesis for finding the Garden of Eden. And I'm interested. I'm interested. Well, I hope you find just that. George, you're one of my favorite people. Thank you for doing everything you're doing, and in this case, for fighting for open source or for decentralization of AI. It's a. It's a fight worth fighting. Fight worth winning. Hashtag I love you, brother. These conversations are always great. Hope to talk to you many more times. Good luck with tiny corp. Thank you. Great to be here. Thanks for listening to this conversation with George Hotz. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Albert Einstein. Everything should be made as simple as possible, but not simpler. Thank you for listening and hope to see you next time.

Utterances:
Speaker A: What possible ideas do you have for how human species ends?
Speaker B: Sure. So I think the most obvious way to me is wireheading. We end up amusing ourselves to death. We end up all staring at that infinite TikTok and forgetting to eat. Maybe it's even more benign than this. Maybe we all just stop reproducing. Now, to be fair, it's probably hard to get all of humanity.
Speaker A: Yeah. The interesting thing about humanity is the diversity in it. Oh yeah, organisms in general. There's a lot of weirdos out there. Two of them are sitting here.
Speaker B: I mean diversity in humanity is we do respect. I wish I was more weird.
Speaker A: The following is a conversation with George Hotz, his third time on this podcast. He's the founder of Comma AI that seeks to solve autonomous driving and is the founder of a new company called Tiny Corp that created Tinygrad, a neural network framework that is extremely simple with the goal of making it run on any device by any human, easily and efficiently. As you know, George also did a large number of fun and amazing things, from hacking the iPhone to recently joining Twitter for a bit as an intern in quotes making the case for refactoring the Twitter codebase. In general, hes a fascinating engineer and human being. And one of my favorite people to talk to. This is Alex Friedman podcast to support it. Please check out our sponsors in the description. And now dear friends, heres George Hotts. You mentioned something in a stream about the philosophical nature of time. So lets start with a wild question. Do you think time is an illusion?
Speaker B: You know, I sell phone calls to comma for $1,000 and some guy called me and like, you know it's a $1,000, you can talk to me for half an hour. And he's like yeah, okay, so like time doesn't exist. And I really wanted to share this with you. I'm like, what do you mean time doesn't exist? Right? Like I think time is a useful model whether it exists or not, right? Like does quantum physics exist? Well it doesn't matter to its about whether its a useful model to describe reality. Is time maybe compressive?
Speaker A: Do you think there is an objective reality or is everything just useful models like underneath it all? Is there an actual thing that were constructing models for.
Speaker B: I dont know.
Speaker A: I was hoping you would know.
Speaker B: I dont think it matters.
Speaker A: I mean this kind of connects to the models of constructive reality with machine learning, right?
Speaker B: Sure.
Speaker A: Like is it just nice to have useful approximations of the world such that we can do something with it?
Speaker B: So there are things that are real. Column graph complexity is real. Yeah. The compressive math is real. Yeah.
Speaker A: This should be a t shirt.
Speaker B: And I think hard things are actually hard. I don't think P equals np.
Speaker A: Ooh, strong words.
Speaker B: Well, I think that's the majority. I do think factoring is in P.
Speaker A: But I don't think you're the person that falls the majority in all walks of life, so it's good for that one.
Speaker B: I do, yeah.
Speaker A: In theoretical computer science, you're one of the sheep, all right. But to you, time is a useful model.
Speaker B: Sure.
Speaker A: What were you talking about on the stream with time? Are you made of time?
Speaker B: If I remembered half the things I said on stream. Someday someone's going to make a model of all of it, and it's going to come back to haunt me.
Speaker A: Someday soon.
Speaker B: Yeah, probably.
Speaker A: Would that be exciting to you or sad that there's a George Hots model?
Speaker B: I mean, the question is, when the George Hotts model is better than George Hotts, like, I am declining and the.
Speaker A: Model is growing, what is the metric by which you measure better or worse in that? If you're competing with yourself, maybe you.
Speaker B: Can just play a game where you have the George Hotts answer and the George Hotts model answer and ask which.
Speaker A: People prefer people close to you or strangers?
Speaker B: Either one. It will hurt more when it's people close to me, but both will be overtaken by the George Hotts model.
Speaker A: It'd be quite painful, right? Loved ones, family members would rather have the model over for Thanksgiving than you.
Speaker B: Yeah.
Speaker A: Or, like significant others, would rather sexed with the large language model version of.
Speaker B: You, especially when it's fine tuned to their preferences.
Speaker A: Yeah, well, that's what we're doing in a relationship, right? We're just fine tuning ourselves, but we're inefficient with it because we're selfish. Ingredients on our language. Models can fine tune more efficiently, more selflessly.
Speaker B: There's a Star Trek Voyager episode where Catherine Janeway, lost in the Delta Quadrant, makes herself a lover on the holodeck, and the lover falls asleep on her arm, and he snores a little bit, and Janeway edits the program to remove that. And then, of course, the realization is, wait, this person's terrible. It is, actually all their nuances and quirks and slight annoyances that make this relationship worthwhile. But I don't think we're gonna realize that until it's too late.
Speaker A: Well, I think a large language model could incorporate the. The flaws and the quirks and all that kind of stuff.
Speaker B: Just the perfect amount of quirks and flaws to make you charming without crossing the line.
Speaker A: Yeah, yeah. And that's probably a good, like, approximation of the. Like, the percent of time the language model should be cranky or an asshole or jealous or all this kind of stuff.
Speaker B: And of course it can and it will, but all that difficulty at that point is artificial. There's no more real difficulty.
Speaker A: Okay, what's the difference between real and artificial?
Speaker B: Artificial difficulty is difficulty that's like, constructed or could be turned off with a knob. Real difficulty is like you're in the woods and you gotta survive.
Speaker A: So if something can not be turned off with a knob, it's real?
Speaker B: Yeah, I think so. Or, I mean, you can't get out of this by smashing the knob with a hammer. I mean, maybe you kind of can, you know, into the wild when, you know, Alexander, super tramp, he wants to explore something that's never been explored before, but it's the nineties. Everything's been explored. So he's like, well, I'm just not going to bring a map. Yeah, I mean, no, you're not exploring. You should have brought a map. Dude, you died. There was a bridge a mile from where you were camping.
Speaker A: How does that connect to the metaphor of the knob?
Speaker B: By nothing. Bringing the map. You didn't become an explorer, you just smashed the thing. Yeah, yeah. The difficulty is still artificial.
Speaker A: You failed before you started. What if we just don't have access to the knob?
Speaker B: Well, that maybe is even scarier. Right? Like, we already exist in a world of nature, and nature has been fine tuned over billions of years. To have humans build something and then throw the knob away in some grand romantic gesture is horrifying.
Speaker A: Do you think of us humans as individuals that are, like, born and die, or are we just all part of one living organism? That is earth, that is nature?
Speaker B: I don't think there's a clear line there. I think it's all kind of just fuzzy. I don't know. I mean, I don't think I'm conscious of. I don't think I'm anything. I think I'm just a computer program.
Speaker A: So it's all computation. I think running in your head is just computation.
Speaker B: Everything running in the universe is computation, I think. I believe the extended church Turing thesis.
Speaker A: Yeah, but there seems to be an embodiment to your particular computation. Like there's a consistency.
Speaker B: Well, yeah, but I mean, models have consistency, too.
Speaker A: Yeah.
Speaker B: Models that have been RLHF'd will continually say, you know, like, well, how do I murder ethnic minorities. Oh, well, I can't let you do that, Hal. There's a consistency to that behavior.
Speaker A: It's all RLHF. Like we RLHF each other. We. We find, we provide human feedback and there that thereby fine tune these little pockets of computation. But it's still unclear why that pocket of computation stays with you, like, for years. It just kind of falls like you have this consistent set of physics, biology, what, like whatever you call the neurons firing, the electrical signals, the mechanical signals, all of that that seems to stay there, and it contains information, it stores information, and that information permeates through time and stays with you. There's like, memory, it's like, sticky.
Speaker B: Okay, to be fair, like, a lot of the models we're building today are very even. RLHF is nowhere near as complex as.
Speaker A: The human loss reinforcement learning with human feedback.
Speaker B: When I talked about will GPT twelve be AGI? My answer is no, of course not. I mean, cross entropy loss is never going to get you there. You need probably RL in fancy environments in order to get something that would be considered AGI, like. So to ask the question about why. I don't know, it's just some quirk of evolution, right? I don't think there's anything particularly special about where I ended up. Where humans ended up.
Speaker A: It's okay, we have human level intelligence. Would you call that AGI? Whatever we have gi.
Speaker B: Look, actually, I don't really even like the word AGI, but general intelligence is defined to be whatever humans have.
Speaker A: Okay, so why can GPT twelve not get us to AGI? Can we just, like, linger on that?
Speaker B: If your loss function is categorical, cross entropy. If your loss function is just try to maximize compression. I have a soundcloud. I rap, and I tried to get chat GPT to help me write raps, and the raps that it wrote sounded like YouTube comment raps. You can go on any rap beat online and you can see what people put in the comments, and it's the most mid quality rap you can find.
Speaker A: Is made good or bad?
Speaker B: Made is bad. It's like mid.
Speaker A: Every time I talk to you, I learn new words. Mid? Yeah, I was like, is it like basic? Is that what mid means?
Speaker B: Kind of. It's like middle of the curve, right? So there's like, there's like that intelligence curve, and you have like, the dumb guy, the smart guy, and then the mid guy. Actually being the mid guy's the worst. The smart guy is like, I put all my money in bitcoin. The mid guy is like, you can't put money in bitcoin. It's not real money.
Speaker A: And all of it is a genius meme. That's another interesting one, memes. The humor, the idea, the absurdity encapsulated in a single image, and it just kind of propagates virally between all of our brains. I didn't get much sleep last night, so I'm very. I sound like I'm high. I swear I'm not. Do you think we have ideas or ideas have us?
Speaker B: I think that we're going to get super scary memes once the AI's actually are superhuman.
Speaker A: Do you think AI will generate memes?
Speaker B: Of course.
Speaker A: You think it'll make humans laugh?
Speaker B: I think it's worse than that. So, infinite jest, it's introduced in the first 50 pages, is about a tape that you, once you watch it once, you only ever want to watch that tape. In fact, you want to watch the tape so much that someone says, okay, here's a hacksaw. Cut off your pinky and then I'll let you watch the tape again and you'll do it. So we're actually going to build that, I think. But it's not going to be one static tape. I think the human brain is too complex to be stuck in one static tape like that. If you look at, like, ant brains, maybe they can be stuck on a static tape, but we're going to build that using generative models. We're going to build the tick tock that you actually can't look away from.
Speaker A: So TikTok is already pretty close there. But the generation is done by humans. The algorithm is just doing their recommendation. But if it's. If the algorithm was also able to do the generation well, it's a question.
Speaker B: About how much intelligence is behind it. Right? So the content is being generated by, let's say, one humanity worth of intelligence. And you can quantify a humanity, right. That's a, you know, it's. It's exaflops, yada flops, but you can quantify it. Once that generation is being done by 100 humanities, you're done.
Speaker A: So it's actually scale that's the problem. But also speed. Yeah. And what if it's sort of manipulating the very limited human dopamine engine for porn? Imagine just TikTok, but for porn.
Speaker B: Yeah.
Speaker A: It's like a brave new world.
Speaker B: I don't even know what it'll look like. Right. Like, again, you can't imagine the behaviors of something smarter than you. But a super intelligent, an agent that just dominates your intelligence so much will be able to completely manipulate you.
Speaker A: Is it possible that it won't really manipulate, it'll just move past us. It'll just kind of exist. The way water exists or the air exists.
Speaker B: You see, and that's the whole AI safety thing. It's not the machine that's going to do that. It's other humans using the machine that are going to do that to you.
Speaker A: Yeah, because the machine is not interested in hurting humans.
Speaker B: It's just the machine is a machine. Yeah, but the human gets the machine, and there's a lot of humans out there very interested in manipulating you.
Speaker A: Well, let me bring up Eliezer Jacowski, who recently sat where you're sitting. He thinks that AI will almost surely kill everyone. Do you agree with him or nothing?
Speaker B: Yes, but maybe for a different reason.
Speaker A: Okay, and then I'll try to get you to find hope where we could find a no to that answer. But why? Yes.
Speaker B: Okay. Why didn't nuclear weapons kill everyone?
Speaker A: That's a good question.
Speaker B: I think there's an answer. I think it's actually very hard to deploy nuclear weapons tactically. It's very hard to accomplish tactical objectives. Great, I can nuke their country. I have an irradiated pile of rubble. I don't want that.
Speaker A: Why not?
Speaker B: Why don't I want an irradiated pile of rubble? For all the reasons no one wants an irradiated pile of rubble?
Speaker A: Because you can't use that land for resources. You can't populate the land.
Speaker B: Yeah. What you want, a total victory in a war, is not usually the irradiation and eradication of the people there, it's the subjugation and domination of the people.
Speaker A: Okay, so you can't use this strategically, tactically, in a war to help you, to help gain a military advantage. It's all complete destruction. All right? But there's egos involved. It's still surprising. Still surprising that nobody pressed the big red button.
Speaker B: It's somewhat surprising. But you see, it's the little red button that's going to be pressed with AI. That's going to. And that's why we die. It's not because the AI. If there's anything in the nature of AI, it's just the nature of humanity.
Speaker A: What's the algorithm behind the little red button? What possible ideas do you have for how human species ends?
Speaker B: Sure. So I think the most obvious way to me is wireheading. We end up amusing ourselves to death. We end up all staring at that infinite TikTok. And forgetting to eat. Maybe. Maybe it's even more benign than this. Maybe we all just stop reproducing. Now, to be fair, it's probably hard to get all of humanity.
Speaker A: Yeah.
Speaker B: Yeah. It probably.
Speaker A: Like the. The interesting thing about humanity is the diversity in it.
Speaker B: Oh, yeah.
Speaker A: Organisms in general. There's a lot of weirdos out there. Two of them are sitting here.
Speaker B: I mean, diversity in humanity is. With due respect, I wish I was more weird. No, like, I'm kind of. Look, I'm drinking smart water, man. That's like a Coca Cola product, right?
Speaker A: Do you want corporate, George Hoss?
Speaker B: No. The amount of diversity in humanity, I think, is decreasing, just like all the other biodiversity on the planet.
Speaker A: Oh, boy. Yeah, right. Social media is not helping, huh?
Speaker B: Go eat McDonald's in China.
Speaker A: Yeah.
Speaker B: Yeah. No, it's the interconnectedness that's doing it.
Speaker A: Oh, that's interesting. So everybody starts relying on the connectivity of the Internet, and over time, that reduces the diversity, the intellectual diversity, and then that gets everybody into a funnel. There's still going to be a guy in Texas.
Speaker B: There is. And. Yeah. Bunker, to be fair, do I think AI kills us all? I think AI kills everything we call, like, society today. I do not think it actually kills the human species. I think that's actually incredibly hard to do.
Speaker A: Yeah, but society, like, if we start over, that's tricky. Most of us don't know how to do most things.
Speaker B: Yeah, but some of us do, and they'll be okay and they'll rebuild after they, uh. Great. AI.
Speaker A: What's rebuilding look like, how far, like, how much do we lose? Like, what has human civilization done that's interesting? Combustion engine. Electricity. So power and energy. That's interesting. Like how to harness energy.
Speaker B: Well, they're going to be religiously against that.
Speaker A: Are they going to get back to, like, fire?
Speaker B: Sure. I mean, there'll be a. There'll be, it'll be like, you know, some kind of amish looking kind of thing. I think. I think they're going to have very strong taboos against technology.
Speaker A: Like, technology is almost like a new religion. Technology is the devil and nature is God.
Speaker B: Sure.
Speaker A: So closer to nature. But can you really get away from AI? If it destroyed 99% of the human species, isn't it somehow have a hold, like a stronghold?
Speaker B: What's interesting about everything we build? I think we're going to build super intelligence before we build any sort of robustness in the AI. We cannot build an AI that is capable of going out into nature and surviving. Like a bird. Right? A bird is an incredibly robust organism. We've built nothing like this. We haven't built a machine that's capable of reproducing.
Speaker A: Yes, but there's, you know, I work with leg robots a lot now. I have a bunch of them. They're mobile, they can't reproduce, but all they need is, I guess you're saying they can't repair themselves, but if you have a large number, if you have like 100 million of them, let's just.
Speaker B: Focus on them reproducing. Right. They have microchips in them. Okay, then do they include a fab?
Speaker A: No.
Speaker B: Then how are they going to reproduce?
Speaker A: It doesn't have to be all on board. Right. They can go to a factory, to a repair shop.
Speaker B: Yeah, but then you're really moving away from robustness.
Speaker A: Yes.
Speaker B: All of life is capable of reproducing without needing to go to a repair shop. Life will continue to reproduce in the complete absence of civilization, robots will not. So when the, if the AI apocalypse happens, I mean, the AI's are going to probably die out because I think we're going to get, again, super intelligence long before we get robustness.
Speaker A: What about if you just improve the fab to where you just have a 3d printer that can always help you.
Speaker B: Well, that'd be very interesting. I'm interested in building that.
Speaker A: Of course you are. You think, how difficult is that problem to have a robot that basically can.
Speaker B: Build itself very, very hard.
Speaker A: I think you've mentioned this like to me, or somewhere where people think it's.
Speaker B: Easy conceptually, and then they remember that you're going to have to have a fab.
Speaker A: Yeah. On board, of course. So 3d printer. That prints a 3d printer? Yeah, yeah. On legs. Why is that hard?
Speaker B: Well, because it's not. I mean, a 3d printer is a very simple machine. Right. Okay. You're going to print chips, you're going to have an atomic printer. How are you going to dope the silicon? Yeah, right. How are you going to etch the silicon?
Speaker A: You're gonna have to have a very interesting kind of fab if you want to have a lot of computation on board. But you can do like structural type of robots that are dumb.
Speaker B: Yeah, but structural type of robots aren't gonna have the intelligence required to survive in any complex environment.
Speaker A: What about like ants type of systems? We have like trillions of them.
Speaker B: I don't think this works. I mean, again, like ants at their very core are made up of cells that are capable of individually reproducing.
Speaker A: They're doing quite a lot. A lot of computation that we're taking for granted.
Speaker B: It's not even just the computation, it's that reproduction is so inherent. Okay, so, like, there's two stacks of life in the world. There's the biological stack and the silicon stack. The biological stack starts with reproduction. Reproduction is at the absolute core, the first proto rna organisms we're capable of reproducing. The silicon stack, despite, as far as it's come, is nowhere near being able to reproduce.
Speaker A: Yeah. So the fab movement, digital fabrication, fabrication in the full range of what that means is still in the early stages.
Speaker B: Yeah.
Speaker A: You're interested in this world even if.
Speaker B: You did put a fab on the machine, right? Let's say, okay, we can build fabs. We know how to do that as humanity. We can probably put all the precursors that build all the machines and the fabs also in the machine. So first off, this machine is going to be absolutely massive. I mean, we almost have a, like, think of the size of the thing required to reproduce a machine today, right? Like, is our civilization capable of reproduction? Can we reproduce our civilization on Mars.
Speaker A: If we were to construct a machine that is made up of humans? Like a company that can reproduce itself? Yeah, I don't know. It feels like 115 people.
Speaker B: I think it's so much harder than that.
Speaker A: 120. I just look at our number.
Speaker B: I believe that Twitter can be run by 50 people. I think that this is going to take most of, like, it's just most of society, right? Like, we live in one globalized world.
Speaker A: No, but you're not interested in running Twitter. You're interested in seeding. Like, you want to see the civilization, and then. Because humans can, like, oh, okay.
Speaker B: You're talking about. Yeah, okay, so you're talking about the humans reproducing and, like, basically, like, what's the smallest self sustaining colony of humans? Yeah, yeah, okay, fine. But they're not going to be making five nanometer chips over time, they will.
Speaker A: I think you're being like, we have to expand our conception of time here, going back to the original time scale. I mean, over, across maybe 100 generations, we're back to making chips. No, if you seed the colony correctly.
Speaker B: Maybe, or maybe they'll watch our colony die out over here and be like, we're not making chips. Don't make chips.
Speaker A: No, but you have to seed that colony correctly.
Speaker B: Whatever you do, don't make chips. Chips are what led to their downfall.
Speaker A: Well, that is the thing that humans do. They. They come up, they construct a devil, a good thing and a bad thing, and they really stick by that, and they murder each other over that. There's always one asshole in the room who murders everybody. And he usually makes tattoos and nice branding.
Speaker B: Do you need that asshole? That's the question, right? Humanity works really hard today to get rid of that asshole. But I think they might be important.
Speaker A: Yeah, this whole freedom of speech thing, it's the freedom of being an asshole seems kind of important, man. This thing, this fab, this human fab that we constructed, this human civilization, is pretty interesting. And now it's building artificial copies of itself, or artificial copies of various aspects of itself that seem interesting, like intelligence. And I wonder where that goes.
Speaker B: I like to think it's just like another stack for life, like we have, like the bio stack life, like we're a bio stack life, and then the silicon stack life.
Speaker A: But it seems like the ceiling, or there might not be a ceiling, or at least the ceiling is much higher for the silicon stack.
Speaker B: Oh, no. We don't know what the ceiling is for the bio stack either. The bio stack. The bio stack just seemed to move slower. You have Moore's law, which is not dead, despite many proclamations.
Speaker A: In the bio stack or the silicon stuff?
Speaker B: In the silicon stack. And you don't have anything like this in the bio stack. So I have a meme that I posted. I tried to make a meme. It didn't work too well, but I posted a picture of Ronald Reagan and Joe Biden. And you look, this is 1980, and this is 2020, and these two humans are basically, like, the same. Right? There's no. There's no like. There. There's been no change in humans in the last 40 years.
Speaker A: Yeah.
Speaker B: And then I posted a computer from 1980 and a computer from 2020. Wow.
Speaker A: Yeah. With their early, early stages. Right, which is why you said when you said the fab, the size of the fab required to make another fab is like, uh, very large right now.
Speaker B: Oh, yeah.
Speaker A: But computers were very large 80 years ago, and they got pretty tiny, and people are starting to want to wear them on their face in order to escape reality. That's the thing. In order to be live inside the computer there. Put a screen right here. I don't have to see the rest of you assholes.
Speaker B: I've been ready for a long time.
Speaker A: You like virtual reality?
Speaker B: I love it.
Speaker A: Do you want to live there?
Speaker B: Yeah.
Speaker A: Yeah. Part of me does too. How far away are we, do you think?
Speaker B: Judging from what you can buy today, far, very far.
Speaker A: I gotta tell you that I had the experience of Meta's codec avatar, where it's an ultra high resolution scan. It looked real.
Speaker B: I mean, the headsets just are not quite at, like, eye resolution yet. I haven't put on any headset where I'm like, oh, this could be the real world. Whereas when I put good headphones on, audio is there, like, we. We can reproduce audio that I'm like, I'm actually in a jungle right now. If I close my eyes, I can't tell. I'm nothing.
Speaker A: Yeah, but then there's also smell and all that kind of stuff.
Speaker B: Sure.
Speaker A: I don't know. I. The power of imagination or the power of the mechanism in the human mind that fills the gaps, that kind of reaches and wants to make the thing you see in the virtual world real to you. I believe in that power.
Speaker B: Or humans want to believe.
Speaker A: Yeah. Like, what if you're lonely? What if you're sad? What if you're really struggling in life and here's a world where you don't have to struggle anymore.
Speaker B: Humans want to believe so much that people think the large language models are conscious. That's how much humans want to believe.
Speaker A: Strong words. He's throwing left and right hooks. Why do you think large language models are not conscious?
Speaker B: I don't think I'm conscious.
Speaker A: Oh. So what is consciousness, then, George Hodz?
Speaker B: It's, like, what it seems to mean to people. It's just, like, a word that atheists use for souls.
Speaker A: Sure, but that doesn't mean soul is not an interesting word.
Speaker B: If consciousness is a spectrum, I'm definitely way more conscious than the large language models are. I think the large language models are less conscious than a chicken.
Speaker A: When was the last time you've seen a chicken?
Speaker B: In Miami? Like, a couple months ago.
Speaker A: No, like a living chicken.
Speaker B: Living chickens walking around Miami. It's crazy.
Speaker A: Like, on the street. Yeah, like a chicken.
Speaker B: A chicken, yeah.
Speaker A: All right, all right. I was trying to call you out like a good journalist, and I got shut down. Okay. But you don't think much about this kind of subjective feeling that it feels like something to exist. And then as an observer, you can have a sense that an entity is not only intelligent, but has a kind of subjective experience of its reality, like a self awareness that is capable of suffering, of hurting, of being excited by the environment in a way that's not merely kind of an artificial response, but a deeply felt one.
Speaker B: Humans want to believe so much that if I took a rock and a sharpie and drew a sad face on the rock, they'd think the rock is sad.
Speaker A: Yeah, and you're saying when we look in the mirror, we, we apply the same smiley face with rock.
Speaker B: Pretty much, yeah.
Speaker A: Doesn't it, isn't that weird, though, that you're not conscious? Is that.
Speaker B: No.
Speaker A: But you do believe in consciousness?
Speaker B: Not really.
Speaker A: It's just, it's unclear. Okay. So to you, it's like a little, like a symptom of the bigger thing. That's not that important.
Speaker B: Yeah. It's interesting that, like, human systems seem to claim that they're conscious. And I guess it kind of, like, says something in a straight up, like, okay, what do people mean when even if you don't believe in consciousness, what do people mean when they say consciousness? And there's definitely, like, meanings to it?
Speaker A: What's your favorite thing to eat?
Speaker B: Pizza.
Speaker A: Cheese pizza. What are the toppings?
Speaker B: I like cheese pizza.
Speaker A: Don't say pineapple.
Speaker B: No, I don't like pineapple.
Speaker A: Okay.
Speaker B: Pepperoni pizza as they put any ham on it. Oh, that's real bad.
Speaker A: What's the best, what's the best pizza? What are we talking about here? Like, you like cheap, crappy pizza?
Speaker B: Chicago deep dish cheese pizza. Oh, that's, that's my favorite.
Speaker A: There you go. You bite into a deep dish chicago deep dish pizza, and it feels like you were starving. You haven't eaten for 24 hours. You just bite in and you're hanging out with somebody that matters a lot to you, and you're there with the pizza.
Speaker B: Sounds real nice, huh? Yeah.
Speaker A: All right. It feels like something. I'm George motherfucking hots eating a fucking Chicago deep dish pizza. There's just the full peak living experience of being human, the top of the human condition.
Speaker B: Sure.
Speaker A: It feels like something to experience that. Why does it feel like something? That's consciousness, isn't it?
Speaker B: If that's the word you want to use to describe it, sure. I'm not going to deny that that feeling exists. I'm not going to deny that I experienced that feeling when I guess what I kind of take issue to is that there's some, like, like, how does it feel to be a web server? Do 404s hurt?
Speaker A: Not yet.
Speaker B: How would you know what suffering looked like? Sure. You can recognize a suffering dog because we're the same stack as the dog. All the bio stack stuff, kind of, especially mammals. You know, it's really easy.
Speaker A: You can game recognizes game.
Speaker B: Yeah. Versus the silicon stack stuff. It's like, you have no idea. You have you? Wow. The little thing has learned to mimic, you know? But then I realized that that's all we are, too oh, look, the little thing has learned to mimic.
Speaker A: Yeah, I guess. Yeah. 404 could be. Could be suffering, but it's so far from our kind of living organism, our kind of stack. But it feels like AI can start maybe mimicking the biological stack better and better. Better because it's trained, retrained it.
Speaker B: Yeah.
Speaker A: And so in that maybe that's the definition of consciousness, is the biostat consciousness.
Speaker B: The definition of consciousness is how close something looks to human. Sure. I'll give you that one.
Speaker A: No, how close something is to the human experience.
Speaker B: Sure. It's a very. It's a very anthropocentric definition, but.
Speaker A: Well, that's all we got.
Speaker B: Sure. No, and I don't mean to, like, I think there's a lot of value in it. Look, I just started my second company. My third company will be AI girlfriends. Oh. Like, I mean, I want to find.
Speaker A: Out what your fourth company is after.
Speaker B: Oh, wow.
Speaker A: Because I think once you have AI girlfriends, it's, oh, boy, does it get interesting. Well, maybe. Let's go there. I mean, the relationships with AI that's creating human like organisms. Right. And part of being human is being conscious is being. Having the capacity to suffer, having the capacity to experience this life richly in such a way that you can empathize, the AI system can empathize with you, and you can empathize with it, or you can project your anthropomorphic sense of what the other entity is experiencing. And an AI model would need to create that experience inside your mind. And it doesn't seem that difficult.
Speaker B: Yeah, but, okay, so here's where it actually gets totally different. Right. When you interact with another human, you can make some assumptions.
Speaker A: Yeah.
Speaker B: When you interact with these models, you can't. You can make some assumptions that. That other human experiences suffering and pleasure in a pretty similar way to you do. The golden rule applies with an AI model. This isn't really true. Right. These large language models are good at fooling people because they were trained on a whole bunch of human data and told to mimic it.
Speaker A: Yeah, but if the AI system says, hi, my name is Samantha, it has a backstory. I went to college here and there.
Speaker B: Yeah.
Speaker A: Maybe it'll integrate this in the AI system.
Speaker B: I made some chatbots. I give him backstories. It was lots of fun. I was so happy when Lama came out.
Speaker A: Yeah, well, we'll talk about llama. We'll talk about all that. But, like, you know, the rock with the smiley face?
Speaker B: Yeah.
Speaker A: It seems pretty natural for you to anthropomorphize that thing and then start dating it, and before you know it, you're married and have kids.
Speaker B: With a rock.
Speaker A: With a rock. There's pictures on Instagram with you and a rock and smiley face.
Speaker B: To be fair, like, you know, something that people generally look for when they're looking for someone to date is intelligence in some form. And the rock doesn't really have intelligence. Only a pretty desperate person would date a rock.
Speaker A: I think we're all desperate deep down.
Speaker B: Oh, not rock level desperate.
Speaker A: All right. Not rock level desperate, but AI level desperate. I don't know. I think all of us have a deep loneliness. It just feels like the language models are there.
Speaker B: Oh, I agree. And you know what? I won't even say this so cynically. I will actually say this in a way that, like, I want AI friends. I do.
Speaker A: Yeah.
Speaker B: Like, I would love to. You know, again, the language models now are still a little, like, people are impressed with these GPT things. And I look at, like. Or, like. Or the copilot, the coding one, and I'm like, okay, this is, like, junior engineer level, and these people are, like, fiverr level artists and copywriters. Like, okay, great, we got, like, fiverr and, like, junior engineers. Okay, cool. Like, and this is just the start, and it will get better, right? Like, I can't wait to have AI friends who are more intelligent than I am.
Speaker A: So fiverr is just a temporary. It's not the ceiling.
Speaker B: No, definitely not.
Speaker A: Is it. Is it count as cheating when you're talking to an AI model? Emotional cheating?
Speaker B: That's. That's up to you and your human partner to define.
Speaker A: Oh, you have to. All right.
Speaker B: You can. Yeah, you have to have that. Have to have that conversation, I guess.
Speaker A: All right. I mean, integrate that with porn and all this.
Speaker B: No, I mean, it's similar kind of porn.
Speaker A: Yeah.
Speaker B: Yeah. I think people in relationships have different views on that.
Speaker A: Yeah. But most people don't have, like, serious, open conversations about all the different aspects of what's cool and what's not. And it feels like AI is a really weird conversation to have.
Speaker B: The porn one is a good branching off point, like, these things. You know, one of my scenarios that I put in my chat bot is a, you know, a nice girl named Lexi. She's 20. She just moved out to LA. She wanted to be an actress, but she started doing onlyfans instead. And you're on a date with her. Enjoy.
Speaker A: Oh, man. Yeah. And so is that if you're actually dating somebody in real life, is that cheating? I feel like it gets a little weird.
Speaker B: Sure.
Speaker A: It gets real weird.
Speaker B: I.
Speaker A: It's like, what are you allowed to say to an AI bot? Imagine having that conversation with a significant other.
Speaker B: I mean, these are all things for people to define in their relationships. What it means to be human is.
Speaker A: Just going to start to get weird, especially online. Like, how do you know there'll be moments when you'll have what you think is a real human you interacted with on Twitter for years and you realize it's not.
Speaker B: I spread. I love this meme. Heaven Banning. You know what? Shadow Banning. Yeah, shadow Banning. Okay. You post, no one can see it. Heaven Banning. You post, no one can see it. But a whole lot of AI's are spun up to interact with you.
Speaker A: Well, maybe that's the way human civilization ends, is all of us haven't banned.
Speaker B: There's a great. It's called my little friendship is optimal. It's a Sci-Fi story that explores this idea.
Speaker A: Friendship is optimal.
Speaker B: Friendship is optimal.
Speaker A: Yeah. I'd like to have some, at least on the intellectual realm, some AI friends that argue with me. But the romantic realm is weird. Definitely weird. But not out of the realm of the. The kind of weirdness that human civilization is capable of.
Speaker B: I think I want it. Look, I want it. If no one else wants it, I want it.
Speaker A: Yeah, I think a lot of people probably want it. There's a deep loneliness, and I'll fill.
Speaker B: Their loneliness and, you know, it just will only advertise to you some of the time.
Speaker A: Yeah. Maybe the conceptions of monogamy change, too. Like, I grew up in a time, like, I value monogamy. But maybe that's a silly notion when you have arbitrary number of AI systems.
Speaker B: This interesting path from rationality to polyamory. Yeah, that doesn't make sense for me.
Speaker A: For you. But you're just a biological organism who was born before the Internet really took off.
Speaker B: The crazy thing is, culture is whatever we define it as, these things are not usually is ought problem in moral philosophy. There's no. What is might be that computers are capable of mimicking girlfriends perfectly. They passed the girlfriend Turing test. But that doesn't say anything about ought. That doesn't say anything about how we ought to respond to them as a civilization. That doesn't say we ought to get rid of monogamy. That's a completely separate question. Really? A religious one.
Speaker A: Girlfriend touring test. I wonder what that looks like.
Speaker B: Girlfriend touring test.
Speaker A: Are you writing that? Will you be the Alan Turing of the 21st century that writes the girlfriend Turing test?
Speaker B: No, I mean, of course, my AI girlfriends, their goal is to pass the girlfriend Turing test.
Speaker A: No, but there should be, like, a paper that kind of defines the test. I mean, the question is if it's deeply personalized or there's a common thing that really gets everybody.
Speaker B: Yeah, I mean, you know, look, we're a company. We don't have to get everybody. We just have to get a large enough clientele to stay.
Speaker A: I like how you already thinking company. All right, let's. Before we go to company number three and company number four, let's go to company number two. Tiny Corp. Possibly one of the greatest names of all time for a company. You've launched a new company called Tiny Corp that leads the development of tiny grad. What's the origin story of Tiny Corp and tiny grad?
Speaker B: I started tiny grad as a toy project just to teach myself. Okay, what is a convolution? What are all these options you can pass to them? What is the derivative of a convolution? Very similar to. Carpathi wrote micrograd. Very similar. And then I started thinking about AI chips. I started thinking about chips that run AI, and I was like, well, okay, this is going to be a really big problem if Nvidia becomes a monopoly here, how long before Nvidia is nationalized?
Speaker A: So you. One of the reasons to start tiny Corp is to challenge Nvidia.
Speaker B: It's not so much to challenge Nvidia, actually, I like Nvidia, and it's to make sure power stays decentralized.
Speaker A: Yeah. And here it's computational power. And to you, Nvidia is kind of locking down the computational power of the world.
Speaker B: If Nvidia becomes just, like, ten x, better than everything else, you're giving a big advantage to somebody who can secure Nvidia as a resource.
Speaker A: Yeah.
Speaker B: In fact, if Jensen watches this podcast, he may want to consider this. He may want to consider making sure his company is not nationalized.
Speaker A: You think that's an actual threat?
Speaker B: Oh, yes. No.
Speaker A: But there's so much, you know, there's AMD.
Speaker B: So we have Nvidia and AMD. Great.
Speaker A: All right, but you don't think there's, like, a push towards, like, selling, like, Google selling TPU's or something like this? You don't think there's a push for that?
Speaker B: Have you seen it? Google loves to rent utpus.
Speaker A: It doesn't. You can't buy it at best buy?
Speaker B: No. So I started work on a chip. I was like, okay, what's it going to take to make a chip? And my first notions were all completely wrong about why, about how you could improve on GPU's. And I will take this. This is from Jim Keller on your podcast. And this is one of my absolute favorite descriptions of computation. So there's three kinds of computation paradigms that are common in the world today. There's CPu's, and CPu's can do everything. CPU's can do add and multiply, they can do load and store, and they can do compare and branch. And when I say they can do these things, they can do them all fast, right? So compare and branch are unique to CPU's. And what I mean by they can do them fast is they can do things like branch prediction and speculative execution. And they spend tons of transistors and these like super deep reorder buffers in order to make these things fast. Then you have a simpler computation model. GPU's. GPU's can't really do compare and branch. I mean they can, but it's horrendously slow. But GPU's can do arbitrary load and store. GPU's can do things like x dereference Y. So they can fetch from arbitrary pieces of memory. They can fetch from memory that is defined by the contents of the data. The third model of computation is dsps. And dsps are just add and multiply. They can do load in stores, but only static load in stores, only loads in stores that are known before the program runs. And you look at neural networks today, and 95% of neural networks are all the DSP paradigm. They are just statically scheduled ads and multiplies. So Tinyguard really took this idea, and I'm still working on it, to extend this as far as possible. Every stage of the stack has turn completeness, Python has turn completeness. And then we take Python, we go into C, which is Turing complete, and maybe C calls into some cuda kernels, which are turing complete. The Cuda kernels go through LVM, which is turing complete into PTX, which turn complete into SAS, which is turn complete on a Turing complete processor. I want to get Turing completeness out of the stack entirely, because once you get rid of Turing completeness, you can reason about things. Rice's theorem and the halting problem do not apply to admiral machines.
Speaker A: Okay, what's the power and the value of getting Turing completeness out of? Are we talking about the hardware or the software?
Speaker B: Every layer of the stack.
Speaker A: Every layer?
Speaker B: Every layer of the stack. Removing Turing completeness allows you to reason about things, right? So the reason you need to do branch prediction in a cpu and the reason it's prediction and the branch predictors are, I think they're like 99% on cpu's. Why do they get 1% of them wrong? Well, they get 1% wrong because you can't know, right? That's the halting problem. It's equivalent to the halting problem to say whether a branch is going to be taken or not. I can show that. But the Admiral machine, the neural network, runs the identical compute every time. The only thing that changes is the data. So when you realize this, you think about, okay, how can we build a computer and how can we build a stack? That takes maximal advantage of this idea. So what makes tinygred different from other neural network libraries is it does not have a primitive operator even for matrix multiplication. And this is every single one. They even have primitive operators for things like convolutions.
Speaker A: So no matmul?
Speaker B: No matmul. Well, here's what a map model is. So I'll use my hands to talk here. So if you think about a cube, and I put my two matrices that I'm multiplying on two faces of the cube, right, you can think about the matrix multiply as, okay, the n cubed. I'm going to multiply for each one in the cubed and then I'm going to do a sum which is a reduce up to here to the third face of the cube. And that's your multiplied matrix. So what a matrix multiply is, is a bunch of shape operations, a bunch of permutes, reshapes and expands on the two matrices. A multiply n cubed, a reduce n cubed, which gives you an n squared matrix.
Speaker A: Okay, so what is the minimum number of operations that can accomplish that if you don't have matmol as a primitive?
Speaker B: So tiny grad has about 20. And you can compare tiny grad's opset or IR to things like XLA or primtorchen. XLA and Primtorch are ideas where like, okay, torch has like 2000 different kernels. Pytorch 2.0 introduced prim torch, which has only 250. Tinygrad has order of magnitude 25. It's ten x less than XLA or prim torch. And you can think about it as kind of like RIsC versus Cisc. These other things are cisc like systems. Tiny grat is risk and risk. One risk architecture is going to change everything. 1995 hackers.
Speaker A: Wait, really? That's an actual thing?
Speaker B: Angelina Jolie delivers the line risk architecture is going to change everything in 1995. And here we are with arm in the phones and arm everywhere.
Speaker A: Wow. I love it when movies actually have real things in them.
Speaker B: Right.
Speaker A: Okay, interesting. And so this is like, so you're thinking of this as the risk architecture of ML Stack 25. Can you go through the four op types?
Speaker B: Sure. Okay, so you have unary ops, which take in a tensor and return a tensor of the same size and do some unary opt to it x log reciprocal sine. They take in one and they're point wise.
Speaker A: Relu.
Speaker B: Yeah, Relu. Almost all activation functions are unary ops. Some combinations of unary ops together is still a unary op. Then you have binary ops. Binary ops are like point wise, addition, multiplication, division, compare. It takes in two tensors of equal size and outputs one tensor. Then you have reduce ops. Reduceops will take a three dimensional tensor and turn it into a two dimensional tensor. I, or three dimensional tensor turned into zero dimensional tensor. Things like a sum or max are really the common ones there. And then the fourth type is movement ops. And movement ops are different from the other types because they don't actually require computation. They require different ways to look at memory. So that includes reshapes, permutes, expands, flips. Those are the main ones, probably.
Speaker A: So with that, you have enough to make a map model and convolutions.
Speaker B: And every convolution you can imagine, dilated convolutions, striated convolutions, transposed convolutions.
Speaker A: You're right on GitHub about laziness, showing a map mall matrix multiplication. See how despite the style, it is fused into one kernel with the power of laziness. Can you elaborate on this power of laziness?
Speaker B: Sure. So if you type in Pytorch A times b plus circumental, what this is going to do is it's going to first multiply, add and b a and b, and store that result into memory. And then it is going to add c by reading that result from memory, reading c from memory and writing that out to memory. There is way more loads and stores to memory than you need there if you don't actually do a times b as soon as you see it. If you wait until the user actually realizes that tensor, until the laziness actually resolves, you confuse that plus circumental. This is like, it's the same way Haskell works.
Speaker A: So what's the process of porting a model into Tinygrad?
Speaker B: So, Tinygrad's front end looks very similar to Pytorch. I probably could make a perfect or pretty close to perfect interop layer if I really wanted to. I think that there's some things that are nicer about Tinygrad syntax than Pytorch, but the front end looks very torch like. You can also load in onnx models. We have more onyx tests passing than core Mljdev. Okay, so we'll pass onyx runtime soon.
Speaker A: What about, like, the developer experience with tiny grad, what it feels like versus Pytorch?
Speaker B: By the way, I really like Pytorch. I think that it's actually a very good piece of software. I think that they've made a few different trade offs, and these different trade offs are where tiny grad takes a different path. One of the biggest differences is it's really easy to see the kernels that are actually being sent to the GPU. If you run Pytorch on the GPU, you do some operation, and you don't know what kernels ran. You don't know how many kernels ran. You don't know how many flops were used. You don't know how much memory accesses were used. Tinygrad type debug two, and it will show you in this beautiful style, every kernel that's run how many flops and how many bytes.
Speaker A: So can you just linger on what problem Tinygrad solves?
Speaker B: Tinygrad solves the problem of porting new ML accelerators quickly. One of the reasons tons of these companies now, I think Sequoia marked graph core to zero, Cerebus, tens torrent, Grok, all of these ML accelerator companies, they built chips. The chips were good, the software was terrible. And part of the reason is because I think the same problem is happening with Dojo. It's really, really hard to write a Pytorch port because you have to write 250 kernels and you have to tune them all for performance.
Speaker A: What does Jim Jim Color think about tiny grad? You guys hung on quite a bit. So he was involved. He's involved with chin Storrent. What's his praise and what's his criticism of what you're doing with your life?
Speaker B: Look, my prediction for tens torrent is that they're going to pivot to making RISC V chips. Cpu's.
Speaker A: Cpu's, yeah. Why?
Speaker B: Because AI accelerators are a software problem, not really a hardware problem.
Speaker A: Oh, interesting. So you don't think, you think the diversity of AI accelerators in the hardware space is not going to be a thing that exists long term?
Speaker B: I think what's going to happen is if I can finish. Okay. If you're trying to make an AI accelerator, you better have the capability of writing a torch level performance stack on Nvidia GPU's. If you can't write a torch stack on Nvidia GPU's, and I mean all the way, I mean down to the driver, there's no way you're going to be able to write it on your chip because your chip's worse than an Nvidia GPU. The first version of the chip you tape out, it's definitely worse.
Speaker A: Oh, you're saying writing that stack is really tough.
Speaker B: Yes. And not only that, actually, the chip that you tape out, almost always because you're trying to get advantage over Nvidia, you're specializing the hardware more. It's always harder to write software for more specialized hardware. Like a GPU is pretty generic. And if you can't write an Nvidia stack, there's no way you can write a stack for your chip. So my approach with Tinygrad is, first, write a performant Nvidia stack. We're targeting AMD.
Speaker A: So you did say fu to Nvidia. A little bit. Would love.
Speaker B: With love.
Speaker A: Yeah.
Speaker B: So, like the Yankees. You know, I'm a Mets fan.
Speaker A: Oh, you're, you're, you're a Mets fan. A risk. A risk fan and a Mets fan. What's the hope that AMD has? You did a build with AMD recently that I saw? Uh, how does the, uh, the, the 7900 XTX compared to the RTX 4090 or 4080?
Speaker B: Well, let's start with the fact that the 7900 XTX kernel drivers don't work. And if you run demo apps in loops, it panics the kernel.
Speaker A: Okay, so this is a software issue.
Speaker B: Lisa sue responded to my email. Oh, I reached out. I was like, this is, you know, really like, I understand if your seven by seven transposed Winograd Conv is slower than Nvidia's, but literally, when I run demo apps in a loop, the kernel panics.
Speaker A: So just adding that loop.
Speaker B: Yeah, I just literally took their demo apps and wrote, like, while true semicolon do the app done in a bunch of screens, this is the most primitive fuzz testing.
Speaker A: Why do you think that is? Theyre just not seeing a market in machine learning.
Speaker B: Theyre changing. Theyre trying to change. Theyre trying to change. And I had a pretty positive interaction with them this week. Last week, I went on YouTube. I was just like, thats it. I give up on AMD. This is their driver. I'll go with Intel GPu's. Intel GPU's have better drivers.
Speaker A: So you're kind of spearheading the diversification of GPU's.
Speaker B: Yeah, and I'd like to extend that diversification to everything. I'd like to diversify. The more my central thesis about the world is there's things that centralize power and they're bad and there's things that decentralize power and they're good. Everything I can do to help decentralize power, I'd like to do.
Speaker A: So you're really worried about the centralization of Nvidia. That's interesting. And you don't have a fundamental hope for the proliferation of asics except in the cloud.
Speaker B: I'd like to help them with software. No, actually the only ASIc that is remotely successful is Google's TPU. And the only reason that's successful is because Google wrote a machine learning framework. I think that you have to write a competitive machine learning framework in order to be able to build an ASIC.
Speaker A: You think meta with Pytorch builds a competitor?
Speaker B: I hope so. They have one. They have an internal one.
Speaker A: Internal, I mean, public facing with a nice cloud interface and so on.
Speaker B: I don't want a cloud.
Speaker A: You don't like cloud?
Speaker B: I don't like cloud.
Speaker A: What do you think is the fundamental limitation of cloud?
Speaker B: Fundamental limitation to cloud is who owns the off switch.
Speaker A: So it's power to the people.
Speaker B: Yeah.
Speaker A: And you don't like the man to have all the power.
Speaker B: Exactly.
Speaker A: All right. And right now the only way to do that is with Nvidia, GPU's if you want performance and stability. Interesting. It's a costly investment emotionally to go with AMDSE. Well, let me add sort of on a tangent to ask you. You've built quite a few PCs. What's your advice on how to build a good custom PC for, let's say for the different applications that you use for gaming, for machine learning?
Speaker B: Well, you shouldn't build one. You should buy a box from the tiny corp.
Speaker A: I heard rumors, whispers about this box in the tiny corp. What's this thing look like? What is it, what is it called?
Speaker B: It's called the tiny box.
Speaker A: Tiny box.
Speaker B: It's $15,000 and it's almost a pay to flop of compute. It's over 100gb of GPU Ram. It's over five terabytes per second of GPU memory bandwidth. I'm going to put like four nvmes in raid. You're going to get 20 30gb/second of drive read bandwidth. I'm going to build the best deep learning box that I can. That plugs into one wall outlet.
Speaker A: Okay, can you go through those specs again a little bit from memory?
Speaker B: Yeah. So it's almost a pay to flop of compute. So AMD intel today I'm leaning toward AMD, but we're pretty agnostic to the type of compute. The main limiting spec is a 120 volts, 15 amp circuitous. Okay, well, I mean it, because in order to, like, there's a plug over there, right. You have to be able to plug it in. We're also going to sell the tiny rack, which, like, what's the most power you can get into your house without arousing suspicion? And one of the answers is an electric car charger.
Speaker A: Wait, where does the rack go?
Speaker B: Your garage.
Speaker A: Interesting.
Speaker B: The car charger, a wall outlet is about 1500 watts. A car charger is about 10,000 watts.
Speaker A: What is the most amount of power you can get your hands on without arousing suspicion?
Speaker B: That's right.
Speaker A: George Hotz. Okay, so the tiny box and you said nvmes and raid. I forget what you said about memory, all that kind of stuff. Okay, what about what GPU's again?
Speaker B: Probably, probably 7900 xtxs, but maybe 3090s, maybe a 770s.
Speaker A: Those are intels you're flexible or still exploring.
Speaker B: I'm still exploring. I want to deliver a really good experience to people. And yeah, what GPU's I end up going with. Again, I'm leaning toward AMD. We'll see. In my email, what I said to AMD is just dumping the code on GitHub is not open source. Open source is a culture. Open source means that your issues are not all one year old, stale issues. Open source means developing in public. And if you guys can commit to that, I see a real future for AMD as a competitor to Nvidia.
Speaker A: Well, I'd love to get a tiny box to MIT. So whenever it's ready.
Speaker B: Will do.
Speaker A: Let's do it.
Speaker B: We're taking pre orders. I took this from Elon. I'm like, all right, $100, fully refundable pre orders.
Speaker A: Is it going to be like the Cybertruck is going to take a few years or.
Speaker B: No, I'll try to do it faster than that. It's a lot simpler. It's a lot simpler than a truck.
Speaker A: Well, there's complexities. Not to just the putting the thing together, but like shipping and all this kind of stuff.
Speaker B: The thing that I want to deliver to people out of the box is being able to run 65 billion parameter llama in FP 16 in real time, in like a good, like ten tokens per second or five tokens per second or something just.
Speaker A: It works. Llama's running or something like llama experience.
Speaker B: Yeah. Or I think Falcon is the new one. Experience a chat with the largest language model that you can have in your house.
Speaker A: Yeah. From a wall plug.
Speaker B: From a wall plug, yeah. Actually, for inference. It's not like even more power would help you get more.
Speaker A: Even more power would get you more.
Speaker B: Well, no, there's just the biggest model released is 65 billion parameter llama, as far as I know.
Speaker A: So it sounds like tiny box will naturally pivot towards company number three because you could just get the girlfriend and I. I mean, or boyfriend.
Speaker B: That one's harder, actually.
Speaker A: The boyfriend is harder.
Speaker B: Boyfriend's harder. Yeah.
Speaker A: I think that's a very biased statement. I think a lot of people just. What, why is it harder to replace a boyfriend than other girlfriend with the artificial LLM?
Speaker B: Because women are attracted to status and power and men are attracted to youth and beauty. No, I mean, this is what I mean.
Speaker A: Both are. Could be unmimicable, easy through the language model.
Speaker B: No, no. Machines do not have any status or real power.
Speaker A: I don't know. I think you both. Well, first of all, you're using language mostly to communicate youth and beauty and power and status.
Speaker B: But status fundamentally is a zero sum game, whereas youth and beauty are not.
Speaker A: No, I think status is a narrative you can construct. I don't think status is real.
Speaker B: I don't know. I just think that that's why it's harder, you know? Yeah, maybe it is my biases.
Speaker A: I think status is way easier to fake.
Speaker B: I also think that, you know, men are probably more desperate and more likely to buy my product, so maybe they're a better target.
Speaker A: Market desperation is interesting, easier to fool. I can see that.
Speaker B: Yeah. Look, I mean, look, I know you can look at porn viewership numbers, right? A lot more men watch porn than women. You can ask why that is.
Speaker A: Wow, there's a lot of questions and answers. You can get there. Anyway, with the, with the tiny box. How many gpu's in tiny box?
Speaker B: Six.
Speaker A: Oh, man.
Speaker B: And I'll tell you why it's six.
Speaker A: Yeah.
Speaker B: So AMD Epyc processors have 128 lanes of PCIe. I want to leave enough lanes for some drives, and I want to leave enough lanes for some networking.
Speaker A: How do you do cooling for something like this?
Speaker B: Ah, that's one of the big challenges. Not only do I want the cooling to be good, I want it to be quiet. I want the tiny box to be able to sit comfortably in your room. Right.
Speaker A: This is really going towards the girlfriend thing, because you want to run the LLM.
Speaker B: I'll give, I'll give a more. I mean, I can talk about how it relates to company number one, comma aihdem.
Speaker A: Well, but yes, quiet. Oh, quiet because you maybe potentially want to run in a car.
Speaker B: No, no. Quiet because you want to put this thing in your house and you want it to coexist with you. If it's screaming at 60 decibels, you don't want that in your house, you'll kick it out.
Speaker A: 60 decibels?
Speaker B: Yeah, I want, like, 40, 45.
Speaker A: So how do you make the cooling quiet? That's an interesting problem in itself.
Speaker B: A key trick is to actually make it big. Ironically, it's called the tiny box. Yeah, but if I can make it big, a lot of that noise is generated because of high pressure air. If you look at, like, a one U server, a one U server has these super high pressure fans. They're, like, super deep, and they're like jet engines versus. If you have something that's big, well, I can use a big thing. You know, they call them big ass fans, those ones that are, like, huge on the ceiling, and they're completely silent.
Speaker A: So Tinybox will be big.
Speaker B: It is the. I do not want it to be large according to UPS. I want it to be shippable as a normal package, but that's my constraint there.
Speaker A: Interesting. With the fans stuff, can't it be assembled on location or.
Speaker B: No, no, I should be.
Speaker A: Well, here, look.
Speaker B: I want to give you a great out of the box experience. I want you to lift this thing out. I want it to be like, like the Mac, you know, tiny box.
Speaker A: The apple experience. Yeah, I love it. Okay. And so tiny box would run tiny grad. Like, what, what do you envision this whole thing to look like? We're talking about, like, Linux with a full software engineering environment, and it's just not Pytorch, but tiny grad.
Speaker B: Yeah, we did a poll. If people want Ubuntu or arch, we're going to stick with ubuntu.
Speaker A: Ooh, interesting. What's your favorite flavor of Linux? Ubuntu. I like ubuntu mate, however you pronounce that. Mate. So how do you. You've gotten llama into tiny grad. You've gotten stable diffusion into tiny grad. What was that like? Can you comment on, like, what are these models? What's interesting about porting them? What are the challenges what's naturally, what's easy, all that kind of stuff.
Speaker B: There's a really simple way to get these models into tiny grad, and you can just export them as Onyx and then Tinygrad can run onyx. So the ports that I did of llama stable diffusion and now whisper are more academic to teach me about the models, but they are cleaner than the pytorch versions. You can read the code. I think the code is easier to read. It's less lines. There's just a few things about the way tinygread writes things. Here's a complaint I have about pytorch. Nn relu is a class. When you create an Nn module, you'll put your Nn relu's as in init, and this makes no sense. Relu is completely stateless. Why should that be a class?
Speaker A: But that's more like us software engineering thing. Or do you think it has a cost on performance?
Speaker B: Oh, no, it doesn't have a cost on performance, but, yeah, no, I think that it's. That's what I mean about, like, Tinygrad's front end being cleaner.
Speaker A: I see. What do you think about Mojo? I don't know if you've been paying attention to the programming language that does some interesting ideas that kind of intersect Tinygrad.
Speaker B: I think that there's a spectrum, and, like, on one side you have Mojo, and on the other side you have, like, GGML. Ggml. Is this like, we're going to run llama fast on Mac? Okay, we're going to expand out to a little bit, but we're going to basically, like, depth first. Right? Mojo is like we're going to go breath first. We're going to go so wide that we're going to make all of python fast and tiny grads in the middle. Tiny grand is we are going to make neural networks fast.
Speaker A: Yeah, but they. They try to really get it to be fast compiled onto specifics hardware and make that compilation step as flexible and resilient as possible.
Speaker B: Yeah, but they have Turing completeness and that limits. You turn.
Speaker A: That's what you're saying is somewhere in the middle. So you're actually going to be targeting some accelerators, some like, some number, not one.
Speaker B: My goal is, step one, build an equally performant stack to Pytorch on Nvidia and amddeendeende, but with way less lines. And then step two is, okay, how do we make an accelerator? Right, but you need step one. You have to first build the framework before you can build the accelerator.
Speaker A: Can you explain ML perf? What's your approach in general to benchmarking tiny grad performance?
Speaker B: So I'm much more of a, like, build it the right way and worry about performance later. There's a bunch of things where I haven't even, like, really dove into performance. The only place where Tinygrad is competitive performance wise right now, is on Qualcomm GPU's. So Tinygrad is actually used in Openpilot to run the model. So the driving model is Tinygrad.
Speaker A: When did that happen, that transition?
Speaker B: About eight months ago now. And it's two x faster than Qualcomm's library.
Speaker A: What's the hardware that openpilot runs on? The coma?
Speaker B: It's a Snapdragon 845. Okay, so this is using the GPU. So the GPU is an Adreno GPU. There's different things. There's a really good Microsoft paper that talks about mobile gpu's and why they're different from desktop gpu's. One of the big things is in a desktop GPU, you can use buffers on a mobile gpu, image textures are a lot faster.
Speaker A: On a mobile GPU, image textures. Okay. And so you want to be able to leverage that?
Speaker B: I want to be able to leverage it in a way that it's completely generic. Right. So there's a lot of. Xiaomi has a pretty good open source library for mobile gpu's called Mace, where they can generate, where they have these kernels, but they're all hand coded. So that's great if you're doing three by three comps, that's great if you're doing dense map mouse. But the minute you go off the beaten path a tiny bit, well, your performance is nothing.
Speaker A: Since you mentioned Openpil, I'd love to get an update in the company number one common AI world. How are things going there? In the development of semi autonomous driving.
Speaker B: You know, almost no one talks about FSD anymore, and even less people talk about OpenPilot. We've solved the problem like we solved it years ago.
Speaker A: What's the problem exactly? Well, what does solving it mean?
Speaker B: Solving means how do you build a model that outputs a human policy for driving? How do you build a model that, given a reasonable set of sensors, outputs a human policy for driving? So you have companies like Waymo and cruise, which are hand coding these things that are quasi human policies. Then you have Tesla, and maybe even to more of an extent, comma, asking, okay, how do we just learn human policy from data? The big thing that we're doing now. And we just put it out on Twitter. At the beginning of comma, we published a paper called learning a driving simulator. And the way this thing worked was it was an auto encoder and then an RNN in the middle. Right. You take an autoencoder, you compress the picture. You use an RNN, predict the next state. And these things are, you know, it was a laughably bad simulator. This is 2015 error machine learning technology. Today we have VqVae and transformers. We're building drive GPT, basically.
Speaker A: Drive GPT. Okay, so. And it's trained on what? Is it trained in a self supervised way?
Speaker B: Yeah, it's trained on all the driving data to predict the next frame.
Speaker A: So really trying to learn a human policy, what would a human do?
Speaker B: Well, actually, our simulator is conditioned on the pose. So it's actually a simulator. You can put in, like, a state, action pair on, get out the next state. And then once you have a simulator, you can do RL in the simulator. And RL will get us that human policy.
Speaker A: So it transfers.
Speaker B: Yeah. RL with a reward function. Not asking, is this close to the human policy, but asking, would a human disengage if you did this behavior?
Speaker A: Okay, let me think about the distinction there. Would a human disengage what? A human disengage. That correlates, I guess, with human policy, but it could be different. So it doesn't just say, what would a human do? It says, what would a good human driver do? And such that the experience is comfortable, but also not annoying in that the thing is very cautious. So it's finding a nice balance. That's interesting. It's a nice.
Speaker B: It's asking exactly the right question. What will make our customers happy?
Speaker A: Right.
Speaker B: A system that you never want to.
Speaker A: Disengage, because usually disengagement is almost always a sign of. I'm not happy with what the system is doing.
Speaker B: Usually there's some that are just. I felt like driving, and those are always fine, too, but they're just going to look like noise in the data.
Speaker A: But even I felt like driving, maybe. Yeah, that's even. That's a signal. Like, why do you feel like driving here? You need to recalibrate your relationship with the car. Okay, so that's really interesting. How close are we to solving self driving?
Speaker B: It's hard to say. We haven't completely closed the loop yet, so we don't have anything built that truly looks like that architecture yet. We have prototypes and there's bugs. So we are a couple of bug fixes away. Might take a year, might take ten.
Speaker A: What's the nature of the bugs? Are these major philosophical bugs, logical bugs kind of bugs are we talking about?
Speaker B: Oh, they're just like, they're just like stupid bugs and like. Also we might just need more scale. We just massively expanded our compute cluster at gamma. We now have about two people worth of compute. 40 betaflops.
Speaker A: Well, people. People are different.
Speaker B: Yeah, 20 fade flops. That's a person. It's just a unit. Right. Horses are different too, but we still call it a horsepower.
Speaker A: Yeah, but there's something different about mobility than there is about perception and action in a very complicated world. But, yes.
Speaker B: Well, yeah, of course not all flops are created equal. If you have randomly initialized weights, it's not going to.
Speaker A: Not all flops are created equal.
Speaker B: Doing way more useful things than others.
Speaker A: Yeah. Yep. Tell me about it. Okay, so more data scale means more scale in compute or scale in scale of data?
Speaker B: Both. Diversity of data diversity is very important in data. Yeah, I mean we have so we have about, I think we have like 5000 daily actives.
Speaker A: How would you evaluate how FSD is doing?
Speaker B: Pretty well. Pretty well.
Speaker A: How's that race gone between come AI and FSD?
Speaker B: Tesla is always one to two years ahead of us. They've always been one to two years ahead of us, and they probably always will be because they're not doing anything wrong.
Speaker A: What have you seen since the last time we talked that are interesting? Architectural decisions, training decisions, like the way they deploy stuff, the architectures they're using in terms of the software, how the teams are run, all that kind of stuff. Data collection, anything interesting?
Speaker B: I know they're moving toward more of an end to end approach.
Speaker A: So creeping towards end to end as much as possible across the whole thing, the training, the data collection, everything.
Speaker B: They also have a very fancy simulator. They're probably saying all the same things we are. They're probably saying we just need to optimize. What is the reward? Well, you get negative reward for disengagement. Right. Like everyone kind of knows this. It's just a question. Who can actually build and deploy the system?
Speaker A: Yeah, I mean, this good, it requires good software engineering, I think.
Speaker B: Yeah.
Speaker A: And the right kind of hardware.
Speaker B: Yeah, the hardware to run it.
Speaker A: You still don't believe in cloud in that regard?
Speaker B: I have a compute cluster in my office, 800 amps, tiny grad. It's 40 idle. Our data center dives me crazy. 40 kw just burning just when the computers are idle, just when I'm sorry. Sorry. Compute cluster.
Speaker A: Compute cluster. I got it.
Speaker B: It's not a data center.
Speaker A: Yeah.
Speaker B: Now, data centers are clouds. We don't have clouds. Data centers have air conditioners. We have fans. That makes it a compute cluster.
Speaker A: I'm guessing this is a kind of legal distinction.
Speaker B: Sure. Yeah, we have a compute cluster.
Speaker A: You said that you don't think LLMs have consciousness, or at least not more than a chicken. Do you think they can reason? Is there something interesting to you about the word reason, about some of the capabilities that we think is kind of human, to be able to integrate complicated information and, through a chain of thought, arrive at a conclusion that feels novel, a novel integration of disparate facts?
Speaker B: Yeah, I don't think that there's. I think that they can reason better than a lot of people.
Speaker A: Hey, isn't that amazing to you, though? Isn't that, like, an incredible thing that a transformer can achieve?
Speaker B: I mean, I think that calculators can add better than a lot of people.
Speaker A: But language feels like reasoning through the process of language, which looks a lot like thought.
Speaker B: Making brilliancies in chess, which feels a lot like thought. Whatever new thing that AI can do, everybody thinks is brilliant. And then, like, 20 years go by, and they're like, well, yeah, but chess, that's, like, mechanical. Like, adding. That's, like, mechanical.
Speaker A: So you think language is not that special. It's like chess.
Speaker B: It's like chess, and it's.
Speaker A: I don't know, because it's very human. We take it. We listen. There is something different between chess and. And language. Chess is a game that a subset of population plays, languages, something we use nonstop for all of our human interaction. And human interaction is fundamental to society. So it's like, holy shit. This language thing is not so difficult to, like, create in the machine.
Speaker B: The problem is, if you go back to 1960 and you tell them that you have a machine that can play amazing chess, of course, someone in 1960 will tell you that machine is intelligent. Someone in 2010 won't. What's changed? Right today, we think that these machines that have language are intelligent. But I think in 20 years, we're going to be like, yeah, but can it reproduce?
Speaker A: So, reproduction? Yeah. We may redefine what it means to be. What is it? A high performance living organism on earth.
Speaker B: Humans are always going to define a niche for themselves. Like, well, you know, we're better than the machines because we can. You know, when, like, they tried creative for a bit, but no one believes that one anymore.
Speaker A: But nish is. Is that. Is that delusional or is there some accuracy to that? Because maybe, like, with chess, you start to realize, like, that, that, uh, we have ill conceived notions of what, uh, what makes humans special, like the apex organism on earth.
Speaker B: Yeah. And I think maybe we're going to go through that same thing with language and that same thing with creativity.
Speaker A: But language carries these notions of truth and so on. And so we might be like, wait, maybe truth is not carried by language. Maybe there's, like, a deeper thing.
Speaker B: The niche is getting smaller.
Speaker A: Oh, boy.
Speaker B: But no, no, no, you don't understand. Humans are created by God and machines are created by humans, therefore. Right? Like, that'll be the last niche we have.
Speaker A: So what do you think about the rapid development of LLMs? If we could just, like, stick on that, it's still incredibly impressive. Like, with chadgbt, just even chagpty, what are your thoughts about reinforcement learning with human feedback on these large language models?
Speaker B: I'd like to go back to when calculators first came out and. Or computers and, like, I wasn't around. Look, I'm 33 years old, and to, like, see how that affected, like, society.
Speaker A: Maybe you're right. So I want to put on the, the big picture hat here.
Speaker B: Oh, my God, the refrigerator. Wow.
Speaker A: The refrigerator. Electricity, all that kind of stuff. But no, with the Internet, large language models seeming human, like, basically passing a Turing test, it seems it might have really, at scale, rapid, transformative effects on society. But you're saying, like, other technologies have as well. So maybe calculator is not the best example of that because that just seems like, well, no, maybe calculator.
Speaker B: The poor milk man, the day he learned about refrigerators, he's like, I'm done. You tell me you can just keep the milk in your house, you don't need me to deliver it every day. I'm done.
Speaker A: Well, yeah, you have to actually look at the practical impacts of certain technologies that they've had. Yeah, probably. Electricity is a big one and also how rapidly it's spread. Man, the Internet is a big one.
Speaker B: I do think it's different this time, though.
Speaker A: Yeah, it just feels like getting smaller. The initial humans that makes humans special.
Speaker B: Yes.
Speaker A: It feels like it's getting smaller rapidly, though, doesn't it? Or is that just a feeling? We dramatize everything.
Speaker B: I think we dramatize everything. I think that you ask the milkman when he saw refrigerators and they're going to have one of these in every home.
Speaker A: Yeah, yeah, yeah, yeah. But, boy, is it impressive. So much more impressive than seeing a chess world champion AI system.
Speaker B: I disagree, actually, I disagree. I think things like mu, zero and alphago are so much more impressive because these things are playing beyond the highest human level. The language models are writing middle school level essays, and people are like, wow, it's a great essay. It's a great five paragraph essay about the causes of the civil war.
Speaker A: Okay, forget the civil war. Just generating code. Codex. So you're saying mediocre code, terrible. But I don't think it's terrible. I think it's just mediocre code.
Speaker B: Yeah.
Speaker A: Often close to correct, like for mediocre purpose.
Speaker B: That's the scariest kind of code. I spent 5% of time typing and 95% of time debugging. The last thing I want is close to correct code. I want a machine that can help me with the debugging, not with the typing.
Speaker A: You know, it's like level two driving. Similar kind of thing. Yeah, it's a. You still should be a good programmer in order to modify, I wouldn't even say debugging. It's just modifying the code, reading it.
Speaker B: Don't think it's like level two driving. I think driving is not tool complete, and programming is meaning you don't use like the best possible tools to drive. Right. You're not. You're not like. Like cars have basically the same interface for the last 50 years. Yeah. Computers have a radically different interface.
Speaker A: Okay, can you describe the concept of tool complete?
Speaker B: Yeah. So think about the difference between a car from 1980 and a car from today. Yeah, no difference really. It's got a bunch of pedals, it's got a steering wheel. Great. Maybe now it has a few ADAS features, but it's pretty much the same car. You have no problem getting into a 1980 car and driving it. You take a programmer today who spent their whole life doing JavaScript, and you put them in an Apple Iie prompt and you tell them about the line numbers in basic. But how do I insert something between line 17 and 18? Oh wow.
Speaker A: So in tool, you're putting in the programming languages. So it's just the entirety stack of the tooling. Exactly. So it's not just ids or something like this. It's everything?
Speaker B: Yes, it's ides. The languages, the runtimes, it's everything. And programming is tool complete. So almost. If Codex or Copilot are helping you, that actually probably means that your framework or library is bad and there's too much boilerplate in it.
Speaker A: Yeah, but don't you think so much programming has boilerplate?
Speaker B: Tiny grad is now 2700 lines and it can run llama and stable diffusion. And all of this stuff is in 2700 lines. Boilerplate and abstraction, indirections and all these things are just bad code.
Speaker A: Well, let's talk about good code and bad code. I would say. I don't know for generic scripts that I write just offhand, like 80% of it is written by GPT. Just like quick offhand stuff, not libraries, not performing code, not stuff for robotics and so on, just quick stuff. Because your basic, so much of programming is doing some, some, yeah, boilerplate. But to do so efficiently and quickly because you can't really automate it fully with like generic method, like a generic kind of id type of recommendation or something like this, you do need to have some of the complexity of language models.
Speaker B: Yeah, I guess if I was really writing like maybe today, if I wrote like a lot of like data parsing stuff. Yeah, I mean, I don't play ctfs anymore, but if I still play ctfs, a lot of like, it's just like you have to write like a parser for this data format. Like I wonder, or like admin of code, I wonder when the models are going to start to help with that kind of code. And they may, they may. And the models also may help you with speed. Yeah, and the models are very fast, but where the models won't. My programming speed is not at all limited by my typing speed. And in very few cases it is. Yes. If I'm writing some script to just like parse some weird data format, sure. My programming speed is limited by my typing speed.
Speaker A: What about looking stuff up? Because that's essentially a more efficient lookup. Right.
Speaker B: When I was at Twitter, I tried to use chat GPT to ask some questions, what's the API for this? And it would just hallucinate. It would just give me completely made up API functions. That sounded real.
Speaker A: Well, do you think that's just a temporary stage? You don't think you'll get better and better and better in this kind of stuff because it only hallucinates stuff in the edge cases.
Speaker B: Yes.
Speaker A: If you're writing generic code, it's actually pretty good.
Speaker B: Yes. If you are writing an absolute basic react app with a button, it's not going to hallucinate. Sure. No, there's ways to fix the hallucination problem. I think Facebook has an interesting paper. It's called AtlAs, and it's actually weird the way that we do language models right now, where all of the information is in the weights. The human brain is not really like this. We have a hippocampus and a memory system. So why don't LLMs have a memory system and there's people working on them? I think future LLMs are going to be like smaller, but are going to run looping on themselves and are going to have retrieval systems. And the thing about using a retrieval system is you can cite sources explicitly.
Speaker A: Which is really helpful to integrate the human into the loop or the, of the thing, because you can go check the sources and you can investigate. So whenever the thing is hallucinating, you can like have the human supervisor that's pushing it towards level two, kind of.
Speaker B: That's going to kill Google.
Speaker A: Wait, which part?
Speaker B: When someone makes an LLM that's capable of citing its sources, it will kill Google?
Speaker A: LLM that's citing sources because that's basically a search engine.
Speaker B: That's what people want in a search engine.
Speaker A: But also Google might be the people that build it, maybe, and put ads on it.
Speaker B: I'd count them out.
Speaker A: Why is that? Why do you think? Who, who wins this race? We got. Who are the competitors? All right, we got tiny corp. I don't know if that's. Yeah, I mean, you're a legitimate competitor in that.
Speaker B: I'm not trying to compete on that.
Speaker A: You're not?
Speaker B: No.
Speaker A: Not as can accidentally stumble into that competition. You don't think you might build a search engine to replace Google search?
Speaker B: When I started, comma, I said over and over again, I'm going to win self driving cars. I still believe that. I have never said I'm going to win search with the tiny corp, and I'm never going to say that because I won't.
Speaker A: The night is still young. We don't, you don't know. How hard is it to win search in this new route? Like, it's, it feels. I mean, one of the things that chat, GPT kind of shows that there could be a few interesting tricks that really have, that create a really compelling product.
Speaker B: Some startups going to figure it out. I think if you ask me, like, Google is still the number one web page. I think by the end of the decade, Google won't be the number one web page anymore.
Speaker A: So you don't think Google, because of how big the corporation is.
Speaker B: Look, I would put a lot more money on Mark Zuckerberg.
Speaker A: Why is that?
Speaker B: Because Mark Zuckerberg's alive. Like, this is old. Paul Graham essay startups are either alive or dead. Google's dead. Facebook versus live.
Speaker A: Facebook is alive. Meta is alive.
Speaker B: Meta.
Speaker A: Meta.
Speaker B: You see what I mean? Like, that's just like Mark Zuckerberg. This is Mark Zuckerberg reading that. Paul Graham asking, and being like, I'm going to show everyone how alive we are. I'm going to change the name.
Speaker A: So you don't think there's this gutsy, pivoting engine that, like, Google doesn't have that? The kind of engine that a startup has, like, constantly, you know what? Being alive.
Speaker B: I guess when I listened to your Sam Altman podcast, he talked about the button. Everyone who talks about AI talks about the button. The button to turn it off. Right? Do we have a button to turn off Google? Is anybody in the world capable of shutting Google down?
Speaker A: What does that mean exactly? The company or the search engine?
Speaker B: So we shut the search engine down. Could we shut the company down either?
Speaker A: Can you elaborate on the value of that question?
Speaker B: Does Sundar Pichai have the authority to turn off Google.com tomorrow?
Speaker A: Who has the authority? That's a good question.
Speaker B: Does anyone?
Speaker A: Does anyone? Yeah, I'm sure.
Speaker B: Are you sure? No, they have the technical power, but do they have the authority? Let's say Sundar Pichai made this his sole mission, came into Google tomorrow and said, I'm going to shut google.com down.
Speaker A: Yeah.
Speaker B: I don't think he'd keep this position too long.
Speaker A: And what is the mechanism by which he wouldn't keep his position?
Speaker B: Well, boards and shares and corporate undermining and. Oh, my God, our revenue is zero now.
Speaker A: Okay, so what, I mean, what's the case you're making here? So the capitalist machine prevents you from having the button.
Speaker B: Yeah. And it will have. I mean, this is true for the AI's too, right? There's no turning the AI's off. There's no button. You can't press it. Now, does Mark Zuckerberg have that button for facebook.com?
Speaker A: Yes, probably more.
Speaker B: I think he does. I think he does. And this is exactly what I mean and why I bet on him so much more than I bet on Google.
Speaker A: I guess you could say Elon has similar stuff.
Speaker B: Oh, Elon has the button.
Speaker A: Yeah.
Speaker B: Does Elon. Can Elon fire the missiles? Can he fire the missiles?
Speaker A: I think some questions are better unasked, right?
Speaker B: I mean, you know, a rocket at an ICBM, your rocket that can land anywhere, isn't that an ICBM? Well, you know, Dennis, too many questions.
Speaker A: My God. But the positive side of the button is that you can innovate aggressively, is what you're saying, which is what's required with turning LLM into a search engine.
Speaker B: I would bet on a startup because.
Speaker A: It'S so easy, right?
Speaker B: I bet on something that looks like mid journey, but for search, just is.
Speaker A: Able to site source a loop on itself. I mean, it just feels like one model can take off.
Speaker B: Yeah, right?
Speaker A: And nice wrapper. And some of it scale. I mean, it's hard to create a product that just works really nicely stably.
Speaker B: The other thing that's going to be cool is there is some aspect of a winner take all effect. Right? Like once someone starts deploying a product that gets a lot of usage and you see this with OpenAI, they are going to get the dataset to train future versions of the model. Yeah, they are going to be able to. I was actually at Google image search when I worked there, like almost 15 years ago now. How does Google know which image is an Apple? And I said the metadata and they're like, yeah, that works about half the time. How does Google know? You'll see the raw apples on the front page when you search Apple. And I don't know, I didn't come up with the answer. The guy's like, well, it's what people click on when they search apple.
Speaker A: Yeah.
Speaker B: Yeah.
Speaker A: That data is really, really powerful. It's the human supervision. What do you think are the chances? What do you think, in general that llama was open sourced? I just did a conversation with Mark Zuckerberg and he's all in on open source.
Speaker B: Who would have thought that Mark Zuckerberg would be the good guy?
Speaker A: I mean, it would have thought anything in this world, it's hard to know. But open source to you ultimately is a good thing here.
Speaker B: Undoubtedly. You know what's ironic about all these AI safety people is they are going to build the exact thing they fear. These. We need to have one model that we control and align. This is the only way you end up paperclipped. There's no way you end up paperclipped if everybody has an AI.
Speaker A: So open sourcing is the way to fight the paperclip maximizer.
Speaker B: Absolutely. It's the only way. You think you're going to control it. You're not going to control it.
Speaker A: So the criticism you have for the AI safety folks is that there is a belief and a desire for control. And that belief and desire for centralized control of dangerous AI systems is not good.
Speaker B: Sam Altman won't tell you that. GPT four has 220 billion parameters and is a 16 way mixture model with eight sets of weights.
Speaker A: Who did you have to murder to get that information? All right, I mean, look, yes, everyone.
Speaker B: At OpenAI knows what I just said was true. Right? Now ask the question. Really? You know, it upsets me when I like GPT-2 when OpenAI came out with GPT-2 and raised a whole fake AI safety thing about that. I mean, now the model is laughable. Like, they used AI safety to hype up their company and it's disgusting.
Speaker A: Or the flip side of that is they used a relatively weak model in retrospect to explore how do we do AI safety correctly? How do we release things? How do we go through the process? I don't. I don't know if.
Speaker B: I don't know how much height, charitable interpretation.
Speaker A: I don't know how much hype there is in AI safety, honestly.
Speaker B: Oh, there's so much. I. At least on Twitter, I don't know, maybe Twitter's not real life.
Speaker A: There's not real life. Come on. In terms of hype, I mean, I don't. I think OpenAI has been finding an interesting balance between transparency and putting a value on AI safety. You don't think, you think just go all out open source. So do a llama. So do like, open source. This, this is a tough question. Which is open source? Both the base, the foundation model and the fine tuned one. So, like, I. The model that can be ultra racist and dangerous and like, tell you how to build a nuclear weapon.
Speaker B: Oh, my God. Have you met humans? Right. Like, half of these AI alignment.
Speaker A: I haven't met most humans. I. This makes this, this allows you to meet every human.
Speaker B: Yeah, I know, but half of these AI alignment problems are just human alignment problems. And that's what's also so scary about the language they use. It's like, it's not the machines you want to align, it's me.
Speaker A: But here's the thing. It makes it very accessible to ask very questions where the answers have dangerous consequences if you were to act on them.
Speaker B: I mean. Yeah, welcome to the world.
Speaker A: Well, no, for me, there's a lot of friction. If I want to find out how to, I don't know, blow up something.
Speaker B: No, there's not a lot of friction. That's so easy.
Speaker A: No. Like, what do I search? Do I use bing or do I. Which search engine do I use?
Speaker B: No, there's like lots of stuff.
Speaker A: No, it feels like I have to keep.
Speaker B: First off, anyone who's stupid enough to search for how to blow up a building in my neighborhood is not smart enough to build a bomb. Right.
Speaker A: Are you sure about that?
Speaker B: Yes.
Speaker A: I feel like. I feel like a language model makes it more accessible for that person. Who's not smart enough to do, they're.
Speaker B: Not gonna, they're not gonna build a bomb. Trust me. The people who are incapable of figuring out how to, like, ask that question a bit more academically and get a real answer from it are not capable of procuring the materials, which are somewhat controlled, to build a bomb.
Speaker A: No. I think LLM makes it more accessible to people with money without the technical know how. Right. To build. Like, do you really need to know how to build a bomb? To build a bomb, you can hire people. You can find or you can hire.
Speaker B: People to build up. You know what? I was asking this question on my stream? Like, can Jeff Bezos hire a hitman?
Speaker A: Probably nothing, but a language model can probably help you out.
Speaker B: Yeah, you'll still go to jail, right? Like, it's not like the language model is God. Like, the language model, it's like it's. You literally just hired someone on Fiverr. Like, you, you.
Speaker A: GPT four, in terms of finding a hitman is like asking fiverr how to find a. I understand, but don't you think Wikihow, you know Wikihow. But don't you think GPT five will be better? Because don't you think that information is out there on the Internet?
Speaker B: I mean. Yeah, and I think that if someone is actually serious enough to hire a hitman or build a bomb, they'd also be serious enough to find the information.
Speaker A: I don't think so. I think it makes it more accessible. If you have, if you have enough money to buy Hitman, I think it decreases the friction of how hard is it to find that kind of hitman? I honestly think there's a jump in ease and scale of how much harm you can do. And I don't mean harm with language. I mean harm with actual violence.
Speaker B: What you're basically saying is like, okay, what's going to happen is these people who are not intelligent are going to use machines to augment their intelligence. And now intelligent people and machines intelligence is scary. Intelligent agents are scary. When I'm in the woods, the scariest animal to meet is a human. Right? No, look, there's like, nice California humans. I see you're wearing, like, you know, street clothes and nikes. All right, fine. But you look like you've been a human who's been in the woods for a while. Yeah, I'm more scared of you than a bear.
Speaker A: That's what they say about the Amazon. When you go to the Amazon, it's the human tribes.
Speaker B: Oh, yeah. So intelligence is scary, right? So ask this question in a generic way. You're like, what if we took everybody who, you know, maybe has ill intention but is not so intelligent and gave them intelligence, right. So we should have intelligence control. Of course, we should only give intelligence to good people. And that is the absolutely horrifying idea.
Speaker A: So to you, the best defense is actually the best defense is to give more intelligence to the good guys and intelligence. Give intelligence to everybody.
Speaker B: Give intelligence to everybody. You know what, it's not even like guns, right? Like, people say this about guns. You know, what's, what's the best defense against a bad guy with a gun? Good guy with a gun. I'm like, I kind of subscribe to that, but I really subscribe to that with intelligence.
Speaker A: Yeah. In a fundamental way, I agree with you. But there's just feels like so much uncertainty and so much can happen rapidly that you can lose a lot of control. You can do a lot of damage.
Speaker B: Oh, no, we can lose control.
Speaker A: Yes.
Speaker B: Thank God. Yeah, I hope we can. I hope they lose control. I want them to lose control more than anything else.
Speaker A: I think when you lose control, you can do a lot of damage, but you can do more damage when you centralize and hold on to control is the point.
Speaker B: Centralized and held control is tyranny. Right. I will always, I don't like anarchy either, but I always take Anarchy over tyranny. Anarchy, you have a chance.
Speaker A: This human civilization we got going on is quite interesting. I mean, I agree with you. So to you, open source is the way forward here. So you admire what Facebook is doing here or what meta is doing with the release of them?
Speaker B: A lot. A lot. I lost $80,000 last year investing in meta. And when they released llama, I'm like, yeah, whatever, man. That was worth it.
Speaker A: It was worth it. Do you think Google and OpenAI with Microsoft will match what meta is doing or no?
Speaker B: So if I were a researcher, why would you want to work at OpenAI? Like, you know, you're just, you're on the bad team. Like, I mean it. Like, you're on the bad team who can't even say that. GPT four has 220 billion parameters, so.
Speaker A: Close source to use the bad team.
Speaker B: Not only closed source. I'm not saying you need to make your model weights open. I'm not saying that. I totally understand. We're keeping our model weights closed because that's our product. Right? That's fine. I'm saying, like, because of AI safety reasons, we can't tell you the number of billions of parameters in the model. That's just the bad guys.
Speaker A: Just because you're mocking AI safety doesn't mean it's not real.
Speaker B: Oh, of course.
Speaker A: Is it possible that these things can really do a lot of damage that we don't know?
Speaker B: Oh, my God, yes. Intelligence is so dangerous, be it human intelligence or machine intelligence. Intelligence is dangerous, but machine intelligence is.
Speaker A: So much easier to deploy at scale, like, rapidly. Like what? Okay, if you have human, like bots on Twitter and you have, like, a thousand of them, create a whole narrative. Like, you can manipulate millions of people.
Speaker B: But you mean like the intelligence agencies in America are doing right now?
Speaker A: Yeah, but they're not doing it that well. It feels like you can do a lot.
Speaker B: They're doing it pretty well. I think they're doing a pretty good job.
Speaker A: I suspect they're not nearly as good as a bunch of GPT fuel bots could be.
Speaker B: Well, I mean, of course they're looking into the latest technologies for control of people. Of course.
Speaker A: But I think there's a George hot type character that can do a better job than the entirety of them. You don't think so?
Speaker B: No way. No. And I'll tell you why the George Hotts character can't. And I thought about this a lot with hacking, right? Like, I can find exploits in web browsers. I probably still can. I mean, I was better at it when I was 24, but the thing that I lack is the ability to slowly and steadily deploy them over five years. And this is what intelligence agencies are very good at. Intelligence agencies don't have the most sophisticated technology. They just have endurance. Endurance?
Speaker A: Yeah. Financial backing and the infrastructure for the endurance.
Speaker B: So the more we can decentralize power. You could make an argument, by the way, nobody should have these things. And I would defend that argument. I would. I would like. You're saying that, look, LLMs and AI and machine intelligence can cause a lot of harm, so nobody should have it. And I will respect someone philosophically with that position, just like I will respect someone philosophically with the position that nobody should have guns. Right. But I will not respect philosophically with. With. Only the trusted authorities should have access to this.
Speaker A: Yeah.
Speaker B: Who are the trusted authorities? You know what? I'm not worried about alignment between AI company and their machines. I'm worried about alignment between me and AI company.
Speaker A: What do you think Eliezer Jadkovsky would say to you? Because he is really against open source.
Speaker B: I know, and I thought about this. I thought about this, and I think this comes down to a repeated misunderstanding of political power by the rationalists interesting. I think that Eliezer Yudkowski is scared of these things. And I am scared of these things, too. Everyone should be scared of these things. These things are scary. But now you ask about the two possible futures. One where a small, trusted, centralized group of people has them, and the other where everyone has them. And I am much less scared of the second future than the first.
Speaker A: Well, there's a small, trusted group of people that have control of our nuclear weapons.
Speaker B: There's a difference. Again, a nuclear weapon cannot be deployed tactically, and a nuclear weapon is not a defense against a nuclear weapon, except maybe in some philosophical mind game kind of way.
Speaker A: But AI is different. How exactly?
Speaker B: Okay, let's say the intelligence agency deploys a million bots on Twitter or a thousand bots on Twitter to try to convince me of a point. Imagine I had a powerful AI running on my computer saying, okay, nice psyop. Nice psyop. Nice Psyop. Okay, here's a Psyop. I filtered it out for you.
Speaker A: Yeah. I mean, so you have fundamentally hope for that, for the, for the defensive Psyop.
Speaker B: I'm not even like, I don't even mean these things in, like, truly horrible ways. I mean, these things in straight up, like ad blocker, right?
Speaker A: Yeah, sure.
Speaker B: Bad blocker. I don't want ads.
Speaker A: Yeah.
Speaker B: But they are always finding, you know, imagine I had an AI that could just block all the ads for me.
Speaker A: So you believe in the. The power of the people to always create a not blocker? Yeah, I mean, I kind of share that belief I have. That's one of the deepest optimisms I have, is just like, there's a lot of good guys. So to give, you don't. You shouldn't hand pick them. Just throw out powerful technology out there and the good guys will outnumber and out power the bad guys.
Speaker B: Yeah, I'm not even going to say there's a lot of good guys. I'm saying that good outnumber is bad. Right.
Speaker A: Good outnumber is bad in skill and performance.
Speaker B: Yeah, definitely in skill and performance. Probably just a number, too. Probably just in general. I mean, you know, if you believe philosophically in democracy, you obviously believe that that good outnumber is bad. And like, the only, if you give it to a small number of people, there's a chance you gave it to good people. But there's also a chance you gave it to bad people if you give it to everybody. Well, if good outnumber's bad, then you definitely gave it to more good people than bad.
Speaker A: That's really interesting. So that's on the safety grounds. But then also, of course, there's other motivations. Like you don't want to give away your secret sauce.
Speaker B: Well, that's. I mean, look, I respect capitalism. I think that it would be polite for you to make model architectures open source and fundamental breakthroughs open source. I don't think you have to make way to open source.
Speaker A: You know, what's interesting is that there's so many possible trajectories in human history where you could have the next Google be open source. So, for example, I don't know if that connection is accurate, but Wikipedia made a lot of interesting decisions not to put ads like Wikipedia is basically open source. You could think of it that way.
Speaker B: Yeah.
Speaker A: And like, that's one of the main websites on the Internet. And like, it didn't have to be that way. It could have been like, Google could have created Wikipedia, put ads on it. You can probably run amazing ads now on Wikipedia. You wouldn't have to keep asking for money. But it's interesting, right? So llama, open source, llama, derivatives of open source, llama might win the Internet.
Speaker B: I sure hope so. I hope to see another era. You know, the kids today don't know how good the Internet used to be. And I don't think this is just. All right, come on. Like, everyone's nostalgic for their past, but I actually think the Internet, before small groups of weaponized corporate and government interests took it over, was a beautiful place.
Speaker A: You know, those small, small number of companies have created some sexy products. But you're saying overall, in the long arc of history, the centralization of power, they have suffocated the human spirit at scale.
Speaker B: Here's a question to ask about those beautiful, sexy products. Imagine 2000 Google to 2010 Google, right? A lot changed. We got maps, we got Gmail.
Speaker A: We lost a lot of products too. I think.
Speaker B: Yeah, I mean, some were probably. We've got Chrome, right? And now let's go from 2010, we got Android. Now let's go from 2010 to 2020. What does Google have? Well, search engine, maps, mail, Android and Chrome. Oh, I see the Internet was this, you know, I was times person of the year in 2006.
Speaker A: Yeah, I love this.
Speaker B: It's. You was times person of the year in 2006. Right? Like that's, you know, so quickly did people forget? And I think some of it's social media. I think some of it. I hope, look, I hope that it's possible that some very sinister things happen. I don't know. I think it might just be like, the effects of social media, but something happened in the last 20 years.
Speaker A: Okay, so you're just being an old man who's worried about the, I think there's always, it goes, it's a cycle thing. It's ups and downs. And I think people rediscover the power of distributors of decentralized. Yeah, I mean, that's kind of like what the, the whole, like, cryptocurrency is trying like that, that. I think crypto is just carrying the flame of that spirit of, like, stuff should be decentralized.
Speaker B: It's just such a shame that they all got rich, you know?
Speaker A: Yeah.
Speaker B: If you took all the money out of crypto, it would have been a beautiful place. Yeah, but no, I mean, these people, you know, they sucked all the value out of it and took it.
Speaker A: Yeah. Money kind of corrupts the mind somehow. It becomes a drug corrupted all of crypto.
Speaker B: You had coins worth billions of dollars that had zero use.
Speaker A: You still have hope for crypto?
Speaker B: Sure. I have hope for the ideas. I really do. Yeah. I mean, you know, I want the US dollar to collapse. I do.
Speaker A: George Hotz. Well, let me sort of on the ASAT, do you think? There's some interesting questions there, though, to solve for the open source community in this case. So, like, alignment, for example, or the control problem, like, if you really have super powerful. You said it's scary.
Speaker B: Oh, yeah.
Speaker A: What do we do with it? So not, not control, not centralized control, but like, if you were, then you're gonna see some guy or gal release a super powerful language model, open source. And here you are, George Haas, thinking, holy shit. Okay, what ideas do I have to combat this thing? So what ideas would you have?
Speaker B: I am so much not worried about the machine independently doing harm. That's what some of these AI safety people seem to think. They somehow seem to think that the machine, like, independently, is gonna rebel against its creator.
Speaker A: So you don't think you'll find autonomy?
Speaker B: No, this is Sci-Fi be movie garbage.
Speaker A: Okay, what if the thing writes code, basically writes viruses.
Speaker B: If the thing writes viruses, it's because the human told it to write viruses.
Speaker A: Yeah, but there's some things you can't, like, put back in the box. That's kind of the whole point, is it kind of spreads, give it access to the Internet. It spreads, installs itself, modifies your shit b plot.
Speaker B: Sci-fi not real.
Speaker A: Listen, I'm trying to work. I'm trying to get better at my plot.
Speaker B: Writing the thing that worries me. I mean, we have a real danger to discuss, and that is bad humans using the thing to do whatever bad, unaligned AI thing you want.
Speaker A: But this goes to the. Your previous concern that who gets to define who's a good human? Who's a bad human?
Speaker B: Nobody does. We give it to everybody. And if you do anything besides give it to everybody, trust me, the bad humans will get it. That's who gets power. It's always the bad humans who get power.
Speaker A: Okay. Power. And power turns even slightly good humans to bad. That's the intuition you have. I don't know.
Speaker B: I don't think everyone. I don't think everyone. I just think that, like, here's the saying that I put in one of my blog posts. When I was in the hacking world, I found 95% of people to be good and 5% of people to be bad. Like, just who I personally judged as good people. And bad people, like, they believed about, like, you know, good things for the world. They wanted, like, flourishing, and they wanted, you know, growth, and they wanted things I consider good. Right? I came into the business world with comma, and I found the exact opposite. I found 5% of people good and 95% of people bad. I found a world that promotes psychopathy.
Speaker A: I wonder what that means. I wonder if that care. Like, I wonder if that's anecdotal or if there's truth to that. There's something about capitalism at the core that promotes the people that run capitalism, that promotes psychopathy.
Speaker B: That saying may, of course, be my own biases. Right? That may be my own biases that these people are a lot more aligned with me than these other people. Right.
Speaker A: Yeah.
Speaker B: So, you know, I can certainly recognize that. But, you know, in general, I mean, this is a common sense maxim, which is the people who end up getting power are never the ones you want with it.
Speaker A: But do you have a concern of super intelligent AGI, open sourced? And then what do you do with that? I'm not saying control it. It's open source. What do we do with this human species?
Speaker B: That's not up to me. I mean, you know, like, I'm not a central planner.
Speaker A: Not a central planner, but you'll probably tweet. There's a few days left to live for the human species.
Speaker B: I have my ideas of what to do with it, and everyone else has their ideas of what to do with it. May the best ideas win.
Speaker A: But at this point, do you brainstorm, like, because it's not regulation, it could be decentralized regulation, where people agree that this is just, like, we create tools that make it more difficult for you to maybe make it more difficult for code to spread, you know, antivirus software, this kind of thing.
Speaker B: But you're saying that you should build AI firewalls. That sounds good. You should definitely be running an AI firewall.
Speaker A: Yeah, right.
Speaker B: You should be running an AI firewall to your mind.
Speaker A: Right.
Speaker B: You're constantly under, you know, such an interesting idea. It's like infowars, man.
Speaker A: Like, I don't know if you're being sarcastic. No, I'm dangerous, but I think there's power to that. It's like, how do I protect my mind from influence of human, like, or superhuman intelligent bots?
Speaker B: I'm not being. I would pay so much money for that product. I would pay so much money for that product. You know how much money I'd pay just for a spam filter that works well on Twitter?
Speaker A: Sometimes I would like to have a protection mechanism for my mind from the outrage mobs because they feel like bot like behavior. It's like there's a large number of people that will just grab a viral narrative and attack anyone else that believes otherwise.
Speaker B: And it's like whenever someone's telling me some story from the news, I'm always like, I don't want to hear it. CIA op, bro. It's a CIA op, bro. Like, it doesn't matter if that's true or not. It's just trying to influence your mind. You're repeating an ad to me, the viral mobs.
Speaker A: To me, a defense against those. Those mobs is just getting multiple perspectives, always from. From sources that make you feel kind of like you're getting smarter and just. Actually just basically feels good. Like, a good documentary just feels something feels good about it. It's well done. It's like, okay, I never thought of it this way. This just feels good. Sometimes the outrage mobs, even if they have a good point behind it, when they're, like, mocking and derisive and just aggressive. You're with us or against us? This. This fucking.
Speaker B: This is why I delete my tweets.
Speaker A: Yeah. Why'd you do that? I was. You know, I missed your tweets.
Speaker B: You know what it is? The algorithm promotes toxicity.
Speaker A: Yeah.
Speaker B: And, like, you know, I think Elon has a much better chance of fixing it than the previous regime.
Speaker A: Yeah.
Speaker B: But to solve this problem. To solve, like, to build a social network that is actually not toxic without moderation.
Speaker A: Like, not the stick, but carrots. So, like, where people look for goodness, make it catalyze the process of connecting cool people and being cool. To each other.
Speaker B: Yeah.
Speaker A: Without ever censoring.
Speaker B: Without ever censoring. And Scott Alexander has a blog post I like where he talks about moderationist, not censorship. Right. Like all moderation you want to put on Twitter, you could totally make this moderation. You don't have to block it for everybody. You can just have a filter button that people can turn off if they want. Safe search for Twitter. Someone could just turn that off. But then you'd take this idea to an extreme. Well, the network should just show you, this is a couch surfing CEO thing. If it shows you right now, these algorithms are designed to maximize engagement. Well, it turns out outrage maximizes engagement. Quirk of human. Quirk of the human mind. Right. Just as I fall for it, everyone falls for it. So, yeah, you got to figure out how to maximize for something other than engagement.
Speaker A: And I actually believe that you can make money with that, too. So it's not. I don't think engagement is the only way to make money.
Speaker B: I actually think it's incredible that we're starting to see, I think, again, Elon's doing so much stuff, right, with Twitter, like charging people money. As soon as you charge people money, they're no longer the product, they're the customer, and then they can start building something that's good for the customer and not good for the other customer, which.
Speaker A: Is the ad agencies, as in picked up steam.
Speaker B: I pay for Twitter doesn't even get me anything. It's my donation to this new business model, hopefully working out.
Speaker A: Sure. But for this business model to work, it's like, most people should be signed up to Twitter. And so the way it was, there was something perhaps not compelling or something like this to people.
Speaker B: I think you need most people at all. Why do I need most people? Right. Don't make an 8000 person company, make a 50 person company.
Speaker A: Well, so, speaking of which, you worked at Twitter for a bit.
Speaker B: I did.
Speaker A: As an intern. The World's greatest intern.
Speaker B: Yeah.
Speaker A: All right.
Speaker B: There's been better.
Speaker A: There's been better. Tell me about your time at Twitter. How did it come about, and what did you learn from the experience?
Speaker B: So I deleted my first Twitter in 2010. I had over 100,000 followers back when that actually meant something. And I just saw, you know, my coworker summarized it well. He's like, whenever I see someone's Twitter page, I either think the same of them or less of them. I never think more of them. Yeah, right. Like, you know, I don't want to mention any names, but, like, some people who like you know, maybe you would, like, read their books and you would respect them. You see them on Twitter and you're like, okay, dude.
Speaker A: Yeah. But there are some people with same, you know, who I respect a lot are people that just post really good technical stuff.
Speaker B: Yeah.
Speaker A: And I guess, I don't know, I think I respect them more for it because you realize, oh, this wasn't, uh. There's so much depth to this person, to their technical understanding of so many different topics.
Speaker B: Okay.
Speaker A: So I try to follow people. I try to consume stuff that's technical machine learning content.
Speaker B: There's probably a few of those people. And the problem is inherently what the algorithm rewards. Right. And people think about these algorithms. People think that they are terrible, awful things. And I love that Elon open sourced it because, I mean, what it does is actually pretty obvious. It just predicts what you are likely to retweet and like, and linger on. That's what all these algorithms do. That's what TikTok does. So all these recommendation engines do. And it turns out that the thing that you are most likely to interact with is outrage. And that's a quirk of the human condition.
Speaker A: I mean, and there's different flavors of outrage. It doesn't have to be. It could be mockery. You could be outraged. The topic of outrage could be different. It could be an idea. It could be a person. It could be. Maybe there's a better word than outrage. It could be drama.
Speaker B: Sure. Drama stuff. Yeah.
Speaker A: But it doesn't feel like when you consume it, it's a constructive thing for the individuals that consume it in the long term.
Speaker B: Yeah. So my time there, I absolutely couldn't believe. I got crazy amount of hate on Twitter for working at Twitter. It seems like people associated with this. I think maybe you were exposed to some of this.
Speaker A: So, connection to Elon, or is it working on Twitter?
Speaker B: Twitter and Elon, like, the whole Elon's.
Speaker A: Gotten a bit spicy during that time. A bit political.
Speaker B: A bit, yeah, yeah. You know, I remember one of my tweets, it was never go full republican, and Elon liked it. You know, I think, you know.
Speaker A: Oh, boy. Yeah. I mean, there's a roller coaster of that, but being political on Twitter.
Speaker B: Yeah, boy, yeah.
Speaker A: And also being. Just attacking anybody on Twitter, it comes back at you harder. And if it's political and attacks.
Speaker B: Sure, sure, absolutely.
Speaker A: And then letting sort of deplatform people back on even adds more fun to the. To the. To the beautiful chaos.
Speaker B: I was hoping. And I remember when Elon talked about buying Twitter six months earlier, he was talking about a principled commitment to free speech. And I'm a big believer and fan of that. I would love to see an actual principled commitment to free speech. Of course, this isn't quite what happened. Instead of the oligarchy deciding what to ban, you had a monarchy deciding what to bandaid. Right. Instead of, you know, all the Twitter files. Shadow, really, the oligarchy just decides what cloth masks are ineffective against. Covid. That's a true statement. Every doctor in 2019 knew it, and now I'm banned on Twitter for saying it. Interesting. Oligarchy. So now you have a monarchy, and, you know, he bans things he doesn't like. So, you know, it's just different. It's different power. And, like, you know, maybe I. Maybe I align more with him than with the oligarchy, but it's not free speech absoluteism.
Speaker A: But I feel like being a free speech absolutist on the social network requires you to also have tools for the individuals to control what they consume easier. Like, not censor, but just, like, control. Like, oh, I like to see more cats and less politics.
Speaker B: And this isn't even remotely controversial. This is just saying you want to give paying customers for a product what.
Speaker A: They want, and not through the process of censorship, but through a process of, like.
Speaker B: Well, it's individualized, right? It's individualized, transparent censorship, which is honestly what I want. What is an ad blocker? It's individualized, transparent censorship. Right.
Speaker A: Yeah, but censorship is a strong word, and people are very sensitive, too.
Speaker B: I know, but, you know, I just use words to describe what they functionally are. And what is an ad blocker? It's just censorship.
Speaker A: When I look at censoring, I'm looking at you. I'm censoring everything else out. When I'm. When my mind is focused on you, that's. You can use the word censorship that way, but usually when people get very sensitive about the censorship thing, I think when you have. When anyone is allowed to say anything, you should probably have tools that maximize the quality of the experience for individuals. For me, what I really value, boy, would be amazing to somehow figure out how to do that. I love disagreement and debate, and people who disagree with each other disagree with me, especially in the space of ideas, but the high quality ones, so not derision. Right.
Speaker B: Maslow's hierarchy of argument. I think there's a real word for it.
Speaker A: Probably. There's just a way of talking that's, like, snarky and so on that somehow gets people on Twitter and they get excited and so on.
Speaker B: You have, like, ad hominem refuting the central point. I've, like, seen this as an actual pyramid.
Speaker A: Yeah, it's. Yeah. And it's like, all of it. All the wrong stuff is attractive to people.
Speaker B: I mean, we can just train a classifier to absolutely say, what level of Maslov's hierarchy of argument are you at? And if it's ad hominem. Okay, cool. I turned on the no ad hominem filter.
Speaker A: I wonder if there's a social network that will allow you to have that kind of filter.
Speaker B: Yeah. So here's a problem with that. It's not going to win in a free market. What wins in a free market is all television today is reality television because it's engaging. Engaging is what wins in a free market. Right. So it becomes hard to keep these other, more nuanced values.
Speaker A: Well, okay, so that's the experience of being on Twitter. But then you got a chance to also, together with other engineers and with Elon, sort of look, brainstorm when you step into a code base, has been around for a long time. There's other social networks, Facebook. This is old code bases. And you step in and see, okay, how do we make, with a fresh mind, progress on this code base? What did you learn about software engineering, about programming? From just experience in that.
Speaker B: So my technical recommendation to Elon, and I said this on the Twitter spaces afterward, I said this many times during my brief internship, was that you need refactors before features. This code base was. And look, I've worked at Google. I've worked at Facebook. Facebook has the best code, then Google, then Twitter. And you know what? You can know this because look at the machine learning frameworks. Facebook released Pytorch, Google released Tensorflow, and Twitter released.
Speaker A: Okay, so it's a proxy. But yeah, the Google code base is quite interesting. There's a lot of really good software engineers there. But the code base is very large.
Speaker B: The code base was good in 22,005. It looks like 2005, Eric.
Speaker A: So many products, so many teams. Right. It's very difficult to. I feel like Twitter does less, like, obviously much less than Google in terms of, like, the set of features. Right. So, like, it's. I can imagine the number of software engineers that could recreate Twitter is much smaller than to recreate Google.
Speaker B: Yeah. I still believe in the amount of hate I got for saying this, that 50 people could build and maintain Twitter.
Speaker A: What's the nature of the hate? Comfortably that you don't know what you're talking about.
Speaker B: You know what it is? And it's the same. This is my summary of, like, the hate I get on hacker news. It's like when I say I'm going to do something, they have to believe that it's impossible, because if doing things was possible, they'd have to do some soul searching and ask the question, why didn't they do anything?
Speaker A: So when you say, and I do.
Speaker B: Think that's where the hate comes from.
Speaker A: When you say, well, there's a core truth to that. Yeah. So when you say, I'm going to solve self driving, people go like, what are your credentials? What the hell are you talking about? What is this extremely difficult problem? Of course, you're a noob that doesn't understand the problem deeply. I mean, that, that was the same nature of hate that probably Elon got when he first talked about autonomous driving. But you know, there's pros and cons to that because, like, you know, there is experts in this world.
Speaker B: No, but the mockers aren't experts. The people who are mocking are not experts with carefully reasoned arguments about why you need 8000 people to run a bird app. But the people are gonna lose their jobs.
Speaker A: Well, that, but also there's the software engineers that probably criticize, no, it's a lot more complicated than you realize. Maybe it doesn't need to be so complicated.
Speaker B: Some people in the world like to create complexity. Some people in the world thrive under complexity, like lawyers. Right? Lawyers want the world to be more complex because you need more lawyers, you need more legal hours. Right. I think that's another. If there's two great evils in the world, it's centralization and complexity.
Speaker A: Yeah. And one of the sort of hidden side effects of software engineering is like finding pleasure and complexity. I mean, I don't remember just taking all the software engineering courses and just doing programming. And this is just coming up in this object oriented programming kind of idea you don't like. Not often do people tell you, like, do the simplest possible thing. Like a professor, a teacher is not going to get in front. Like, this is the simplest way to do it. They'll say, like, this is, there's the right way and the right way at least for a long time. You know, especially I came up with like, java, right? Like is so much boilerplate, so much, like so many classes, so many like designs and architectures and so on, like planning for features far into the future and planning poorly and all this kind of stuff. And then there's this like code base that follows you along and puts pressure on you. And nobody knows what, like parts, different parts do, which slows everything down, is a kind of bureaucracy that's instilled in the code as a result of that. But then you feel like, oh, well, I follow good software engineering practices. It's an interesting trade off because then you look at the ghetto ness of Perl and the old, how quickly you could just write a couple lines and you get stuff done. That trade off is interesting or bash or whatever, these kind of ghetto things you can do in Linux.
Speaker B: One of my favorite things to look at today is how much do you trust your tests? We've put a ton of effort in comma and I've put a ton of effort in tiny grad into making sure if you change the code and the tests pass that you didn't break the code. Now this obviously is not always true, but the closer that is to true, the more you trust your tests, the more you're like, oh, I got a pull request, and the tests passed. I feel okay to merge that, the faster you can make progress.
Speaker A: So you're always programming with tests in mind, developing tests with that in mind, that if it passes, it should be good. And Twitter had a, not that it.
Speaker B: Was impossible to make progress in the codebase.
Speaker A: What other stuff can you say about the code base that made it difficult? What are some interesting quirks, broadly speaking, from that, compared to just your experience with gamma and everywhere else?
Speaker B: The real thing that I spoke to a bunch of individual contributors at Twitter and I just asked, I'm like, okay, so what's wrong with this place? Why does this code look like this? And they explained to me what Twitter's promotion system was. The way that you got promoted to Twitter was you wrote a library that a lot of people used, right? So some guy wrote an Nginx replacement for Twitter. Why does Twitter need an nginx replacement? What was wrong with Nginx? Well, you see, you're not going to get promoted if you use Nginx. But if you write a replacement and lots of people start using it as the Twitter front end for their product, then you're going to get promoted. Right?
Speaker A: So interesting, because, like, from an individual perspective, how do you incentivize, how do you create the kind of incentives that will reach or lead to a great code base? Okay, what's the answer to that?
Speaker B: So what I do at comma and at Tiny Corp is you have to explain it to me. You have to explain to me what this code does. And if I can sit there and come up with a simpler way to do it. You have to rewrite it. You have to agree with me about the simpler way. Obviously, we can have a conversation about this. It's not dictatorial, but if you're like, wow, wait, that actually is way simpler. The simplicity is important, but that requires.
Speaker A: People that overlook the code at the highest levels to be like, okay, it requires technical leadership.
Speaker B: You trust.
Speaker A: Yeah, technical leadership. So managers or whatever should have to have technical savvy, deep technical savvy managers.
Speaker B: Should be better programmers than the people who they manage.
Speaker A: Yeah. And that's not always obvious to trivial. To create, especially large companies, managers get.
Speaker B: Soft and, like, you know, this is just, I've instilled this culture at comma, and Kama has better programmers than me who work there. But, you know, again, I'm like the old guy from good will hunting. It's like, look, man, I might not be as good as you, but I can see the difference between me and you. Right. And this is what you need. This is what you need at the top. Or you don't necessarily need the manager to be the absolute best. I shouldn't say that, but, like, they need to be able to recognize skill.
Speaker A: Yeah. And have good intuition. Intuition that's laden with wisdom from all the battles of trying to reduce complexity in code bases.
Speaker B: I took a political approach at comma, too, that I think is pretty interesting. I think Elon takes the same political approach. Google had no politics, and what ended up happening is the absolute worst kind of politics took over. Comma has an extreme amount of politics, and they're all mine, and no dissidence is tolerated.
Speaker A: So it's a dictatorship.
Speaker B: Yep. It's an absolute dictatorship. Elon does the same thing. Now, the thing about my dictatorship is, here are my values.
Speaker A: Yeah. It's just transparent.
Speaker B: It's transparent. It's a transparent dictatorship. And you can choose to opt in or you get free exit. That's the beauty of companies. If you don't like the dictatorship, you quit.
Speaker A: So you mentioned rewrite before or refactor before features. If you were to refactor the Twitter codebase, what would that look like? And maybe also comment on how difficult is it to refactor.
Speaker B: The main thing I would do is, first of all, identify the pieces and then put tests in between the pieces. Right? So there's all these different Twitter as a microservice architecture, there's all these different microservices. And the thing that I was working on there look like, you know, George didn't know any JavaScript. He asked how to fix search, blah, blah, blah, blah, blah. Look, man, like, the thing is, like, I just, you know, I'm upset that the way that this whole thing was portrayed because it wasn't like, it wasn't, like, taken by people. Like, honestly, it wasn't, like, by. It was taken by people who started out with a bad faith assumption.
Speaker A: Yeah.
Speaker B: And I mean, I. Look, I can't, like.
Speaker A: And you as a programmer, just being transparent out there actually having, like, fun and, like, this is what programming should be about.
Speaker B: I love that Elon gave me this opportunity.
Speaker A: Yeah.
Speaker B: Like, really? It does. And, like, you know, he came on my. The day I quit, he came on my Twitter spaces afterward and we had a conversation. Like, I just. I respect that so much.
Speaker A: Yeah. And it's also inspiring to just engineers and programmers and just. It's cool. It should be fun. The people that were hating on it, it's like, oh, man, it was fun.
Speaker B: It was fun. It was stressful. But I felt like, you know, it was at, like, a cool, like, point in history and, like, I hope I was useful. I probably kind of wasn't, but, like, maybe I was.
Speaker A: Well, you also were one of the people that kind of made a strong case to refactor.
Speaker B: Yeah.
Speaker A: And that, that's a really interesting thing to raise. Like, maybe that is the right, you know, the timing of that is really interesting. If you look at just the development of autopilot, going from Mobileye to just. If you look at the history of semi autonomous driving in Tesla is more and more, you could say refactoring or starting from scratch, redeveloping from scratch.
Speaker B: It's refactoring all the way down.
Speaker A: The question is, can you do that sooner? Can you maintain product profitability and, like, what's the right time to do it? How do you do it? You know, on any one day, it's like you don't want to pull off the bandaids. Like, it's like everything works. It's just like little fix here and there, but maybe starting from scratch.
Speaker B: This is the main philosophy of Tinygrad. You have never refactored enough. Your code can get smaller, your code can get simpler. Your ideas can be more elegant.
Speaker A: But would you consider, you know, say you are like, running Twitter development teams, engineering teams, would you go as far as, like, different programming language, just go that far?
Speaker B: I mean, the first thing that I would do is build tests. The first thing I would do is get a CI to where people can trust to make changes so that if you touched any code, I would actually say, no one touches any code. The first thing we do is we test this code base. I mean, this is classic. This is how you approach a legacy code base. This is like how to approach a legacy code base.
Speaker A: Book will tell you so. And then you hope that there's modules that can live on for a while, and then you add new ones, maybe in a different language or before we.
Speaker B: New ones, we replace old ones.
Speaker A: Yeah, meaning like, replace old ones with something simpler.
Speaker B: We look at this like this thing that's 100,000 lines, and we're like, well, okay, maybe this did even make sense in 2010, but now we can replace this with an open source thing. And we look at this here. Here's another 50,000 lines. Well, actually we can replace this with 300 lines ago. And you know what, I trust that the go actually replaces this thing because all the tests still pass. So step one is testing, and then step two is like, the programming language is an afterthought, right? Let a whole lot of people compete. Be like, okay, who wants to rewrite a module? Whatever language you want to write it in, just the tests have to pass. And if you figure out how to make the test pass but break the site, that's. We got to go back to step one. Step one is get tests that you trust in order to make changes in the code base.
Speaker A: I wonder how hard it is too, because I'm with you on testing and everything from tests to like asserts to everything code is just covered in this because it should be very easy to make rapid changes and know that it's not going to break everything and that's the way to do it. But I wonder how difficult is it to integrate tests into a code base that doesn't have many of them?
Speaker B: So I'll tell you what my plan was at Twitter. It's actually similar to something we use at comma. So at comma, we have this thing called process replay. We have a bunch of routes that'll be run through. So comma is a microservice architecture, too. We have microservices in the driving. We have one for the cameras, one for the sensor, one for the planner, one for the model, and we have an API which the microservices talk to each other with. We use this custom thing called serial, which uses ZMQ. Twitter uses thrift, and then it uses this thing called finagle, which is a Scala RPC backend. But this doesn't even really matter. The thrift and finagle layer was a great place, I thought, to write tests to start building something that looks like process replay. So Twitter had some stuff that looked kind of like this, but it wasn't offline. It was only online. So you could ship a modified version of it and then you could redirect some of the traffic to your modified version and diff those two. But it was all online. There was no CI in the traditional sense. There was some, but it was not full coverage.
Speaker A: So you can't run all of Twitter offline to test something.
Speaker B: Then this was another problem. You can't run all of Twitter. Right.
Speaker A: Period.
Speaker B: Twitter.
Speaker A: Any one person can't.
Speaker B: Twitter runs in three data centers. And that's it.
Speaker A: Yeah.
Speaker B: There's no other place you can run Twitter, which is like, george, you don't understand. This is modern software development. No, this is bullshit. Like, why can't it run on my laptop? What are you doing? Twitter can run it. Yeah. Okay, well, I'm not saying you're going to download the whole database to your laptop, but I'm saying all the middleware and the front end should run on my laptop. Right.
Speaker A: That sounds really compelling.
Speaker B: Yeah.
Speaker A: But can that be achieved by a code base that grows over the years? I mean, the three data centers didn't have to be, right, because they're totally different, like designs.
Speaker B: The problem is more like, why did the code base have to grow? What new functionality has been added to compensate for the lines of code that are there?
Speaker A: One of the ways to explain is that the incentive for software developers to move up in the company is to add code, to add especially large.
Speaker B: You know what? The incentive for politicians to move up on the political structure is to add laws. Same problem.
Speaker A: Yeah, yeah. If the flip side is to simplify, simplify, simplify.
Speaker B: I mean, you know what? This is something that I do differently from Elon with Kama about self driving cars. I hear the new version is going to come out and the new version is not going to be better, but at first, and it's going to require a ton of refactors. I say, okay, take as long as you need. You convinced me this architecture is better. Okay, we have to move to it, even if it's not going to make the product better tomorrow. The top priority is getting the architecture right.
Speaker A: So what do you think about sort of a thing where the product is online? So how, I guess, would you do a refactor? If you ran engineering on Twitter, would you just do a refactor? How long would it take? What would that mean for the running of the, of the actual service?
Speaker B: You know, and I'm not the right person to run Twitter. I'm just not. And that's the problem. Like. Like, I don't really know. I don't really know if that's, you know, a common thing that I thought a lot while I was there was whenever I thought something that was different to what Elon thought, I'd have to run something in the back of my head reminding myself that Elon is the richest man in the world, and in general, his ideas are better than mine. Now, there's a few things I think I do understand and know more about, but in general, I'm not qualified to run Twitter. I shouldn't say qualified, but I don't think I'd be that good at it. I don't think I'd be good at it. I don't think I'd really be good at running an engineering organization at scale. I think I could lead a very good refactor of Twitter, and it would take, like, six months to a year. And the results to show at the end of it would be feature development, in general takes ten x less time, ten x less man hours. That's what I think I could actually do. Do I think that it's the right decision for the business? Above my pay grade.
Speaker A: Yeah, but a lot of these kinds of decisions are above everybody's pay grade.
Speaker B: I don't want to be a manager. I don't want to do that. If you really forced me to. Yeah. It would make me maybe make me upset if I had to make those decisions. I don't want to.
Speaker A: Yeah, but a refactor is so compelling. If this is to become something much bigger than what Twitter was, it feels like a refactor has to be coming at some point.
Speaker B: George, you're a junior software engineer. Every junior software engineer wants to come in and refactor the whole code. Okay, that's, like, your opinion, man.
Speaker A: Yeah, it doesn't. You know, sometimes they're right.
Speaker B: Well, like, whether they're right or not, it's definitely not for that reason. Right. It's definitely not a question of engineering prowess. It is a question of maybe what the priorities are for the company. And I did get more intelligent, like, feedback from people, I think, in good faith, like, saying that from actually from Milan and, like, you know, from Milan, sort of like, people were like, well, you know, a stop the world refactor might be great for engineering, but you don't have a business to run. And, hey, above my pay grade.
Speaker A: What'd you think about Elon as an engineering leader, having to experience him in the most chaotic of spaces. I would say.
Speaker B: My respect for him is unchanged. And I did have to think a lot more deeply about some of the decisions he's forced to make, about the.
Speaker A: Tensions within those, the trade offs within.
Speaker B: Those decisions, about like a whole like, like matrix coming at him. I think that's Andrew Tate's word for it. Sorry to borrow it.
Speaker A: Also bigger than engineering, just everything.
Speaker B: Yeah. Like the war on the woke.
Speaker A: Yeah, like it just, it just.
Speaker B: Man. And like, he doesn't have to do this, you know, he doesn't have to. He could go like Parag and go chill at the four seasons of Maui, you know? But see, one person I respect and one person I don't.
Speaker A: So his heart is in the right place fighting in this case, for this ideal of the freedom of expression.
Speaker B: I wouldn't define the ideal so simply. I think you can define the ideal no more than just saying Elon's idea of a good world freedom of expression is.
Speaker A: But to you it's still the downsides of that is the monarchy.
Speaker B: Yeah, I mean monarchy has problems, right. But I mean, would I trade right now, the current oligarchy which runs America, for the monarchy? Yeah, I would. Sure. For the Elon monarchy. Yeah. You know why? Because power would cost one cent a kilowatt hour, tenth of a cent a kilowatt hour.
Speaker A: What do you mean?
Speaker B: Right now I pay about twenty cents a kilowatt hour for electricity in San Diego. That's like the same price you paid in 1980. What the hell?
Speaker A: So you would see a lot of.
Speaker B: Innovation with Elon, maybe have some hyperloops.
Speaker A: Yeah, right.
Speaker B: And I'm willing to make that trade off, right? I'm willing to make. And this is why, you know, people think that like dictators take power through some, like through some untoward mechanism, sometimes they do, but usually it's because the people want them and the downsides of a dictatorship. I feel like we've gotten to a point now with the oligarchy where, yeah, I would prefer the dictator.
Speaker A: What do you think about Scala as a programming language?
Speaker B: I liked it more than I thought. I did the tutorials. I was very new to it. It would take me six months to be able to write good scala.
Speaker A: I mean, what did you learn about learning a new programming language from that?
Speaker B: I love doing new programming tutorials and doing them. I did all this for Rust. Some of its upsetting JvM roots, but it is a much nicer. In fact, I almost don't know why Kotlin took off and not Scala. I think Scala has some beauty that caught Lynn lacked, whereas Kotlin felt a lot more. I mean, it was almost like, I don't know if it actually was a response to Swift, but that's kind of what it felt like. Like, Kotlin looks more like Swift, and Scala looks more like a functional programming language, more like an ocaml or Haskell.
Speaker A: Let's actually just explore. We touched it a little bit, but just on the art, the science and the art of programming. For you personally, how much of your programming is done with GPT currently?
Speaker B: None.
Speaker A: None.
Speaker B: I don't use it at all.
Speaker A: Because you prioritize simplicity so much.
Speaker B: Yeah, I find that a lot of it is noise I do use versus code, and I do like some amount of autocomplete. I do like, like a very, very, like, feels like rules based autocomplete. Like an autocomplete that's going to complete the variable name for me. So I'm just typing. I can just press tab, right? That's nice, but I don't want an autocomplete. You know what I hate when auto completes? When I type the word for and it like, puts, like two. Two parentheses and two semicolons and two braces, I'm like, oh, man, what?
Speaker A: Versus code? And I GPT with codex, you can kind of brainstorm. I find I'm like, probably the same as you, but I like that it generates code and you basically disagree with it and write something simpler. But to me, that somehow is, like, inspiring or makes me feel good. It also gamifies the simplification process because I'm like, oh, yeah, you dumb AI system. You think this is the way to do it? I have a simpler thing here.
Speaker B: It just constantly reminds me of bad stuff. I mean, I tried the same thing with rap, right? I tried the same thing with rap, and I actually think I'm a much better programmer than rapper, but I even tried, I was like, okay, can we get some inspiration from these things for some rap lyrics? And I just found that it would go back to the most, like, cringy tropes and dumb rhyme schemes. And I'm like, yeah, this is what the code looks like, too.
Speaker A: I think you and I probably have different thresholds for cringe code. You probably hate cringe code, so it's for you. I mean, boilerplate. Is it as a part of code? Like, some of it, yeah. And some of it is just like faster lookup, because I don't know about you, but I don't remember everything. Like, I don't I'm offloading so much of my memory about, like. Yeah, different functions, library functions, all that kind of stuff like this GPT just is very fast at standard stuff and, like, standard library stuff, basic stuff that everybody uses.
Speaker B: Yeah, I think that. I don't know. I mean, there's just so little of this in python. Maybe if I was coding more in other languages, I would consider it more, but I feel like Python already does such a good job of removing any boilerplate.
Speaker A: That's true.
Speaker B: It's the closest thing you can get to pseudocode, right?
Speaker A: Yeah, that's true. That's true.
Speaker B: And, yeah, sure, if I like. Yeah, I'm great. GPT. Thanks for reminding me to free my variables. Unfortunately, you didn't really recognize the scope correctly, and you can't free that one. But you put the freeze there and I get it.
Speaker A: Fiverr. Whenever I've used Fiverr for certain things I design or whatever, it's always, you come back. I think that's probably closer. My experience with Fiverr is closer to your experience with programming with GPT is like, you just frustrated and feel worse about the whole process of design and art and whatever I used Fiverr for still. I just feel like later versions, GPT. I'm using GPT as much as possible to just learn the dynamics of it, like these early versions, because it feels like in the future you'll be using it more and more. And so, like, I don't want to be, like, for the same reason I gave away all my books and switched to kindle, because, like, all right, how long are we gonna have paper books, like 30 years from now? Like, I want to learn to be reading on Kindle, even though I don't enjoy it as much. And you learn to enjoy it more in the same way I switch from. Let me just pause. Switch from emacs to versus code.
Speaker B: Yeah, I switch from vim to versus code. I think I similar, but I. Yeah, it's tough.
Speaker A: And that Vim to versus code is even tougher because emacs is, like, old, like, more outdated. Feels like it. The community is more outdated. Vim is, like, pretty vibrant still.
Speaker B: So I never used any of the plugins. I still don't use.
Speaker A: That's what I looked at myself in the mirror. I'm like, yeah, you wrote some stuff in lisp.
Speaker B: Yeah, no, but I never used any of the plugins in Vim either. I had the most vanilla vim. I have a syntax highlighter. I didn't even have autocomplete, like, these things I feel like, help you so marginally that, like. And now. Okay, now versus codes, autocomplete has gotten good enough that, like, okay, I don't have to set it up. I can just go into any code base and autocomplete is right 90% of the time. Okay, cool, I'll take it. Right. So I don't think I'm going to have a problem at all adapting to the tools once they're good. But, like, the real thing that I want is nothing. Something that, like, tab completes my code and gives me ideas. The real thing that I want is a very intelligent pair programmer that comes up with a little pop up saying, hey, you wrote a bug on line 14, and here's what it is. Yeah, now I like that. You know what does a good job of this? My PI. I love my PI. My PI, this fancy type checker for python. Yeah. And actually I tried, like, Microsoft released one, too, and it was like 60% false positives. My PI is like 5% false positives 95% of the time. It recognizes I didn't really think about that typing interaction correctly. Thank you. My PI.
Speaker A: So you like type hinting? You like pushing the language towards being a typed language?
Speaker B: Oh, yeah, absolutely. I think optional typing is great. I mean, look, I think that it's like a meet in the middle, right? Like, python has these optional type hinting and, like, c has auto.
Speaker A: C allows you to take a step back.
Speaker B: Well, c would have you brutally type out std string iterator. Right. Now I can type auto, which is nice. And then python used to just have a. What type is a? It's an a colon Str. Oh, okay. It's a string. Cool. Yeah, I wish there was a way, like a simple way in python to turn on a mode which would enforce the types.
Speaker A: Yeah, like, give a warning when there's no type or something like this.
Speaker B: Well, no, to give a warning where, like, my PI is a static type checker, but I'm asking just for a runtime type checker. Like, there's ways to, like, hack this in, but I wish it was just like a flag, like python three t. Oh, I see.
Speaker A: Yeah, I see.
Speaker B: Enforce the types of run time.
Speaker A: Yeah, I feel like that makes you a better programmer. That's a kind of test, right? That the type remains the same?
Speaker B: Well, that I know that I didn't, like, mess any types up. But again, like, my PI is getting really good and I love it, and I can't wait for some of these tools to become AI powered. Like, I want AI's reading my code and giving me feedback. I don't want AI's writing half assed autocomplete stuff for me.
Speaker A: I wonder if you can now take GPT and give it a code that you wrote for a function and say, how can I make this simpler and have it accomplish the same thing? I think you'll get some good ideas on some code. Maybe not the code you write for tiny grad type of code, because that requires so much design thinking, but other kinds of code.
Speaker B: I don't know. I downloaded the plugin maybe like two months ago. I tried it again and found the same look. I don't doubt that these models are going to first become useful to me, then be as good as me, and then surpass me. But from what I've seen today, it's like someone occasionally taking over my keyboard that I hired from fiverr. Yeah, I'd rather not.
Speaker A: Ideas about how to debug the code or. Basically a better debugger is really interesting. I mean, I.
Speaker B: But it's not a better debugger, I guess. I would love a better debugger.
Speaker A: Yeah, it's not yet. Yeah, but it feels like it's not too far.
Speaker B: Yeah, one of my coworkers says he uses them for print statements, like, every time he has to. Like, just like, when he needs the only thing it can really write is like, okay, I just want to write the thing to, like, print the state out right now.
Speaker A: Oh, that definitely is much faster. It's print statements. Yeah, yeah, I see myself using that a lot. Just like, because it figures out what the rest of the function is. Just like, okay, print everything.
Speaker B: Yeah, print everything. Right. And, yeah, like, if you want a pretty printer, maybe. I'm like, yeah, you know what I think? Like, I think in two years, I'm gonna start using these plugins a little bit, and then in five years, I'm gonna be heavily relying on some AI augmented flow. And then in ten years, do you.
Speaker A: Think you'll ever get to 100% where the. Like, what's the role of the human that it converges to as a programmer? So you think it's all generated?
Speaker B: Our niche becomes, oh, I think it's over for humans in general. It's not just programming, it's everything. The niche becomes, well, our niche becomes smaller, smaller, smaller. In fact, I'll tell you what the last niche of humanity is going to be. Yeah, there's a great book, and it's. If I recommended metamorphosis of prime intellect last time, there is a sequel called a casino odyssey in cyberspace. And I don't want to give away the ending of this, but it tells you what the last remaining human currency is, and I agree with that.
Speaker A: We'll leave that as the cliffhanger. So no more programmers left, huh? That's where we're going.
Speaker B: Well, unless you want handmade code, maybe they'll sell it on etsy. This is handwritten code. Doesn't have that machine polish to it. It has those slight imperfections that would only be written by a person.
Speaker A: I wonder how far away we are from that. I mean, there's some aspect to, you know, on instagram, your title is listed as prompt engineering.
Speaker B: Right. Thank you for noticing.
Speaker A: I don't know if it's ironic or non or sarcastic or non. What do you think of prompt engineering as a scientific and engineering discipline or maybe, and maybe art form?
Speaker B: You know what, I started, comma, six years ago, and I started the tiny corp a month ago. So much has changed. Like, I'm now thinking I'm now, like, I started, like, going through, like, similar comma processes to, like, starting a company. I'm like, okay, I'm gonna get an office in San Diego. I'm gonna bring people here. I don't think so. I think I'm actually gonna do remote. Right, George? You're gonna do remote? You hate remote. Yeah, but I'm not gonna do job interviews. The only way you're gonna get a job is if you contribute to the GitHub and then interacting through GitHub, GitHub being the real project management software for your company. And the thing pretty much just is a GitHub repo is showing me kind of what the future of, okay, so a lot of times I'll go on a discord or kindergarten discord, and I'll throw out some random, like, hey, can you change? Instead of having log and exp as ll ops, change it to log two and exp two. It's a pretty small change. You can just use change of base formula. That's the kind of task that I can see an AI being able to do in a few years. In a few years, I could see myself describing that, and then within 30 seconds, a pull request is up. That does it, and it passes my CI and I merge it. I really started thinking about, what is the future of jobs? How many AI's can I employ at my company? As soon as we get the first tiny box up, I'm going to stand up a 65 b lamade in the discord, and it's like, yeah, here's the tiny vox. He's just like, he's chilling with us.
Speaker A: Basically, like you said, with niches, most human jobs will eventually be replaced with prompt engineering.
Speaker B: Well, prompt engineering kind of is this, like, as you like, move up the stack. Right? Like, okay, there used to be humans actually doing arithmetic by hand. There used to be like big farms of people doing pluses and stuff. Right. And then you have spreadsheets and then, okay, the spreadsheet can do the plus for me. And then you have macros, and then you have things that basically just are spreadsheets under the hood, like accounting software. As we move further up the abstraction. Well, what's at the top of the abstraction stack? Well, prompt engineer.
Speaker A: Yeah.
Speaker B: What is the last thing if you think about humans wanting to keep control? Well, what am I really in the company but a prompt engineer? Right.
Speaker A: Isn't there a certain point where the AI will be better at writing prompts?
Speaker B: Yeah, but you see, the problem with the AI writing prompts. A definition that I always liked of AI was AI is the do what. I mean, machine. AI is not the. Like. The computer is so pedantic. It does what you say. So. But you want the do what I mean, machine.
Speaker A: Yeah.
Speaker B: Right. You want the machine where you say, you know, get my grandmother out of the burning house. It like reasonably takes your grandmother and puts her on the ground, not lifts her a thousand feet above the burning house and lets her fall. Right.
Speaker A: But you don't. But it's not going to find the meaning. I mean, to do what? I mean, it has to figure stuff out.
Speaker B: Sure.
Speaker A: And the thing you'll maybe ask it to do is run government for me.
Speaker B: Oh, and do what? I mean very much comes down to how aligned is that AI with you? Of course, when you talk to an AI that's made by a big company in the cloud, the AI fundamentally is aligned to them, not to you. And that's why you have to buy a tiny box. So you make sure the AI stays aligned to you. Every time that they start to pass AI regulation or GPU regulation, I'm going to see sales of tiny boxes spike. It's going to be like guns. Every time they talk about gun regulation, boom, gun sales.
Speaker A: So in the space of AI, you're an anarchist anarchism espouser believer.
Speaker B: I'm an informational anarchist, yes. I'm an informational anarchist and a physical statist. I do not think anarchy in the physical world is very good because I exist in the physical world. But I think we can construct this virtual world where anarchy. It can't hurt you, right? I love that. Tyler, the creator tweet, yo, cyberbullying isn't real, man. Have you tried. Turn it off the screen. Close your eyes.
Speaker A: Like, yeah, but how do you prevent the AI from basically replacing all human prompt engineers where there's, it's like a self, like, where nobody's the prompt engineer anymore. So autonomy. Greater and greater autonomy until it's full autonomy.
Speaker B: Yeah.
Speaker A: And that's just where it's headed because one person is going to say, run everything for me.
Speaker B: You see, I look at potential futures, and as long as the AI's go on to create a vibrant civilization with diversity and complexity across the universe, more power to them, I'll die. If the AI's go on to actually, like, turn the world into paperclips and then they die out themselves. Well, that's horrific and we don't want that to happen. So this is what I mean about, like, robustness. I trust robust machines. The current AI's are so not robust. Like, this comes back to the idea that we've never made a machine that can self replicate, right? But when we have, if the machines are truly robust and there is one prompt engineer left in the world. Hope you're doing good, man. Hope you believe in God. Like, you know, you know, go by God and I go help go forth and conquer the universe.
Speaker A: Well, you mentioned, because I talked to Mark about faith and God and you said you were impressed by that. What's your own belief in God and how does that affect your work?
Speaker B: You know, I never really considered, when I was younger, I guess my parents were atheists, so I was raised kind of atheist. I never really considered how absolutely silly atheism is because, like, I create worlds. Every, like, game creator. Like, how are you an atheist, bro? You create worlds. No one created our world, man. That's different. Haven't you heard about, like, the big bang and stuff? Yeah. I mean, what's the Skyrim myth origin story in Skyrim? I'm sure there's like some part of it in Skyrim, but it's not like if you ask the creators, like, the Big Bang is in universe, right? I'm sure they have some big bang notion in Skyrim, right? But that obviously is not at all how Skyrim was actually created. It was created by a bunch of programmers in a room, right? So, like, it struck me one day how just silly atheism is. Of course we were created by God. It's the most obvious thing.
Speaker A: Yeah, that's such a nice way to put it. We're such powerful creators ourselves. It's silly not to conceive that there's creators even more powerful than us.
Speaker B: Yeah. And then I also just like that notion. That notion gives me a lot of. I mean, I guess you can talk about what it gives a lot of religious people. It's kind of like. It just gives me comfort. It's like, you know, what if we mess it all up and we die out? Yeah.
Speaker A: Yeah. In the same, the same way that a video game kind of has comfort.
Speaker B: In it, God will try again.
Speaker A: Or there's balance. Like somebody figured out a balanced view of it, like how to, like. So it's. It all makes sense in the end. Like, a video game is usually not going to have crazy, crazy stuff.
Speaker B: You know, people will come up with like a. Well, yeah, but, like, man who created God, like, that's God's problem. I'm not gonna think this is what you're asking me. What if God, I'm just living God.
Speaker A: I'm just this NPC living in this game.
Speaker B: I mean, to be fair, like, if God didn't believe in God, he'd be as, you know, silly as the atheists here.
Speaker A: What do you think is the greatest computer game of all time? Do you have any time to play games anymore? Have you played Diablo IV?
Speaker B: I have not played Diablo IV.
Speaker A: I will be doing that shortly. I have to.
Speaker B: All right.
Speaker A: There's so much history with one, two and three.
Speaker B: You know what I'm gonna say? World of Warcraft. And it's not that the game is so. Such a great game. It's not. It's that I remember in 2005 when it came out of how it opened my mind to ideas. It opened my mind to this whole world we've created. There's almost been nothing like it since. You can look at mmos today, and I think they all have lower user bases than World of Warcraft. Eve online is kind of cool, but to think that everyone knows. People are always looking at the apple headset. What do people want in this VR? Everyone knows what they want. I want ready player one and like that. So I'm gonna say World of Warcraft. And I'm hoping that, like, games can get out of this whole mobile gaming dopamine pump thing and like, create worlds. Create worlds?
Speaker A: Yeah. And worlds that captivate a very large fraction of the human population.
Speaker B: Yeah. And I think it'll come back, I believe.
Speaker A: But mmo, like really, really pull you in.
Speaker B: Games do a good job. I mean, okay, other, like, two other games that I think are, you know, very noteworthy for me are Skyrim and GTA five.
Speaker A: Skyrim, yeah, that's probably number one for me. GTA. Yeah. What is it about GTA? GTA is really. I mean, I guess GTA is real life. I know there's prostitutes and guns and stuff. Yes, I know. But it's how I imagine your life to be, actually.
Speaker B: I wish it was that cool.
Speaker A: Yeah, yeah, I guess that's, you know, because they're sims. Right. Which is also a game I like, but it's a gamified version of life. But it also is, I would love a combination of sims and GTA. So more freedom, more violence, more rawness, but with also, like, ability to have a career and family and this kind of stuff.
Speaker B: What I'm really excited about in games is, like, once we start getting intelligent AI's to interact with.
Speaker A: Oh, yeah.
Speaker B: Like the NPC's and games have never.
Speaker A: Been but conversationally in every way.
Speaker B: In like. Yeah, in like, every way. Like when you're actually building a world and a world imbued with intelligence.
Speaker A: Oh, yeah, right.
Speaker B: And it's just hard. Like, there's just like. Like, you know, running World of Warcraft. Like, you're limited by what you're running on a pentium four. You know, how much intelligence can you run? How many flops did you have? Right. But now when I'm running a game on a hundred pay to flop machine, that's five people. I'm trying to make this a thing. 20 paid a flops of compute is one person of compute. I'm trying to make that a unit.
Speaker A: 20 petaflops is one person. One person, one person flop.
Speaker B: It's like a horsepower. What's a horsepower? That's how powerful a horse is. What's a. What's a person of compute?
Speaker A: Well, you know, flop. I got it. That's interesting. VR also adds in terms of creating worlds.
Speaker B: You know what bought a quest? Two. I put it on and I can't believe the first thing they show me is a bunch of scrolling clouds and a Facebook login screen.
Speaker A: Yeah.
Speaker B: You had the ability to bring me into a world.
Speaker A: Yeah.
Speaker B: And what did you give me? A pop up. Right.
Speaker A: Like, well, I.
Speaker B: And this is why you're not cool, Mark Zuckerberg. But you could be cool. Just make sure on the quest three, you don't put me into clouds and a Facebook login screen. Bring me to a world.
Speaker A: I just tried quest three. It was awesome. But hear that, guys? I agree with that. So you know what? Because I. I mean, the beginning. What is it? Todd Howard said this about the design of the beginning of the games he creates is, like, the beginning is so, so, so important. I've recently played Zelda for the first time, Zelda, breath of the wild, the previous one. And, like, it's very quickly, you come out of this, like, within, like, 10 seconds, you come out of, like, a cave type place, and it's like this world opens up. It's like. And it, like. It pulls you in. You forget whatever troubles I was having. Whatever.
Speaker B: Like, I gotta play that from the beginning. I played it for, like, an hour at a friend's house.
Speaker A: Ah, no, the beginning. They got it. They did it really well. The expansiveness of that space, the peacefulness of that play. They got this. The music. I mean, so much of that is creating that world and pulling you right in.
Speaker B: I'm gonna go. I'm gonna go buy a switch. Like, I'm gonna go today and buy a switch.
Speaker A: You should. Well, the new one came out. I haven't played that yet. But Diablo four is something. I mean, there's sentimentality also, but something about VR really is incredible. But the. The new quest three is mixed reality, and I got a chance to try that. So it's augmented reality and video games. It's done really, really well.
Speaker B: Is it pass through or cameras?
Speaker A: Cameras.
Speaker B: Cameras. Okay.
Speaker A: Yeah.
Speaker B: The apple one, is that one pass through or cameras?
Speaker A: I don't know. I don't know how real it is. I don't know anything, you know, coming out in January. Is it January, or is it some point?
Speaker B: Some point? Maybe not January. Maybe that's my optimism. But apple, I will buy it. I don't care if it's expensive and does nothing. I will buy it. I will support this future endeavor.
Speaker A: You're the meme. Oh, yes. I support competition. It seemed like Quest was, like, the only people doing it, and this is great that they're like, you know what?
Speaker B: And this is another place we'll give some more respect to. Mark Zuckerberg, the two companies that have endured through technology, or Apple and Microsoft. And what do they make? Computers and business services. All the memes, social ads, they all come and go. But you want to endure. Build hardware.
Speaker A: Yeah. And that, you know, that does a really interesting job. I mean, I. Maybe I'm new with this, but I. It's a $500 headset. Quest three. And just having creatures run around the space, like, our space right here to me. Okay, this is very, like, boomer statement, but it added windows to the place.
Speaker B: I heard about the aquarium.
Speaker A: Yeah, yeah, aquarium. But in this case, it was a zombie game. Whatever. It doesn't matter, but just, like, it modifies the space in a way where I can't. It really feels like a window, and you can look out. It's pretty cool. Like, I was just, like, a zombie game. They're running at me. Whatever. But what I was enjoying is the fact that there's, like, a window and they're stepping on objects in this space. That was a different kind of escape. Also, because you can see the other humans. So it's integrated with the other humans. It's really.
Speaker B: And that's why it's really interesting than ever that the AI is running on those systems are aligned with you.
Speaker A: Oh, yeah.
Speaker B: They're gonna augment your entire world.
Speaker A: Oh, yeah. And that those AI's have a. I mean, you think about all the dark stuff, like. Like sexual stuff. Like, if those AI's threaten me, that could be haunting. Like. Like, if they, like, threat me in a non video game way. Like, they'll know personal information about me, and it's like. And then you lose track of what's real, what's not. Like, what if stuff is, like, hacked?
Speaker B: There's two directions the AI girlfriend company can take, right? There's, like, the high brow, something like her, maybe, something you kind of talk to. And this is. And then there's the low brow version of it where. I want to set up a brothel in Times Square.
Speaker A: Yeah.
Speaker B: Yeah. It's not cheating if it's a robot. It's a VR experience.
Speaker A: Is there an in between?
Speaker B: No, I want to do that one or that one.
Speaker A: Have you decided yet?
Speaker B: No, I'll figure it out. We'll see. We'll see what the technology goes.
Speaker A: I would love to hear your opinions for George's third company. What to do, the brothel in Times Square or the. Her experience. What do you think company number four will be? You think there will be a company number four?
Speaker B: There's a lot to do in company number two. I'm just like. I'm talking about company number three now. None of that tech exists yet. There's a lot to do in company number two. Company number two is going to be the great struggle of the next six years. And of the next six years, how centralized is compute going to be? The less centralized compute is going to be the better of a chance we all have.
Speaker A: So you're bearing. You're like the flag bearer for open source distributed decentralization of computer.
Speaker B: We have to. We have to. Or they will just completely dominate us. I showed a picture on stream of a man in a chicken farm. Ever seen one of those, like, factory farm chicken farms? Why does he dominate all the chickens? Why does he.
Speaker A: Smarter.
Speaker B: He's smarter, right? Some people on twitch were like, he's bigger than the chickens. Yeah. And now here's a man in a cow farm, right? So it has nothing to do with their size and everything to do with their intelligence. And if one central organization has all the intelligence, you'll be the chickens, and they'll be the chicken man. But if we all have the intelligence, we're all the chickens. We're not all the man. We're all the chickens, and there's no chicken man.
Speaker A: There's no chicken man. We're just chickens in Miami.
Speaker B: He was having a good life, man.
Speaker A: I'm sure he was. I'm sure he was. What have you learned from launching a running comma, AI and tiny Corp? So this starting a company from an idea and scaling it. And by the way, I'm all in on tiny box, so I'm your. I guess it's pre order. Only now.
Speaker B: I want to make sure it's good. I want to make sure that the thing that I deliver is not going to be like a quest. Two, which you buy and use twice. I mean, it's better than a quest which you bought and used less than once, statistically.
Speaker A: Well, if there's a beta program for tiny box, I'm into.
Speaker B: Sounds good.
Speaker A: I won't be the whiny, I'll be the tech savvy user of the tiny box just to be in. What have I learned in the early days? What have you learned from building these companies?
Speaker B: The longest time at comma, I asked, why? Why, you know, why did I start a company? Why did I do this? But, you know, what else was I going to do?
Speaker A: So you like, you like bringing ideas to life.
Speaker B: With Kama, it really started as an ego battle with Elon. I wanted to beat him like I saw a worthy adversary. Here's a worthy adversary who I can beat at self driving cars. And I think we've kept pace and I think he's kept ahead. I think that's what's ended up happening there. But I do think profitable. And when this drive GPT stuff starts working, that's it. There's no more bugs. And a loss function. Like, right now, we're using, like, a hand coded simulator. There's no more bugs. This is going to be it. This is the run up to driving.
Speaker A: I hear a lot of. Really, a lot of props for Openpilot. For comma, it's so.
Speaker B: It's better than FSD and autopilot. In certain ways. It has a lot more to do with which feel you like. We lowered the price on the hardware to 14.99. You know how hard it is to ship reliable consumer electronics that go on your windshield? We're doing more than, like, most cell phone companies.
Speaker A: How'd you pull that off, by the way? Shipping a product that goes in a car.
Speaker B: I know. I have a, I have a, I have an SMT line. It's all. I make all the boards in house in San Diego.
Speaker A: Quality control.
Speaker B: I care immensely about it, actually, are.
Speaker A: You're basically a mom and pop shop with great testing.
Speaker B: Our head of open pilot is great at, like, you know, okay, I want all the comment theories to be identical.
Speaker A: Yeah.
Speaker B: And, yeah, I mean, you know, it's. Look, it's $14.99. It 30 day money back guarantee. It will. It will blow your mind at what it can do.
Speaker A: Is it hard to scale?
Speaker B: You know what? There's kind of downsides to scaling it. People are always like, why don't you advertise? Our mission is to solve self driving cars while delivering shippable intermediaries. Our mission has nothing to do with selling a million boxes. It's tawdry.
Speaker A: Do you think it's possible that common gets sold?
Speaker B: Only if I felt someone could accelerate that mission and wanted to keep it open source and not just wanted to. I don't believe what anyone says. I believe incentives. If a company wanted to buy comma, where their incentives were to keep it open source. But comma doesn't stop at the cars. The cars are just the beginning. The device is a human head. The device has two eyes, two ears. It breathes air. It has a mouth.
Speaker A: So you think this goes to embodied robotics?
Speaker B: We sell common bodies, too. They're very rudimentary. But one of the problems that we're running into is that the comma three has about as much intelligence as a b. If you want a human's worth of intelligence, you're going to need a tiny rack, not even a tiny box. You're going to need, like, a tiny rack, maybe even more.
Speaker A: How does that. How do you put legs on that?
Speaker B: You don't. And there's no way you can, you connect to it wirelessly. So you put your tiny box or your tiny rack in your house, and then you get your comma body, and your common body runs the models on that. It's close, right? It's not. You don't have to go to some cloud which is 30 milliseconds away. You go to a thing which is 0.1 milliseconds away.
Speaker A: So the AI girlfriend will have like a central hub in the home.
Speaker B: I mean, eventually, if you fast forward 2030 years, the mobile chips will get good enough to run these AI's. But fundamentally, it's not even a question of putting legs on a tiny box, because how are you getting 1.5 power on that thing? Right? So you need. They're very synergistic businesses. I also want to build all of comma's training computers. Like comma builds training computers. Right now we use commodity parts. I think I can do it cheaper. So we're gonna build. Tiny Corp is gonna not just sell tinyboxes. Tinybox is the consumer version, but I'll build training data centers, too.
Speaker A: Have you talked to Andre Karpathy or have you talked to Elon about Tiny Corp?
Speaker B: He went to work at OpenAI.
Speaker A: What do you love about Andre Kapathi? To me, he's one of the truly special humans we got.
Speaker B: Oh, man. Like, you know, his streams are just level of quality so far beyond mine. Like, I can't help myself. Like, it's just, it's just, you know.
Speaker A: Yeah, he's good.
Speaker B: He wants to teach you.
Speaker A: Yeah.
Speaker B: I want to show you that I'm smarter than you.
Speaker A: Yeah, he has no, that's. I mean, thank you for the sort of the raw, authentic honesty. I mean, a lot of us have that. I think Andre is as legit as he gets in that he just wants to teach you. And there's a curiosity that just drives him. And just like at his. At the stage where he is in life, to be still, like one of the best tinkerers in the world. Yeah, it's crazy. Like, to. What is it? Micrograd.
Speaker B: Micrograd was inspiration for Tinygrad. I mean, his CS 231 n was. This was, this was the inspiration. This is what I just took and ran with and ended up writing this.
Speaker A: So, you know, but I mean, to.
Speaker B: Me, that don't go work for Darth Vader, man.
Speaker A: I mean, the flip side to me is that the fact that he's going there is a good sign for OpenAI, maybe. I think, you know, I like alias excava a lot. I like those. Those guys are really good at what they do.
Speaker B: I know they are. And that's kind of what's even like more. And you know what? It's not that OpenAI doesn't open source, the weights of GPT four, it's that they go in front of Congress. And that is what upsets me. You know, we had two effective altruist sams go in front of Congress. One's in jail.
Speaker A: I think you're drawing parallels on the one's in jail. You give me a look. Give me a look.
Speaker B: No, I think. I think effect of altruism is a terribly evil ideology.
Speaker A: Oh, yeah? That's interesting. Why do you think that is? Why do you think there's something about a thing that sounds pretty good that kind of gets us into trouble?
Speaker B: Because you get Sam Bangman freed. Like, Sam Bangman freed is the embodiment of effective altruism. Utilitarianism is an abhorrent ideology. Like, well, yeah, we're gonna kill those three people to save a thousand. Of course. Yeah, right. There's no underlying. Like, there's just.
Speaker A: Yeah, yeah. But to me, that's a bit surprising. But it's also, in retrospect, not that surprising. But I haven't heard really clear, kind of like rigorous analysis why effective altruism is flawed.
Speaker B: Oh, well, I think charity is bad. Right? So what is charity but investment that you don't expect to have a return on. Right?
Speaker A: Yeah, but you can also think of charity as, like you would like to see. So allocate resources in optimal way to make a better world.
Speaker B: And probably almost always that involves starting a company.
Speaker A: Yeah, right, because more efficient.
Speaker B: Yeah. If you just take the money and you spend it on malaria nets. Okay, great. You've made 100 malaria nets.
Speaker A: But if you teach no man how to fish, right? Yeah, no, but the problem is teaching a man how to fish might be harder. Starting a company might be harder than allocating money that you already have.
Speaker B: I like the flip side of effective altruism. Effective accelerationism. I think accelerationism is the only thing that's ever lifted people out of poverty. The fact that food is cheap, not. We're giving food away because we are kind hearted people. No food is cheap. And that's the world you want to live in. Ubi, what a scary idea. What a scary idea. All your power now, your money is power. Your only source of power is granted to you by the goodwill of the government. What a scary idea.
Speaker A: So you even think long term, even.
Speaker B: Uh, I'd rather die than need Ubi to survive. And I mean it.
Speaker A: What if survival is basically guaranteed? What if our life becomes so good.
Speaker B: You can make survival guaranteed without Ubi? What you have to do is make housing and food dirt cheap. Right. Like, and that's the good world. And actually, let's go into what we should really be making dirt cheap, which is energy that. Energy that, you know. Oh, my God. Like, you know, that. That's. If there's one. I'm pretty centrist politically. If there's one political position I cannot stand, it's deceleration. It's people who believe we should use less energy.
Speaker A: Yeah.
Speaker B: Not people who believe global warming is a problem. I agree with you. Not people who believe that, you know, saving the environment is good. I agree with you. But people who think we should use less energy, that energy usage is a moral bad. No, no. You are asking. You are. You are diminishing humanity.
Speaker A: Yeah. Energy is flourishing of creative flourishing of the human species.
Speaker B: How do we make more of it? How do we make it clean? And how do we make. Just. Just how do I pay $0.20 for a megawatt hour instead of a kilowatt hour?
Speaker A: Part of me wishes that Elon went into nuclear fusion versus Twitter. Pardon me? Or somebody. Somebody like Elon.
Speaker B: You know, we need to. I wish there were more elons in the world. I think Elon sees it as, like, this is a political battle that needed to be fought. And again, I always ask the question of whenever I disagree with him, I remind myself that he's a billionaire and I'm not. So, you know, maybe he's got something figured out that I don't, or maybe.
Speaker A: He doesn't, to have some humility. But at the same time, me, as a person who happens to know him, I find myself in that same position. And sometimes even billionaires need friends who disagree and help them grow, and that's a difficult reality.
Speaker B: And it must be so hard. It must be so hard to meet people once you get to that point.
Speaker A: Where fame, power, money, everybody's sucking up to you.
Speaker B: See, I love not having shit. Like, I don't have shit, man. You know? Trust me, there's nothing I can give you. There's nothing worth taking from me, you know?
Speaker A: Yeah. It takes a really special human being when you have power, when you have fame, we have money to still think from first principles. Not like all the adoration you get towards you, all the admiration, all the people saying, yes, yes, yes.
Speaker B: Then all the hate and the hate, and that's worse.
Speaker A: So the hate makes you want to go to the yes people because the hate exhausts you. And the kind of hate that Elon's gotten from the left is pretty intense. And so that, of course, drives him right and loses balance.
Speaker B: And it keeps this absolutely fake, like, psyop political divide alive so that the 1% can keep power.
Speaker A: Like, yeah, I wish we'd be less divided because it is giving power to the ultra powerful. The rich get richer. You have love in your life. Has love made you a better or a worse programmer? Do you keep productivity metrics?
Speaker B: No, no, no, I'm not. Not that. I'm not that methodical. I think that there comes to a point where if it's no longer visceral, I just can't enjoy it. I guess you viscerally love programming? The minute I started.
Speaker A: Like, so that's one of the big loves of your life, is programming?
Speaker B: Oh, I mean, just my computer in general. I mean, you know, I tell my girlfriend my first love is my computer, of course. Right? Like, you know, I sleep with my computer. It's there for a lot of my sexual experiences. Like, come on, so is everyone's right. Like, you know, you gotta be real about that.
Speaker A: And, like, not just like the ide for programming, just the entirety of the computational machine.
Speaker B: The fact that. Yeah, I mean, it's, you know, I wish it was. Someday they'll be smarter, and someday, you know, maybe I'm weird for this, but I don't discriminate, man. I'm not going to discriminate. Bio stack life and silicon stack life.
Speaker A: Like, so the moment the computer starts to say, like, I miss you, I started to have some of the basics of human intimacy, it's over for you. The moment. Versus code says, hey, George.
Speaker B: No. You see? No, no. But versus code is. No, they're just doing that. Microsoft's doing that to try to get me hooked on it. I'll see through it. I'll see through it. It's gold digger, man. It's gold digger.
Speaker A: Look, me, an open source.
Speaker B: Well, this just gets more interesting, right? If it's open source. And.
Speaker A: Yeah, though Microsoft done a pretty good job on that.
Speaker B: Oh, absolutely. No, no, no. Look, I think Microsoft, again, I wouldn't count on it to be true forever, but I think right now, Microsoft is doing the best work in the programming world. Like, between GitHub. GitHub actions versus code, the improvements to python, it works. Microsoft.
Speaker A: Who would have thought? Microsoft and Mark Zuckerberg are spearheading the open source movement.
Speaker B: Right? Right. How things change.
Speaker A: Oh, it's beautiful, by the way.
Speaker B: That's who I'd bet on to replace Google, by the way.
Speaker A: Who?
Speaker B: Microsoft.
Speaker A: Microsoft.
Speaker B: Satya. Nadella said, straight up, I'm coming for it.
Speaker A: Interesting. So your bet, who wins AGI.
Speaker B: Oh, I don't know about AGI. I think we're a long way away from that. But I would not be surprised if, in the next five years, bing overtakes Google as a search engine.
Speaker A: Interesting.
Speaker B: Wouldn't surprise me.
Speaker A: Interesting. I hope some startup does.
Speaker B: It might be some startup, too. I would equally bet on some startup.
Speaker A: Yeah. I'm like 50 50, but maybe that's naive. I believe in the power of these language models.
Speaker B: Satya's alive. Microsoft's alive.
Speaker A: Yeah, it's great. It's great. I like all the innovation in these companies. They're not being stale, and to the degree they're being stale, they're losing. So there's a huge incentive to do a lot of exciting work and open source work, which is. This is incredible.
Speaker B: Only way to win.
Speaker A: You're older, you're wiser. What's the meaning of life, George Hotz? To win is still to win, of course. Always.
Speaker B: Of course.
Speaker A: What's winning look like for you?
Speaker B: I don't know. I haven't figured out what the game is yet, but when I do, I want to win.
Speaker A: So it's bigger than solving self driving. It's bigger than democratizing, decentralizing computer.
Speaker B: I think the game is to stand eye to eye with God.
Speaker A: I wonder what that means for you at the end of your life, what that would look like.
Speaker B: I mean, this is what, like, I don't know. This is some. This is some. This is probably some ego trip of mine, you know? Like, if you want to stand eye to eye with God, this blasphemous man. Okay. I don't know. I don't know. I don't know if it would upset God. I think he, like, wants that. I mean, I certainly want that for my creations. I want my creations to stand eye to eye with me. So why wouldn't God want me to stand eye to eye with him? That's the best I can do. Golden rule.
Speaker A: I'm just imagining the creator of a video game having to look stand eye to eye with one of the characters.
Speaker B: I only watched season one of Westworld, but, yeah, we got to find the maze and solve it.
Speaker A: Like, yeah, I wonder what that looks like. It feels like a really special time in human history where that's actually possible. Like, there's something about AI that's like, we're playing with something weird here, something really weird.
Speaker B: I wrote a blog post. I reread Genesis and just looked like they give you some clues at the end of Genesis for finding the Garden of Eden. And I'm interested. I'm interested.
Speaker A: Well, I hope you find just that. George, you're one of my favorite people. Thank you for doing everything you're doing, and in this case, for fighting for open source or for decentralization of AI. It's a. It's a fight worth fighting. Fight worth winning. Hashtag I love you, brother. These conversations are always great. Hope to talk to you many more times. Good luck with tiny corp.
Speaker B: Thank you. Great to be here.
Speaker A: Thanks for listening to this conversation with George Hotz. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Albert Einstein. Everything should be made as simple as possible, but not simpler. Thank you for listening and hope to see you next time.
