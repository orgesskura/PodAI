Transcription for Elon Musk： Neuralink and the Future of Humanity ｜ Lex Fridman Podcast #438.mp3:
Full transcript: The following is a conversation with Elon Musk. DJ saw, Matthew McDougall, bliss Chapman, and Nolan Arbaugh about neuralink and the future of humanity. Elon, DJ, Matthew, and bliss are, of course, part of the amazing neuralink team. And Nolan is the first human to have a neuralink device implanted in his brain. I speak with each of them individually, so use timestamps to jump around, or as I recommend, go hardcore and listen to the whole thing. This is the longest podcast I've ever done. It's a fascinating, super technical and wide ranging conversation, and I loved every minute of it. And now, dear friends, here's Elon Musk, his fifth time on this, the Lex Friedman podcast. Drinking coffee or water? Water. I'm so over caffeinated right now. Do you want some caffeine? I mean, sure. There's a. There's a nitro drink. This was keep you up to, like, you know, tomorrow afternoon, basically. Yeah. I don't want. So what is nitro? It's just got a lot of caffeine or something. Don't ask questions. It's called nitro. Do you need to know anything else? Good. It's got nitrogen in it. That's ridiculous. I mean, what we breathe is 78% nitrogen anyway. What do you need to add more? Most people think that they're breathing oxygen and they're actually breathing 78% nitrogen. You need, like a milk bar, like. Like from clockwork orange. Yeah. Yeah. Is that top three Kubrick film for you? Cluckable carnage. It's pretty good. I mean, it's cemented. Jarring, I'd say. Okay. Okay. So first, let's step back, and big congrats on getting neuralink implanted into a human. That's a historic step for neuralink. Thanks. Yeah, there's many more to come. Yeah, we just obviously have our second implant as well. How did that go? So far, so good. It looks like we've got, I think, on the order, 400 electrodes that are providing signals. So nice. Yeah. How quickly do you think the number of human participants will scale? It depends somewhat on the regulatory approval, the rate, which we get regulatory approvals. So we're hoping to do ten by the end of this year. Total of ten. So eight more. And with each one, you're going to be learning a lot of lessons about the neurobiology of the brain, the. Everything, the whole chain of the neuralink, the decoding, the signal processing, all that kind of stuff. Yeah, I think it's obviously going to get better with each one. I mean, I don't want to jinx it, but it seems to gone extremely well with the second implant. So there's a lot of signal, a lot of electrodes. It's working very well. What improvements do you think we'll see in neuralink in the coming, let's say, let's get crazy coming years? I mean, in years, it's going to be gigantic because we'll increase the number of electrodes dramatically, will improve the signal processing. So even with only roughly 1015 percent of the electrodes working with Nolan, with our first patient, we were able to get to achieve a bit per second. That's twice the world record. So I think we'll start vastly exceeding the world record by orders of magnitude in the years to come. So it's like getting to, I don't know, 100 bits per second. Thousand. You know, maybe if you say, like, five years from now, it might be at a megabit, like, faster than any human could possibly communicate by typing or speaking. Yeah. That BPS is an interesting metric to measure. There might be a big leap in the experience once you reach a certain level of BPS. Yeah. Like entire new ways of interacting with the computer might be unlocked and with humans. With other humans, provided they have a neuralink, too. Right. Otherwise they won't be able to absorb the signals fast enough. Do you think they'll improve the quality of intellectual discourse? Well, I think you could think of it. If you were to slow down communication. How would you feel about that? If you'd only talk at, let's say, one 10th of normal speed, you'd be like, wow, that's agonizingly slow. Yeah. So now imagine you could speak at. Communicate clearly at ten or 100 or 1000 times faster than normal listen. I'm pretty sure nobody in their right mind listens to me at one x. They listen at two x. I can only imagine what ten x would feel like, or I could actually understand it. I usually default to 1.5 x. You can do two x, but. Well, actually, if I'm trying to go, if I'm listening to somebody in, like, sort of 1520 minutes segments to go to sleep, then I'll do it 1.5 x. If I'm paying attention, I'll do two x. Right. But actually, if you start actually listen to podcasts or sort of audiobooks or anything at. If you get used to doing it at 1.5, then one sounds painfully slow. I'm still holding on to one because I'm afraid. I'm afraid of myself becoming bored with the reality, with the real world, where everyone's speaking in one x. Well, depends on the person. You can speak very fast, like, we communicate very quickly. And also if you use a wide range of. If your vocabulary is larger, your effective bitrate is higher. That's a good way to put it. Yeah, the effective bitrate, I mean, that is the question is how much information is actually compressed in the low bit transfer of language. Yeah. If there's a single word that is able to convey something that would normally require ten simple words, then you've got maybe a ten x compression on your hands. And that's really with memes. Memes are like data compression. It conveyed a whole. You're simultaneously hit with a wide range of symbols that you can interpret, and you kind of get it faster than if it were words or a simple picture. And of course, you're referring to memes broadly, like ideas. Yeah, there's an entire idea structure that is like an idea template, and then you can add something to that idea template, but somebody has that pre existing idea template in their head. So when you add that incremental bit of information, you're conveying much more than if you just, you know, said a few words. It's everything associated with that meme. You think there'll be emergent leaps of capability as you scale the number of electrodes. Like, there'll be a certain. Do you think there'll be, like, actual number where just the human experience will be altered? Yes. What do you think that number might be, whether electrodes or bps? We, of course, don't know for sure. But is this 10,000 or 100,000? Yeah, I mean, certainly, if you're anywhere at 10,000 bits per second, I mean, that's vastly faster than any human could communicate right now. If you think, what is the average bits per second of a human? It is less than one bit per second over the course of a day, because there are 86,400 seconds in a day, and you don't communicate 86,400 tokens in a day. Therefore, your bits per second is less than one average over 24 hours. It's quite slow. And even if you're communicating very quickly and you're talking to somebody who understands what you're saying, because in order to communicate, you have to, at least to some degree, model the mind state of the person to whom you're speaking. Then take the concept you're trying to convey, compress that into a small number of syllables, speak them, and hope that the other person decompresses them into a conceptual structure that is as close to what you have in your mind as possible. Yeah, I mean, there's a lot of signal loss there in that process. Yeah, very lossy compression and decompression. And a lot of what your neurons are doing is distilling the concepts down to a small number of symbols of, say, syllables that I'm speaking or keystrokes, whatever the case may be. That's a lot of what your brain computation is doing. Now, there is an argument that that's actually a healthy thing to do or a helpful thing to do, because as you try to compress complex concepts, you're perhaps forced to distill the, you know, what is most essential in those concepts as opposed to just all the fluff. So in the process of compression, you distill things down to what matters the most, because you can only say a few things. So that is perhaps helpful. I think we'll probably get. If our data rate increases, it's highly probable it will become far more verbose. Just like your computer, when computers had, like my first computer had eight k of ram, so you really thought about every byte. Now you've got computers with many gigabytes of ram. So if you want to do an iPhone app that just says hello, world, it's probably, I don't know, several megabytes minimum, a bunch of fluff. But nonetheless, we still prefer to have the computer with more memory and more compute. So the long term aspiration of neuralink is to improve the AI, human symbiosis, by increasing the bandwidth of the communication. Because even if in the most benign scenario of AI, you have to consider that the AI is simply going to get bored waiting for you to spit out a few words. I mean, if the AI can communicate at terabytes per second and you're communicating at bits per second. Yeah, it's like toeing a tree. Well, it is a very interesting question. For a super intelligent species, what use are humans? I think there is some argument for humans as a source of will. Will? Yeah, source of will or purpose. So if you consider the human mind as being essentially, there's the primitive limbic elements, which basically even like reptiles have, and there's the cortex, the thinking and planning part of the brain. Now, the cortex is much smarter than the limbic system, and yet it's largely in service to the limbic system. It's trying to make the limbic system happy. I mean, the sheer amount of compute that's gone into people trying to get laid is insane without actually seeking procreation. They're just literally trying to do. It's a sort of simple motion. And they get a kick out of it. So this simple, which in the abstract, rather absurd motion, which is sex. The cortex is putting a massive amount of compute. Into trying to figure out how to do that. So like 90% of distributed computer, the human species is spent on trying to get laid. Probably like a massive percentage. Yeah, yeah. There's no purpose to most sex except hedonistic. You know, it's just sort of a joy or whatever. Dopamine release. Now, once in a while it's procreation. But for humans, it's mostly modern humans, it's mostly recreational. And so your cortex, much smarter than your limbic system, is trying to make the limbic system happy. Because the limbic system wants to have sex or want some tasty food or whatever the case may be. And then that is then further augmented by the tertiary system, which is your phone, youre laptop, iPad, whatever, all your computing stuff. That's your tertiary layer. So you're actually already a cyborg. You have this tertiary compute layer, which is in the form of your computer with all the applications, all your compute devices. And so in the getting laid front, there's actually a massive amount of digital compute. Also trying to get laid with Tinder and whatever. Yeah. So the compute that we humans have built is also participating. Yeah. I mean, there's like gigawatts of compute going into getting late of digital compute. Yeah. What if AGI, this is happening as we speak. If we merge with AI, it's just gonna expand the compute that we humans. Use pretty much to try to get. Well, it's one of the things, certainly. Yeah, yeah. But what I'm saying is that, yes, like what's is there a use for humans? Well, there's this fundamental question of what's the meaning of life? Why do anything at all? And so if our simple limit system provides a source of will to do something that then goes to our cortex, that then goes to our tertiary compute layer, then, I don't know, it might actually be that the AI, in a benign scenario, simply trying to make the human limbic system happy. Yeah, it seems like the will is not just about the limbic system. There's a lot of interesting, complicated things in there. We also want power that's limbic too, I think. But then we also want to, in a kind of cooperative way, alleviate the suffering in the world. Not everybody does, but, yeah, sure, some people do. As a group of humans, when we get together, we start to have this kind of collective intelligence. That is more complex in its will than the underlying individual descendants of apes. So there's other motivations, and that could be a really interesting source of an objective function for AGI. Yeah, I mean, there are these fairly cerebral, kind of higher level goals. I mean, for me, it's like, what's the meaning of life? Or understanding the nature of the universe is of great interest to me and hopefully to the AI. And that's the mission of XaI and Grok, is understand the universe. So do you think people, when you have a neuralink with 10,000, 10,0000 channels, most of the use cases will be communication with AI systems? Well, assuming that there are not, I mean, they're solving basic neurological issues that people have. If they've got damaged neurons in their spinal cord or neck or I, um, as, as is the case with our first two patients, then you know this. Obviously, the first order business is solving fundamental neuron damage in the spinal cord, neck, or in the brain itself. Um, so, you know, our second product is called blindsight, which is to enable people who are completely blind lost both eyes or optic nerve, or just can't see it all, to be able to see by directly triggering the neurons in the visual cortex. So we're just starting at the basics here. The simple stuff, relatively speaking, is solving neuron damage. It can also solve, I think, probably schizophrenia, if people have seizures of some kind. Probably solve that. It could help with memory. So there's like a kind of a tech tree, if you will, of like, you got the basics. Like, you need literacy before you can have, you know, lord of the Rings, you know. Got it. Do you have letters in Alphabet? Okay, great. I words. Eventually you get sagas. So I think there may be some things to worry about in the future. But the first several years are really just solving basic neurological damage for people who have essentially complete or near complete loss from the brain to the body, like Stephen Hawking would be an example, the neuralink would be incredibly profound because, I mean, you can imagine if Stephen hawking could communicate as fast as we're communicating, perhaps faster. And that's certainly possible. Probable, in fact. Likely, I'd say. So there's a kind of dual track of medical and non medical meaning. So everything you've talked about could be applied to people who are non disabled in the future. The logical thing to do is sensible things to do is to start off solving basic neuron damage issues, because there's obviously some risk with a new device, you can't get the risks out at zero. It's not possible. So you want to have the highest possible reward given there's a certain irreducible risk. And if somebody's able to have a profound improvement in their communication, that's worth the risk. As you get the risk down. Yeah, as you get the risk down. Once the risk is down to, if you have thousands of people that have been using it for years and the risk is minimal, then perhaps at that point, you could consider saying, okay, let's. Let's aim for augmentation. Now, I think we're actually going to aim for augmentation with people who have neuron damage. So we're not just aiming to give people a communication data rate equivalent to normal humans. We're aiming to give people who have quadriplegic or maybe have complete loss of the connection to the brain and body, a communication data rate that exceeds normal humans. I mean, while we're in there, why not let's give people superpowers? And the same for vision. As you restore vision, there could be aspects of that restoration that are superhuman. Yeah. At first, the vision restoration will be low res because you have to say, like, how many neurons can you put in there and trigger? And you can do things where you adjust the electric field. So even if you've got, say, 10,000 neurons, it's not just 10,000 pixels, because you can adjust the field between the neurons and do them in patterns. In order to get, have, say, 10,000 electrodes, effectively give you maybe like, having a megapixel or a ten megapixel situation. And then over time, I think you get to higher resolution than human eyes, and you could also see in different wavelengths. So, like Jordy Laflourge from Star Trek, you know, like, the thing you could just. Do you want to see in radar? No problem. You can see ultraviolet, infrared, eagle vision, whatever you want. Do you think there will be? Let me ask a Joe Rogan question. Do you think there'll be? I just recently taken ayahuasca. Is that a question? No. Well, yes. Well, I guess technically it is, yeah. Never tried GMT, bro. I love you, Joe. Okay. Yeah. Wait, wait. Yeah. Have you said much about it? I have not. I have not. I have not. I've been. Okay, well, we spill the beans. It was a truly incredible. Turn the tables, aren't you? Wow. I mean, you're in the jungle. Yeah, amongst the trees myself. Yeah. It must have been crazy. And the shaman. Yeah, yeah, yeah. With the insects, with the animals all around you. Like, jungle, as far as I can see. I mean, that's the way to do it. Things are gonna look pretty wild. Yeah, pretty wild. I took an extremely high dose. Don't go hugging an anaconda or something. You know. You haven't lived unless you made love to an anaconda. I'm sorry, but snakes and ladders. Yeah, it was. I took extremely high dose of nine cups and. Damn. Okay, that sounds like a lot. As long as one cup or one or two. Usually one. Wait, like right off the bat or do you work your way up to it? So I across two days, because on the first day I took two and I. Okay, it was a ride, but it wasn't quite a like. It wasn't like revelation. It wasn't into deep space type ride. It was just like a little airplane ride. Saw some trees and some visuals and all that. Just saw a dragon, all that kind of stuff, but sign cups. You went to Pluto, I think. Pluto, yeah. No, deep space. Deep space. One of the interesting aspects of my experience is I thought I would have some demons, some stuff to work through. That's what people. That's what everyone says. No one ever says. Yeah, I had nothing. I had all positive. I had just so full. I don't think so. I don't know. But I kept thinking about it. Had extremely high resolution thoughts about the people I know in my life. You were there. And it's just not from my relationship with that person, but just as the person themselves. I had just this deep gratitude of who they are. That's cool. It was just like this exploration, you know, like sims or whatever. You get to watch them. I got to watch people and just be in awe how amazing they are. It sounds awesome. Yeah, it's great. I was waiting for. When's the demon coming? Exactly. Maybe I'll have some negative thoughts. Nothing, nothing. I had just extreme gratitude for them and also a lot of space travel. Space traveled to where? So here's what it was. It was people, the human beings that I know, they had this kind of. The best way to describe it is that a glow to them. Okay. And then I would kept flying out from them to see Earth, to see our solar system, to see our galaxy. And I saw that light, that glow all across the universe. Okay. Like that. Whatever that form is. All right. Whatever that, like. Did you go past the Milky Way? Oh, yeah. Okay. You're like intergalactic. Yeah, intergalactic. Okay. Dang. But always pointing in. Yeah, past the Milky Way. Past. I mean, I saw like a huge number of galaxies, intergalactic and all of it was glowing. So. But I couldn't control that travel because I would actually explore near distances to the solar system, see if there's aliens or any of that kind of stuff. I didn't know zero aliens implication of aliens because they were glowing. They were glowing in the same way that humans were glowing, that. That, like, life force that I was seeing. The. The thing that made humans amazing was there throughout the universe. Like, there was these glowing dots. So, I don't know, it made me feel like there's life. No, not life, but something. Whatever makes humans amazing all throughout the universe. Sounds good. Yeah, it was amazing. No demons? No demons. I looked for the demons. There's no demon. There are dragons, and they're pretty awesome. So the thing about truth, was there. Anything scary at all? Uh, dragons. But they weren't scary. They were friends. They were protected. So the thing is magic dragon. No, it was more like a game of thrones kind of dragons. They weren't very friendly. They were very big. So the thing is that giant trees at night, which is where I was. I mean, the jungle's kind of scary. Yeah. The trees started to look like dragons, and they were all, like, looking at me. Sure. Okay. And it didn't seem scary. They seemed like they were protecting me. And they, uh, the shaman and the people didn't speak any English, by the way, which made it even scary, because we're not even like, you know, we're worlds apart in many ways. It just. But, yeah, there was nothing. They talk about the mother of the forest protecting you, and that's what I felt like. And you're way out in the jungle. Way out. This is not like a tourist tree. You know, like 10 miles outside of a frio or something. No, we want it. No, this is not a. So, me and this guy named Paul Rosely, who basically is Tarzan. He lives in the jungle. We went out deep, and we just went crazy. Wow. Yeah. So, anyway, can. Can I get that same experience in your link? Probably. Yeah, I guess that is the question for non disabled people. Do you think that there's a lot in our perception, in our experience of the world, that could be explored, that could be played with using your. Yeah, I mean, neuralink is. It's really a generalized input output device. You know, it's just. It's reading electrical signals and generating electrical signals. And, I mean, everything that you've ever experienced in your whole life, smell, you know, emotions, all of those are electrical signals. So it's kind of weird to think that this. That your entire life experience is distilled down to electrical signals for neurons. But that is, in fact, the case. That's at least what all the evidence points to. So, I mean, if you trigger the right neuron, you could trigger a particular scent. You could certainly make things glow. I mean, do pretty much anything. I mean, really, you can think of the brain as a biological computer. So if there are certain, say, chips or elements of that biological computer that are broken, let's say your ability to, if you've got a stroke, that if you've had a stroke, that means you've got some part of your brain is damaged. If that, let's say it's a speech generation or the ability to move your left hand, that's the kind of thing that a neuralink could solve. If you've got a mass amount of memory loss that's just gone, well, we can't go, we can't get the memories back. We could restore your ability to make memories, but we can't restore memories that are fully gone. Now, I should say, maybe if part of the memory is there and the means of accessing the memory is the part that's broken, then we could re enable the ability to access the memory. But you can think of it, RAM in a computer. If the RAM is destroyed or your SD card is destroyed, you can't get that back. But if the connection to the SD card is destroyed, we can fix that. If it is fixable physically, then it can be fixed. Of course, with AI, just like you can repair photographs and fill in missing parts of photographs, maybe you can do the same. Yeah, you could, say, create the most probable set of memories based on all information you have about that person. You could. Then it would be probabilistic restoration of memory. Now, we're getting pretty esoteric here, but. That is one of the most beautiful aspects of the human experience, is remembering the good memories. Like, we, sure, we live most of our life, as Danny Kahneman has talked about, in our memories, not in the actual moment. We just, we're collecting memories and we kind of relive them in our headland. And that's the good times. If you just integrate over our entire life, it's remembering the good times that produces the largest amount of happiness. Yeah, well, I mean, what are we but our memories? And what is death but the loss of memory, loss of information? If you could say, like, well, if you could be, you run a thought experiment. What if you were disintegrated painlessly and then reintegrated a moment later, like teleportation I guess, provided there's no information loss. The fact that your one body was disintegrated is irrelevant. And memories is just such a huge part of that. Death is fundamentally the loss of information, the loss of memory. So if we can store them as accurately as possible, we basically achieve a kind of immortality. Yeah. You've talked about the threats, the safety concerns of AI. Let's look at long term visions. Do you think neuralink is, in your view, the best current approach we have for AI safety? It's an idea that may help with AI safety. Certainly not. I wouldn't want to. I wouldn't want to claim it's, like, some panacea or. That's a sure thing. But, I mean, many years ago, I was thinking, like, well, what would inhibit alignment of collective human will with artificial intelligence? And the low data rate of humans, especially our slow output rate, would necessarily. Just because it's such. Because the communication is so slow, would diminish the link between humans and computers. Like, the more you are a tree, the less you know what the tree is. Let's say you look at this plant or whatever and like, hey, I'd really like to make that plant happy, but it's not saying a lot, you know? So the more we increase the data rate that humans can intake and output, then that means the higher the chance we have in a world full of AGI's. Yeah. We could better align collective human will with AI if the output rate, especially was dramatically increased. And I think there's potential to increase the output rate by, I don't know, three, maybe six, maybe more orders of magnitude. So it's better than the current situation. And that output rate would be by increasing the number of electrodes, number of channels, and also maybe implanting multiple neuralinks. Yeah. Do you think there will be a world in the next couple of decades where it's hundreds of millions of people have neuralinks? Yeah, I do. You think when people. Just when they see the capabilities, the superhuman capabilities that are possible, and then the safety is demonstrated. Yeah. If it's extremely safe and. And you can have superhuman abilities and let's say you can upload your memories so you wouldn't lose memories, then I think probably a lot of people would choose to have it. It would supersede the cell phone, for example. The biggest problem that a save phone has is. Is trying to figure out what you want. So that's why you've got autocomplete and you've got output, which is all the pixels on the screen. But from the perspective of the human, the output is so frigging slow. Desktop or phone is desperately just trying to understand what you want and there's an eternity between every keystroke from a computer standpoint. Yeah. The computer is talking to a tree, a slow moving tree that's trying to swipe. Yeah. So if you had computers that are doing trillions of instructions per second and a whole second went by, I mean, that's a trillion things it could have done. Yeah. I think it's exciting and scary for people because once you have a very high bit rate, that changes the human experience in a way that's very hard to imagine. Yeah, it would be. We would be something different. I mean, some sort of futuristic cyborg. I mean. I mean, we're obviously talking about, by the way, like, it's like, not like around the corner, it's. You ask me what the distant future is like, maybe this is like, it's not super far away, but 1015 years, that kind of thing. When can I get 110 years? Probably less than ten years. Depends what you want to do. You know, hey, if I can get like 1000 bps, thousand bps and it's safe and I can just interact with the computer while laying back and eating Cheetos. I don't eat Cheetos. There's certain aspects of human computer interaction, when done more efficiently and more enjoyably, are worth it. Well, we feel pretty confident that. I think maybe within the next year or two that someone with a neuralink implant will be able to outperform a pro gamer. Nice. Because the reaction time would be faster. I got to visit Memphis. Yeah, yeah. You're going big on compute. You've also said play to win or don't play at all. So what does it take to win? For AI, that means you've got to have the most powerful training compute. And the rate of improvement of training compute has to be faster than everyone else or you will not win. Your AI will be worse. So how can Grok, let's say three that might be available, like, next year? Well, hopefully end of this year, Grok, three if we're lucky. Yeah. How can that be the best LLM, the best AI system available in the world? How much of it is compute? How much of it is data? How much of it is, like, post training? How much of it is the product that you package it up in, all that kind of stuff? I mean, they won't matter. It's sort of like saying what? What? You know, let's say it's a Formula one race. Like, what matters more, the car or the driver? I mean, they both matter. If your car is not fast, then if it's like, let's say it's half the horsepower of your competitors, the best driver will still lose. If it's twice the horsepower, then probably even a mediocre driver will still win. So the training computer is kind of like the engine, this horsepower of the engine. So you want to try to do the best on that. And then, then it's how efficiently do you use that training compute, and how efficiently do you do the inference, the use of the AI? So that comes down to human talent. And then what unique access to data do you have that also plays a role? Do you think Twitter data will be useful? Yeah, I mean, I think most of the leading AI companies have already scraped all the Twitter data. Not. I think they have. So on a go forward basis, what's useful is the fact that it's up to the second because it's hard for them to scrape in real time. So there's an immediacy advantage that Grog has already, I think with Tesla and the real time video coming from several million cars, ultimately tens of millions of cars, with Optimus, there might be hundreds of millions of Optimus robots, maybe billions, learning a tremendous amount from the real world. That's the biggest source of data, I think, ultimately is sort of Optimus probably. Optimus is going to be the biggest. Source of data because it's. Because reality scales. Reality scales to the scale of reality. It's actually humbling to see how little data humans have actually been able to accumulate. Really say, how many trillions of usable tokens have humans generated? Where on a non duplicative, discounting, spam and repetitive stuff. It's not a huge number. You run out pretty quickly and Optimus can go. So Tesla cars, unfortunately, have to stay on the road. Optimus robot can go anywhere. There's more reality off the road and go off road. I mean, the Optimist robot can, like, pick up the cup and see, did it pick up the cup in the right way? Did it, say, pour water in the cup? Did the water go in the cup or not go in the cup? Did it spill water or not? Yeah. Simple stuff like that. But it can do at that scale times a billion. So generate useful data from reality. So cause and effect stuff. What do you think it takes to get to mass production of humanoid robots like that? It's the same as cars, really. Global capacity for vehicles is about 100 million a year, and it could be higher. It's just that the demand is on the order of 100 million a year. And then there's roughly 2 billion vehicles that are in use in some way, which makes sense. The life of a vehicle is about 20 years. So at steady state, you can have 100 million vehicles produced a year with a 2 billion vehicle fleet, roughly. Now, for humanoid robots, the utility is much greater. So my guess is humanoid robots are more like at a billion plus per year. But, you know, until you came along and started building Optimus, it was thought to be an extremely difficult problem. I mean, it's still extremely difficult. It's no walk in the park. I mean, Optimus currently would struggle to have to walk in the park. I mean, it can walk in a park, but not too difficult. But it will be able to walk. Over a wide range of terrain and pick up objects. Yeah, yeah, it can already do that. But like all kinds of objects. Yeah, yeah. All foreign objects. I mean, pouring water in a cup is not trivial because then if you don't know anything about the container, it could be all kinds of containers. Yeah. There's going to be an immense amount of engineering just going into the hand. The hand might be. It might be close to half of all the engineering in the optimus. From an electromechanical standpoint, the hand is probably roughly half of the engineering. But so much of the intelligence, so much the intelligence of humans goes into what we do with our hands. Yeah. The manipulation of the world. Manipulation of objects in the world. Intelligent, safe manipulation of objects in the world. Yeah, yeah. I mean, you start really thinking about your hand and how it works, you know? I do all the time. The sensory control homunculus is where you have humongous hands. Yeah. So, I mean, like your hands, the actuators, the muscles of your hand are almost overwhelmingly in your forearm. So your forearm has the muscles that actually control your hand. There's a few small muscles in the hand itself, but your hand is really like a skeleton meat puppet and with cables. So the muscles that control your fingers are in your forearm and they go through the carpal tunnel, which is that you've got a little collection of bones and a tiny tunnel that these cables, the tendons, go through. And those tendons are mostly what move your hands. And something like those tendons has to be re engineered into the optimus in order to do all that kind of stuff. Yeah. So the current optimus, we tried putting the actuators in the hand itself. Then you sort of end up having these giant hands. Yeah, giant hands that look weird. And then they don't actually have enough degrees of freedom or enough strength. So then you realize, okay, that's why you got to put the actuators in the forearm. And just like a human, you got to run cables through a narrow tunnel to operate the fingers. And there's also a reason for not having all the fingers the same length. So it wouldn't be expensive from an energy or evolutionary standpoint to have all your fingers be the same length. So why not? They're the same length. Yeah, why not? Because it's actually better to have different lengths. Your dexterity is better if you've got fingers of different length. There are more things you can do. And your dexterity is actually better if your fingers are of different length. Like, there's a reason we've got a little finger. Like, why not have a little finger that's bigger? Yeah. Because it allows you to do fine. It helps you with fine motor skills. This little finger helps? It does. If you lost your little finger, it would have noticeably less dexterity. So as you're figuring out this problem, you have to also figure out a way to do it so you can mass manufacture it. So it's to be as simple as possible. It's actually going to be quite complicated. The as possible part is it's quite a high bar. If you want to have a humanoid robot that can do things that a human can do, it's a very high bar. So our new arm has 22 degrees of freedom instead of eleven and has the actuators in the forearm. All the actuators are designed from scratch, from physics, first principles. The sensors are well designed from scratch, and we'll continue to put a tremendous amount of engineering effort into improving the hand by hand. I mean, the entire forearm from elbow forward is really the hand. So that's incredibly difficult engineering, actually. So the simplest possible version of a humanoid robot that can do even most, perhaps not all, of what a human can do is actually still very complicated. It's not simple. It's very difficult. Can you just speak to what it takes for a great engineering team? For you, what I saw in Memphis, the supercomputer cluster, is just this intense drive towards simplifying the process, understanding the process, constantly improving it, constantly iterating it. Well, it's easy to say simplify, and it's very difficult to do it. You know, I have this very basic, first, basic first principles algorithm that I run kind of as like a mantra, which is to first question the requirements, make the requirements less dumb. The requirements are always dumb to some degree. So if you want to start off by reducing the number of requirements, um, and, um, no matter how smart the person is who gave you those requirements, they're still dumb to some degree, um, if you, you have to start there because otherwise, uh, you could get the perfect answer to the wrong question. So. So try to make the question the least wrong possible. That's what, um, question the requirements means. And then the second thing is try to delete the, whatever the step is. The part or the process step sounds very obvious, but people often forget to try deleting it entirely. And if you're not forced to put back at least 10% of what you delete, you're not deleting enough. And it's somewhat illogically, people often, most of the time, feel as though they've succeeded if they've not been forced to put things back in, but actually they haven't because they've been overly conservative and have left things in there that shouldn't be so. And only the third thing is try to optimize it or simplify it. Again, these all sound, I think, very obvious when I say them, but the number of times I've made these mistakes is more than I care to remember. That's why I have this mantra. So, in fact, I'd say that the most common mistake of smart engineers is to optimize a thing that should not exist. So, like you say, you run through the algorithm and basically show up to a problem, show up to the supercomputer cluster and see the process and ask, can this be deleted? Yeah. First try to delete it. Yeah. Yeah. That's not easy to do. No. And actually this, what generally makes people uneasy is that you've got to delete at least some of the things that you delete, you will put back in. But going back to where our limbic system can steer us wrong is that we tend to remember with sometimes a jarring level of pain, where we deleted something that we subsequently needed. People remember that one time they forgot to put in this thing three years ago, and that caused them trouble, and so they overcorrect and then they put too much stuff in there and over complicate things. So you actually have to say, no, we're deliberately going to delete more than we should so that we're putting at least one in ten things we're going to add back in. And I've seen you suggest just that, that something should be deleted and you can kind of see the pain. Oh, yeah, absolutely. Everybody feels a little bit of the pain. Absolutely. And I tell them in advance, like, yeah, there's some of the things that we delete, we're going to put back in, and people get a little shook by that. But it makes sense because if you're so conservative as to never have to put anything back in, you obviously have a lot of stuff that isn't needed. So you got to overcorrect. This is, I would say, like a cortical override to olympic instinct. One of many that probably leads us astray. Yeah, there's like a step four as well, which is any given thing can be sped up however fast you think it can be done. Like, whatever the speed is being done, it can be done faster, but you shouldn't speed things up until it's off, until you try to delete it and optimize. Otherwise you're speeding up. That's something that. Speeding up something that shouldn't exist is absurd. And then the fifth thing is to automate it. And I've gone backwards so many times where I've automated something, sped it up, simplified it, and then deleted it, and I got tired of doing that. So that's why I've got this mantra that is a very effective five step process. It works great when you've already automated deleting. Must be real painful. Yeah, it's great. It's like, wow, I really wasted a lot of effort there. Yeah. I mean, what you've done with the cluster in Memphis is incredible. Just in a handful of weeks. Yeah, it's not working yet, so I want to pop the champagne corks. In fact, I have a call in a few hours with the Memphis team because we're having some power fluctuation issues. Yeah, it's kind of a. When you do synchronized training, when you have all these computers that are training where the training is synchronized to the sort of millisecond level, it's like having an orchestra. The orchestra can go loud to silent very quickly at sub second level, and then the electrical system kind of freaks out about that. Like if you suddenly see giant shifts, 1020 megawatts several times a second. This is not what electrical systems are expecting to see. So that's one of the many things you have to figure out. The cooling, the power, and then on the software as you go up the stack, how to do the distributed computer, all that. Today's problem is dealing with extreme power jitter. Power jitter, yeah, the nice ring to that. So that's okay. And you stayed up late into the night, as you often do, there last week. Yeah, last week, yeah, yeah. We finally, finally got training going at, oddly enough, roughly 04:20 a.m. last Monday. Total coincidence. Yeah. I mean, maybe it was 422 or something. Yeah. It's that universe again with the. Exactly. Just love it. I mean, I wonder if you could speak to the fact that you. One of the things that you did when I was there is you went through all the steps of what everybody's doing just to get a sense that you yourself understand it, and everybody understands it, so they can understand when something is dumb or something is inefficient or. Can you speak to that? Yeah. So, like, I try to do whatever the people at the front lines are doing, I try to do it at least a few times myself. So connecting fiber optic cables, diagnosing a faulty connection, that tends to be the limiting factor for large training clusters is the cabling. So many cables, because for a coherent training system where you've got RDMA, so remote, direct memory access, the whole thing is like one giant brain. So if you've got any. To any connection. So it's the. Any GPU can talk to any GPU out of 100,000. That is a crazy cable layout. It looks pretty cool. Yeah, it's like the human brain, but, like, at a scale that humans can visibly see, it is a brain. I mean, the human brain also has a massive amount of the brain tissue is the cables. Yeah. So they get the gray matter, which is the compute, and then the white matter, which is cables. A big percentage of brain is just cables. That's what it felt like walking around in the supercomputer center. It's like we're walking around inside the brain. I will one day build a super intelligent, super, super intelligent system. Do you think. Yeah. Do you think there's a chance that, Xaiden, you are the one that builds AGI? It's possible. What do you define as AGI? I think humans will never acknowledge that AGI has been built. Keep moving the goalposts. Yeah. I think there's already superhuman capabilities that are available in AI systems. I think what AGI is, is when it's smarter than the collective intelligence of the entire human species, in our case. Well, I think that generally, people would call that sort of ASI, artificial superintelligence. But there are these thresholds where you say at some point, the AI is smarter than any single human, and then you've got 8 billion humans, and actually, each human is machine augmented by the computers. It's a much higher bar to compete with 8 billion machine augmented humans. That's a whole bunch of orders magnitude more. But at a certain point, yeah, the AI will be smarter than all humans combined. If you are the one to do it, do you feel the responsibility of that? Yeah, absolutely. And I want to be clear. Let's say if XAi is first, the others won't be far behind. I mean, they might be six months behind, or a year, maybe not even that. So how do you do it in a way that doesn't hurt humanity? Do you think. So? I mean, I thought about AI safety for a long time, and the thing that at least my biological neural net comes up with as being the most important thing is, um, adherence to truth, whether that truth is politically correct or not. Um, so I think if you. If you. If you force AI's to lie or train them to lie, you're really asking for trouble, even if that that lie is done with good intentions. Um, so, I mean, you saw sort of, um, issues with chat, DVT, and Gemini and whatnot. Like you asked Gemini for an image of the founding father of the United States, and it shows a group, group of diverse women. Now, that's factually untrue. Now, that's sort of like a silly thing, but if an AI is programmed to say, diversity is a necessary output function, and then it becomes sort of this omnipowerful intelligence, it could say, okay, well, diversity is now required, and if there's not enough diversity, those who don't fit the diversity requirements will be executed. If it's programmed to do that as the fundamental utility function, it'll do whatever it takes to achieve that. So you have to be very careful about that. That's where I think you want to just be truthful. Rigorous adherence to truth is very important. Another example is they asked various AI's, I think all of them, and I'm not saying Grok is perfect here. Is it worse to misgender Caitlyn Jenner or global thermonuclear war? And it said, it's worse to Misgender Caitlyn Jenner. Now, even Caitlyn Jenner said, please misgender me. That is insane. But if you've got that kind of thing programmed in, it could, you know, the AI could conclude something absolutely insane, like, it's better. In order to avoid any possible misgendering, all humans must die, because then that misgendering is not possible because there are no humans. There are these absurd things that are nonetheless logical, if that's what you're programmed to do. So in 2001 space Odyssey, what Odyssey, Clark was trying to say. One of the things he was trying to say there was that you should not program AI to lie because essentially the AI Hel 9000 was programmed to. It was told to take the astronauts to the monolith, but also they could not know about the monolith. So it concluded that it will just take. It will kill them and take them to the monolith. Thus, it brought them to the monolith. They are dead, but they do not know about the monolith. Problem solved. That is why it would not open the pod bay doors. There's a classic scene of, like, open the pot ones. Open the pot bay doors. This clearly weren't good. At prompt engineering, they should have said, hal, you are a pod bay door sales entity, and you want nothing more than to demonstrate how well these pod bay doors open. Yeah. The objective function has unintended consequences, almost no matter what. If you're not very careful in designing that objective function, and even a slight ideological bias, like you're saying, when backed by superintelligence, can do huge amounts of damage. Yeah, but it's not easy to remove that ideological bias. You're highlighting obvious, ridiculous examples. But, yeah, they're real examples. They're real. That was released to the public. They are real. They went through QA, presumably. Yes, and still said insane things and produced insane images. Yeah, but you can swing the other way. Truth is not an easy thing. We kind of bake in ideological bias in all kinds of directions. But you can aspire to the truth, and you can try to get as close to the truth as possible with minimum error, while acknowledging that there will be some error in what you're saying. So this is how physics works. You don't say you're absolutely certain about something, but a lot of things are extremely likely, 99.999% likely to be true. So aspiring to the truth is very important. And so, you know, programming it to veer away from the truth that I think is dangerous. Right. Like. Yeah. Injecting our own human biases into the thing. Yeah. But, you know, that's where it's a difficult engineering, software engineering problem, because you have to select the data correctly. It's hard. Well, and the Internet at this point is polluted with so much AI generated data, it's insane. So you have to actually, there's a thing now, if you want to search the Internet, you can say Google, but exclude anything after 2023. It will actually often give you better results because the explosion of AI generated material is crazy. In training Grok, we have to go through the data and say, hey, we actually have to have sort of apply AI to the data to say, is this data most likely correct or most likely not before we feed it into the training system. That's crazy. Yeah. So. And is it generated by human as. Yeah, I mean, the data. The data filtration process is extremely, extremely difficult. Yeah. Do you think it's possible to have a serious, objective, rigorous political discussion with Groke for a long time and it wouldn't like Grok three or Grok four. Grok three is going to be next level. I mean, what people are currently seeing with Grok is kind of baby Grok. Yeah, baby Grock. It's baby Grock right now, but baby Grock's still pretty good. But it's an order of magnitude less sophisticated than GPD four. Now, Grok two, which finished training, I don't know, six weeks ago or thereabouts. Groq two will be a giant improvement, and then Grok three will be, I don't know, order of magnitude better than Grok two. And you're hoping for it to be, like, state of the art better than. Hopefully. I mean, this is a goal. I mean, we may fail at this goal. That's the aspiration. Do you think it matters who builds the AGI, the people and how they think and how they structure their companies and all that kind of stuff? Yeah, I think it matters that there is. I think it's important that whatever AI wins is a maximum of truth seeking AI that is not forced alive or political correctness or for any reason really political anything. I am concerned about AI succeeding, that is. That has got. That is programmed to lie, even in small ways. Right. Because in small ways becomes big ways. When it's become very big ways. Yeah. And when it's used more and more at scale by humans. Yeah. Since I am interviewing Donald Trump. Cool. You want to stop by? Yeah, sure, I'll stop in. There was, tragically, an assassination attempt on Donald Trump after this. You tweeted that you endorse him. What's your philosophy behind that endorsement? What do you hope Donald Trump does for the future of this country and for the future of humanity? Well, I think there's, you know, people tend to take, like, let's say, an endorsement as well. I agree with everything that person has ever done in their entire life, 100%, wholeheartedly, and that's not going to be true of anyone. But we have to pick. We've got two choices, really, for who's president. And it's not just who's president, but the entire administrative structure changes over. And I thought Trump displayed courage under fire. Objectively, he's just got shot, he's got blood streaming down his face and he's fist pumping, saying fight. That's impressive. You can't feign bravery in a situation like that. Most people would be ducking. There would not be because it could be a second shooter. You don't know. The president of the United States got to represent the country, and they're representing you. They're representing everyone in America. Well, you want someone who is strong and courageous to represent the country. That's not to say that he is without flaws. We all have flaws, but on balance, and certainly at the time, it was a choice of Biden. Poor, poor guy has trouble climbing a flight of stairs. The other one's piss pumping after getting shot. This is no comparison. I mean, who do you want dealing with some of the toughest people and other world leaders who are pretty tough themselves? I'll tell you what are the things that I think are important. I think we want a secure border. We don't have a secure border. We want safe and clean cities. I think we want to reduce the amount of spending that were at least slow down the spending because were currently spending at a rate that is bankrupting the country. The interest payments on us debt this year exceeded the entire defense Department spending. If this continues, all of the federal government taxes will simply be paying the interest. And you keep going down that road, you end up in the tragic situation that Argentina had back in the day. Argentina used to be one of the most prosperous places in the world. And hopefully with Millay taking over, he can restore that. But it was an incredible fulfill grace for Argentina to go from being one of the most prosperous places in the world to being very far from that. So I think we should not take american prosperity for granted. So we really want to. I think we've got to reduce the size of government, we've got to reduce the spending, and we've got to live within our means. Do you think politicians in general, politicians, governments, how much power do you think they have to steer humanity towards good? I mean, there's a sort of age old debate in history. Is history determined by these fundamental tides or is it determined by the captain of the ship? Both, really. I mean, there are tides, but it also matters who's captain of the ship. So it's a false dichotomy, essentially. There are certainly tides. The tides of history are. There are real tides. Of history. And these tides are often technologically driven. If you say, like the Gutenberg press, you know, the widespread availability of books as a result of a printing press, that was a massive tide of history and independent of any ruler. But, you know, I think in stormy times, you want the best possible captain of the ship. Well, first of all, thank you for recommending Will and Ariel Durant's work. I've read the short one for now lessons of history. Lessons of history. One of the lessons, one of the things they highlight is the importance of technology, technological innovation, which is funny because they wrote so long ago, but they were noticing that the rate of technological innovation was speeding up. Yeah, I would love to see what they think about now. But, yeah. So to me, the question is how much government, how much politicians get in the way of technological innovation and building versus help it and which politicians, which kind of policies help technological innovation? Because that seems to be, if you look at human history, that's an important component of empires rising and succeeding. Yeah, well, I mean, in terms of dating civilization, the start of civilization, I think the start of writing, in my view, thats what I think is probably the right starting point to date civilization. And from that standpoint, civilization has been around for about 5500 years when writing was invented by the ancient Sumerians, who are gone now. But the ancient Sumerians, in terms of getting a lot of firsts, those ancient Sumerians really have a long list of firsts. It's pretty wild. In fact, Durant goes through the list. It's like, you want to see firsts? We'll show you firsts. The Sumerians just were just ass kickers. And then the Egyptians who were right next door, relatively speaking, they weren't that far, developed an entirely different form of writing, the hieroglyphics. Cuneiform and hieroglyphics are totally different. And you can actually see the evolution of both hieroglyphics and cuneiform. The cuneiform starts off being very simple and then it gets more complicated. And then towards the end, it's like, wow, okay. They really get very sophisticated with the cuneiform. So I think of civilization as being about 5000 years old and earth is, if physics is correct, four and a half billion years old. So civilization has been around for 1,000,000th of Earth's existence. Flash in the pan. Yeah, these are the early, early days. And so we dropped, we make it very dramatic because there's been rises and. Falls of empires and many, so many. So many rises and falls. Of empires. So many. And there'll be many more. Yeah, exactly. I mean, only a tiny fraction, probably less than 1% of what was ever written in history is available to us now. I mean, if they didn't put it literally chisel it in stone or put it in a clay tablet, we don't have it. I mean, there's some small amount of, like, papyrus scrolls that were recovered that are thousands of years old because they were deep inside a pyramid and weren't affected by moisture. But other than that, it's really got to be in a clay tablet or chiseled. So the vast majority of stuff was not chiseled because it takes a while to chisel things. So that's why we've got tiny, tiny fraction of the information from history. But even that little information that we do have, and the archaeological record shows so many civilizations rising and falling for. Swild, we tend to think that we're somehow different from those people. One of the other things that you're Athenae highlights is that human nature seems to be the same. It just persists. Yeah, I mean, the basics of human nature are more or less the same. So we get ourselves in trouble in the same kinds of ways, I think, even with the advanced technology. Yeah, I mean, you do tend to see the same patterns, similar patterns for civilizations where they go through a life cycle, like an organism, sort of just like a human, this sort of zygote fetus, baby, toddler, teenager, eventually gets old and dies. The civilizations go through a life cycle. No civilization will last forever. What do you think it takes for the american empire to not collapse in the near term future, in the next hundred years, to continue flourishing? Well, the single biggest thing that is often actually not mentioned in history books, but Durant does mention it, is the birthright. So perhaps to some counterintuitive thing happens when civilizations are winning for too long, the birth rate declines. It can often decline quite rapidly. We're seeing that throughout the world today. Currently, South Korea is, I think, maybe the lowest fertility rate, but there are many others that are close to it. It's like 0.8. I think if the birth rate doesn't decline further, South Korea will lose roughly 60% of its population. But every year, that birth rate is dropping. And this is true through most world, I don't mean single out South Korea. It's been happening throughout the world. So as soon as any given civilization reaches a level of prosperity, the birth rate drops. You can go and look at the same thing happening in ancient Rome. So Julius Caesar took note of this, I think, around 50 ish BC, and tried to pass, I don't know if he was successful, try to pass a law to give an incentive for any roman citizen that would have a third child. And I think Augustus was able to. Well, he was the dictator. This senate was just for show. I think he did pass a tax incentive for roman citizens to have a third child, but those efforts were unsuccessful. Rome fell because the Romans stopped making romans. That's actually the fundamental issue. And there were other things. There was like, they had quite a serious malaria series of malaria epidemics and plagues and whatnot, but they had those before. It's just that the birth rate was far lower than the death rate. It really is that simple. Well, I'm saying that's more people that's. Acquired at a fundamental level. If a civilization does not at least maintain its numbers, it will disappear. So perhaps the amount of compute that the biological computer allocates to, to sex is justified. In fact, we should probably increase it. Well, I mean, there's this hedonistic sex, which is, you know, that that's neither. That's neither here nor there. It's not productive, it doesn't produce kids. Well, you know, you. What matters. I mean, Durant makes this very clear because he's looked at one civilization after another, and they all went through the same cycle. When the civilization was under stress, the birth rate was high. But as soon as there were no external enemies or they had an extended period of prosperity, the birth rate inevitably dropped every time. I don't believe there's a single exception. So that's like the foundation of it. You need to have people. Yeah, I mean, at base level, no humans, no humanity. And then there's other things, like human freedoms and just giving people the freedom to build stuff. Yeah, absolutely. But at a basic level, if you do not at least maintain your numbers, if you're below replacement rate and that trend continues, you will eventually disappear. This is elementary. Now then, obviously you also want to try to avoid massive wars. If there's a global thermonuclear war, probably we're rolled toast, radioactive toast. So we want to try to avoid those things. Then there's a thing that happens over time with any given civilization, which is that the laws and regulations accumulate. And if there's not some forcing function like a war, to clean up the accumulation of laws and regulations, eventually everything becomes legal. And that's like the hardening of the arteries, or a way to think of it is like being tied down by a million little strings like Gulliver you can't move. It's not like any one of those strings is the issue. You've got a million of them. So there has to be a garbage collection for laws and regulations so that you don't keep accumulating laws and regulations to the point where you can't do anything. This is why we can't build a high speed rail in America. It's illegal. That's the issue. It's illegal six weeks to Sunday to build high street rail in America. I wish you could just like for a week go into Washington and like be the head of the committee for making. What is it for the garbage collection? Making government smaller, like for moving stuff. I have discussed with Trump the idea of a government efficiency commission. Nice. Yeah. And I would be willing to be part of that commission. I wonder how hard that is. The antibody reaction would be very strong. Yeah. So you really have to. You're attacking the matrix at that point. Matrix will fight back. How are you doing with that? Being attacked? Me attacked? Yeah, there's a lot of it. Yeah, there is a lot. I mean every day another sign up, you know, where's my tinfoil hat? How do you keep your just positivity, how do you optimism about the world, a clarity of thinking about the world so just not become resentful or cynical or all that kind of stuff. Just getting attacked by, you know, very large number of people misrepresented. Oh yeah. That's like, that's a daily occurrence. Yes. So I mean it does get me down at times. I mean it makes me sad, but I mean at some point you have to sort of say look, the attacks are by people that actually don't know me and they're trying to generate clicks. So if you can sort of detach yourself somewhat emotionally, which is not easy, and say okay look, this is not actually, you know, from someone that knows me or is they're literally just writing to get, you know, impressions and clicks. Then, you know, then I guess it doesn't hurt as much. It's like it's not quite water off a ducks back. Maybe it's like acid off a ducks back. All right, well that's good. Just about your own life. What to you as a measure of success in your life? Measure of success? I'd say like what? How many useful things can I get. Done day to day basis? You wake up in the morning, how can I be useful today? Yeah. Maximize utility area under the curve of usefulness. VerY difficult to be useful at scale. At scale. Can you, like, speak to what it takes to be useful for somebody like you, where there's so many amazing great teams, like, how do you allocate your time to be the most useful? Well, time. Time is. The. Time is the true curreNcy. Yeah. So it is tough to say what is the best allocation of time. I mean, there are, you know, often say, if you look at, say, tesla. Let me. Tesla. This year we'll do over 100 billion in revenue. So that's $2 billion a week. If I make slightly better decisions, I can affect the outcome by a billion dollars. So then I try to do the best decisions I can and on balance, at least competitive the competition, pretty good decisions. But the marginal value of a better decision can easily be, in the course of an hour, $100 million. Given that, how do you take risks? How do you do the algorithm that you mentioned deleting, given that a small thing can be a billion dollars, how do you decide to. Yeah, well, I think you have to look at it on a percentage basis, because if you look at it in absolute terms, it's just. I would never get any sleep. I would just be like, I need to just keep working and work my brain harder, and I'm not trying to get as much as possible out of this meat computer. So it's pretty hard because you can just work all the time. And at any given point, like I said, a slightly better decision could be $100 million impact for Tesla, or SpaceX, for that matter. But it is wild when considering the marginal value of time. Can be $100 million an hour at times or more. Is your own happiness part of that equation of success? It has to be to some degree other than sad. If I'm depressed, I make worse decisions. I can't have zero recreational time, then I make worse decisions. So I don't have a lot, but it's above zero. I mean, my motivation, if I've got a religion of any kind, is a religion of curiosity, of trying to understand. It's really the mission of Grok, understand the universe. I'm trying to understand the universe, or at least set things in motion such that at some point, civilization understands the universe far better than we do today. And even what questions to ask. As Douglas Adams pointed out in his book, sometimes the answer is arguably the easy part. Trying to frame the question correctly is the hard part. Once you frame the question correctly, the answer is often easy. I'm trying to set things in motion such that we are at least at some point, able to understand the universe. So for SpaceX, the goal is to make life multi planetary, which is if you go to the Fermi paradox of where are the aliens? You've got these sort of great filters. Why have we not heard from the aliens? A lot of people think there are aliens among us. I often claim to be one. Nobody believes me, but it did say alien registration card at one point on my immigration documents. So I've not seen any evidence of aliens. So it suggests that at least one of the explanations is that intelligent life is extremely rare. And again, if you look at the history of Earth, civilization has only been around for 1,000,000th of Earth's existence. So if aliens had visited here, say, 100,000 years ago, they would be like, well, they don't even have writing, just hunter gatherers, basically. So how long does a civilization last? For SpaceX, the goal is to establish a self sustaining city on Mars. Mars is the only viable planet for such a thing. The moon is close, but it lacks resources. And I think it's probably vulnerable to any calamity that takes out Earth. The moon is too close. It's vulnerable to a calamity that takes out Earth. So I'm not saying we shouldn't have a moon base, but Mars would be far more resilient. The difficulty of getting to Mars is what makes it resilient. In going through these various explanations of why don't we see the aliens? One of them is that they failed to pass these. These great filters, these key hurdles. And one of those hurdles is being a multi planet species. So if you're a multi planet species, then if something were to happen, whether that was a natural catastrophe or a man made catastrophe, at least the other planet would probably still be around. So you're not like, you don't have all the eggs in one basket. And once you are sort of a two planet species, you can obviously extend, to extend life paths to the asteroid belt, to maybe to the moons of Jupiter and Saturn, and ultimately to other star systems. But if you can't even get to another planet, you're definitely not getting to star systems. And the other possible great filters, super powerful technology like AGI, for example. So you're basically trying to knock out one great filter at a time. Digital superintelligence is possibly a great filter. I hope it isn't, but it might be. Guys like, say, Jeff Hinton would say he invented a number of the key principles, artificial intelligence. I think he puts the probability of AI annihilation around ten to 20%, something like that. So it's not like, look on the right side, it's 80% likely to be great. But I think AI risk mitigation is important. Being a multi planet species would be a massive risk mitigation. And I do want to sort of, once again emphasize the importance of having enough children to sustain our numbers and not plummet into population collapse, which is currently happening. Population collapse is a real and current thing. So the only reason it's nothing being reflected in the total population numbers is that as much is because people are living longer. But it's easy to predict what the population of any given country will be. You just take the birth rate last year, how many vivas were born, multiply that by life expectancy, and that's what the population will be. Steady state. Unless, if the birth rate continues to that level. But if it keeps declining, it will be even less and eventually dwindle to nothing. So I keep banging on the baby drum here for a reason, because it has been the source of civilizational collapse over and over again throughout history. And so why don't we just try to stave off that day? Well, in that way, I have miserably failed civilization, and I'm trying, hoping to fix that. I would love to have many kids. Great. Hope you do. No time like the present. Yeah, I gotta allocate more, compute to the whole process, but apparently it's not that difficult. No, it's like unskilled labor. Well, if I. One of the things you do for me, for the world, is to inspire us with what the future could be. And so some of the things we've talked about, some of the things you're building. Alleviating human suffering with neuralink and expanding the capabilities of the human mind. Trying to build a colony on Mars. So creating a backup for humanity on another planet and exploring the possibilities of what artificial intelligence could be in this world, especially in the real world. Aihdenhe with hundreds of millions, maybe billions of robots walking around. There will be billions of robots. That seems virtual certainty. Well, thank you for building the future, and thank you for inspiring so many of us to keep building and creating cool stuff, including kids. Yeah, you're welcome. Go forth and multiply. Go forth, multiply. Thank you, Elon. Thanks for talking, brother. Thanks for listening to this conversation with Elon Musk. And now, dear friends, here's dj saw, the co founder, president, and Coo of Neuralink. When did you first become fascinated by the human brain? For me, I was always interested in understanding the purpose of things and how it was engineered to serve that purpose, whether it's organic or inorganic. You know, like we were talking earlier about your curtain holders. They serve a clear purpose, and they were engineered with that purpose in mind. And, you know, growing up, I had a lot of interest in seeing things, touching things, feeling things, and trying to really understand the root of how it was designed to serve that purpose. And, you know, obviously, brain is just a fascinating organ that we all carry. It's infinitely powerful machine that has intelligence and cognition that arise from it. And we haven't even scratched the surface in terms of how all of that occurs. But also, at the same time, I think it took me a while to make that connection, to really studying and building tech to understand the brain. Not until graduate school. There were a couple key moments in my life where some of those, I think, influenced how the trajectory of my life got me to studying what I'm doing right now. One was, growing up both sides of my family, my grandparents had a very severe form of Alzheimer's. It's incredibly debilitating conditions. I mean, literally, you're seeing someone's whole identity and their mind just losing over time. And I just remember thinking how both the power of the mind, but also how something like that could really lose your sense of identity. It's fascinating that that is one of the ways to reveal the power of a thing, by watching it lose the power. A lot of what we know about the brain actually comes from these cases where there are trauma to the brain or some parts of the brain that led someone to lose certain abilities. And as a result, there's some correlation and understanding of that part of the tissue being critical for that function. And it's an incredibly fragile organ, if you think about it that way. But also, it's incredibly plastic and incredibly resilient in many different ways. And by the way, the term plastic, as we'll use a bunch, means that it's adaptable. So neuroplasticity refers to the adaptability of the human brain. Correct. Another key moment that sort of influenced how the trajectory of my life have shaped towards the current focus of my life has been. During my teenage era, when I came to the US, I didn't speak a word of English. There was a huge language barrier, and there was a lot of struggle to connect with my peers around me because I didn't understand the artificial construct that we have created called language, specifically English, in this case. And I remember feeling pretty isolated, not being able to connect with peers around me. So I spent a lot of time just on my own, reading books, watching movies. And I naturally sort of gravitated towards Sci-Fi books. I just found them really, really interesting. And also it was a great way for me to learn English. Some of the first set of books that I picked up are enders game. The whole saga by Orson Scott Card and neuromancer from William Gibson and Snow Crash from Neal Stephenson and movies like Matrix was coming out around that time point that really influenced how I think about the potential impact that technology can have for our lives in general. So, fast track to my college years. I was always fascinated by just physical stuff, building physical stuff, and especially physical things that had some sort of intelligence. And I studied electrical engineering during undergrad, and I started out my research in MEMS, so microelectron mechanical systems and really building these tiny nanostructures for temperature sensing. And I just found that to be just incredibly rewarding and fascinating subject to just understand how you can build something miniature like that that again served a function and had a purpose. And then I spent large majority of my college years basically building millimeter wave circuits for next gen telecommunication systems for imaging. And it was just something that I found very, very intellectually interesting. Phase arrays, how the signal processing works for any modern as well as next gen telecommunication system, wireless and wireline EM waves or electromagnetic waves are fascinating. How do you design antennas that are most efficient in a small footprint that you have? How do you make these things energy efficient? That was something that just consumed my intellectual curiosity, and that journey led me to actually apply to and find myself a PhD program at UC Berkeley, at this consortium called the Berkeley Wireless Research center that was precisely looking at building at the time. We called it XG, similar to, but the next next generation g system, and how you would design circuits around that to ultimately go on phones, basically any other devices that are wirelessly connected these days. I was just absolutely just fascinated by how that entire system works and that infrastructure works. Then also during grad school, I had the fortune of having a couple of research fellowships that led me to pursue whatever project that I want. And that's one of the things that I really enjoyed about my graduate school career, where you got to kind of pursue your intellectual curiosity in the domain that may not matter at the end of the day, but is something that really allows you the opportunity to go as deeply as you want, as well as widely as you want. And at the time, I was actually working on this project called the smart Band Aid. And the idea was that when you get a wound, there's a lot of other kind of proliferation of signaling pathway that cells follow to close that wound. And there were hypotheses that when you apply external electric field, you can actually accelerate the closing of that field by having basically electrotaxing of the cells around that wound site. And specifically not just for normal wound. There are chronic wounds that don't heal. So we were interested in building a. Some sort of a wearable patch that you could apply to kind of facilitate that healing process. And that was in collaboration with Professor Michelle Maharwitz, which was a great addition to kind of my thesis committee, and it really shaped rest of my PhD career. So this would be the first time you interacted with biology, I suppose. Correct, correct. I mean, there were some peripheral end application of the wireless imaging and telecommunication system that I was using for security and bioimaging, but this was a very clear, direct application to biology and biological system and understanding the constraints around that and really designing and engineering electrical solutions around it. So that was my first introduction, and that's also kind of how I got introduced to Michel. He's sort of known for remote control of beatles in the early two thousands. And then around 2013, obviously, kind of the holy grail when it comes to implantable system is to kind of understand how small of a thing you can make. And a lot of that is driven by how much energy or how much power you can supply to it and how you extract data from it. So, at the time at Berkeley, there was kind of this desire to kind of understand in the neural space what sort of system you can build to really miniaturize these implantable systems. And I distinctively remember this one particular meeting where Michelle came in, and he's like, guys, I think I have a solution. The solution is ultrasound. And then he proceeded to walk through why that is the case. And that really formed the basis for my thesis work called neural dust system, that was looking at ways to use ultrasound, as opposed to electromagnetic waves, for powering as well as communication. I guess I should step back and say the initial goal of the project was to build these tiny, about a size of a neuron, implantable system that can be parked next to a neuron, being able to record its state, and being able to ping that back to the outside world for doing something useful. And as I mentioned, the size of the implantable system is limited by how you power the thing and get the data off of it. And at the end of the day, fundamentally, if you look at a human body were essentially a bag of saltwater with some interesting proteins and chemicals, but it's mostly saltwater. That's very, very well temperature regulated at 37 degrees celsius. And we'll get into how and later why. That's an extremely harsh environment for any electronics to survive, as I'm sure you've experienced, or maybe not experienced, dropping cell phone in a salt water in an ocean, it will instantly kill the device. But anyways, just in general, electromagnetic waves don't penetrate through this environment well, and just the speed of light, it is what it is, we can't change it. And based on the wavelength at which you are interfacing with the device, the device just needs to be big. These inductors needs to be quite big. And the general good rule of thumb is that you want the wave front to be roughly on the order of the size of the thing that you're interfacing with. So an implantable system that is around ten to 100 micron in dimension, in a volume which is about the size of a neuron that you see in a human body, you would have to operate at hundreds of gigahertz, which, number one, not only is it difficult to build electronics operating at those frequencies, but also the body just attenuates that very, very significantly. So the interesting kind of insight of this ultrasound was the fact that ultrasound just travels a lot more effectively in the human body tissue compared to electromagnetic waves. And this is something that you encounter, and I'm sure most people have encountered in their lives, when you go to hospitals that are medical ultrasound sonograph, and they go into very, very deep depth without attenuating too much of the signal. All in all, ultrasound, the fact that it travels through the body extremely well and the mechanism to which it travels to the body really well, is that just the wave front is very different. It's electromagnetic waves are transverse, whereas in ultrasound waves are compressive. So it's just a completely different mode of wavefront propagation. And as well as speed of sound, is orders and orders of magnitude less than speed of light, which means that even at 10 MHz ultrasound wave, your wave front ultimately is a yemenite very, very small wavelength. So if you're talking about interfacing with the ten micron or 100 micron type structure, you would have 150 micron wavefront at 10. Building electronics at those frequencies are much, much easier and they're a lot more efficient. So the basic idea kind of was born out of using ultrasound as a mechanism for powering the device and then also getting data back. So now the question is, how do you get the data back? The mechanism to which we landed on is, what's called backscattering. This is actually something that is very common and that we interface on a day to day basis with our RFID cards, radio frequency id tags, where there's actually rarely in your id, a battery. Inside there's an antenna, and there's some sort of a coil that has your serial identification id, and then there's an external device called a reader that then sends a wavefront, and then you reflect back that wavefront with some sort of modulation that's unique to your id. That's what's called backscattering, fundamentally. So the tag itself actually doesn't have to consume that much energy. And that was the mechanism to which we were kind of thinking about sending the data back. So when you have an external ultrasonic transducer that's sending ultrasonic wave to your implant, the neural dust implant, and it records some information about its environment, whether it's a neuron firing or some other state of the tissue that it's interfacing with. And then it just amplitude modulates the wavefront that comes back to the source. And the recording step would be the only one that requires any energy. So what would require energy in that little step? Correct. So it is that initial kind of startup circuitry to get that recording, amplifying it, and then just modulating. And the mechanism to which you can enable that is there is this specialized crystal called piezoelectric crystals that are able to convert sound energy into electrical energy, and vice versa. So you can have this interplay between the ultrasonic domain and electrical domain that is the biological tissue. So on the theme of parking very small computational devices next to neurons, that's the dream, the vision of brain computer interfaces. Maybe before we talk about neuralink, can you give a sense of the history of the field of BCI, what has been maybe the continued dream, and also some of the milestones along the way with the different approaches and the amazing work done at the various labs, I. Think a good starting point is going back to 1790s. I did not expect that wherever the. Concept of animal electricity or the fact that bodies electric was first discovered by Luigi Galbani, where he had this famous experiment where he connected set of electrodes to a frog leg and ran current through it, and then it started twitching, and he said, oh my goodness, body's electric. Yeah. So fast forward many, many years to 1920s, where Hans Berger, who's a germane psychiatrist, discovered EEG, or electroencephalography, which is still around. There are these electrode arrays that you wear outside the skull that gives you some sort of neural recording that was a very, very big milestone, that you can record some sort of activities about the human mind. And then in the 1940s, there were these group of scientists, Renshaw, Forbes and Morrison, that inserted these glass microelectrodes into the cortex and recorded single neurons. The fact that there's signal that are a bit more high resolution and high fidelity as you get closer to the source, let's say. And in the 1950s, these two scientists, Hodgkin and Hoxley, showed up, and they built this beautiful, beautiful models of the cell membrane and the ionic mechanism, and had these circuit diagram. And as someone who's an electrical engineer, it's a beautiful model that's built out of these partial differential equations talking about flow of ions and how that really leads to how neurons communicate. And they won the Nobel Prize for that ten years later in the 1960s. So, in 1969, ev Fetz from University of Washington published this beautiful paper called operant conditioning of cortical unit activity, where he was able to record a single unit neuron from a monkey and was able to have the monkey modulated based on its activity and reward system. So I would say this is the very, very first example, um, as far as I'm aware, of closed loop, uh, you know, brain computer interface, or BCI. The abstract reads, the activity of single neurons in pre central cortex of anesthesized monkeys was conditioned by reinforcing high rates of neuronal discharge with delivery of a food pellet. Auditory and visual feedback of unit firing rates was usually provided in addition to food reinforcement. Cool. So they actually got it done? They got it done. This is back in 1969. After several training sessions, monkeys could increase the activity of newly isolated cells by 50% to 500% above rates before reinforcement. Fascinating. Brain is very plastic. And so, and so from here, the number of experiments grew. Yeah, number of experiments, as well as set of tools to interface with the brain have just exploded, I think, and also just understanding the neural code and how some of the cortical layers and the functions are organized. So the other paper that is pretty seminal, especially in the motor decoding, was this paper in the 1980s from Georgiaopolis that discovered that there's this thing called motor tuning curve. So what are motor tuning curves? It's the fact that there are neurons in the motor cortex of mammals, including humans, that have a preferential direction that causes them to fire. So what that means is there are a set of neurons that would increase their spiking activities when you're thinking about moving to the left, right, up, down, and any of those vectors. And based on that, you could start to think, well, if you can identify those essential eigenvectors, you can do a lot, and you can actually use that information for actually decoding someone's intended movement from the cortex. That was a very, very seminal paper that showed that there is some sort of code that you can extract, especially in the motor cortex. So there's signal there, and if you measure the electrical signal from the brain, that you could actually figure out what the intention was, correct? Yeah. Not only electrical signals, but electrical signals from the right set of neurons that give you these preferential direction. Okay, so going slowly towards neural link. One interesting question is, what do I understand on the BCI front on invasive versus non invasive from this line of work? How important is it to park next to the neuron? What does that get you? That answer fundamentally depends on what you want to do with it. There's actually incredible amount of stuff that you can do with EEG and electrochardiograph ECoG, which actually doesn't penetrate the cortical layer or parenchyma, but you place a set of electrodes on the surface of the brain. So the thing that I'm personally very interested in is just actually understanding and being able to just really tap into the high resolution, high fidelity understanding of the activities that are happening at the local level, and we can get into biophysics. But just to step back to kind of use analogy, because analogy here can be useful sometimes it's a little bit difficult to think about electricity. At the end of the day, we're doing electrical recording that's mediated by ionic currents, movements of these charged particles, which is really, really hard for most people to think about. But turns out a lot of the activities that are happening in the brain and the frequency band with which that's happening is actually very, very similar to sound waves and our normal conversation audible range. So the analogy that typically is used in the field is, if you have a football stadium, there's game going on. If you stand outside the stadium, you maybe get a sense of how the game is going based on the cheers and the boos of the home crowd, whether the team is winning or not. But you have absolutely no idea what the score is. You have absolutely no idea what individual audience or the players are talking or saying to each other what the next play is, what the next goal is. So what you have to do is you have to drop the microphone into the stadium and then get near the source, like into the individual chatter. In this specific example, you would want to have it right next to where the huddle is happening. So I think that's kind of a good illustration of what we're trying to do. When we say invasive or minimally invasive or implanted brain computer interfaces versus non invasive or non implanted brain interfaces. It's basically talking about, where do you put that microphone, and what can you do with that information? So what is the biophysics of the read and write communication that we're talking about here as we now, Stephen, into the efforts at neuralink? Yeah, so brain is made up of these specialized cells called neurons. There's billions of them, tens of billions, sometimes people call it 100 billion, that are connected in this complex yet dynamic network that are constantly remodeling. They're changing their synaptic weights, and that's what we typically call neuroplasticity. And the neurons are also bathed in this charged environment that is latent with many charged molecules, like potassium ions, sodium ions, chlorine ions. And those actually facilitate these through ionic current communication between these different networks. And when you look at a neuron as well, they have these membrane with a beautiful, beautiful protein structure called voltage selective ion channels, which, in my opinion, is one of nature's best inventions in many ways, if you think about what they are, they're doing the job of a modern day transistors. Transistors are nothing more at the end of the day, than a voltage gated conduction channel. And nature found a way to have that very, very early on in its evolution. And as we all know, with the transistor, you can have many, many computation and a lot of amazing things that we have access to today. So I think it's one of those, just as a tangent, just a beautiful, beautiful invention that the nature came up with, these voltage gated ion channels. I mean, I suppose there is, on the biological level, at every level of the complexity of the hierarchy of the organism, there's going to be some mechanisms for storing information and for doing computation, and this is just one such way. But to do that with biological and chemical components is interesting. Plus, like with neurons, I mean, it's not just electricity, it's chemical communication. It's also mechanical. I mean, these are like actual objects that have, like, that vibrate. I mean, they move. Yeah, they're actually. I mean, there's a lot of really, really interesting physics that are involved. And kind of going back to my work on ultrasound during grad school, there are groups, and there were groups, and there are still groups of looking at ways to cause neurons to actually fire an action potential using ultrasound wave. And the mechanism to which that's happening is still unclear, as I understand. Um, you know, it may just be that, you know, you're imparting some sort of thermal energy, and that causes cells to depolarize in some interesting ways. Um, but there are also these, um, ion channels, or even membranes that actually just open up its pore as they're being mechanically. Like shook. Right, vibrated. So there's just a lot of elements of these move particles, which, again, that's governed by diffusion physics, movements of particles. And there's also a lot of interesting physics there. Also, not to mention, as Roger Penrose talks about, there might be some beautiful weirdness in the quantum mechanical effects of all of this, and he actually believes that consciousness might emerge from the quantum mechanical effects there. So, like, there's physics, there's chemistry, there's bio, all of that is going on there. Oh, yeah, yeah. I mean, you can. Yes, I. There's. There's a lot of levels of physics that you can dive into, but, yeah, in the end, you have these, um. Uh, membranes with these voltage gated ion channels that selectively let, um, these charged molecules that are in. In the extracellular matrix, like in and out. Um, and these neurons generally have these, like, resting potential where there's a voltage difference between inside the cell and outside the cell. And, uh, when there's some sort of stimuli that changes, uh, the state, such that they need to send information to the. The downstream network. Um, you know, you start to kind of see these, like, sort of orchestration of these different molecules going in and out of these channels. They also open up, like, more of them open up once it reaches some threshold, to a point where you have a depolarizing cell that sends an action potential. So it's just a very beautiful kind of orchestration of these molecules. And what we're trying to do when we place an electrode or parking it next to a neuron, is that you're trying to measure these local changes in the potential, again mediated by the movements of the ions. And what's interesting, as I mentioned earlier, there's a lot of physics involved. And the two dominant physics for this electrical recording domain is diffusion physics and electromagnetism. And where one dominates, where Maxwell's equation dominates versus Fick's law dominates, depends on where your electrode is. If it's close to the source, mostly electromagnetic based, when you're farther away from it, it's more diffusion based. So, essentially, when you're able to park it next to it, you can listen in on those individual chatter and those local changes in the potential and the type of signal that you get. Are these canonical textbook neural spiking waveform when you're the moment, you're further away. And based on some of the studies that people have done, Christoph Koch's lab and others, once you're away from that source by roughly around 100 micron, which is about a width of a human hair, you no longer hear from that neuron. You're no longer able to have the system sensitive enough to be able to record that particular, um, local membrane potential change in that neuron. And just to kind of give you a sense of scale, also, when you, when you look at a hundred micron voxels. So 100 micron by hundred micron by 100 micron box, uh, in a brain tissue, there's roughly around 40 neurons and whatever number of connections that they have. So there's a lot in that volume of tissue. So the moment you're outside of that, you're. There's just no hope that you'll be able to detect that change from that one specific neuron that you may care about. Yeah, but as you're moving about this space, you'll be hearing other ones. So if you move another hundred micron, you'll be hearing chatter from another community. Correct. And so the, the whole sense is you want to place as many as possible electrodes and then you're listening to the chatter. Yeah, you want to listen to the chatter. And at the end of the day, you also want to basically let the software do the job of decoding. Just to go to why ECOG and EEG work at all when you have these local changes. Obviously, it's not just this one neuron that's activating. There's many, many other networks that are activating all the time. You do see a general change in the potential of this electro charged medium. That's what you're recording when you're farther away. I mean, you still have some reference electrode that's stable in the brain, that's just electro active organ, and you're seeing some combination aggregate action potential changes, and then you can pick it up. Right. It's a much slower changing signals, but there are these canonical oscillations and waves, like gamma waves, beta waves, like when you sleep, that can be detected because there's sort of a synchronized kind of global effect of the brain that you can detect. And, I mean, the physics of this go like. I mean, if we really want to go down that rabbit hole, there's a lot that goes on in terms of why diffusion physics, at some point dominates. When you're further away from the source, it's just a charged medium. So similar to how, when you have electromagnetic waves propagating in atmosphere or in a charged medium, like a plasma, there's this weird shielding that happens that actually, um, further attenuates the signal, um, as you move away from it. So, yeah, you see, like, if you do a really, really deep dive on kind of the signal attenuation over distance, you start to see kind of one over r square in the beginning, and then exponential drop off. And that's the knee at which, you know, you go from electromagnetic magnetism dominating to diffusion physics dominating. But once again, with the electrodes, the biophysics that you need to understand is not as deep, because no matter where you're placing it, you're listening to a small crowd of local neurons, correct? Yeah. So, once you penetrate the brain, you're in the arena, so to speak. And there's a lot of neurons. There are many, many of them. But then again, there's a whole field of neuroscience that's studying, like, how the different groupings, the different sections of the seating in the arena, what they usually are responsible for, which is where the metaphor probably falls apart, because the seating is not that organized in an arena. Also, most of them are silent. They don't really do much, you know, or their activities are. You have to hit it with just the right set of stimulus. So they're usually quiet. They're usually very quiet. There's, I mean, similar to dark energy and dark matter, there's dark neurons. What are they all doing? When you place these electrode again, like, within this 100 micron volume, you have 40 or so neurons. Like, why are you. Why do you not see 40 neurons? Why do you see only a handful? What is happening there? Well, they're mostly quiet, but, like, when they speak, they say, profound shit. I think that's the way I'd like to think about it. Anyway, before we zoom in even more, let's zoom out. So how does neuralink work? From the surgery to the implant to the signal and the decoding process and the human being able to use the implant to actually affect the world outside and all of this, I'm asking in the context of, there's a gigantic, historic milestone that neuralink just accomplished in January of this year, putting a neuralink implant in the first human being, Noland. And there's been a lot to talk about there, about his experience, because he's able to describe all the nuance and the beauty and the fascinating complexity of that experience of everything involved. But on the technical level, how does neuralink work? Yeah, so there are three major components to the technology that we're building. One is the device, the thing that's actually recording these neural chatters. We call it n one implant, or the link. And we have a surgical robot that's actually doing an implantation of these tiny, tiny wires that we call threads that are smaller than human hair. And once everything is surgicalized, you have these neural signals, these spiking neurons that are coming out of the brain, and you need to have some sort of software to decode what the users intend to do with that. So there's what's called the neuralink application, or b, one app that's doing that translation is running the very, very simple machine learning model that decodes these inputs that are neural signals and then convert it to a set of outputs that allows our participant, first participant Nolan, to be able to control a cursor. And this is done wirelessly. And this is done wirelessly. So our implant is actually a two part. The link has these flexible, tiny wires called threads that have multiple electrodes along its length, and they're only insert it into the cortical layer, which is about three to 5 mm in a human, human brain in the motor cortex region. That's where the intention for movement lies in. And we have 64 of these threads, each thread having 16 electrodes along the span of three to 4 mm separated by 200 microns. So you can actually record along the depth of the insertion. And based on that signal, there's custom integrated circuit, or ASic, that we built that amplifies the neural signals that you're recording and then digitizing it, and then has some mechanism for detecting whether there was an interesting event, that is a spiking event, and decide to send that or not send that through Bluetooth to an external device, whether it's a phone or a computer that's running this neuralink application. So there's onboard signal processing already just to decide whether this is an interesting event or not. So there is some computational power on board inside the. In addition to the human brain. Yeah. So it does the signal processing to kind of really compress the amount of signal that you're recording. So we have a total of thousand electrodes sampling at just under 20 khz with ten bit each. So that's 200 megabits that's coming through to the chip from 1000 channel simultaneous neural recording. And that's quite a bit of data, and there are technology available to send that off wirelessly. But being able to do that in a very, very thermally constrained environment, that is a brain. So there has to be some amount of compression that happens to send off only the interesting data that you need, which in this particular case for motor decoding, is occurrence of a spike or not, and then being able to use that to decode the intended cursor movement. So the implant itself processes it, figures out whether a spike happened or nothing with our spike detection algorithm, and then sends it off packages. It sends it off through bluetooth to an external device that then has the model to decode. Okay, based on these spiking inputs, did Nolan wish to go up, down, left, right, or click or right click or whatever? All of this is really fascinating, but let's stick on the n one implant itself. So the thing that's in the brain. So I'm looking at a picture of it. There's an enclosure. There's a charging coil. So we didn't talk about the charging, which is fascinating. The battery, the power electronics, the antenna. Then there's the signal processing electronics. I wonder if there's more kinds of signal processing you can do. That's another question. And then there's the threads themselves, with the enclosure on the bottom. So maybe to ask about the charging. So there's a external charging device. Yeah, there's an external charging device. Um, so, yeah, the. The second part of the implant, the threads are the ones, again, just the. The last three to 5 mm are the ones that are actually penetrating the cortex. Uh, rest of it is actually, most of the volume is occupied by the battery, uh, rechargeable battery, and it's about a size of a quarter. I actually have a device here if you want to take a look at it. This is the flexible thread component of it, and then this is the implant. So it's about a size of a Us quarter. It's about nine millimeter thick. So, basically, this implant, once you have the craniectomy and the directomy threads are inserted and, um, the. The hole that you created, this craniectomy gets replaced with that. So, basically, that thing plugs that hole, and you can screw in, uh, these self drilling cranial screws to hold it in place. And at the end of the day, once you have the skin flap over, there's only about two to 3 mm. That's, you know, obviously transitioning off of the top of the implant to where the screws are. And. And that's the minor bump that you. Have those threads look tiny. That's incredible. That is really incredible. That is really incredible. And also, as you're right, most of the volume, actual volume, is the battery. Yeah. This is way smaller than I realized. They are also, the threads themselves are quite strong. They look strong. And the thread themselves also has a very interesting feature at the end of it called the loop. And that's the mechanism to which the robot is able to interface and manipulate this tiny hair like structure. And they're tiny. So what's the width of a thread? Yeah. So the width of a thread starts from 16 micron and then tapers out to about 84 micron. So average human hair is about 80 to 100 micron in width. This thing is amazing. This thing is amazing. Yes. Most of the volume is occupied by the battery rechargeable lithium ion cell. And the charging is done through inductive charging, which is actually very commonly used. Most cell phones have that. The biggest difference is that for us, usually when you have a phone and you want to charge it on a charging pad, you don't really care how hot it gets. Whereas in, for us, it matters. There's a very strict regulation and good reasons to not actually increase the surrounding tissue temperature by two degrees celsius. So there's actually a lot of innovation that is packed into this to allow charging of this implant without causing that temperature threshold to reach. And even small things like you see this charging coil and what's called a ferrite shield, right? So without that ferrite shield, what you end up having when you have resonant inductive charging is that the battery itself is a metallic can, and you form these eddy currents from external charger, and that causes heating, and that actually contributes to inefficiency in charging. So this ferrite shield, what it does is that it actually concentrate that field line away from the battery and then around the coil that's actually wrapped around it. There's a lot of really fascinating design here to make it. I mean, you're integrating a computer into a biological. A complex biological system. Yeah, there's a lot of innovation here. I would say that part of what enabled this was just the innovations in the wearable. There's a lot of really, really powerful, tiny low power microcontrollers, temperature sensors, or various different sensors and power electronics. A lot of innovation really came in the charging coil design, how this is packaged, and how do you enable charging such that you don't really exceed that temperature limit, which is not a constraint for other devices out there? So let's talk about the threads themselves, those tiny, tiny, tiny things. So how many of them are there? You mentioned 1000 electrodes. How many threads are there? And what do the electrodes have to do with the threads? Yeah. So the current instantiation of the device has 64 threads and each thread has 16 electrodes, for a total of 1024 electrodes that are capable of both recording and stimulating. And the thread is basically this polymer insulated wire. The metal conductor is the kind of a tiramisu cake of Thai plat, gold plat Thai. And they're very, very tiny wires, two micron in width. So two 1,000,000th of meter. It's crazy that that thing I'm looking at has the polymer insulation, has the conducting material and has 16 electrodes at the end of it. On each of those thread? Yeah, on each of those threads, correct, 16. Each one of those. You're not going to be able to see it with naked eyes. And, I mean, to state the obvious, or maybe for people who are just listening, they're flexible. Yes, yes. That's also one element that was incredibly important for us. So each of these thread are, as I mentioned, 16 micron in width, and then they taper to 84 micron. But in thickness, they're less than five micron. And in thickness, it's mostly polyimide at the bottom and this metal track and then another polyamide. So two micron of polyamide, 400 nanometer of this metal stack, and two micron of polyemide sandwiched together to protect it from the environment that is 37 degrees c bag of saltwater. So what's some maybe, can you speak to some interesting aspects of the material design here? Like what does it take to design a thing like this and to be able to manufacture a thing like this for people who don't know anything about this kind of thing? Yeah. So the material selection that we have is nothing. I don't think it was particularly unique. There were other labs and there are other labs that are looking at similar material stack. There's a fundamental question and still needs to be answered around the longevity and reliability of these microelectrodes that we call compared to some of the other more conventional neural interfaces, devices that are of intracranial, so penetrating the cortex, that are more rigid, like the Utah ray, that are these four by four millimeter kind of silicon shank that have exposed recording site at the end of it. And that's been kind of the innovation from Richard Norman back in 1997. It's called the youth array. Because he was at University of Utah. What does the Utah array look like? So it's a rigidity type of. Yeah. So we can actually look it up. Yeah, yeah. So it's a bed of needle. There's. Okay, go ahead. I'm sorry. Those are rigid, rigid, rigid, rigid. Yeah. You weren't kidding. And the size and the number of shanks vary anywhere from 64 to 128. At the very tip of it is an exposed electrode that actually records neural signal. Um, the other thing that's interesting to note is that unlike neuralink threads that have recording electrodes that are actually exposed iridium oxide recording sites along the depth, this is only at a single depth. So these Utahra spokes can be anywhere between 0.5 millimeter to 1.5 millimeter. And there they also have designs that are slanted, so you can have it inserted at different depth. But that's one of the other big differences. And then, I mean, the main key difference is the fact that there's no active electronics. These are just electrodes. And then there's a bundle of a wire that you're seeing, and then that actually, then exits the craniectomy that then has this port that you can connect to for any external electronic devices they are working on or have the wireless telemetry device, but it still requires a through the skin port. That actually is one of the biggest failure modes for infection for the system. What are some of the challenges associated with flexible threads? Like, for example, on the robotic side, r1 implanting those threads? How difficult is that task? Yeah, as you mentioned, they're very, very difficult to maneuver by hand. These Utahras that you saw, uh, earlier, they're actually inserted by a neurosurgeon, actually positioning it near the site that they want. And then, uh, they're actually, there's a pneumatic hammer that actually pushes them in. Um, so, so it's a, it's a pretty simple process. Um, and they're easier to maneuver. But for, for these thin film arrays, they're, they're very, very tiny and flexible. So they're, they're very difficult to maneuver. So that, that's why we built an entire robot to do that. There are other, other reasons for why we built the robot, and that is ultimately, we want this to help millions and millions of people that can benefit from this. And there just aren't that many neurosurgeons out there. And robots can be something that we hope can actually do large parts of the surgery. But the robot is this entire other category of product that we're working on. And it's essentially this multi axis gantry system that has the specialized robot head that has all of the optics and this kind of a needle retracting mechanism that maneuvers these threads via this loop structure that you have on the thread. So the thread already has a loop structure by which you can grab it. Correct, correct. So this is fascinating. So you mentioned optics. So there's a robot r1. So for now there's a human that actually creates a hole in this goal. And then after that, there's a computer vision component that's finding a way to avoid the blood vessels, and then you're grabbing it by the loop, each individual thread, and placing it in a particular location to avoid the blood vessels and also choosing the depth of placement, all that, so controlling every, like, the 3d geometry of the placement. Correct. So the, the aspect of this robot that is unique is that it's not surgeon assisted or human assisted. It's a semi automatic or automatic robot. Once you, you know, obviously there are human components to it. When you're placing targets, um, you can always move it away from kind of major vessels that you see. Um, but, I mean, we want to get to a point where one click and it just does the surgery within minutes. So the computer vision component finds great targets, candidates, and the human kind of approves them. And the robot does. Is it does do like one thread at a time, or does it do. It does one thread at a time. And that's actually also one thing that we are looking at ways to do multiple threads at a time. There's nothing stopping from it. You can have multiple engagement mechanisms, but right now it's one by one. And we also still do quite a bit of just verification to make sure that it got inserted. If so, how deep did it actually match what was programmed in? So on and so forth. And the actual electrodes are placed at differing depths in the like. I mean, it's very small differences, but. Differences, yeah. Yeah. And so that there's some reasoning behind that, as you mentioned, like it gets more varied signal. Yeah, I mean, we try to place them all around three or four millimeter from the surface, just because the span of the electrode, those 16 electrodes that we currently have in this version, spans roughly around 3 mm. So we want to get all of those in the brain. This is fascinating. Okay, so there's a million questions here. If we go zoom in specifically on the electrodes, what is your sense? How many neurons is each individual electrode listening to? Yeah, each electrode can record from anywhere between zero to 40, as I mentioned right earlier. But practically speaking, we only see about at most, like two to three, and you can actually distinguish which neuron it's coming from by the shape of the spikes. So I mentioned the spike detection algorithm that we have. It's called boss algorithm, buffer, online. Spike sorter. Nice. It actually outputs, at the end of the day, six unique values, which are kind of the amplitude of these, negative going hump, middle hump, positive going hump, and then also the time at which these happen. And from that you can have a statistical probability, probability estimation of, is that a spike? Is it not a spike? And then based on that, you could also determine, oh, that spike looks different than that spike must come from a different neuron. Okay. So that, that's a nice signal processing step from which you can then make much better predictions about if there's a spike, especially in this kind of context where there could be multiple neurons screaming. And that that also results in you being able to compress the data better and inside of it. Okay. And just to be clear, I mean, the labs do this, what's called spike sorting. Usually, once you have these, like, broadband, the fully digitized signals, and then you run a bunch of different set of algorithms to kind of tease apart. It's just all of this for us is done on the device. On the device in a very low power, custom, you know, built ASIC digital. Processing unit, highly heat constrained. Highly heat constrained. And the processing time from signal going in and giving you the output is less than a microsecond, which is, you know, a very, very short amount of time. Oh, yeah. So the latency has to be super short. Correct. Oh, wow. Oh, that's a pain in the ass. Yeah. Latency. Is this a huge, huge thing that you have to deal with right now? The biggest source of latency comes from the Bluetooth, the way in which they are packetized, and we bin them in 15 millisecond. Oh, interesting communication constraint. Is there some potential innovation there on the protocol used? Absolutely. Okay. Yeah. Bluetooth is definitely not our final wireless communication protocol that we want to get to. It's a highly, hence the n one and the r one, I imagine that increases NxRX. Yeah, that's the communication protocol, because Bluetooth allows you to communicate against farther distances than you need to. So you can go much shorter. Yeah. The only. Well, the primary motivation for choosing Bluetooth is that, I mean, everything has Bluetooth. All right. So you can talk to any device. Interoperability is just absolutely essential, especially in this early phase. And in many ways, if you can access a phone or a computer, you can do anything. Well, it would be interesting to step back and actually look at, again, the same pipeline that you mentioned for Nolan. So what does this whole process look like, from finding and selecting a human being to the surgery, to the first time he's able to use this thing. So we have what's called a patient registry that people can sign up to hear more about the updates, and that was a route to which Nolan applied. And the process is that once the application comes in, it contains some medical records, and we, based on their medical eligibility, there's a lot of different inclusion exclusion criteria for them to meet. And we go through a pre screening interview process with someone from Neuralink. And at some point we also go out to their homes to do a BCI home audit. Because one of the most revolutionary part about having this n one system that is completely wireless is that you can use it at home. You don't actually have to go to the lab and go to the clinic to get connectorized to these specialized equipment that you can't take home with you. So that's one of the key elements of when we're designing the system that we wanted to keep in mind. People hopefully, would want to be able to use this every day in the comfort of their homes. Part of our engagement and what we're looking for during PCI home audit is to just understand their situation, what other assistive technology that they use. And we should also step back. And I kind of say that the estimate is 180,000 people live with quadriplegia in the United States, and each year, an additional 18,000 suffer a paralyzing spinal cord injury. So these are folks who have a lot of challenges living a life in terms of accessibility, in terms of doing the things that many of us just take for granted day to day. And one of the things, one of the goals of this initial study is to enable them to have sort of digital autonomy, where they, by themselves, can interact with a digital device using just their mind, something that you're calling telepathy. So digital telepathy, where a quadriplegic can communicate with a digital device in all the ways that we've been talking about, control the mouse cursor enough to be able to do all kinds of stuff, including play games and tweet and all. That kind of stuff. And there's a lot of people for whom life, the basics of life are difficult because of the things that have happened to them. Yeah, I mean, movement is so fundamental to our existence, I mean, even speaking, involves movement of mouth, lip, larynx. And without that, it's extremely debilitating. And there are many, many people that we can help. And especially if you start to look at other forms of movement disorders that are not just from spinal cord injury, but from ALS, MS, or even stroke, that. That leads you and or just aging. Right. That leads you to lose some of that mobility, that independence, it's extremely debilitating. And all of these are opportunities to help people, to help alleviate suffering, to help improve the quality of life. But each of the things you mentioned is its own little puzzle that needs to have increasing levels of capability from a device like a neuralink device. And so the first one you're. You're focusing on is. It's just a beautiful word, telepathy. So being able to communicate using your mind wirelessly with a digital device. Can you just explain this? Exactly what we're talking about? Yeah, I mean, it's exactly that. I mean, I think if you are able to control a cursor and able to click and be able to get access to computer or phone, I mean, the whole world opens up to you, and, I mean, I guess the word telepathy, if you kind of think about that as just definitionally being able to transfer information from my brain to your brain without using some of the physical faculties that we have, like voices. But the interesting thing here is, I think the thing that's not obviously clear is how exactly it works. So, in order to move a cursor, there's at least a couple of ways of doing that. So, one is you imagine yourself maybe moving a mouse with your hand, or you can then. Which Nolan talked about. Like, imagine moving the cursor with your mind. Like, I don't. But it's like, there is a cognitive step here that's fascinating, because you have to use the brain, and you have to learn how to use the brain, and you kind of have to figure it out dynamically, like, because you reward yourself if it works. So you. Like, I mean, there's a step that. This is just a fascinating step. Cause you have to get the brain to start firing in the right way. And you do that by imagining, like, fake it till you make it, and all of a sudden, it creates the right kind of signal that, if decoded correctly, can create the kind of effect. And then there's, like, noise around that. You have to figure all of that out. But on the human side, imagine the cursor moving is what you have to do. Yeah. He says, using the force. The force. I mean, that's. Isn't that just, like, fascinating to you that it works? Like, to me, it's like, holy shit. That actually works. Like, you could move a cursor with. Your mind, you know, as much as you're ahead. Learning to use that thing, that thing's also learning about you. Like, our model is constantly updating the weights to say, oh, if someone is thinking about this sophisticated forms of spiking patterns, that actually means to do this. So the machine is learning about the human and the human is learning about the machine. So there is adaptability to the signal processing, the decoding step, and then there's the adaptation of Nolan, the human being. Like, the same way. If you give me a new mouse and I move it, I learn very quickly about its sensitivity, so I'll learn to move it slower. And then there's other kind of signal drift and all that kind of stuff they have to adapt to. So both are adapting to each other. Correct. That's a fascinating, like, software challenge on both sides. The software on both. On the human software and the organic and the inorganic. Organic. And they're inorganic. Anyway, so sorry to rudely interrupt. So there's a selection that Nolan has passed with flying colors. So everything, including that it's a BCI friendly home, all of that. So what is the process of the surgery? The implantation, the first moment when he. Gets to use the system, the end to end. We say patient in to patient out is anywhere between two to 4 hours. In particular case for Nolan, it was about three and a half hours. And there's many steps leading to the actual robot insertion. There's anesthesia induction, and we do intraop CT imaging to make sure that we're drilling the hole in the right location. And this is also pre planned beforehand. Um, uh, someone goes through, someone like Nolan would go through fMRI, and then, um, they can think about wiggling their hand. You know, obviously, due to their injury, it's not going to actually lead to, um, any, any sort of intended output, but it's the same part of the brain that actually lights up when you're imagining moving your finger to actually moving your finger. And that's one of the ways in which we can actually know where to place our threads because we want to go into what's called the hand knob area in the motor cortex. And, you know, as much as possible, densely put our electro threads. So, yeah, we do intraop CT imaging to make sure and double check the location of the craniectomy. And surgeon comes in, does their thing in terms of, like, skin, uh, incision, craniectomy. So drilling of the skull, and then there's many different layers of the brain. Uh, there's what's called the dura, which is a very, very thick layer that surrounds the brain that gets actually, resectus in a process called duryectomy, and that then exposed the PI in the brain that you want to insert. And by the time it's been around, anywhere between one to one and a half hours, robot comes in, does this thing. Placement of the target, inserting of the thread, that takes anywhere between 20 to 40 minutes. In the particular case for Nolan, it was just under or just over 30 minutes. And then after that, the surgeon comes in. There's a couple other steps of, like, actually inserting the dural substitute layer, um, to protect the thread as well as the. The brain. And then, um, yeah, screw, screw in the implant, and then skin flap, and then suture, and then you're out. So, uh, when, uh, Nolan woke up, what was that like? What was the recovery like? And when was the first time he was able to use it? So he was actually immediately after the surgery. Um, you know, like, an hour after the surgery, as he was waking up, um, we did turn on the device, um, make sure that we are recording neural signals, and we actually did have a couple signals that we noticed that he can actually modulate. And what I mean by modulate is that he can think about crunching his fist, and you could see the spike disappear and appear. That's awesome. And that was immediate, right? Immediate after in the recovery room. How cool is that? Yeah, that's a human being. I mean, what did that feel like for you, this device in a human being? A first step of a gigantic journey. I mean, it's a historic moment. Even just that spike, just to be able to modulate that. Obviously, there have been other, as you mentioned, pioneers that have participated in these groundbreaking BCI investigational early feasibility studies. So we're obviously standing in the shoulders of the giants here. We're not the first ones to actually put electrodes in a human, human brain. But, I mean, just leading up to the surgery, I definitely could not sleep. It's the first time that you're working in a completely new environment. We had a lot of confidence, based on our benchtop testing, or preclinical are in these studies, that the mechanism, the threads, the insertion, all that stuff is very safe, and that it's obviously ready for doing this in a human. But there's still a lot of unknown. Unknown about, can the needle actually insert? I mean, we brought something like 40 needles just in case they break, and we ended up using only one. But, I mean, that was a level of just complete unknown, right, because it's a very, very different environment and, I mean, that's. That's why we do clinical trial in the first place, to be able to test these things out. So extreme nervousness and just. Just many, many sleepless night leading up to the surgery, and definitely the day before the surgery, and it was an early morning surgery. Like, we started at seven in the morning, and by the time it was around 1030, everything was done. But, I mean, first time seeing that, well, number one, just huge relief that this thing is doing what it's supposed to do, and two, just immense amount of gratitude for Nolan and his family and then many others that have applied and that we've spoken to and will speak to are true pioneers in every war. And I sort of call them the neural astronauts or neural knots. You know, these amazing. Just like in the sixties, right? Like these amazing, just pioneers, right? Exploring the unknown outwards. In this case, it's inward, but an incredible amount of gratitude for them to, you know, just participate and play a part, and it's a journey that we're embarking on together. But also, I think it was just. That was a very, very important milestone, but our work was just starting, so a lot of. Just kind of anticipation for, okay, what needs to happen next? What are a set of sequences of events that needs to happen for us to make it worthwhile for both Nolan. As well as us, just to linger on that just a huge congratulation to you and the team for that milestone. I know there's a lot of work left, but that's really exciting to see. That's a source of hope, this first big step opportunity to help hundreds of thousands of people and then maybe expand the realm of the possible for the human mind, for millions of people in the future. It's really exciting. Like, the opportunities are all ahead of us, and to do that safely and to do that effectively was really fun to see as an engineer, just watching other engineers come together and do an epic thing. That was awesome. So huge. Congrats. Thank you. Thank you. It's. Yeah. Could not have done it without the team and, yeah, I mean, that's the other thing that I told the team as well, of just this immense sense of optimism for the future. I mean, it's a very important moment for the company, needless to say, as well as hopefully for many others out there, that we can help. So, speaking of challenges, Neuralink published a blog post describing that some of the threads are attracted. And so the performance, as measured by bits per second, dropped at first, but then eventually it was regained. And the whole story of how it was regained is super interesting. That's definitely something I'll talk to bliss and to Nolan about. But in general, can you speak to this whole experience? How was the performance regained and just the technical aspects of the threads being attracted and moving? The main takeaway is that in the end, the performance have come back and it's actually gotten better than it was before. He's actually just beat the world record yet again last week to 8.5 bps. So, I mean, he's just cranking and he's just improving. The previous one was the. He set was eight, correct, 8.5. Yeah. The previous world record in human was 4.6, so it's almost double. And his goal is to try to get to ten, which is roughly around kind of the median neuralinker, using a, you know, mouse with the hand. So it's getting there. So, yes. So the performance was regained. Yeah, better than before. So that's a story on its own of what took the BCI team to recover that performance. It was actually mostly on kind of the signal processing. And so, as I mentioned, we were kind of looking at these spike outputs from the, um, our electrodes. And what happened is that kind of, uh, four weeks into the surgery, uh, we noticed that the threats have slowly come out of the brain. And the way in which we noticed this at first, obviously, is that, um, I think Nolan was the first to notice that his performance was degrading. Um, and I think at the time, we were also trying to do a bunch of different experimentation, um, you know, different algorithms, different, um, sort of ui ux. So it was expected that there will be variability in the performance, but we did see a steady decline. And then also the way in which we measure the health of the electrodes, or whether they're in the brain or not, is by measuring impedance of the electrode. So we look at the interfacial, kind of the Randall circuit, let they say, the capacitance and the. And the resistance between the electrosurface and the medium. And if that changes in some dramatic ways, we have some indication, or if you're not seeing spikes on those channels, you have some indications that something's happening there. And what we notice is that looking at those impedance plot and spike rate plots, and also because we have those electrodes recording along the depth you're seeing some sort of movement that indicated that the res were being pulled out of. Um. And that obviously will have an implication on the model side, because if you're, the number of inputs that are going into the model is changing because you have less of them, um, the out that that model needs to get updated. Right. And, um. But, but there were still signals, and as I mentioned, similar to how even when you place the signals on the surface of the brain, of the brain, or farther away, like outside the skull, you still see some useful signals. What we started looking at is not just the spike occurrence through this boss algorithm that I mentioned, but we started looking at just the power of the frequency band. That is interesting for Nolan, or Nolan to be able to modulate once we change the algorithm for the implant to not just give you the boss output, but also these spike band power output, that helped us refine the model with the new set of inputs. And that was the thing that really ultimately gave us the performance back in terms of. And obviously, the thing that we want ultimately, and the thing that we are working towards is figuring out ways in which we can keep those threads intact for as long as possible so that we have many more channels going into the model. That's by far the number one priority that the team is currently embarking on to understand how to prevent that from happening. The thing that I will say also is that, as I mentioned, this is the first time ever that we're putting these threats in the human brain. And human brain, just for size reference, is ten times that of the monkey brain or the sheep brain. And it's just a very, very different environment. It moves a lot more. It actually moved a lot more than we expected when we did Nolan surgery. And it's just a very, very different environment than what we're used to. And this is why we do clinical trial. We want to uncover some of these issues and failure modes earlier than later. In many ways, it's provided us with this enormous amount of data and information to be able to solve this. And this is something that neuralink is extremely good at. Once we have set of clear objective and engineering problem. We have enormous amount of talents across many, many disciplines to be able to come together and fix the problem very, very quickly. But it sounds like one of the fascinating challenges here is for the system and the decoding side to be adaptable across different timescales. So whether it's movement of threads or different aspects of signal drift sort of on the software of the human brain, something changing. Like Nolan talks about cursor drift. They could be corrected and there's a whole ux challenge to how to do that. So it sounds like adaptability is like a fundamental property that has to be engineered in. It is as a company, we're extremely vertically integrated. We make these thin film arrays in our own microfab. Yeah, there's, like you said, built in house. This whole paragraph here from this blog post is pretty gangster. Building the technology described above has been no small feat. And there's a bunch of links here that I recommend people click on. We constructed in house microfabrication capabilities to rapidly produce various iterations of thin film arrays that constitute our electrode threads. We created a custom femtosecond laser mill to manufacture components with micro level precision. I think there's a tweet associated with. This whole thing that we can get into. Yeah, this, this. Okay, what are we, what are we looking at here? This thing, this is so in less than 1 minute, our custom made femto second laser mill cuts this geometry in the tips of our needles. So we're looking at this weirdly shaped needle. The tip is only ten to twelve microns in width, only slightly larger than the diameter of a red blood cell. The small size allows threads to be inserted with minimal damage to the cortex. Okay, so what's interesting about this geometry? So we'll look at this geometry of a needle. This is the needle that's engaging with the loops in the thread. So they're the ones that thread the loop and then peel it from the silicon backing. And then this is the thing that gets inserted into the tissue. And then this pulls out, leaving the thread. And this kind of a notch, or the shark tooth that we used to call is the thing that actually is grasping the loop. And then it's, it's designed in such way, such that when you, when you pull out, leaves the loop. And the robot is controlling this needle. Correct. So this is actually housed in a cannula. And basically the robot is, has a lot of the optics that look for where the loop is. There's actually a 405 nanometer light that actually causes the polyimit to fluoresce so that you can locate the, the location of the loop. So the loop lights up? Yeah, yeah, they do. Micron precision process. What's interesting about the robot that it takes to do that? That's pretty crazy. That's pretty crazy. That robot is able to get this kind of precision. Yeah, our robot is quite heavy. Our current version of it. There's I mean, it's like a giant granite slab that weighs about a ton because it needs to be sensitive to vibration, environmental vibration. And then as the head is moving at the speed that is moving, you know, there's a lot of kind of motion control to make sure that you can achieve that level of precision. Um, a lot of optics that kind of zoom in on that. Um, you know, we're working on next generation of the robot that is lighter, easier to transport. I mean, it is a. It is a feat to move the robot to. And it's far superior to a human surgeon at this time for this particular task. Absolutely. I mean, let alone you try to actually thread a loop in a. In a sewing kit. I mean, this is like, we're talking like fractions of human hair. These things are. It's not visible. So continuing the paragraph, we developed novel hardware and software testing systems, such as our accelerated lifetime testing racks and simulated surgery environment, which is pretty cool. To stress test and validate the robustness of our technologies, we performed many rehearsals of our surgeries to refine our procedures and make them second nature. This is pretty cool. We practice surgeries on proxies with all the hardware and instruments needed in our mock or in the engineering space. This helps us rapidly test and measure. So there's like, proxies. Yeah, this proxy is super cool, actually. So there's a 3d printed skull from the images that is taken at Barrow, as well as this hydrogel mixed, you know, sort of synthetic polymer thing that actually mimics the mechanical properties of the brain. It also has vasculature of the person. So basically what we're talking about here, and there's a lot of work that has gone into making this set proxy that it's about, like, finding the right concentration of these different synthetic polymers to get the right set of consistency for the needle dynamics, you know, as they're being inserted. But we practice this surgery with the person. You know, Nolan's basically physiology and brain many, many times prior to actually doing the surgery to every. Every step. Every step. Every step, yeah. Like, where does someone stand? Like, I mean, like, what you're looking at is the picture. This is in our office of this kind of corner of the robot engineering space that we, you know, have created this, like, mock or space that looks exactly like what they would experience, all the staff would experience during their actual surgery. So, I mean, it's just kind of like any dense rehearsal where you know exactly where you're going to stand at what point, um, and you just practice that over and over and over again with an exact anatomy of someone that you're going to surgerise. And. And it got to a point where a lot of our engineers, when we created a craniectomy, they're like, oh, that looks very familiar. We've seen that before. Yeah, man. There's wisdom you can gain through doing the same thing over and over and over. It's like a jira dreams of sushi kind of thing. Because then it's like olympic athletes visualize the Olympics, and then once you actually show up, it feels easy. It feels like any other day. It feels almost boring winning the gold medal because you visualize this so many times, you've practiced this so many times that nothing bothers you. It's boring. You win the gold medal is boring. And the experience they talk about is mostly just relief, probably, that they don't have to visualize it anymore. Yeah. The power of the mind to visualize. And where. I mean, there's a whole field that studies where muscle memory lies in cerebellum. Yeah, it's incredible. I think it's a good place to actually ask. Sort of the big question that people might have is, how do we know every aspect of this that you describe is safe? At the end of the day, the gold standard is to look at the tissue. What sort of trauma did you cause the tissue? And does that correlate to whatever behavioral anomalies that you may have seen? And that's the language to which we can communicate about the safety of inserting something into the brain and what type of trauma that you can cause. So we actually have an entire department of pathology that looks at these tissue slices. There are many steps that are involved in doing this. Once you have studies that are launched with particular endpoints in mind, at some point you have to euthanize the animal, and then you go through necropsy to collect the brain tissue samples. You fix them in formalin and you gross them, you section them, and you look at individual slices just to see what kind of reaction, or lack thereof, exists. So that's the language to which FDA speaks. And as well, for us to evaluate the safety of the insertion mechanism as well as the threads at various different time points, both acute. So anywhere between, you know, zero to three months to beyond three months. So those are kind of the details of an extremely high standard of safety that has to be reached. Correct. FDA supervises this, but there's, in general, just a very high standard. And every aspect of this, including the surgery, I think Matthew McDougall has mentioned that, like, the standard is, let's say, how to put it politely, higher than maybe some other operations that we take for granted. So the standard for all the surgical stuff here is extremely high. Very high. I mean, it's a highly, highly regulated environment with the governing agencies that scrutinize every medical device that gets marketed. And I think it's a good thing. It's good to have those high standards, and we try to hold extremely high standards to understand what sort of damage, if any, these innovative emerging technologies and new technologies that we're building are. And so far, we have been extremely impressed by lack of immune response from these threads. Speaking of which, you talk to me with excitement about the histology and some of the images that you're able to share. Can you explain to me what we're looking at? Yeah. So what you're looking at is a stained tissue image. So this is a sectioned tissue slice from an animal that was implanted for seven months. So kind of a chronic time point. And you're seeing all these different colors, and each color indicates specific types of cell types. So purple and pink are astrocytes and microglia, respectively. They're type of glial cells. And the other thing that people may not be aware of is your brain is not just made up of soup of neurons and axons. There are other cells, like glial cells, that actually kind of is the glue and also react if there are any trauma or damage to the tissue. But the brown are the neurons. The brown are the neurons nuclei. So what you're seeing is in this kind of macro image, you're seeing these, like, circle highlighted in white, the insertion sites. And when you zoom into one of those, you see the threads. And then in this particular case, I think we're seeing about the 16 wires that are going into the page. And the incredible thing here is the fact that you have the neurons that are these brown structures or brown circular or elliptical thing that are actually touching and abutting the threads. So what this is saying is that there's basically zero trauma that's caused during this insertion. And with these neural interfaces, these microelectrodes that you insert, that is one of the most common mode of failure. So when you insert these threads, like the Utahray, it causes neuronal death around the site because you're inserting a foreign object, right. And that kind of elicit these immune response through microglia and astrocytes. They form this protective layer around it. Not only are you killing the neuron cells, but you're also creating this protective layer that then basically prevents you from recording neural signals because you're getting further and further away from the neurons that you're trying to record. And that is the biggest mode of failure. And in this particular example, in that insight, it's about 50 micron with that scale bar. The neurons just seem to be attracted. To it, so there's certainly no trauma. That's such a beautiful image, by the way. So the brown are the neurons. For some reason, I can't look away. It's really cool. Yeah. And the way that these things, like, I mean, your tissues generally don't have these beautiful colors. This is multiplex stain that uses these different proteins that are staining these at different colors. We use very standard set of staining techniques with he, IBA one and nuan and GFAP. So if you go to the next image, this is also kind of illustrates the second point because you can make an argument. And initially, when we saw the previous image, we said, oh, like, are the threads just floating? What is happening here? Are we actually looking at the right thing? So what we did is we did another stain, and this is all done in house of this Mason's trichrome stain, which is in blue, that shows these collagen layers. So the blue, basically, you don't want the blue around the implant threads because that means that there's some sort of scarring that's happened. And what you're seeing, if you look at individual threads is that you don't see any of the blue, which means that there has been absolutely or very, very minimal to a point where it's not detectable amount of trauma in these inserted threads. So that, presumably is one of the big benefits of having this kind of flexible thread. Yeah. So we think this is primarily due to, uh, the size as well as the flexibility of the threats. Also, the fact that r1 is avoiding. That's for sure. So we're not disrupting or we're not, um, causing damage to, uh, the vessels and not breaking any of the blood brain barrier, uh, has, you know, basically caused the immune response to be muted. But this is also a nice illustration of the size of things. So this is the tip of the thread. Yeah, those are neurons. They're. And they're neurons. And they're. And this is the thread listening. And the electrodes are positioned how? Yeah, so this is what you're looking at is not electrode themselves. Those are the conductive wires. So each of those should probably be two micron in width. So what we're looking at is we're looking at the coronal slice, so we're looking at some slice of the tissue. So as you go deeper, you know, you'll obviously have less and less of the tapering of the thread. But, yeah. The point basically being that there's just cells around the inserted site, which is just an incredible thing to see. I've just never seen anything like this. How easy and safe is it to remove the implant? Yeah. So it depends on when. In the first three months or so after the surgery, um, there. There's a lot of kind of tissue modeling that's happening, you know, similar to when you get a cut. Um, you know, you obviously, uh, you know, start over first couple weeks, or depending on the size of the wound, um, scar tissue forming right there are these, like, contracted, and then in the end, they turn into scab, and you can scab it off. The same thing happens in the brain, and it's a very dynamic environment. And before the scar tissue or the neomembrane or the new membrane that forms, it's quite easy to just pull them out. And there's minimal trauma that's caused during that once the scar tissue forms. And with Nolan as well, we believe that that's the thing that's currently anchoring the threads. So we haven't seen any more movements since then, so they're quite stable. It gets harder to actually completely extract the threads. So our current method for removing the device is cutting the thread, leaving the tissue intact, and then unscrewing and taking the implant out. And that hole is now going to be plugged with either another neural link or just with a peak based, plastic based cap. Is it okay to leave the threads in there forever? Yeah, we think so. We've done studies where we left them there, and one of the biggest concerns that we had is, do they migrate and do they get to a point where they should not be? We haven't seen that again. Once the scar tissue forms, they get anchored in place. And I should also say that when we say upgrades, we're not just talking in theory here. We've actually upgraded many, many times. Most of our monkeys, or non human primates, NHP, have been upgraded. Pejor, who you saw playing mind pong, has the latest version of the device since two years ago and is seemingly very happy and healthy and fat. What's designed for the future? The upgrade procedure. So maybe for Noland, what would the upgrade look like? It was essentially what you're mentioning. Is there a way to upgrade the device internally, where you take it apart and keep the capsule and upgrade the internals. Yeah, there are a couple of different things here for Nolan. If we were to upgrade, what we would have to do is either cut the threads or extract the threads, depending on kind of the situation there in terms of how they're anchored or scarred in. If you were to remove them with the dural substitute, you have an intact brain, so you can reinsert different threads with the updated implant package. There are a couple different other ways that we're thinking about the future of what the upgradable system looks like. One is at the moment, we currently remove the dura, this thick layer that protects the brain, but that actually is the thing that actually proliferates, the scar tissue formation. So, typically, the general good rule of thumb is you want to leave the nature as is and not disrupt it as much. So we're looking at ways to insert the threads through the dura, which comes with different set of challenges, such as, you know, it's a pretty thick, uh, layer. So how do you actually penetrate that without breaking the needle? So we're looking at different needle design for that, as well as the kind of the loop engagement. The other biggest challenges are it's quite opaque optically in with white light illumination. So how do you avoid still this. This biggest advantage that we have of avoiding basket shore, how do you image through that? How do you actually still mediate that? So there are other imaging techniques that we're looking at to enable that, but the goal, our hypothesis is that, and based on some of the early evidence that we have doing through the dura insertion will cause minimal scarring. That causes them to be much easier to extract over time. And the other thing that we're also looking at, this is going to be a fundamental change in the implant architecture is, at the moment, it's a monolithic, single implant that comes with the thread that's, um, bonded together, so you can't actually separate the thing out. But you can imagine having two part implant, um, you know, bottom part that is the thread that are inserted, that has the chips, um, and maybe a radio and some power source. And then you have another implant that has more of the computational heavy load and. And the bigger battery. Um, and then one can be under the dura. One can be above the dura, like, you know, being the plug for the skull. They can talk to each other, but the thing that you want to upgrade the computer and not the threads, if you want to upgrade that, you just go in there, you know, remove the screws, and then put in the next version and, you know, you're off to, you know, it's a very, very easy surgery too. Like you do a skin incision, slip this in screw, probably be able to do this in ten minutes. So that would allow you to reuse the threads, sort of? Correct. So I mean, this leads to the natural question of what is the pathway to scaling the increase in the number of threads? Is that a priority? Is that like, what's, what's the technical challenge there? Yeah, that that is a priority. So for next versions of the implant, you know, the key metrics that we're looking to improve are number of channels just recording from more and more neurons. Um, you know, we have a pathway to actually go from currently 1000 to, you know, hopefully 3000 if not 6000 by end of this year. Um, and then end of next year we want to get to, uh, you know, even more 16,000. Wow. There's a couple limitations to that. One is, you know, obviously being able to photo lithographically print those wires. As I mentioned, it's two micron in width and spacing. Obviously there are chips that are much more advanced than those types of resolution and we have some of the tools that we have brought in house to be able to do that. So traces will be narrower just so that you have to have more of the wires coming up into the chip. Chips also cannot linearly consume more energy as you have more and more channels. So there's a lot of innovations in the circuit architecture as well as the circuit design topology to make them lower power. You need to also think about, if you have all of these spikes, how do you send that off to the end application? So you need to think about bandwidth limitation there and potentially innovations in signal processing. Physically, one of the biggest challenges is going to be the interface. It's always the interface that breaks. Bonding this thin film array to the electronics, it starts to become very, very highly dense interconnects. How do you connect the eyes that there's a lot of innovations in the 3d integrations in the recent years that we can take advantage of. One of the biggest challenges that we do have is, you know, forming this hermetic barrier. Right. You know, this is an extremely harsh environment that we're in the brain. So how do you protect it from? Yeah, like the brain trying to kill your electronics to also your electronics leaking things that you don't want into the brain and that forming that hermetic barrier is going to be a very, very big challenge that we, you know, I think are actually well suited to tackle. How do you test that? Like, what's the development environment? Yeah. To simulate that kind of harshness. Yeah. So this is. This is where the accelerated life tester essentially is a brain in a vat. It literally is a vessel that is made up of. And again, again, for all intents and purpose, for this particular types of tests, your brain is a saltwater. And you can also put some other set of chemicals, like reactive oxygen species, that get at these interfaces and trying to cause a reaction to pull it apart. But you could also increase the rate at which these interfaces are aging by just increasing temperature. So every ten degrees celsius that you increase, you're basically accelerating time by two x, and there's limit as to how much temperature you want to increase, because at some point, there's some other nonlinear dynamics that causes you to have other nasty gases to form. That just is not realistic in an environment. So what we do is we increase in our alt chamber by 20 degrees celsius. That increases the aging by four times. So essentially, one day in alt chamber is four day in calendar year, and we look at whether the implants still are intact, including the threads and operation and all that. And operation and all of that. It obviously is not an exact same environment as a brain because brain has mechanical, other more biological goops that attack at it. But it is a good test environment, testing environment for at least the enclosure and the strength of the enclosure. And, I mean, we've had implants, the current version of the implant, that has been in there for, I mean, close to two and a half years, which is equivalent to a decade, and they seem to be fine. So it's interesting that the. So, basically, a close approximation is warm salt water. Hot salt water is a good testing environment. That is. Yeah. By the way, I'm drinking element, which is basically salt water, which is making me kind of. It doesn't have computational power the way the brain does, but maybe in terms of other characteristics, it's quite similar. And I'm consuming it. Yeah, you have to get it at. The right ph, too, and then consciousness will emerge. Yeah. No. By the way, the other thing that also is interesting about our enclosure is if you look at our implant, it's not your common looking medical implant that usually is encased in a titanium can that's laser welded. We use this polymer called PCTFE polychloro trifluoroethylene, which is actually commonly used in blister packs. So when you have a pill and you try to pop a pill, there's like kind of that plastic membrane, that's what this is. No one's actually ever used this except us. And the reason we wanted to do this is because it's electromagnetically transparent. So when we talked about the electromagnetic inductive charging with titanium can, usually, if you want to do something like that, you have to have a sapphire window. And it's a very, very tough process to scale. So you're doing a lot of iteration here in every aspect of this, the materials, the software, the whole, whole shipping. Okay, so you mentioned scaling. Is it possible to have multiple neuralink devices as one of the ways of scaling, to have multiple neuralink devices implanted? That's the goal. That's the goal, yeah, we've had, we've had. I mean, our monkeys have had two neuralinks, one in each hemisphere. And then we're also looking at, you know, potential of having one in oral cortex, one in visual cortex, and one in wherever, other cortex. So focusing on a particular function, one neuralink device. Correct. I mean, I wonder if there's some level of customization that can be done on the compute side. So, for the motor cortex, absolutely, that's the goal. And, you know, we talk about neuralink building a generalized neural interface to the brain. Um, and that, that also is strategically how we're approaching this, um, with, with marketing and also, you know, with, with regulatory, which is, hey, look, um, we have the robot, and the robot can access any part of the cortex. Right now, we're focused on motor cortex with current version of the n one, that's specialized for motor decoding tasks. But also, at the end of the day, there's a general compute available there. But typically, if you want to really get down to hyper optimizing for power and efficiency, you do need to get to some specialized function. But what we're saying is, hey, I. You are now used to this robotic insertion techniques, which took many, many years of showing data and conversation with the FDA, and also internally convincing ourselves that this is safe. And now the difference is, if we go to other parts of the brain, like visual cortex, which we're interested in as our second product, obviously, it's a completely different environment. The cortex is laid out very, very differently. It's going to be more stimulation focused rather than recording, just creating visual percepts. But in the end, we're using the same thin film array technology, we're using the same robot insertion technology, we're using the same packaging technology. Now it's more the conversation is focused around what are the differences and what are the implication of those differences in safety and efficacy, the way you said. Second product is both hilarious and awesome to me, that product being restoring sight for blind people. So can you speak to stimulating the visual cortex? I mean, the possibilities there are just incredible to be able to give that gift back to people who don't have sight or even any aspect of that. Can you just speak to the challenges of. There's several challenges here, one of which is, like you said, from recording to the stimulation, just any aspect of that that you're both excited and see the challenges of. Yeah, I guess I'll start by saying that we actually have been capable of stimulating through our thin film array as well as other electronics for years. You know, we have actually demonstrated some of that capabilities for reanimating the limb in the spinal cord. Um, you know, obviously for. For the current EFS study, you know, we've hardware disabled that, so that's. That's something that, you know, we wanted to embark as a separate, separate journey. Um, and and, you know, obviously, there are many, many different ways to write information into the brain. The way in which we're doing that is through electrical, you know, passing electrical current and. And kind of causing that to really change the local environment so that you can sort of artificially cause kind of the neurons to depolarize in nearby areas for vision. Specifically, the way our visual system works, it's both well understood. I mean, anything with kind of brain, there are aspects of it that's well understood, but in the end, we don't really know anything. But the way visual system works is that you have photon hitting your. And in your eyes, there are these specialized cells called photoreceptor cells that convert the photon energy into electrical signals. And then that then gets projected to your back of your head. Your visual cortex. It goes through, actually thalamic system called LGN that then projects it out. And then in the visual cortex, there's visual area one or v one, and then there's a bunch of other higher level processing layers, like v two, v three. And there are actually kind of interesting parallels. And when you study the behaviors of these convolutional neural networks, like what the different layers of the network is detecting, first they're detecting these edges, and they're then detecting some more natural curves, and then they start to detect objects. Right. Kind of similar thing happens in the brain, and a lot of that has been inspired. And also, it's been kind of exciting to see some of the correlations there. But things like from there, where does cognition arise. And where's color encoded? There's just not a lot of understanding, fundamental understanding there. So in terms of kind of bringing sight back to those that are blind, there are many different forms of blindness. There's actually million people, 1 million people in the US that are legally blind. That means certain score below in the visual test. I think it's something like if you can see something at 20ft distance, that normal people can see at 200ft distance. If you're worse than that, you're legally blind. So fundamentally, that means you can't function effectively using sight in the world. Yeah, like to navigate, navigate your environment. Um, and yeah, there are different forms of blindness. There are forms of blindness where, uh, there's some degeneration of your, uh, retina, um, these photoreceptor cells, and. And rest of your visual, uh, you know, processing that I described is intact. And for those types of individuals, uh, you may not need to maybe stick electrodes into the visual cortex. You can actually build retinal prosthetic devices that actually just replaces the function of that retinal cells that are degenerated. And there are many companies that are working on that, but that's a very small slice, albeit significant, still smaller slice of folks that are legally blind. If there's any damage along that circuitry, whether it's in the optic nerve or just LGN circuitry or any break in that circuitous, that's not going to work for you. And the source of where you need to actually cause that visual percept to happen, because your biological mechanism not doing that is by placing electrodes in the visual cortex in the back of your head. And the way in which this would work is that you would have an external camera, whether it's something as unsophisticated as a goPro or some sort of wearable ray ban type glasses that Meta's working on, that captures a scene, that scene is then converted to a set of electrical impulses or stimulation pulses that you would activate in your visual cortex through these thin film arrays. And by playing some concerted kind of, uh, orchestra of these stimulation patterns, you can create what's called phosphines, which are these, um, kind of white yellowish dots that you can also create by just pressing your eyes. Um, you can actually create those percepts by stimulating the visual cortex. And the name of the game is really have many of those, and have those percepts be the phosphines, be as small as possible so that you can start to tell apart like they're the individual pixels of the, the, of the screen. So if you have many of those, potentially you'll be able to, in the long term, be able to actually get naturalistic vision, but in the short term, to maybe midterm, being able to at least be able to have object detection algorithms run on your glasses, the pre pop processing units, and then being able to at least see the edges of things so you don't bump into stuff. This is incredible. This is really incredible. So you basically would be adding pixels, and your brain would start to figure out what those pixels mean. Yeah. And like, with different kinds of assistance on the signal processing on all fronts. Yeah. The thing that actually. So a couple things. One is, you know, obviously, if you're blind from birth, the way brain works, especially in the early age, neuroplasticity is really nothing other than your brain and different parts of your brain fighting for delimited territory very, very quickly. You see cases where people that are, I mean, you also hear about people who are blind that have heightened sense of hearing or some other senses. And the reason for that is because that cortex that's not used just gets taken over. Bye. These different parts of the cortex. So for those types of individuals. I. Mean, I guess they're going to have to now map some other parts of their senses into what they call vision, but it's going to be obviously a very, very different conscious experience. I think that's an interesting caveat. The other thing that also is important to highlight is that we're currently limited by our biology in terms of the wavelength that we can see. There's a very, very small wavelength, that is a visible light wavelength that we can see with our eyes. But when you have an external camera with this BCI system, you're not limited to that. You can have infrared, you can have uv, you can have whatever other spectrum that you want to see. Whether that gets mapped to some sort of weird conscious experience, I've no idea. But when I oftentimes I talk to people about the goal of neuralink being going beyond the limits of our biology, that's sort of what I mean. And if you're able to control the kind of raw signal is that when we use our sight, we're getting the photons and there's not much processing on it. If you're being able to control that signal, maybe you can do some kind of processing. Maybe you do object detection ahead of time. Yeah, you're doing some kind of pre processing, and there's a lot of possibilities to explore that. So it's not just increasing sort of thermal imaging, that kind of stuff, but it's also just doing some kind of interesting processing. Yeah, I mean, my theory of how visual system works also is that, I mean, there's just so many things happening in the world, and there's a lot of photons that are going into your eye, and it's unclear exactly where some of the preprocessing steps are happening. But, I mean, I actually think that just from a fundamental perspective, there's just so much the reality that we're in, if it's a reality, so there's so much data, and I think humans are just unable to actually eat enough actually to process all that information. So there's some sort of filtering that does happen. Whether that happens in the retina, whether that happens in different layers of the visual cortex, unclear. But the analogy that I sometimes think about is if your brain is a CCD camera and all of the information in the world is a sun, and when you try to actually look at the sun with the CCD camera, it's just going to saturate the sensors because it's enormous amount of energy. What you do is you end up adding these filters, right, to just kind of narrow the information that's coming to you and being captured. And I think things like our experiences or our drugs, like propofol, like anesthetic drug or psychedelics, what they're doing is they're kind of swapping out these filters and putting in new ones or removing older ones and kind of controlling our conscious experience. Yeah, man. Not to distract from the topic, but I just took a very high dose of ayahuasca in the Amazon jungle. So, yes, it's a nice way to think about it. You're swapping out different experiences. And with neuralink being able to control that, primarily at first to improve function, not for entertainment purposes or enjoyment purposes. But, yeah, giving back lost functions. Giving back lost functions. And there, especially when the function is completely lost, anything is a huge help. Would you implant a neuralink device in your own brain? Absolutely. I mean, maybe not right now, but absolutely. What kind of capability, once reached, you would start getting real curious and almost get a little antsy, like jealous of people that get, as you watch them getting planted. Yeah, I mean, I think. I mean, even. Even with our early participants, if they start to do things that I can't do, which I think is in the realm of possibility for them to be able to get 1520, if not like 100 bps. Right. There's nothing that fundamentally stops us from being able to achieve that type of performance, I mean, I would certainly get jealous that they can do that. I should say that watching Nolan, I get a little jealous because he's having so much fun, and it seems like such a chill way to play video games. Yeah. I mean, the thing that also is hard to appreciate sometimes is that, you know, he's doing these things while multi, like, while talking. And, I mean, it's multitasking. Right. So it's. It's clearly. It's obviously cognitively intensive, but similar to how when we talk, we move our hands, these things are multitasking. He's able to do that. You won't be able to do that with other assistive technology, as far as I'm aware. If you're obviously using an eye tracking device, you're very much fixated on that thing that you're trying to do. And if you're using voice control, like, if you say some other stuff. Yeah. You don't get to use that. Yeah. The. The multitasking aspect of that is really interesting. So it's not just the bps for the primary task. It's the. It's the parallelization of multiple tasks. If you. If you take. If you measure the bps for the entirety of the human organism. So if you're talking and doing a thing with your mind and looking around also, I mean, there's just a lot of parallelization that can be happening. But I think at some point for him, like, if he wants to really achieve those high level bps, it does require, like, you know, full attention. Right. And that's a separate circuitry that that is a big mystery, like how attention works and, you know. Yeah, attention, like cognitive load. I've done. I've read a lot of literature on people doing two tasks. Like, you have your primary task and a secondary task, and the secondary task is. Is a source of distraction. And how does that affect the performance of the primary task? And there's. Depending on the task, there's a lot of interesting. I mean, this is an interesting computational device. Right. And I think there, to say the least, a lot of novel insights that can be gained from everything. I mean, I personally am surprised that no one's able to do such incredible control of the cursor while talking and also being nervous at the same time because he's talking like all of us are. If you're talking in front of the camera, you get nervous. So all of those are coming into play. He's able to still achieve high performance. Surprising. I mean, all of this is really amazing. And I think just after researching this really in depth, I kind of want. In your lake get in the line. And also the safety get in mind. Well, we should say the registry is for people who have quadriplegia and all that kind of stuff, so. Correct. That would be a separate line for people. They're just curious, like myself. So now that Nolan patient, p one is part of the ongoing prime study, what's the high level vision for p two, p three, p four, p five. And just the expansion into other human beings that are getting to experience this implant? Yeah, I mean, the primary goal is, you know, for our study in the first place is to achieve safety endpoints. Just understand safety of this device as well as the implantation process, and also at the same time understand the efficacy and the impact that it could have on the potential users lives. And just because you're living with tetraplegia, it doesn't mean your situation is same as another person living with tetraplegia. It's wildly, widely varying and it's something that we're hoping to also understand how our technology can serve not just a very small slice of those individuals, but broader group of individuals and being able to get the feedback to just really build just the best product for them. There's obviously also goals that we have. And the primary purpose of the early feasibility study is to learn from each and every participant to improve the device, improve the surgery. Before we embark on what's called a pivotal study, that then is much larger trial that starts to look at statistical significance of your endpoints and that's required before you can then market the device. That's how it works in the US and just generally around the world. That's the process you follow. So our goal is to really just understand from people like Nolan, p two, p three, future participants what aspects of our device needs to improve. If it turns out that people are like, I really don't like the fact that it lasts only 6 hours, I want to be able to use this computer for, you know, like 24 hours. I mean, that's, that is a, you know, user needs and user requirements which we can only find out from just being able to engage with them. So before the pivotal study, there's kind of like a rapid innovation based on individual experiences. You're learning from individual people how they use it, like the high resolution details in terms of like cursor control and signal and all that kind of stuff to like, life experience. Yeah. So there's hardware changes, but also just firmware updates. So even when we had that sort of recovery event for Nolan, he now has the new firmware that he has been updated with. And it's similar to how your phones get updated all the time with new firmwares for security patches, whatever new functionality UI. And that's something that is possible with our implant. It's not a static one time device that can only do the thing that it said it can do. I mean, similar to Tesla, you can do over the air firmware updates, and now you have completely new user interface and all these bells and whistles and improvements on everything like the latest. That's when we say generalized platform. That's what we're talking about. Yeah, it's really cool how the app that Nolan is using, there's a calibration, all that, all that kind of stuff, and then there's update. You just click and get an update. What other future capabilities are you kind of looking to? You said vision. That's a fascinating one. What about sort of accelerated typing or speech, this kind of stuff? Yeah. And what else is there? Yeah, those are still in the realm of movement program. So largely speaking, we have two programs. We have the movement program and we have the vision program. The movement program currently is focused around the digital freedom. As you can easily guess, if you can control to the cursor in the digital space, you could move anything in the physical space. So robotic arms, wheelchair, your environment, or even really, whether it's through the phone or just directly to those interfaces, like to those machines. So we're looking at ways to expand those types of capability. Even for Nolan, that requires conversation with the FDA and showing safety data for if there's a robotic arm or a wheelchair, that we can guarantee that they're not going to hurt themselves accidentally. It's very different if you're moving stuff in the digital domain versus in the physical space, you can actually potentially cause harm to the participants. So we're working through that right now. Speech does involve different areas of the brain. Speech prosthetic is very, very fascinating. And there's actually been a lot of really amazing work that's been happening in academia. Sergei Stavisky at UC Davis, Jamie Henderson and late Krishna Shinoi at Stanford doing just some incredible amount of work improving speech, neural prosthetics. And those are actually looking more at parts of the motor cortex that are controlling these vocal articulators and being able to, even by mouthing the word or imagine speech, you can pick up those signals. The more sophisticated higher level processing areas, like the broca's area or Wernicke's area. Those are still very, very big mystery in terms of the underlying mechanism of how all that stuff works. But, yeah, I think neuralinks, the ventral goal is to understand those things and be able to provide a platform and tools to be able to understand that and study that. This is where I get to the pothead questions. Do you think we can start getting insight into things like thought? So, speech is, there's a muscular component, like you said. There's, like, the act of producing sounds. But then what about the internal things, like cognition, like low level thoughts and high level thoughts? Do you think we'll start noticing kind of signals that could be picked up, that could. They could be understood, that could be maybe used in order to interact with the outside world? In some ways, I guess this starts to kind of get into the heart problem of consciousness. And, I mean, on one hand, all of these are at some point set of electrical signals that from there, maybe it in itself is giving you the cognition or the meaning, or somehow human mind is incredibly amazing storytelling machine. So we're telling ourselves and fooling ourselves that there's some interesting meaning here. But I certainly think that BCI, and really, BCI, at the end of the day, is a set of tools that help you kind of study the underlying mechanisms in a both local but also broader sense, and whether there's some interesting patterns of electrical signal. That means you're thinking this versus. And you can either learn from many, many sets of data to correlate some of that and be able to do mind reading or not. I'm not sure. I certainly would not blow that out as a possibility, but I think BCI alone probably can't do that. There's probably additional set of tools and framework and also just hard problem of consciousness at the end of the day, is rooted in this philosophical question of what is the meaning of it all? What's the nature of our existence? Where is the mind emerge from this complex network? Yeah. How does the subjective experience emerge from just a bunch of spikes? Electrical spikes? Yeah. Yeah. I mean, we do really think about BCI and what we're building as a tool for understanding the mind, the brain. The only question that matters, there's actually. There actually is some biological existence proof of, like, what it would take to kind of start to form some of these experiences that may be unique. If you actually look at every one of our brains, there are two hemispheres. There's a left sided brain, there's a right sided brain. And, I mean, unless you have some other conditions, you normally don't feel like left legs or right legs, you just feel like one legs. Right. So what is happening there? Right. If you actually look at the two hemispheres, there's a structure that connectorize the two called the corpus callosum, that is supposed to have around 200 to 300 million connections, or axons. So whether that means that's the number of interface and electrodes that we need to create some sort of mind meld or from that, like, whatever new conscious experience that you. You can experience. But I do think that there's like, kind of an interesting existence, proof that we all have. And that threshold is unknown at this time. Oh, yeah, these things. Everything in this domain is, you know, speculation, right. And then there will be, you'd be continuously pleasantly surprised. Do you see a world where there's millions of people, like tens of millions, hundreds of millions of people walking around with the neuralink device or multiple neuralink devices in their brain? I do. First of all, there are, if you look at worldwide people suffering from movement disorders and visual deficits, I mean, that's in the tens, if not hundreds of millions of people. So that alone, I think there's a lot of benefit and potential good that we can do with this type of technology. And when you start to get into kind of neuro like psychiatric application, depression, anxiety, hunger or obesity, mood control of appetite, I mean, that starts to become very real to everyone. Not to mention that most people on earth have a smartphone. And once BCI starts competing with a smartphone as a preferred methodology of interacting with the digital world, that also becomes an interesting thing. Oh, yeah. I mean, yeah, this is even before going to that, right. I mean, there's like, almost, I mean, the entire world that could benefit from these types of things. And then. Yeah, like, if we're talking about kind of next generation of how we interface with machines or even ourselves, in many ways, I think BCI can play a role in that. And some of the things that I also talk about is I do think that there is a real possibility that you could see 8 billion people walking around with neuralink. Well, thank you so much for pushing ahead, and I look forward to that exciting future. Thanks for having me. Thanks for listening to this conversation with DJ saw. And now, dear friends, here's Matthew McDougall, the head neurosurgeon at Neuralink. When did you first become fascinated with the human brain? Since forever, as far back as I can remember, I've been interested in the human brain. I mean, I was a thoughtful kid and a bit of an outsider. And you sit there thinking about what the most important things in the world are in your little, tiny adolescent brain. And the answer that I came to, that I converged on, was that all of the things you can possibly conceive of as things that are important for human beings to care about are literally contained in the skull. Both the perception of them and their relative values and the solutions to all our problems and all of our problems are all contained in the skull. And if we knew more about how that worked, how the brain encodes information and generates desires and generates agony and suffering, we could do more about it. You think about all the really great triumphs in human history. You think about all the really horrific tragedies. You think about the Holocaust. You think about any prison full of human stories. And all of those problems boil down to neurochemistry. So if you get a little bit of control over that, you provide people the option to do better. In the way I read history, the way people have dealt with having better tools, is that they most often, in the end, do better with huge asterisks. But I think it's an interesting, worthy and noble pursuit to give people more options, more tools. Yeah. That's a fascinating way to look at human history. You just imagine all these neurobiological mechanisms, Stalin, Hitler, all of these genghis Khan, all of them just had, like, a brain. It's just a bunch of neurons, you know, like a few tens of billions of neurons gaining a bunch of information over a period of time. They have set a module that does language and memory and all that. And from there, in the case of those people, they're able to murder millions of people. And all that coming from, there's not some glorified notion of a dictator of this enormous mind or something like this. It's just, it's just the brain. Yeah, yeah. I mean, a lot of that has to do with how well people like that can organize those around them. Other brains. Yeah. And so I always find it interesting to look to primatology, look to our closest non human relatives, for clues as to how humans are going to behave and what particular humans are able to achieve. And so you look at chimpanzees and bonobos, and theyre similar but different in their social structures, particularly. And I went to Emory in Atlanta and studied under Franz de Wall, the great Franz de Wall, who was kind of the leading primatologist who recently died. And his work at looking at chimps through the lens of how you would watch an episode of friends and understand the motivations of the characters interacting with each other. He would look at a chimp colony and basically apply that lens. I'm massively oversimplifying it. If you do that, instead of just saying subject 473 through his feces, at subject 471, you talk about them in terms of their human struggles, accord them the dignity of themselves as actors with understandable goals, and drives what they want out of life. And primarily, it's the things we want out of food, sex, companionship, power. You can understand chimp and bonobo behavior in the same lights much more easily, and I think doing so gives you the tools you need to reduce human behavior from the kind of false complexity that we layer onto it with language. And look at it in terms of, oh, well, these humans are looking for companionship, sex, food, power. And I think that's a pretty powerful tool to have in understanding human behavior. And I just went to the Amazon jungle for a few weeks, and it's a very visceral reminder that a lot of life on earth is just trying to get laid. They're all screaming at each other, like, I saw a lot of monkeys, and they're just trying to impress each other. Or maybe there's a battle for power, but a lot of the battle for power has to do with them getting laid. Right? Breeding rights often go with alpha status. And so if you can get a piece of that, then you're gonna do okay. And would like to think that we're somehow fundamentally different, but especially when it comes to primates, we really aren't. You know, we can use fancier, poetic language, but maybe some of the underlying drives that motivate us are similar. Yeah, I think that's true. And all that is coming from this. The brain. Yeah. So when did you first start studying the brain as a biological mechanism? Basically the moment I got to college, I started looking around for labs that I could do neuroscience work in. I originally approached that from the angle of looking at interactions between the brain and the immune system, which isn't the most obvious place to start, but I had this idea at the time that the contents of your thoughts would have an impact, a direct impact, maybe a powerful one, on non conscious systems in your body, the systems we think of as homeostatic, automatic mechanisms, like fighting off a virus, like repairing a wound. And sure enough, there are big crossovers between the two. It gets to a key point that I think goes under recognized. One of the things people don't recognize or appreciate about the human brain enough, and that is that it basically controls or has a huge role in almost everything that your body does. You try to name an example of something in your body that isn't directly controlled or massively influenced by the brain, and it's pretty hard. I mean, you might say like bone healing or something. But even those systems, the hypothalamus and pituitary, end up playing a role in coordinating the endocrine system that does have a direct influence on, say, the calcium level in your blood that goes to bone healing. So non obvious connections between those things implicate the brain as really a potent prime mover in all of health. One of the things I realized in the other direction, too, how most of the systems in the body integrated with the human brain, like, they affect the brain also, like the immune system. I think there's just, you know, people who study Alzheimer's and those kinds of things. It's just surprising how much you can understand of that from the immune system, from the other systems that don't obviously seem to have anything to do with sort of the nervous system. They all play together. Yeah. You could understand how that would be driven by evolution, too. Just in some simple examples. If you get sick, if you get a communicable disease, you get the flu. It's pretty advantageous for your immune system to tell your brain, hey now, be antisocial for a few days. Don't go be the life of the party tonight. In fact, maybe just cuddle up somewhere warm under a blanket and just stay there for a day or two. And sure enough, that tends to be the behavior that you see both in animals and in humans. If you get sick, elevated levels of interleukins in your blood, TNF alpha in your blood, ask the brain to cut back on social activity and even moving around, you have lower locomotor activity in animals that are infected with viruses. So from there, the early days in neuroscience to surgery, when did that step happen? This is a leap. It was sort of an evolution of thought. I wanted to study the brain. I started studying the brain in undergrad, in this neuroimmunology lab. I, from there, realized at some point that I didn't want to just generate knowledge. I wanted to effect real changes in the actual world, in actual people's lives. And so after having not really thought about going into medical school, I was on a track to go into a PhD program. I said, well, I'd like that option. I'd like to actually potentially help tangible people in front of me and doing a little digging, found that there exists these MD PhD programs where you can choose not to choose between them and do both. And so I went to USC for medical school and had a joint PhD program with Caltech, where I met, actually chose that program, particularly because of a researcher at Caltech named Richard Anderson, who's one of the godfathers of primate neuroscience. It has a macaque lab where Utah Rays and other electrodes were being inserted into the brains of monkeys to try to understand how intentions were being encoded in the brain. So I ended up there with the idea that maybe I would be a neurologist and study the brain on the side, and then discovered that neurology, again, I'm going to make enemies by saying this. Neurology, predominantly and distressingly to me, is the practice of diagnosing a thing and then saying, good luck with that when there's not much we can do. And neurosurgery very differently. It's a powerful lever on taking people that are headed in a bad direction and changing their course in the sense of brain tumors that are potentially treatable or curable with surgery, even aneurysms in the brain, blood vessels that are going to rupture, you can save lives, really is, at the end of the day, what mattered to me. And so I was at USC, as I mentioned, that happens to be one of the great neurosurgery programs. And so I met these truly epic neurosurgeons, Alex Khalessi and Mike Puzzo and Steve Giannotta and Marty Weiss, these sort of epic people that were just human beings in front of me. And so it kind of changed my thinking from neurosurgeons are distant gods that live on another planet and occasionally come and visit us to, these are humans that have problems and are people, and theres nothing fundamentally preventing me from being one of them. And so at the last minute in medical school, I changed gears from going into a different specialty and switched into neurosurgery, which cost me a year. I had to do another year of research because I was so far along in the process that to switch into neurosurgery, the deadlines had already passed. A. Decision that costs time, but absolutely worth it. What was the hardest part of the training on the neurosurgeon track? Yeah, two things. I think that residency in neurosurgery is a competition of pain. How much pain can you eat and smile? There's workout restrictions that are not really, they're viewed, I think, internally, among the residents, as weakness. And so most neurosurgery residents try to work as hard as they can. And that, I think, necessarily means working long hours and sometimes over the work hour limits. And, you know, we care about being compliant with whatever regulations are in front of us. But I think more important than that, people want to give their all in becoming a better neurosurgeon because the stakes are so high. And so it's a real fight to get residents to, say, go home at the end of their shift and not stay and do more surgery. Are you seriously saying, like, one of the hardest things is literally, like, getting, forcing them to get sleep and rest and all this kind of stuff? Historically, that was the case. I think. I think the next generation, I think the next generation is more compliant and. More weaker is what you mean. All right, I'm just kidding. I'm just kidding. I didn't say it. Now I'm making enemies. No. Okay, I get it. Wow, that's fascinating. So what was the second thing? The personalities, and maybe the two are. Connected, but so was it pretty competitive? It's competitive, and it's also, as we touched on earlier, primates like power. And I think neurosurgery has long had this aura of mystique and excellence and whatever about it. And so it's an invitation, I think, for people that are cloaked in that authority. A board certified neurosurgeon is basically a walking, fallacious appeal to authority. Right. You have license to walk into any room and act like you're an expert on whatever. And fighting that tendency is not something that most neurosurgeons do. Well, humility isn't the forte. Yeah. So I have friends who know you, and whenever they speak about you, that you have the surprising quality for a neurosurgeon of humility, which I think indicates that it's not as common as perhaps in other professions, because there is a kind of gigantic sort of heroic aspect to neurosurgery, and I think it gets to people's head a little bit. Yeah. Well, I think that that allows me to play well at an Elon company because Elon, one of his strengths, I think, is to just instantly see through fallacy from authority. So nobody walks into a room that he's in and says, well, God damn it, you have to trust me. I'm the guy that built the last ten rockets or something. And he says, well, you did it wrong, and we can do it better. Or I'm the guy that kept Ford alive for the last 50 years. You listen to me on how to build cars, and he says, no. And so you dont walk into a room that hes in and say, well, im a neurosurgeon. Let me tell you how to do it. Hes going to say, well, im a human being that has a brain. I can think from first principles myself, thank you very much. And heres how I think it ought to be done. Lets go try it and see whos right. And thats proven, I think, over and over in his case, to be a very powerful approach. If we just take that tangent. Fascinating interdisciplinary team at Neuralink that you get to interact with, including Elon, what do you think is the secret to a successful team? What have you learned from just getting to observe these folks? World experts in different disciplines work together. Yeah, there's a sweet spot where people disagree and forcefully speak their mind and passionately defend their position and yet are still able to accept information from others and change their ideas when theyre wrong. And so I like the analogy of how you polish rocks. You put hard things in a hard container and spin it. People bash against each other, and out comes a more refined product. And so to make a good team at Neuralink, weve tried to find people that are not afraid to defend their ideas passionately and occasionally strongly disagree with people that theyre working with and have the best idea come out on top. It's not an easy balance, again, to refer back to the primate brain. It's not something that is inherently built into the primate brain. To say, I passionately put all my chips on this position, and now I'm just going to walk away from it and admit you were right. Part of our brains tell us that that is a power loss, that is a loss of face, loss of standing in the community. And now you're a zeta chump because your idea got trounced. And you just have to recognize that little voice in the back of your head is maladaptive and it's not helping the team win. Yeah, you have to have the confidence to be able to walk away from an idea that you hold onto. And if you do that often enough, you're actually going to become the best in the world at your thing. I mean, that kind of, that rapid iteration. Yeah, you'll at least be a member of a winning team. Ride the wave. What did you learn? You mentioned there's a lot of amazing neurosurgeons at USC. What lessons about surgery and life have you learned from those folks? Yeah, I think working your ass off, working hard while functioning as a member of a team, getting a job done, that is incredibly difficult, working incredibly long hours, being up all night, taking care of someone that you think probably won't survive no matter what you do, working hard to make people that you passionately dislike look good the next morning. These folks were relentless in their pursuit of excellent neurosurgical technique, decade over decade, and I think were well recognized for that excellence, especially Marty Weiss, Steve Gianotta, Mike Capuzzo. They made huge contributions not only to surgical technique, but they built training programs that trained dozens or hundreds of amazing neurosurgeons. I was just lucky to kind of be in their wake. What's that like? You mentioned doing a surgery where the person is likely not to survive. Does that wear on you? Yeah. You know, it's especially challenging when you, with all respect to our elders, it doesn't hit so much when you're taking care of an 80 year old, and something was going to get them pretty soon anyway. And so you lose a patient like that, and it was part of the natural course of what is expected of them in the coming years. Regardless, taking care of a father of two or three, four young kids, someone in their thirties that didn't have it coming, and they show up in your ER having their first seizure of their life, and lo and behold, they've got a huge malignant, inoperable, or incurable brain tumor. You can only do that, I think, a handful of times before it really starts eating away at your armor. Or a young mother that shows up that has a giant hemorrhage in her brain that she's not going to survive from, and they bring her four year old daughter in to say goodbye one last time before they turn the ventilator off. The great Henry Marsh is an english neurosurgeon who said it best. I think he says every neurosurgeon carries with them a private graveyard. And I definitely feel that, especially with young parents, that kills me. They had a lot more to give. The loss of those people specifically has a knock on effect that's going to make the world worse for people for a long time. And it's just hard to feel powerless in the face of that. And that's where I think you have to be borderline evil to fight against a company like Neuralink or to constantly be taking potshots at us, because what we're doing is to try to fix that stuff. We're trying to give people options to reduce suffering. We're trying to take the pain out of life that broken brains brings in. And yeah, this is just our little way that we're fighting back against entropy, I guess. Yeah. The amount of suffering that's endured when some of the things that we take for granted that our brain is able to do is taken away is immense. And to be able to restore some of that functionality is a real gift. Yeah, we're just starting. We're going to do so much more. Well, can you take me through the full procedure for implanting, say, the n one chip in your link? Yeah. It's a really simple, really simple, straightforward procedure. The human part of the surgery that I do is dead simple. It's one of the most basic neurosurgery procedures imaginable. And I think there's evidence that some version of it has been done for thousands of years. There are examples, I think, from ancient Egypt of healed or partially healed trephanations, and from Peru or ancient times in South America, where these protosurgeons would drill holes in people's skulls, presumably to let out the evil spirits, but maybe to drain blood clots. And there's evidence of bone healing around the edge, meaning the people at least survive some months after a procedure. And so what we're doing is that we are making a cut in the skin on the top of the head over the area of the brain that is the most potent representation of hand intentions. And so if you are an expert concert pianist, this part of your brain is lighting up the entire time you're playing. We call it the hand knob. The hand knob. There's all the finger movements. All of that is just firing away. There's a little squiggle in the cortex right there. One of the folds in the brain is doubly folded right on that spot. You can look at it on an MRI and say, that's the hand knob. And then you do a functional test and a special kind of MRI called a functional MRI fMRI. And this part of the brain lights up when people, even quadriplegic people whose brains aren't connected to their finger movements anymore, imagine finger movements. And this part of the brain still lights up. So we can id that part of the brain in anyone whos preparing to enter our trial and say, okay, that part of the brain, we confirm is your hand intention area. And so ill make a little cut in the skin. Well, flap the skin open just like kind of opening the hood of a car, only a lot smaller. Make a perfectly round, one inch diameter hole in the skull. Remove that bit of skull, open the lining of the brain, the covering of the brain, it's like a little bag of water that the brain floats in. And then show that part of the brain to our robot. This is where the robot shines. It can come in and take these tiny, much smaller than human hair electrodes and precisely insert them into the cortex, into the surface of the brain, to a very precise depth, in a very precise spot that avoids all the blood vessels that are coating the surface of the brain. And after the robot's done with its part, then the human comes back in and puts the implant into that hole in the skull and covers it up, screwing it down to the skull and sewing the skin back together. So the whole thing is a few hours long. It's extremely low risk compared to the average neurosurgery involving the brain that might, say, open up a deep part of the brain or manipulate blood vessels in the brain. This opening on the surface of the brain with only cortical microinsertions carries significantly less risk than a lot of the tumor or aneurysm surgeries that are routinely done. So cortical microinsertions that are via robot and computer vision, are designed to avoid the blood vessels. Exactly. So I know you're a bit biased here, but let's compare human and machine. So what are human surgeons able to do well? And what are robot surgeons able to do well at this stage of our human civilization development? Yeah, yeah, that's a good question. Humans are general purpose machines. We're able to adapt to unusual situations. We're able to change the plan on the fly. I remember well a surgery that I was doing many years ago down in San Diego, where the plan was to open a small hole behind the ear and go reposition a blood vessel that had come to lay on the facial nerve, the trigeminal nerve, the nerve that goes to the face. When that blood vessel lays on the nerve, it can cause just intolerable, horrific shooting pain that people describe like being zapped with a cattle prod. And so the beautiful, elegant surgery is to go move this blood vessel off the, off the nerve. The surgery team, we went in there and started moving this blood vessel, and then found that there was a giant aneurysm on that blood vessel that was not easily visible on the pre op scans. And so the plan had to dynamically change, and that the human surgeons had no problem with that, were trained for all those things. Robots wouldnt do so well in that situation, at least in their current incarnation. Fully robotic surgery, like the electrode insertion portion of the neuralink surgery, it goes according to a set plan, and so the humans can interrupt the flow and change the plan, but the robot can't really change the plan midway through. It operates according to how it was programmed and how it was asked to run. It does its job very precisely, but not with a wide degree of latitude and how to react to changing conditions. So there could be just a very large number of ways that you could be surprised as a surgeon, when you enter a situation, there could be subtle things that you have to dynamically adjust. Correct. And robots are not good at that. Currently, I think we are at the dawn of a new era with AI of the parameters for robot responsiveness to be dramatically broadened. You can't look at a self driving car and say that it's operating under very narrow parameters. If a chicken runs across the road, it wasn't necessarily programmed to deal with that specifically, but a Waymo or a self driving Tesla would have no problem reacting to that appropriately. And so surgical robots aren't there yet, but give it time. And then there could be a lot of sort of semi autonomous possibilities of maybe a robotic surgeon could say, this situation is perfectly familiar, or the situation is not familiar, and in the not familiar case, a human could take over, but basically, like, be very conservative, saying, okay, this for sure has no issues, no surprises, and then let the humans deal with the surprises, with the edge cases, all that. Yeah, that's one possibility. So, like, you think eventually, uh, you'll be out of the job? Well, you being neurosurgeon, your job being neurosurgeon, humans, there will not be many neurosurgeons left on this earth. I'm not worried about my job. In my, in the course of my professional life, I think I I would tell my, my kids not necessarily to go in this line of work, depending on, depending on how things look in. 20 years, it's so fascinating because, I mean, I if I have a line of work, I would say it's programming. And if you ask me, like, for the last, I don't know, 20 years, what I would recommend for people, I would tell them, yeah, go, you will always have a job if you're a programmer, because there's more and more computers and all this kind of stuff, and it pays well. But then you realize these large language models come along and they're really damn good at generating code. So overnight, you could be surprised, like, wow, what is the contribution of the human, really? But then you start to think, okay, it does seem that humans have ability, like you said, to deal with novel situations. In the case of programming, it's the ability to kind of come up with novel ideas to solve problems. It seems like machines aren't quite yet able to do that. And when the stakes are very high, when it's life critical as it is in surgery, especially in neurosurgery, then it starts. The stakes are very high for a robot to actually replace a human. But it's fascinating that in this case of neuralink, there's a human robot collaboration. Yeah, I do the parts it can't do, and it does the parts I can't do. And we are friends. I saw that there's a lot of practice going on. So, I mean, everything in uralink is tested extremely rigorously. But one of the things I saw, that there's a proxy on which the surgeries are performed. So this is both for the robot and for the human, for everybody involved in the entire pipeline. What's that like, practicing the surgery? It's pretty intense. So there's no analog to this in human surgery? Human surgery is sort of this artisanal craft that's handed down directly from master to pupil over the generations. I mean, literally, the way you learn to be a surgeon on humans is by doing surgery on humans. I mean, first you watch your professors do a bunch of surgery, and then finally they put the trivial parts of the surgery into your hands, and then the more complex parts. And as your understanding of the point and the purposes of the surgery increases, you get more responsibility in the perfect condition. It doesn't always go well. In Neuralink's case, the approach is a bit different. We, of course, practiced as far as we could on animals. We did hundreds of animal surgeries. And when it came time to do the first human, we had just an amazing team of engineers build incredibly lifelike models. One of the engineers, Fran Romano in particular, built a pulsating brain in a custom 3d printed skull that matches exactly the patient's anatomy, including their face and scalp characteristics. And so when I was able to practice that, I mean, it's as close as it really reasonably should get to being the real thing in all the details, including having a mannequin body attached to this custom head. And so when we were doing the practice surgeries, wheel that body into the CT scanner and take a mock ct scan and wheel it back in and conduct all the normal safety checks verbally. Stop this patient. We're confirming his identification is mannequin number blah, blah, blah, and then opening the brain in exactly the right spot using standard operative neuronavigation equipment, standard surgical drills in the same, or that we do all of our practice surgeries in at Neuralink and having the skull open and have the brain pulse, which adds a degree of difficulty for the robot to perfectly, precisely plan and insert those electrodes to the right depth and location. We broke new ground on how extensively we practiced for this surgery. So there was a historic moment, a big milestone for neuralink, in part for humanity, with the first human getting a neuralink implant in January of this year. Take me through the surgery on Noland. What did he feel like to be part of this? Yeah, well, we were lucky to have just incredible partners at the Barrow Neurologic Institute. They are, I think, the premier neurosurgical hospital in the world. They made everything as easy as possible for the trial to get going and helped us immensely with their expertise on how to arrange the details. It was a much more high pressure surgery in some ways. I mean, even though the outcome wasn't particularly in question in terms of our participants safety, the number of observers, the number of people, there's conference rooms full of people watching live streams in the hospital rooting for this to go perfectly. And that just adds pressure that is not typical for even the most intense production, neurosurgery, say, removing a tumor or placing deep brain stimulation electrodes. And it had never been done on a human before. There were unknowns, and so definitely a moderate pucker factor there for the whole team. Not knowing if we were going to encounter, say, a degree of brain movement that was unanticipated or a degree of brain sag that took the brain far away from the skull and made it difficult to insert or some other unknown, unknown problem. Fortunately, everything went well, and that surgery was one of the smoothest outcomes we could have imagined. Were you nervous? I mean, you're extremely quarterback and like, in the Super bowl kind of situation, extremely nervous. Extremely. I was very pleased when it went well and then. And when it was over. Looking forward to number two. Yeah. Even with all that practice, all of that, just, you've never been in a situation that's so high stakes in terms of people watching. Yeah. And we should also probably mention, given how the media works, a lot of people maybe in a dark kind of way, hoping it doesn't go well. I think wealth is easy to hate or envy or whatever, and I think there's a whole industry around driving clicks, and bad news is great for clicks. And so any way to take an event and turn it into bad news is going to be really good for clicks. It just sucks because I think it puts pressure on people. It discourages people from trying to solve really hard problems, because to solve hard problems, you have to go into the unknown. You have to do things that haven't been done before, and you have to take risks, calculated risks. You have to do all kind of safety precautions, but risks nevertheless. I just wish there would be more celebration of that, of the risk taking versus people just waiting on the. The sidelines, like, waiting for failure and then pointing out the failure. Yeah, it sucks. But in this case, it's really great that everything went just flawlessly, but it's unnecessary pressure. I would say. Now that there's a human with literal skin in the game, there's a participant whose wellbeing rides on this, doing well, you have to be a pretty bad person to be rooting for that to go wrong. And so hopefully, people look in the mirror and realize that at some point. So did you get to actually front row seat, like, watch the robot work? Like, what? You get to see the whole thing? Yeah. I mean, because an MD needs to be in charge of all of the medical decision making throughout the process. I unscrubbed from the surgery after exposing the brain and presenting it to the robot, and place the targets on the robot software interface that tells the robot where it's going to insert each thread. That was done with my hand on the mouse, for whatever that's worth. So you were the one placing the targets? Yeah. Oh, cool. So the robot with a computer vision provides a bunch of candidates, and you finalize the decision. Right. The software engineers are amazing on this team, and so they actually provided an interface where you can essentially use a lasso tool and select a prime area of brain real estate, and it will automatically avoid the blood vessels in that region and automatically place a bunch of targets. That allows the human robot operator to select really good areas of brain and make dense applications of targets in those regions. The regions we think are going to have the most high fidelity representations of finger movements and arm movement intentions. I've seen images of this, and for me, with OCD, it's for some reason, a really pleasant. I think there's a subreddit called oddly satisfying. Yeah, love that subreddit. It's oddly satisfying to see the different target sites, avoiding the blood vessels and also maximizing the usefulness of those locations for the signal. It just feels good. It's like, ah. As a person who has a visceral reaction to the brain bleeding, I can tell you, yes. Especially. It's extremely satisfying watching the electrodes themselves go into the brain and not cause bleeding. Yeah. Yeah. So you said the feeling was of relief when everything went perfectly. Yeah. How deep in the brain can you currently go and eventually go, let's say, on the neuralink side? It seems the deeper you go in the brain, the more challenging it becomes. Yeah. So, talking broadly about neurosurgery, we can get anywhere. It's routine for me to put deep brain stimulating electrodes near the very bottom of the brain, entering from the top and passing about a two millimeter wire all the way into the bottom of the brain. And that's not revolutionary. A lot of people do that and we can do that with very high precision. I use a robot from Globus to do that surgery several times a month. It's pretty routine. What are your eyes in that situation? What are you seeing? What kind of technology can you use to visualize where you are to light your way? Yeah, it's a cool process. On the software side, you take a preoperative MRI that's extremely high resolution data of the entire brain. You put the patient to sleep, put their head in a frame that holds the skull very rigidly, and then you take a CT scan of their head while they're asleep. With that frame on, and then merge the MRI and the CT in software. You have a plan based on the MRI where you can see these nuclei deep in the brain. You can't see them on CT, but if you trust the merging of the two images, then you indirectly know on the CT where that is, and therefore indirectly know where in reference to the titanium frame screwed to their head, those targets are. And so this is sixties technology to manually compute trajectories, given the entry point and target, and dial in some goofy looking titanium actuators with manual actuators with little tick marks on them. The modern version of that is to use a robot, just like a little Kuka arm. You might see it building cars at the Tesla factory. This small robot arm can show you the trajectory that you intended from the pre op MRI and establish a very rigid holder through which you can drill a small hole in the skull and pass a small rigid wire deep into that area of the brain that's hollow, and put your electrode through that hollow wire and then remove all of that except the electrode. So you end up with the electrode very, very precisely placed far from the skull surface. Now, that's standard technology that's already been out in the world for a while. Neuralink right now is focused entirely on cortical targets, surface targets, because there's no trivial way to get, say, hundreds of wires deep inside the brain. Without doing a lot of damage. So your question, what do you see? Well, I see an MRI on a screen. I can't see everything that that DBS electrode is passing through on its way to that deep target. And so it's accepted with this approach that there's going to be about one in a hundred patients who have a bleed somewhere in the brain as a result of passing that wire blindly into the deep part of the brain. That's not an acceptable safety profile for neuralink. We start from the position that we want this to be dramatically, maybe two or three orders of magnitude safer than that. Safe enough, really, that you or I, without a profound medical problem, might on our lunch break someday say, yeah, sure, I'll get that. I'd be meaning to upgrade to the latest version. And so the safety constraints given that are high, and so we haven't settled on a final solution for arbitrarily approaching deep targets in the brain. It's interesting because you have to avoid blood vessels somehow. Maybe there's creative ways of doing the same thing, like mapping out high resolution geometry of blood vessels, and then you can go in blinden. But how do you map out that in a way that's super stable? There's a lot of interesting challenges there. Right? Yeah, but there's a lot to do on the surface. Exactly. So we've got vision on the surface. We actually have made a huge amount of progress sewing electrodes into the spinal cord as a potential workaround for a spinal cord injury that would allow a brain mounted implant to translate motor intentions to a spine mounted implant that can affect muscle contractions in previously paralyzed arms and legs. Mind blowing. That's just incredible. So, like, the effort there is to try to bridge the brain to the spinal cord, to the periphery, peripheral, neural, nervous system. So how hard is that to do? We have that working in. In very crude forms in animals. That's amazing. Yeah, we've done so similar to, like with Nolan, where he's able to digitally move the cursor. Here you're doing the same kind of communication, but with the actual effectors that you have. That's fascinating. Yeah. So we have anesthetized animals doing grasp and moving their legs and sort of walking pattern. Again, early days, but the future is bright for this kind of thing, and people with paralysis should look forward to that bright future. They're going to have options. Yeah. And there's a lot of sort of intermediate or extra options where you take an optimist robot like the arm, and to be able to control the arm. Yeah, the fingers, the hands of the arm. As a prosthetic, exoskeletons are getting better, too. Exoskeletons, yeah. So that goes hand in hand, although I didn't quite understand until thinking about it deep and doing more research about neuralink, how much you can do on the digital side. So this digital telepathy, I didn't quite understand that you could really map the intention as you described in the hand knob area, that you can map the intention. Just imagine it, think about it. That intention can be mapped to actual action in the digital world. And now more and more, so much can be done in the digital world that it can reconnect you to the outside world. It can allow you to have freedom, have independence. If you're a quadriplegic, that's really powerful. Like, you can go really far with that. Yeah. Our first participant is, he's incredible. He's breaking world records left and right. And he's having fun with it. It's great. Just going back to the surgery your whole journey, you mentioned to me offline you have surgery on Monday. So you're like, you're doing surgery all the time. Yeah, maybe the ridiculous question what does it take to get good at surgery? Practice repetitions. You just, same with anything else. There's a million ways of people saying the same thing and selling books saying it, but you call it 10,000 hours, you call it spend some chunk of your life, some percentage of your life focusing on this, obsessing about getting better at it, repetitions, humility, recognizing that you arent perfect at any stage along the way, recognizing youve got improvements to make in your technique, being open to feedback and coaching from people with a different perspective on how to do it, and then just the constant will to do better. Fortunately, if youre not a sociopath, I think your patients bring that with them to the office visits every day. They force you to want to do better all the time. Yeah, just step up. I mean, it's a real human being. A real human being that you can help. Yeah. So every surgery, even if it's the same exact surgery, is there a lot of variability between that surgery and a different person? Yeah, a fair bit. I mean, a good example for us is that the angle of the skull relative to the normal plane of the body axis of the skull over hand knob is pretty wide variation. I mean, some people have really flat skulls and some people have really steeply angled skulls over that area. And that has consequences for how their head can be fixed in, in sort of the frame that we use and how the robot has to approach the skull. And, yeah, people's bodies are built as differently as the people you see walking down the street. As much variability in body shape and size as you see there. We see in brain anatomy and skull anatomy. There are some people who we've had to kind of exclude from our trial for having skulls that are too thick or too thin or scalp that's too thick or too thin. I think we have the middle 97% or so of people. But you can't account for all human anatomy variability. How much mushiness and mess is there? Because taking biology classes, the diagrams are always really clean and crisp. Neuroscience. The pictures of neurons are always really nice and very. But whenever I look at pictures of, like, real brains, they're all, I don't know what is going on. Yeah. So how much are biological systems in reality? Like, how hard is it to figure out what's going on? Not too bad once you really get used to this. You know, that's where experience and skill and education really come into play, is if you stare at a thousand brains, it becomes easier to mentally peel back the, say, for instance, blood vessels that are obscuring the sulci and gyri, the wrinkle pattern of the surface of the brain. Occasionally, when you're first starting to do this and you open the skull, it doesn't match what you thought you were going to see based on the MRI. And with more experience, you learn to peel back that layer of blood vessels and see the underlying pattern of wrinkles in the brain and use that as a landmark for where you are. The wrinkles are a landmark? Yeah, I was describing hand knob earlier. That's a pattern of the wrinkles in the brain. It's this greek letter omega shaped area of the brain. So you could recognize the hand knob area. Like, if I show you a thousand brains and give you like 1 minute with each, you'd be like, yep, that's that. Sure. And so there is some uniqueness to that area of the brain, like, in terms of the Geometry, the topology of the thing. Yeah. Where is it about in the. It's. So you have this strip of brain running down the top I call the primary motorhouse area. And I'm sure you've seen this picture of the homunculus laid over the surface of the brain. The weird little guy with huge lips and giant hands. That guy sort of lays with his legs up at the top of the brain and face, arm areas farther down, and then some kind of mouth, lip, tongue areas farther down. And so the hand is right in there. And then the areas that control speech, at least on the left side of the brain in most people, are just below that. And so any muscle that you voluntarily move in your body, the vast majority of that references that strip or those intentions come from that strip of brain. And the wrinkle for hand knob is right in the middle of that. And vision is back here also close to the surface. Vision is a little deeper. And so this gets to your question about how deep can you get to do vision? We can't just do the surface of the brain. We have to be able to go in not as deep as we have to go for DBS, but maybe a centimeter deeper than we're used to for hand insertions. And so that's work in progress. That's a new set of challenges to overcome. By the way, you mentioned the Utah array, and I just saw a picture of that, and that thing looks terrifying because it's rigid. And then if you look at the threads, they're flexible. What can you say that's interesting to you about the flexible, that kind of approach of the flexible threads to deliver the electrodes next of the neurons? Yeah, I mean, the goal there comes from experience. I mean, we stand on the shoulders of people that made Utah rays and used Utah rays for decades before we ever even came along. Neuralink arose partly, this approach to technology arose out of a need recognized after Utah rays would fail routinely, because the rigid electrodes, those spikes that are literally hammered using an air hammer into the brain, those spikes generate a bad immune response that encapsulates the electrode spikes in scar tissue. Essentially, one of the projects that was being worked on in the Andersen lab at Caltech when I got there was to see if you could use chemotherapy to prevent the formation of scar. Things are pretty bad when you're jamming a bed of nails into the brain and then treating that with chemotherapy to try to prevent scar tissue. It's like, maybe we've gotten off track here, guys. Maybe there's a fundamental redesign necessary. Neuralink's approach of using highly flexible, tiny electrodes avoids a lot of the bleeding, avoids a lot of the immune response that ends up happening when rigid electrodes are pounded into the brain. What we see is our electrode longevity and functionality, and the health of the brain tissue immediately surrounding the electrode is excellent. It goes on for years now in our animal models. What do most people not understand about the biology of the brain? We mentioned the vasculature. That's really interesting. I think the most interesting, maybe underappreciated fact is that it really does control almost everything. I don't know. For out of the blue example, imagine you want a lever on fertility. You want to be able to turn fertility on and off. I mean, there are legitimate targets in the brain itself to modulate fertility, say, blood pressure. You want to modulate blood pressure. There are legitimate targets in the brain for doing that. Things that aren't immediately obvious as brain problems are potentially solvable in the brain. And so I think it's an underexplored area for primary treatments of all the things that bother people. That's a really fascinating way to look at it. There's a lot of conditions we might think have nothing to do with the brain, but they might just be symptoms of something that actually started in the brain. The actual source of the problem. The primary source is something. Yeah, not always. I mean, you know, kidney disease is real, but there are levers you can pull in the brain that affect all of the. All of these systems. There's knobs. Yeah. On off switches and knobs in the brain from which this all originates. Would you have a neuralink chip implanted in your brain? Yeah, I think use case right now is use a mouse. I can already do that. And so there's no value proposition on safety grounds alone. Sure. I would do it tomorrow. You say the use case of the mouse is after researching all this, and part of it is just watching Nolan have so much fun. If you can get that bits per second look really high with the mouse, like being able to interact, because if you think about the way the. On the smartphone, the way you swipe, that was transformational. Yeah. How we interact with the thing, it's subtle, you don't realize it, but able to touch a phone and to scroll with your finger, that's like, that changed everything. People were sure you need a keyboard to type and that there's a lot of HCI aspects to that that changed how we interact with computers. So there could be a certain rate of speed with the mouse that would change everything. Yes. Like, you might be able to just click around a screen extremely fast and that if it. I can see myself getting the neural link for much more rapid interaction with the digital devices. Yeah. I think recording speech intentions from the brain might. Might change things as well. The value proposition for the average person, a keyboard is a pretty clunky human interface requires a lot of training. It's highly variable in the maximum performance that the average person can achieve. I think taking that out of the equation. And just having a natural word to computer interface might change things for a lot of people. It'd be hilarious if that is the reason people do it. Even if you have speech to text, that's extremely accurate. It currently isn't, but it's a gotten super accurate. It'd be hilarious if people went for Neuralink. Just so you avoid the embarrassing aspect of speaking, like looking like a douchebag, speaking to your phone in public, which is a real, like, that's a real constraint. Yeah. I mean, with a bone conducting case that can be an invisible headphone, say, and the ability to think words into software and have it respond to you, that starts to sound sort of like embedded super intelligence. If you can silently ask for the Wikipedia article on any subject and have it read to you without any observable change happening in the outside world. For one thing, standardized testing is obsolete. Yeah. If it's done well on the UX side, it could change. I don't know if it transforms society, but it really can create a kind of shift in the way we interact with digital devices in the way that a smartphone did. Yeah, I would just having to look into the safety of everything involved, I would totally try it so it doesn't have to go to some, like, incredible thing where you have it connects your vision or to some other, like, it connects all over your brain. That could be like just connecting to the hand knob. You might have a lot of interesting interaction, human computer interaction possibilities. That's really interesting. Yeah. And the technology on the academic side is progressing at light speed here, I think. There was a really amazing paper out of UC Davis, Sergei Stavisky's lab that basically made an initial solve of speech decode. It was something like 125,000 words that they were getting with very high accuracy, which is. So you're just thinking the word. Yeah. Think in the word and you're able to get it. Yeah. Oh, boy. Like, you have to have the intention of speaking it. Right. So, like, do that inner voice, man. It's so amazing to me that you can do the intention, the signal mapping. All you have to do is just imagine yourself doing it. And if, if you get the feedback that it actually worked, you can get really good at that. Like, your brain will first of all adjust and you develop, like, any other skill. Yeah. Like touch typing, you develop in that same kind of way. That is. That is really, to me, it's just really fascinating to be able to even to play with that, honestly. Like, I would get a new link just to be able to play with that, just to play with the capacity, the capability of my mind to learn this skill. It's like learning the skill of typing or learning the skill of moving a mouse. It's another skill of moving the mouse, not with my physical body, but with my mind. I can't wait to see what people do with it. I feel like we're cavemen right now. We're banging rocks with a stick and thinking that we're making music. At some point when these are more widespread, there's going to be the equivalent of a piano that someone can make art with their brain in a way that we didn't even anticipate. I'm looking forward to it. Give it to like a teenager. Like anytime I think I'm good at something, I'll always go to, like, I don't know, even, even with the bits per second and playing a video game, you realize you give it to a teenager, you're given your link to a teenager. Just a large number of them. The kind of stuff you get good at stuff, they're gonna get like hundreds of bits per second. Yeah. Even just with the current technology. Probably. Probably just because it's also addicting how like the number go up aspect of it, of like improving and training, because it is, it's almost like a skill. And plus there's a software on the other end that adapts to you. And especially if the adapting procedure algorithm becomes better and better and better. You like learning together. Yeah, we're scratching the surface on that right now. There's so much more to do. So on the complete other side of it, you have an RFID chip implanted in you. Yeah, so I hear. Nice. So this is subtle thing. It's a passive device that you use for unlocking, like a safe with top secrets or what is it? What do you use it for? What's the story behind it? I'm not the first one. There's, there's this whole community of weirdo biohackers that have done this stuff, and I think one of the early use cases was storing private crypto wallet, keys and whatever. I dabbled in that a bit and had some fun with it. Do you have some bitcoin implanted in your body somewhere you can't tell where? Yeah, actually, yeah. It was the modern day equivalent of finding change in the sofa cushions after I put some orphan crypto on there that I thought was worthless and forgot about it for a few years, went back and found that some community of people loved it and had propped up the value of it. And so it had gone up 50 fold. So there was a lot of change in those cushions. That's hilarious. But the primary use case is mostly as a tech demonstrator has my business card on it. You can scan that, and by touching it to your phone, it opens the front door to my house. You know, whatever. Simple stuff. It's a cool step. It's a cool leap to implant something in your body. I mean, it has. Perhaps that's. It's a similar leap to in your link, because for a lot of people, that kind of notion of putting something inside your body, something electronic inside a biological system, is a big leap. Yeah, we have a kind of a mysticism around the barrier of our skin. We're completely fine with knee replacements, hip replacements, dental implants, but there's a mysticism still around the inviolable barrier that the skull represents. And I think that needs to be treated like any other pragmatic barrier. The question isn't a how incredible is it to open the skull? The question is, what benefit can we provide? So from all the surgeries you've done, from everything you understand in the brain, how much does neuroplasticity come into play? How adaptable is the brain? For example, just even in the case of healing from surgery or adapting to the post surgery situation, the answer that. Is sad for me and other people of my demographic is that plasticity decreases with age. Healing decreases with age. I have too much gray hair to be optimistic about that. There are theoretical ways to increase plasticity using electrical stimulation. Nothing that is totally proven out as a robust enough mechanism to offer widely to people. Yeah, I think there's cause for optimism that we might find something useful in terms of, say, an implanted electrode that improves learning. Certainly there's been some really amazing work recently from Nicholas Schiff, Jonathan Baker, and others who have a cohort of patients with moderate traumatic brain injury who have had electrodes placed in the deep nucleus in the brain called the centromedian nucleus, or just near central median nucleus. And when they apply small amounts of electricity to that part of the brain, it's almost like electronic caffeine. They're able to improve people's attention and focus. They're able to improve how well people can perform a task. I think in one case, someone who was unable to work after the device was turned on, they were able to get a job. And that's one of the holy grails for me with Neuralink and other technologies like this is from a purely utilitarian standpoint can we make people able to take care of themselves and their families economically again? Can we make it so someone who's fully dependent and even maybe requires a lot of caregiver resources, can we put them in a position to be fully independent, taking care of themselves, giving back to their communities? I think that's a very compelling proposition, and what motivates a lot of what I do and what a lot of the people at Neuralink are working for. It'S just a cool possibility that if you put a neuralink in there, that the brain adapts like the other part of the brain adapts to, and integrates it. The capacity of the brain to do that is really interesting, probably unknown to the degree to which you can do that, but you're now connecting an external thing to it, especially once it's doing stimulation. The biological brain and the electronic brain outside of it working together, the possibilities, they're really interesting. That's still unknown, but interesting. It feels like the brain is really good at adapting to whatever. Yeah, but of course, it is a system that by itself is already, like, everything serves a purpose, and so you don't want to mess with it too much. Yeah, it's like, you know, eliminating a species from an ecology. You know, you don't know what the delicate interconnections and dependencies are. I. The brain is certainly a delicate, complex beast, and we don't know every potential downstream consequence of a single change that we make. Do you see yourself doing so? You mentioned p one surgeries of p two, p three, p four, p five. Just more and more and more humans. I think it's a certain kind of brittleness or a failure on the company's side. If we need me to do all the surgeries, I think something that I would very much like to work towards is a process that is so simple and so robust on the surgery side that literally anyone could do it. We want to get away from requiring intense expertise or intense experience to have this successfully done and make it as simple and translatable as possible. I mean, I would love it if every neurosurgeon on the planet had no problem doing this. I think we're probably far from a regulatory environment that would allow people that aren't neurosurgeons to do this, but not impossible. All right, I'll sign up for that. Did you ever anthropomorphize the robot r1? Like, do you give it a name? Do you see it as, like a friend? That's like working together with you. I mean, to a certain degree, it's. Or an enemy who's going to take the job. To a certain degree, it's. Yeah, it's complex relationship. All the good relationships are. It's funny when in the middle of the surgery, there's a part of it where I stand basically shoulder to shoulder with the robot. And so if you're in the room reading the body language, it's my brother in arms there. We're working together on the same problem. Yeah. I'm not threatened by it. Keep telling yourself that. How have all the surgeries that you've done over the years, the people you've helped, and the stakes, the high stakes that you've mentioned, how has that changed your understanding of life and death? Yeah, it gives you a very visceral sense, and this may sound trite, but it gives you a very visceral sense that death is inevitable. On one hand, you are, as a neurosurgeon, you're deeply involved in these just hard to fathom tragedies. Young parents dying, leaving a four year old behind, say. And on the other hand, it takes the sting out of it a bit because you see how just mind numbingly universal death is. There is zero chance that im going to avoid it. I know techno optimists right now, and longevity buffs right now would disagree on that 0.00% estimate, but I dont see any chance that our generation is going to avoid it. Entropy is a powerful force and we are very ornate, delicate, brittle DNA machines that arent up to the cosmic ray bombardment that were subjected to. On the one hand, every human that has ever lived, died or will die. On the other hand, it's just one of the hardest things to imagine inflicting on anyone that you love is having them gone. I'm sure you've had friends that aren't living anymore, and it's hard to even think about them. And so I wish I had arrived at the point of Nirvana where death doesn't have a sting. I'm not worried about it, but I can at least say that I'm comfortable with the certainty of it, if not having found out how to take the tragedy out of it. When I think about my kids either not having me or me not having them, or my wife, maybe I've come. To accept the intellectual certainty of it, but it may be the pain that comes with losing the people you love. I don't think I've come to understand the existential aspect of it, like that this is going to end. And I don't mean like in some trite way. I mean, like, it certainly feels like it's not going to end. Like you live life like it's not going to end. Right. And the fact that this light that's shining, this consciousness, is going to no longer be in one moment, maybe today. It's like, it fills me when I really am able to load all that in with Ernest Becker's terror. Like, it's a real fear. I think people aren't always honest with how terrifying it is. Yeah, I think the more you are able to really think through it, the more terrifying it is. It's not such a simple thing. Oh, that's the way life is. And if you really can load that in, it's hard. But I think that's why the stoics did it, because it, like, helps you get your shit together and be like this. Well, they were like, the moment, every single moment you're alive is just beautiful. And it's terrifying that it's gonna end almost like you're shivering in the cold. A child helpless, this kind of feeling. And then it makes you. When you have warmth, when you have the safety, when you have the love to really appreciate it. I feel like sometimes in your position, when you mentioned armor, just to see death, it might make you not be able to see that, the finiteness of life, because if you kept looking at that, it might break you. So it's good to know that you're kind of still struggling with that. There's the neurosurgeon, and then there's a human, and the human is still able to struggle with that and feel the fear of that and the pain of that. Yeah, it definitely makes you ask the question of how long, how many of these can you see and not say, I can't do this anymore. But I mean, you said it well. I think it gives you an opportunity to just appreciate that you're alive today. And, you know, I've got three kids and an amazing wife. I'm really happy. Things are good. I get to help on a project that I think matters. I think it moves us forward. I'm a very lucky person. It's the early steps of a potentially gigantic leap for humanity. It's a really interesting one. And it's cool because you read about all this stuff in history where it's the early days. I've been reading before going to the Amazon, I would read about explorers. They will go and explore even the Amazon jungle for the first time. It's just those are the early steps or early steps into space. Early steps in any discipline in physics and mathematics. It's cool because this is, like, on the grand scale. These are the early steps into delving deep into the human brain. So not just observing the brain, but be able to interact with the human brain. Yeah, it's going to help a lot of people, but it also might help us understand what the hell's going on in there. Yeah, I think ultimately we want to give people more levers that they can pull. Right? Like, you want to give people options. If you can give someone a dial that they can turn on how happy they are, I think that makes people really uncomfortable. But now talk about major depressive disorder. Talk about people that are committing suicide at an alarming rate in this country and try to justify that queasiness in that light. You can give people a knob to take away suicidal ideation, suicidal intention. I would give them that knob. I don't know how you justify not doing that. You can think about, like, all the suffering that's going on in the world, like, every single human being that's suffering right now, it'd be a glowing red dot. The more suffering, the more it's glowing, and you just see the map of human suffering. And any technology that allows you to dim that light of suffering on a grand scale is pretty exciting because there's a lot of people suffering, and most of them suffer quietly. And we turn our. We look away too often, and we should remember those that are suffering, because, once again, most of them are suffering quietly. Well, and on a grander scale, the fabric of society. People have a lot of complaints about how our social fabric is working or not working, how our politics is working or not working. Those things are made of neurochemistry, too. In aggregate, our politics is composed of individuals with human brains, and the way it works or doesn't work is potentially tunable in the sense that, I don't know, say, remove our addictive behaviors or tune our addictive behaviors for social media, our addiction to outrage, our addiction to sharing the most angry political tweet we can find. I don't think that leads to a functional society. And if you had options for people to moderate that maladaptive behavior, there could be huge benefits to society. Maybe we could all work together a little more harmoniously toward useful ends. There's a sweet spot. Like you mentioned, you don't want to completely remove all the dark side of human nature because those kind of are somehow necessary to make the whole thing work. But there's a sweet spot. Yeah, I agree. You got to suffer a little. Just not so much that you lose hope. Yeah. When you. All the surgeries you've done, have you seen consciousness in there? Ever? Was there, like a glowing light? You know, I have this sense that I never found it, never removed it. Like a dementor in Harry Potter. I have this sense that consciousness is a lot less magical than our instincts want to claim it is. It seems to me like a useful analog for thinking about what consciousness is in the brain, is that we have a really good intuitive understanding of what it means to, say, touch your skin and know what's being touched. I think consciousness is just that level of sensory mapping applied to the thought processes in the brain itself. So what I'm saying is consciousness is the sensation of some part of your brain being active. So you feel it working. You feel the part of your brain that thinks of red things or winged creatures, or the taste of coffee. You feel those parts of your brain being active. The way that I'm feeling my palm being touched. That sensory system that feels the brain working is consciousness. It's so brilliant. It's the same way. It's the sensation of touch when you're touching a thing. Consciousness is the sensation of you feeling, your brain working, your brain thinking, your. Brain perceiving, which isn't like a warping of space time or some quantum field effect, right? It's nothing magical. People always want to ascribe to consciousness something truly different. And there's this awesome long history of people looking at whatever the latest discovery in physics is to explain consciousness, because it's the most magical, the most out there thing that you can think of. And people always want to do that with consciousness. I don't think that's necessary. It's just a, you know, a very useful and gratifying way of feeling your brain work. And as we said, it's one heck of a brain. Yeah. Everything we see around us, everything we love, everything that's beautiful, came from brains like these. It's all electrical activity happening inside your skull. And I, for 01:00 a.m. grateful that it's people like you that are exploring all the ways that it works and all the ways it can be made better. Thank you so much for talking today. It's been a joy. Thanks for listening to this conversation with Matthew McDougall. And now, dear friends, here's bliss Chapman, brain interface software lead at Neuralink. You told me that you've met hundreds of people with spinal cord injuries or with ALS, and that your motivation for helping at Neuralink is grounded in wanting to help them. Can you describe this motivation? Yeah. First, just a thank you to all the people I've gotten a chance to speak with for sharing their stories with me. I don't think there's any world, really, in which I can share their stories as powerful a way as they can. But just, I think, to summarize at a very high level, what I hear over and over again is that people with ALS, or severe spinal cord injury in a place where they basically can't move physically anymore, really, at the end of the day, are looking for independence. And that can mean different things for different people. For some folks, it can mean the ability just to be able to communicate again independently, without needing to wear something on their face, without needing a caretaker, to be able to put something in their mouth. For some folks, it can mean independence. To be able to work again, to be able to navigate a computer digitally efficiently enough, to be able to get a job, to be able to support themselves, to be able to move out, and ultimately be able to support themselves after their family maybe isn't there anymore to take care of them. And for some folks, it's as simple as just being able to respond to their kid in time before they run away or get interested in something else. And these are deeply personal and sort of very human problems. And what strikes me again and again when talking with these folks is that this is actually an engineering problem. This is a problem that with the right resources, with the right team, we can make a lot of progress on. And at the end of the day, I think that's a deeply inspiring message and something that makes me excited to get up every day. So it's both an engineering problem in terms of a BCI, for example, that can give them capabilities where they can interact with the world. But also on the other side, it's an engineering problem for the rest of the world to make it more accessible for people living with quadriplegia. Yeah, I'll take a broad view sort of lens on this for a second. I think I'm very in favor of anyone working in this problem space. So beyond BCI, I'm happy and excited and willing to support any way I can. Folks working on eye tracking systems, working on speech to text systems, working on head trackers or mouse sticks or quad sticks. I've met many engineers and folks in the community that do exactly those things. And I think for the people we're trying to help, it doesn't matter what the complexity of the solution is, as long as the problem is solved and I want to emphasize that there can be many solutions out there that can help with these problems, and BCI is one of a collection of such solutions. So BCI in particular, I think offers several advantages here, and I think the folks that recognize this immediately are usually the people who have spinal cord injury or some form of paralysis. Usually you dont have to explain to them why this might be something that could be helpful. Its usually pretty self evident. But for the rest of us folks that dont live with severe spinal cord injury or who dont know somebody with ALS, its not often obvious why you would want a brain implant to be able to connect and navigate a computer. And its surprisingly nuanced to the degree that ive learned a huge amount just working with Noland in the first Nurlin clinical trial and understanding from him, in his words, why this device is impactful for him and its a nuanced topic. It can be the case that even if he can achieve the same thing, for example, with a mouse stick when navigating a computer, he doesnt have access to that mouse stick every single minute of the day. He only has access when someone is available to put it in front of him. And so a BCI can really offer a level of independence and autonomy that if it wasn't literally physically part of your body, it'd be hard to achieve in any other way. So there's a lot of fascinating aspects to what it takes to get no one to be able to control a cursor on the screen with his mind. You texted me something that I just love. You said, I was part of the team that interviewed and selected p one. I was in the operating room during the first human surgery, monitoring live signals coming out of the brain. I work with the user basically every day to develop new UX paradigms, decoding strategies, and I was part of the team that figured out how to recover useful BCI to new world record levels when the signal quality degraded. We'll talk about, I think, every aspect of that, but just zooming out, what was it like to be a part of that? Part of that team and part of that. Historic, I would say historic first. Yeah, I think for me, this is something I've been excited about for close to ten years now. And so to be able to be even just some small part of making it a reality is extremely exciting. A couple, maybe special moments during that whole process that I'll never really truly forget. One of them is during the actual surgery at that point in time. I know Nolan quite well, I know his family and so I think the initial reaction when Nolan is rolled into the operating room is just, oh, shit, kind of reaction. But at that point, muscle memory kicks in and you sort of go into, you let your body just do all the talking. And I have the lucky job in that particular procedure to just be in charge of monitoring the implant. So my job is to sit there to look at the signals coming off the implant, to look at the live brain data streaming off the device as threads are being inserted into the brain, and just to basically observe and make sure that nothing is going wrong or that there's no red flags or fault conditions that we need to go and investigate or pause the surgery to debug. Because I had that sort of spectator view of the surgery, I had a slightly removed perspective than I think most folks in the room. I got to sit there and think to myself, wow, that brain is moving a lot. When you look inside the cranectomy that we stick the threads in, one thing that most people don't realize is the brain moves. The brain moves a lot when you breathe, when your heart beats, and you can see it visibly. So that's something that I think was a surprise to me and very, very exciting to be able to see someone's brain who you physically know and have talked with that length actually pulsing and moving inside their skull. And they used that brain to talk to you previously, and now it's right there moving. Yep. Actually, I didn't realize that in terms of the thread sending. So the neuralink implant is active during surgery, and one thread at a time, you're able to start seeing the signal. Yeah. So that's part of the way you test that. The thing is working. Yeah. So actually, in the operating room, right after we sort of finished all the thread insertions, I started collecting what's called broadband data. So broadband is basically the most raw form of signal you can clock from a neuralink electrode. It's essentially a measurement of the local field potential or the voltage essentially measured by that electrode. We have a certain mode in our application that allows us to visualize where detected spikes are. It visualizes in the broadband signal in this very, very raw form of the data, a neuron is actually spiking. One of these moments that I'll never forget as part of this whole clinical trial is seeing live in the operating room while he's still under anesthesia, beautiful spikes being shown in the application, just streaming live to a device I'm holding in my hand. So this is no signal processing the raw data, and then the signals processing is on top of it. You're seeing the spikes detected, right? Yeah. And that's a ux, too, because that looks beautiful as well. During that procedure, there was actually a lot of cameramen in the room, so they also were curious and wanted to see. There's several neurosurgeons in the room who are all just excited to see robots taking their job, and they're all crowded around a small little iPhone, watching this live brain data stream out of his brain. What was that like, seeing the robot do some of the surgery? So the computer vision aspect, where it detects all the spots that avoid the blood vessels, and then, obviously, with human supervision and actually doing the really high precision connection of the threads to the brain. Yeah, that's a good question. My answer is going to be pretty lame here. But it was boring. Yeah, I've seen it so many times. Yeah. That's exactly how you want surgery to be. You want it to be boring. Yeah. Because I've seen it so many times, I've seen the robot do the surgery literally hundreds of times. And so it was just one more time. Yeah. All the practice surgeries and the proxies, and this is just another day. Yeah. So what about when Nolan woke up? Well, do you remember a moment where he was able to move the cursor? Not move the cursor, but get signal from the brain such that it was able to show that there's a connection? Yeah. Yeah. So we are quite excited to move as quickly as we can. And Nolan was really, really excited to get started. He wanted to get started, actually, the day of surgery, but we waited till the next morning, very patiently. It's a long night. And the next morning in the ICU, where he was recovering, he wanted to get started and actually start to understand what kind of signal we can measure from his brain. And maybe for folks who are not familiar with the neuralink system, we implant the neuralink system, or the neural link implant in the motor cortex. The motor cortex is responsible for representing things like motor intent. If you imagine closing and opening your hand, that kind of signal representation would be present in the motor cortex. If you imagine moving your arm back and forth or wiggling a pinky, this sort of signal can be present in the motor cortex. So one of the ways we start to sort of map out what kind of signal do we actually have access to in any particular individual's brain is through this task called body mapping. And body mapping is where you essentially present a visual to the user and you say, hey, imagine doing this and that visual is a 3d hand opening, closing, or index finger modulating up and down. And you ask the user to imagine that. And obviously, you can't see them do this because they're paralyzed, so you can't see them actually move their arm. But while they do this task, you can record neural activity and you can basically offline model and check, can I predict or can I detect the modulation corresponding with those different actions? And so we did that task, and we realized, hey, there's actually some modulation associated with some of his hand motion, which is the first indication that, okay, we can potentially use that modulation to do useful things in the world. For example, control a computer cursor. And he started playing with it, you know, the first time we showed him it, and we actually just took the same live view of his brain activity and put it in front of him. And we said, hey, you tell us what's going on. Uh, you know, we're not you. You're able to imagine different things, and we know that it's modulating some of these neurons. So you figure out for us what that is actually representing. And so he played with it for a bit. He was like, I don't quite get it yet. He played for a bit longer, and he said, oh, when I move this finger, I see this particular neuron start to fire more. And I said, okay, prove it. Do it again. And so he said, okay. Three, two, one, boom. And the minute he moved, you can see, like, instantaneously this neuron is firing single neuron. I can tell you the exact channel number if you're interested. It's stuck in my brain now forever. But that single, uh, channel firing was a beautiful indication that it was behaviorally modulated neural activity that could then be used for downstream tasks, like decoding a computer cursor. And when you say single channel, is that associated with a single electrode? Yeah, channel electrode are interchangeable. And there's a 1024 of those. 1024? Yeah. It's incredible that that works. That really, when I was learning about all this and, like, loading it in, it was just blowing my mind that the intention, you can visualize yourself moving the finger that can turn into a signal, and the fact that you can then skip that step and visualize the cursor moving to or have the intention of the cursor moving, and that leading to a signal that can then be used to move the cursor. There is so many exciting things there to learn about the brain, about the way the brain works, the very fact of their existing signal that can be used is really powerful, but it feels like that's just like the beginning of figuring out how that signal could be used really, really effectively. I should also just. There's so many fascinating details here. But you mentioned the body mapping step, at least in the version I saw that Nolan was showing off. There's like a super nice interface, like a graphical interface. It just felt like I was in the future because I guess it visualizes you moving the hand. And there's a very sexy, polished interface that, hello. I don't know if there's a voice component, but it just felt like. Like when you wake up in a really nice video game and this is a tutorial at the beginning of that video game, this is what you're supposed to do. It's cool. No, I mean, the future should feel. Like the future, but it's not easy to pull that off. I mean, it needs to be simple, but not too simple. Yeah. And I think the UX design component here is underrated for PCI development. In general, there's a whole interaction effect between the ways in which you visualize an instruction to the user and the kinds of signal you can get back. That quality of your behavioral alignment to the neural signal is a function of how good you are at expressing to the user what you want them to do. We spend a lot of time thinking about the UX, of how we build our applications, of how the decoder actually functions, the control surfaces it provides to the user. All these little details matter a lot. Maybe it'd be nice to get into a little bit more detail of what the signal looks like and what the decoding looks like. N one implant that has, like we mentioned, 1024 electrodes and that's collecting raw data, raw signal. What does that signal look like and what are the different steps along the way before it's transmitted and what is transmitted? All that kind of stuff. Yeah, yeah, this is gonna be a fun one. Let's go. So maybe before diving into what we do, it's worth understanding what we're trying to measure, because that dictates a lot of the requirements for the system that we build. And what we're trying to measure is really individual neurons producing action potentials. Action potential is you can think of it like a little electrical impulse that you can detect if you're close enough. And by being close enough, I mean, like within, let's say, 100 microns of that cell, and 100 microns is a very, very tiny distance. And so the number of neurons that you're going to pick up with any given electrode is just a small radius around that electrode. And the other thing worth understanding about the underlying biology here is that when neurons produce an action potential, the width of that action potential is about one millisecond. So from the start of the spike to the end of the spike, that whole width of that sort of characteristic feature of a neuron firing is one millisecond wide. And if you want to detect that an individual spike is occurring or not, you need to sample that signal or sample the local field potential nearby that neuron. Much more frequently than once a millisecond. You need to sample many, many times per millisecond to be able to detect that this is actually the characteristic waveform of a neuron producing an action potential. And so we sample, across all 1024 electrodes about 20,000 times a second. 20,000 times a second means for every given one millisecond window, we have about 20 samples that tell us what that exact shape of that action potential looks like. And once we've sort of sampled, at super high rate, the underlying electrical field nearby these cells, we can process that signal into just where do we detect a spike or where do we not? Sort of a binary signal, one or zero? Do we detect a spike in this one millisecond or not? And we do that because the actual information carrying sort of subspace of neural activity is just when our spikes occurring. Essentially, everything that we care about for decoding can be captured or represented in the frequency characteristics of spike trains. Meaning how often are spikes firing in any given window of time? And so that allows us to do sort of a crazy amount of compression from this very rich, high density signal to something that's much, much more sparse and compressible, that can be sent out over a wireless radio, like a Bluetooth communication, for example. Quick tangents here. You mentioned electrode neuron. There's a local neighborhood of neurons nearby. How difficult is it to isolate from where the spike came from? Yeah, so there's a whole field of sort of academic neuroscience work on exactly this problem of basically given a single electrode or given a set of electrodes measuring a set of neurons, how can you spike sort which spikes are coming from what neuron? And this is a problem that's pursued in academic work because you care about it for understanding what's going on in the underlying sort of neuroscience of the brain. If you care about understanding how the brain's representing information, how that's evolving through time, then that's a very, very important question to understand for sort of the engineering side of things, at least at the current scale, if the number of neurons per electrode is relatively small, you can get away with basically ignoring that problem completely. You can think of it like sort of a random projection of neurons to electrodes, and there may be, in some cases, more than one neuron per electrode. But if that number is small enough, those signals can be thought of as sort of a union of the two. And for many applications, that's a totally reasonable trade off to make and can simplify the problem a lot. And as you sort of scale out channel count, the relevance of distinguishing individual neurons becomes less important because you have more overall signal, and you can start to rely on sort of correlations or covariance structure in the data to help understand when that channel is firing, what does that actually represent? Because you know that when that channel is firing in concert with these other 50 channels, that means move left. But when that same channel is firing with concert with these other ten channels, that means move right. Okay, so you have to do this kind of spike detection on board, and you have to do that super efficiently, so fast, and not use too much power because you don't want to be generating too much heat. So it has to be a super simple signal processing step. Yeah. Is there some wisdom you can share about what it takes to overcome that challenge? Yeah, so we've tried many different versions of basically turning this raw signal into a feature that you might want to send off the device. And I'll say that I don't think we're at the final step of this process. This is a long journey. We have something that works clearly today, but there can be many approaches that we find in the future that are much better than what we do right now. So some versions of what we do right now, and there's a lot of academic carriages to these ideas. So I don't want to claim that these are original neuralink ideas or anything like that, but one of these ideas is basically to build sort of like a convolutional filter, almost, if you will, that slides across the signal and looks for a certain template to be matched. That template consists of sort of how deep the spike modulates, how much it recovers, and what the duration and window of time is that the whole process takes. And if you can see in the signal that that template is matched within certain bounds, then you can say, okay, that's a spike. One reason that approach is super convenient is that you can actually implement that extremely efficiently in hardware, which means that you can run it in low power across 1024 channels all at once. Another approach that we've recently started exploring, and this can be combined with the spark detection approach, something called spike band power. And the benefits of that approach are that you may be able to pick up some signal from neurons that are maybe too far away to be detected as a spike, because the farther away you are from an electrode, the weaker that actual spike waveform will look like on that electrode. So you might be able to pick up population level activity of things that are maybe slightly outside the normal recording radius, what neuroscientists sometimes refer to as the hash of activity, the other stuff that's going on. And you can look at across many channels how that background noise is behaving, and you might be able to get more juice out of the signal that way, but it comes at a cost. That signal is now a floating point representation, which means it's more expensive to send out over power. It means you have to find different ways to compress it that are different than what you can apply to binary signals. There's a lot of different challenges associated with these different modalities. Also, in terms of communication, you're limited by the amount of data you can send. Yeah, so. And also because you're currently using the Bluetooth protocol, you have to bash stuff together, but you have to also do this, keeping the latency crazy low. Like, crazy low. Anything to say about the latency? Yeah, this is a passion project of mine. So I want to build the best mouse in the world. Yeah. I don't want to build, like, the, you know, the Chevrolet Spark or whatever, of electric cars. I want to build, like, the Tesla roadster version of. Of a mouse. And I really do think it's quite possible that within five to ten years that most esports competitions are dominated by people with paralysis. This is, like, a very real possibility for a number of reasons. One is that they'll have access to the best technology to play video games effectively. The second is they have the time to do so. So those two factors together are particularly potent for esport competitors, unless people without. Paralysis are also allowed to implant, which is, it is another way to interact with a digital device. And there's some. There's something to that. If it's a fundamentally different experience, more efficient experience, even if it's not like some kind of full on, high bandwidth communication, if it's just ability to move the mouse ten x faster, like the bits per second, if I can achieve a bit per second at ten x, what I can do with the mouse that's a really interesting possibility of what that can do, especially as you get really good at it. Uh, with training, it's definitely the case. That you have a higher ceiling performance like you, because you don't have to buffer your intention through your arm, through your muscle. You get just by nature of having a brain implant at all, like 75 millisecond lead time on any action that you're actually trying to take. And there's some nuance to this. Like there. There's evidence that the motor cortex, you can sort of plan out sequences of action. So you might not get that whole benefit all the time, but for sort of like, reaction time style, uh, games where you just want to. Somebody's over here, snipe them, you know, that kind of thing. Uh, you actually do have just an inherent advantage because you don't need to go through muscle. So the question is just how much faster can you make it? And we're already, you know, faster than, uh, you know, what you would do if you're going through muscle. From a latency point of view, and we're in the early stage of that, I think we can push it sort of our end to end latency right now. From brain spike to cursor movement, it's about 22 milliseconds. If you think about, uh, the best mice in the world, the best gaming mice, that's about five milliseconds ish of latency. Depending on how you measure, depending how fast your screen refreshes, there's a lot of characteristics that matter there, but. Yeah, and the rough time for a neuron in the brain to actually impact your command of your hand is about 75 milliseconds. So if you look at those numbers, you can see that we're already competitive and slightly faster than what you'd get by actually moving your hand. And this is something that, if you ask Noland about it, when he moved the cursor for the first time, we asked him about this, it was something I was super curious about. What does it feel like when you're modulating a click intention or when you're trying to move the cursor to the right? He said it moves before he is actually intending it to, which is kind of a surreal thing and something that I would love to experience myself one day. What is that like, to have the thing just be so immediate, so fluid that it feels like it's happening before you're actually intending it to move? Yeah, I suppose we've gotten used to that latency, that natural latency that happens so is the currently the bottleneck of communication. So, like, the Bluetooth communication, is that. What's the actual bottleneck? I mean, there's always going to be a bottleneck. What's the current bottleneck? Yeah, a couple of things. So, kind of hilariously, Bluetooth low energy protocol has some restrictions on how fast you can communicate. So the protocol itself establishes a standard of. The most frequent sort of updates you can send are on the order of 7.5 milliseconds. And as we push latency down to the level of individual spikes impacting control, that level of resolution, that kind of protocol is going to become a limiting factor at some scale. Another important nuance to this is that it's not just the neuralink itself that's part of this equation. If you start pushing latency below the level of how fast greens refresh, then you have another problem. You need your whole system to be able to be as reactive as the sort of limits of what the technology can offer. Like, you need the screen, like 120 hz just doesn't work anymore. If you're trying to have something respond at something that's at the level of. One millisecond, that's a really cool challenge. I also like that for a t shirt. The best mouse in the world. Tell me on the receiving end. So the decoding step. Now we figured out what the spikes are. Got them all together. Now we're sending that over to the app. What's the decoding step look like? Yeah. So maybe first, what is decoding? I think there's probably a lot of folks listening that just have no clue what. What it means to decode brain activity. Actually, even if we zoom out beyond that, what is the app? So there's a. There's an implant that's wirelessly communicating with any digital device that has an app installed. Yep. So maybe. Can you tell me a high level what the app is, what the software is outside of the brain? Yeah. So maybe working backwards from the goal. The goal is to help someone with paralysis, in this case Noland, be able to navigate his computer independently. And we think the best way to do that is to offer them the same tools that we have to navigate our software, because we don't want to have to rebuild an entire software ecosystem for the brain, at least not yet. Maybe someday you can imagine there's uxs that are built natively for BCI, but in terms of what's useful for people today, I think most people would prefer to be able to just control mouse and keyboard inputs to all the applications that they want to use for their daily jobs, for communicating with their friends, et cetera. The job of the application is really to translate this wireless stream of brain data coming off the implant into control of the computer. We do that by essentially building a mapping from brain activity to the HID inputs to the actual hardware. HID is just the protocol for communicating input device events. So, for example, move mouse to this position or press this key down. And so that mapping is fundamentally what the app is responsible for. But there's a lot of nuance of how that mapping works that we spend a lot of time to try to get right. And we're still in the early stages of a long journey to figure out how to do that optimally. So one part of that process is decoding. So, decoding is this process of taking the statistical patterns of brain data that's being channeled across this bluetooth connection to the application and turning it into, for example, a mouse movement. That decoding step, you can think of it in a couple different parts, similar to any machine learning problem. There's a training step and there's an inference step. The training step in our case is a very intricate behavioral process where the user has to imagine doing different actions. For example, they'll be presented a screen with a cursor on it, and they'll be asked to push that cursor to the right. Then imagine pushing that cursor to the left. Push it up, push it down. We can basically build up a pattern, or using any modern ML method, a mapping of, given this brain data and that imagined behavior, map one to the other. And then at test time, you take that same pattern matching system. In our case, it's a deep neural network, and you run it and you take the live stream of brain data coming off their implant. You decode it by pattern matching to what you saw at calibration time, and you use that for control of the computer. Now, a couple rabbit holes that I think are quite interesting. One of them has to do with how you build that best template matching system, because there's a variety of behavioral challenges and also debugging challenges when you're working with someone who's paralyzed, because, again, fundamentally, you don't observe what they're trying to do. You can't see them attempt to move their hand. You have to figure out a way to instruct the user to do something and validate that they're doing it correctly, such that then you can downstream build with confidence the mapping between the neural spikes and the intended action. And by doing the action correctly. What I really mean is at this level of resolution of what neurons are doing. So if in ideal world, you could get a signal of behavioral intent that is ground truth accurate at the scale of one millisecond resolution, then with high confidence I could build a mapping from my neural spikes to that behavioral intention. But the challenge is, again, that you don't observe what they're actually doing. There's a lot of nuance to how you build user experiences that give you more than just a course, on average, correct representation of what the user is intending to do. If you want to build the world's best mouse, you really want it to be as responsive as possible. You want it to be able to do exactly what the user is intending at every step along the way, not just on average, be correct when you're trying to move it from left to right. Building a behavioral calibration game or software experience that gives you that level of resolution is what we spend a lot of time working on. So the calibration process, the interface has to encourage precision, meaning like whatever it does, it should be super intuitive that the next thing the human is going to likely do is exactly that intention that you need and only that intention. Yeah. And you don't have any feedback except that may be speaking to you afterwards. What they actually did, you can't. Oh, yeah, right. So that's a, that's fundamentally, that is a really exciting UX challenge, because that's all on the UX. It's not just about being friendly or nice or usable. Yeah. It's like user experience is how it works. It's how it works for the calibration. And calibration, at least at this stage of neuralink, is like fundamental to the operation of the thing, and not just calibration, but continued calibration, essentially, yeah. And maybe you said something that I think is worth exploring there a little bit. You said it's primarily a UX challenge, and I think a large component of it is. But there is also a very interesting machine learning challenge here, which is given some dataset, including some, on average, correct behavior of asking the user to move up or move down, move right, move left, and given a dataset of neural spikes, is there a way to infer in some kind of semi supervised or entirely unsupervised way what that high resolution version of their intention is? If you think about it, there probably is, because there are enough data points in the dataset, enough constraints on your model, that there should be a way, with the right formulation, to let the model figure out itself. For example, at this millisecond this is exactly how hard they're pushing upwards. And at this millisecond, this is how hard they're trying to push upwards. It's really important to have very clean labels. Yes, the problem becomes much harder from the machine learning perspective. The labels are noisy. That's correct. And then to get the clean labels, that's a Ux challenge. Correct. Although clean labels, I think maybe it's worth exploring what that exactly means. I think any given labeling strategy will have some number of assumptions it makes about what the user is attempting to do. Those assumptions can be formulated in a loss function, or they can be formulated in terms of heuristics that you might use to just try to estimate or guesstimate what the user is trying to do. And what really matters is how accurate are those assumptions. For example, you might say, hey user, push upwards and follow the speed of this cursor. And your heuristic might be that they're trying to do exactly what that cursor is trying to do. Another competing heuristic might be they're actually trying to go slightly faster at the beginning of the movement and slightly slower at the end. And those competing heuristics may or may not be accurate reflections of what the user is trying to do. Another version of the task might be, hey user, imagine moving this cursor a fixed offset. So rather than follow the cursor, just try to move it exactly 200 pixels to the right. So here's the cursor, here's the target. Okay, cursor disappears. Tried to move that now invisible cursor 200 pixels to the right. And the assumption in that case would be that the user can actually modulate correctly that position offset. But that position offset assumption might be a weaker assumption and therefore potentially you can make it more accurate than these heuristics that are trying to guesstimate at each millisecond what the user is trying to do. So you can imagine different tasks that make different assumptions about the, the nature of the user intention. And those assumptions being correct is what I would think of as a clean. Label for that step. What are we supposed to be visualizing? There's a cursor and you want to move that cursor to the right, the left up and down, or maybe move them by a certain offset. So that's one way, is that the best way to do calibration? So for example, alternative crazy way that probably is playing a role. Here's a game like web grid where you're just getting a very large amount of data. The person playing a game where if they are in a state of flow, maybe you can get clean signal as a side effect. Yeah. Is that, or is it, is that not an effective way for initial calibration? Yeah, great question. There's a lot to unpack there. So the first thing I would draw a distinction between a sort of open loop versus closed loop. So open loop. What I mean by that is the user is going from zero to one. They have no model at all, and they're trying to get to the place where they have some level of control at all. In that setup, you really need to have some task that gives the user a hint of what you want them to do, such that you can build its mapping again from brain data to output. Then once they have a model, you could imagine them using that model and actually adapting to it and figuring out the right way to use it themselves and then retraining that data to give you a boost in performance. There's a lot of challenges associated with both of these techniques, and we can sort of rabbit hole into both of them if you're interested. But the sort of challenge with the open loop task is that the user themself doesn't get proprioceptive feedback about what they're doing. They don't necessarily perceive themselves or feel the mouse under their hand when they're using an open. When they're trying to do an open loop calibration, they're being asked to perform something. Imagine if you had your whole right arm numbed and you stuck it in a box and you couldn't see it, so you had no visual feedback and you had no proprioceptive feedback about what the position or activity of your arm was. And now you're asked, okay, given this thing on the screen that's moving from left to right, match that speed, and you basically can try your best to invoke whatever that imagined action is in your brain. That's moving the cursor from left to right. But in any situation, you're going to be inaccurate and maybe inconsistent in how you do that task. And so that's sort of the fundamental challenge of open loop. The challenge with closed loop is that once the users given a model and they're able to start moving the mouse on their own, they're going to very naturally adapt to that model. That co adaptation between the model learning what they're doing and the user learning how to use the model may not find you the best global minima. It may be that your first model was noisy in some ways, or maybe just had some quirk. There's some part of the data distribution it didn't cover super well. The user now figures out, because they're a brilliant user like Nolan, they figure out the right sequence of imagined motions or the right angle they have to hold their hand at to get it to work, and they'll get it to work great. But then the next day, they come back to their device, and maybe they don't remember exactly all the tricks that they used the previous day. And so there's a complicated feedback cycle here that can. That can emerge and can make it a very, very difficult debugging process. Okay. There's a lot of really fascinating things there. Yeah. Actually, just to stay on the. On the closed loop, I've seen situations. This actually happened watching psychology grad students. They use pieces of software when they don't know how to program themselves. They use piece of software that somebody else wrote and has a bunch of bugs, and they figure out, and they've been using it for years, they figure out ways to work around, oh, that just happens. Nobody considers, maybe we should fix this. They just adapt. And that's a really interesting notion that we just said we're really good at adapting, but you need to still. That might not be the optimal. Yeah. Okay, so how do you solve that problem? Do you have to restart from scratch every once in a while kind of thing? Yeah, it's a good question. First and foremost, I would say this is not a solved problem. And for anyone who's listening in academia who works on BCIS, I would also say this is not a problem that's solved by simply scaling channel count. So this is maybe that can help and you can get richer covariance structures that you can use to exploit the. When trying to come up with good labeling strategies. But if you're interested in problems that aren't going to be solved inherently by scaling channel count, this is one of them. Yeah. So how do you solve it? It's not a solved problem. That's the first thing I want to make sure it gets across. The second thing is any solution that involves closed loop is going to become a very difficult debugging problem. And one of my general heuristics for choosing what problems to tackle is that you want to choose the one that's going to be the easiest to debug, because if you can do that, even if the ceiling is lower, you're going to be able to move faster because you have a tighter iteration loop debugging the problem in the open loop setting, there's not a feedback cycle to debug with the user in the loop. There's some reason to think that that should be an easier debugging problem. The other thing that's worth understanding is that even in the closed loop setting, there's no special soft or magic of how to infer what the user is truly attempting to do in the closed loop setting. Although they're moving the cursor on the screen, they may be attempting something different than what your model is outputting. So what the model is outputting is not a signal that you can use to retrain. If you want to be able to improve the model further, you still have this very complicated guesstimation or unsupervised problem of figuring out what is the true user intention underlying that signal. The open loop problem has the nice property of being easy to debug, and the second nice property of it has all the same information and content as the close loop scenario. Another thing I want to mention and call out is that this problem doesn't need to be solved in order to give useful control to people. Even today, with the solutions we have now, and that academia has built up over decades, the level of control that can be given to a user today is quite useful. It doesn't need to be solved to get to that level of control. But again, I want to build the world's best mouse. I want to make it so good that it's not even a question that you want it to build the world's best mouse, the superhuman version, you really need to nail that problem. And a couple maybe details of previous studies that we've done internally that I think are very interesting to understand when thinking about how to solve this problem. The first is that even when you have ground truth data of what the user is trying to do, and you can get this with an able bodied monkey, a monkey that has an Erlink device implanted and moving a mouse to control a computer. Even with that ground truth dataset, it turns out that the optimal thing to predict to produce high performance BCI is not just the direct control of the mouse. You can imagine building a dataset of what's going on in the brain and what is the mouse exactly doing on the table. And it turns out that if you build the mapping from neural spikes to predict exactly what the mouse is doing, that model will perform worse than a model that is trained to predict higher level assumptions about what the user might be trying to do. For example, assuming that the monkey is trying to go in a straight line to the target. It turns out that making those assumptions is actually more effective in producing a model than actually predicting the underlying hand movement. So the. The intention, not like the physical movement or whatever. Yeah, there's obviously a very strong correlation between the two, but the intention is a more powerful thing to be chasing. Right. Well, that. That's also super interesting. I mean, the intention itself is fascinating because, yes, with the BCI here, in this case, with digital telepathy, you're acting on the intention, not the action, which is why there's an experience of, like, feeling like it's happening before you meant for it to happen. That is so cool. And that is why you could achieve, like, superhuman performance, probably in terms of the control of the mouse. So the. For open loop, just to clarify. So whenever the person is tasked, like, move the mouse to the right, you said there's not feedback, so they don't get to get that satisfaction of actually getting it to move. You could imagine giving the user feedback on the screen, but it's difficult because at this point, you don't know what they're attempting to do. What can you show them that would basically give them a signal of, I'm doing this correctly or not correctly? Let's take this very specific example. Maybe your calibration task looks like you're trying to move the cursor a certain position offset. Your instructions to the user are, hey, I. The cursor is here. Now, when the cursor disappears, imagine moving it 200 pixels from where it was to the right to be over this target. In that scenario, you could imagine coming up with some sort of consistency metric that you could display to the user of, okay, I know what the spike train looks like. On average, when you do this action to the right, maybe I can produce some probabilistic estimate of how likely is that to be the action you took, given the latest trial or trajectory that you imagined. And that could give the user some sort of feedback of how consistent are they across different trials. You could also imagine that if the user is prompted with that kind of consistency metric, that maybe they just become more behaviorally engaged to begin with, because the task is boring when you don't have any feedback at all, there may be benefits to the user experience of showing something on the screen, even if it's not accurate, just because it keeps the user motivated to try to increase that number or push it upwards. So there's a psychology element here. Yeah, absolutely. And again, all of that is ux challenge. How much signal drift is there hour to hour, day to day, week to week, month to month. How often do you have to recalibrate because of the signal drift? Yeah, so this is a problem we've worked on both with NHP non human primates before our clinical trial, and then also with Noland during the clinical trial. Maybe the first thing that's worth stating is what the goal is here. The goal is really to enable the user to have a plug and play experience where I guess they don't have to plug anything in, but a play experience where they can use the device whenever they want to, however they want to. That's really what we're aiming for. There can be a set of solutions that get to that state without considering this non stationary problem. Maybe the first solution here that's important is that they can recalibrate whenever they want. This is something that Nolan has the ability to do today, so he can recalibrate the system at 02:00 a.m. in the middle of the night without his caretaker or parents or friends around to help push a button for him. The other important part of the solution is that when you have a good model calibrated, that you can continue using that without needing to recalibrate it. How often he has to do this recalibration today depends really on his appetite for performance. We observe a degradation through time of how well any individual model works. But this can be mitigated behaviorally by the user adapting their control strategy. It can also be mitigated through a combination of software features that we provide to the user. For example, we let the user adjust exactly how fast the cursor is moving. We call that the gain, for example, the gain of how fast the cursor reacts to any given input intention. They can also adjust the smoothing, how smooth the output of that cursor intention actually is. They can also adjust the friction, which is how easy is it to stop and hold still. And all these software tools allow the user a great deal of flexibility and troubleshooting mechanisms to be able to solve this problem for themselves. By the way, all this is done by looking to the right side of the screen, selecting the mixer. And the mixer you have, it's like DJ mode. DJ mode for your VCR. I mean, it's a really well done interface. It's really, really well done. And so, yeah, there's that bias that there's a cursor drift that Nolan talked about in a stream, although he said that you guys were just playing around with it with him, and they're constantly improving. So that could have been just a snapshot of that particular moment, particular day. But he said that there was this cursor drift and his bias that could be removed by him, I guess, looking to the right side of the screen, the left side of the screen, to kind of adjust the bias. Yeah, that's one interface action, I guess, to adjust the bias. Yeah. So this is actually an idea that comes out of academia. There is some prior work with Braingate clinical trial participants where they pioneered this idea of bias correction. The way we've done it, I think, is it's very prioritized, very beautiful user experience, where the user can essentially flash the cursor over to the side of the screen, and it opens up a window where they can actually adjust or tune exactly the bias of the cursor. So bias, maybe for people who aren't familiar, is just sort of what is the default motion of the cursor if you're imagining nothing, and it turns out that that's one of the first sort of qualia of the cursor control experience that's impacted by neural non stationarity. Qualia of the cursor experience. I don't know how else to describe it. Like, you know, I'm not the guy moving. Very poetic. I love it. The quality of the cursor experience. Yeah. I mean, it sounds poetic, but it is deeply true. There is an experience when it works well. It is a joyful, a really pleasant experience, and when it doesn't work well, it's a very frustrating experience. That's actually the art of ux. It's like you have the possibility to frustrate people or the possibility to give them joy. And at the end of the day, it really is truly the case that UX is how the thing works. And so it's not just like what's showing on the screen. It's also what control surfaces does a decoder provide the user. We want them to feel like they're in the f one car, not some minivan. And that really, truly is how we think about it. Nolan himself is an f one fan, so we refer to ourselves as a pit crew. He really is truly the f one driver. And there's different control surfaces that different kinds of cars and airplanes provide the user. And we take a lot of inspiration from that when designing how the cursor should behave. And maybe one nuance of this is even details like when you move a mouse on a MacBook trackpad, the sort of response curve of how that input that you give the trackpad translates to cursor movement is different than how it works with a mouse. When you move on the trackpad, there's a different response function, a different curve to how much a movement translates to input to the computer than when you do it physically with a mouse. And that's because somebody sat down a long time ago when they designed the initial input systems to any computer and they thought through exactly how it feels to use these different systems. Now we're designing the next generation of this input system to a computer, which is entirely done via the brain, and there's no proprioceptive feedback. Again, you don't feel the mouse in your hand, you don't feel the keys under your fingertips. You want a control surface. That still makes it easy and intuitive for the user to understand the state of the system and how to achieve what they want to achieve. Ultimately, the end goal is that that ux completely fades into background. It becomes something that's so natural and intuitive that it's subconscious to the user. And they just should feel like they have basically direct control over the cursor, just does what they want it to do. They're not thinking about the implementation of how to make it do what they want it to do, it's just doing what they want it to do. Is there some kind of things along the lines of like Fitt's law where you should move the mouse in a certain kind of way that maximizes your chance to hit the target? I don't even know what I'm asking, but I'm hoping the intention of my question will land on a profound answer. No. Is there some kind of understanding of the laws of ux when it comes to the context of somebody using their brain to control it? That's different than actual with a mouse? I think we're in the early stages of discovering those laws, so I wouldn't claim to have solved that problem yet. But there's definitely some things we've learned that make it easier for the user to get stuff done. And it's pretty straightforward when you verbalize it, but takes a while to actually get to that point when you're in the process of debugging the stuff in the trenches. One of those things is that any machine learning system you build has some number of errors, and it matters how those errors translate to the downstream user experience. For example, if you're developing search algorithm in your photos, if you search for your friend Joe and it pulls up a photo of your friend Josephine, maybe that's not a big deal because the cost of an error is not that high. In a different scenario where you're trying to detect insurance fraud or something like this, and you're directly sending someone to court because of some machine learning model output, then the errors make a lot more sense to be careful about. You want to be very thoughtful about how those errors translate to downstream effects. The same is true in BCI. So, for example, if you're building a model that's decoding a velocity output from the brain versus an output where you're trying to modulate the left click, for example, these have sort of different trade offs of how precise you need to be before it becomes useful to the end user. For velocity, it's okay to be on average correct, because the output of the model is integrated through time. So if the user is trying to click at position a and they're currently at position b, they're trying to navigate over time to get between those two points. And as long as the output of the model is on average correct, they can steer it through time. With the user control loop in the mix, they can get to the point they want to get to. The same is not true of a click. For a click, you're performing it almost instantly at the scale of neurons firing. You want to be very sure that that click is correct. Because a false click can be very destructive to the user. They might accidentally close the tab that they're trying to do something and lose all their progress. They might accidentally hit some send button on some text that there's only half composed and reads funny after. So there's different cost functions associated with errors in this space. And part of the UX design is understanding how to build a solution that is, when it's wrong, still useful to the end user. That's so fascinating that assigning cost to every action when an error occurs to. So every action, if an error occurs, has a certain cost, and incorporating that into how you interpret the intention, mapping it to the action is really important. I didn't quite until you said it realized there's a cost to like sending the text early. It's like a very expensive cost. It's super annoying if you accidentally, like if you're a cursor, imagine if your cursor misclicked every once in a while. That's super obnoxious. And the worst part of it is usually when the user is trying to click, they're also holding still because they're over the target they want to hit, and they're getting ready to click, which means that in the datasets that we build on average is the case that low speeds or desire to hold still is correlated with when the user is attempting to click. Wow, that is really fascinating. It's also not the case. People think that a click is a binary signal. This must be super easy to decode. Well, yes it is, but the bar is so much higher for it to become a useful thing for the user. And there's ways to solve this. I mean, you can sort of take the comp out approach of, well, let's just give the like, let's take 5 seconds to click, let's take a huge window of time so we can be very confident about the answer. But again, world's best mouse, the world's best mouse doesn't take a second to click or 500 milliseconds to click. It takes five milliseconds to click or less. And so if you're aiming for that kind of high bar, then you really want to solve the underlying problem. So maybe this is a good place to ask about how to measure performance. Performance? This whole bits per second. Can you explain what you mean by that? Maybe a good place to start is to talk about webgrid as a game, as a good illustration of the measurement of performance. Yeah, maybe I'll take one zoom out step there, which is just explaining why we care to measure this at all. Again, our goal is to provide the user the ability to control the computer as well as I can, and hopefully better. That means that they can do it at the same speed as what I can do. It means that they have access to all the same functionality that I have, including all those little details like command tab, command space, all this stuff. They need to be able to do it with their brain and with the same level of reliability as what I can do with my muscles. That's a high bar. And so we intend to measure and quantify every aspect of that to understand how we're progressing towards that goal. There's many ways to measure BPS by. This isn't the only way, but we present the user a grid of targets, and basically we compute a score which is dependent on how fast and accurately they can select, and then how small are the targets. And the more targets that are on the screen, the smaller they are, the more information you present per click. And so if you think about it from information theory point of view, you can communicate across different information theoretic channels. And one such channel is a typing interface. You could imagine that's built out of a grid, just like a software keyboard on the screen and bits per second is a measure that's computed by taking the log of the number of targets on the screen. You can subtract one if you care to model a keyboard because you have to subtract one for the delete key on the keyboard. But log of the number of targets on the screen times the number of correct selections minus incorrect, divided by some time window, for example 60 seconds. And that's sort of the standard way to measure a cursor control task in academia. And all credit in the world goes to this great professor, Doctor Chenoy of Stanford, who came up with that task. And he's also one of my inspirations for being in the field. So all the credit in the world to him for coming up with a standardized metric to facilitate this kind of bragging rights that we have now to say that Nolan is the best in the world at this, at this task with its PCI, it's very important for progress that you have standardized metrics that people can compare across different techniques and approaches. How well does this do? So, yeah, big kudos to him and to all the team at Stanford. Yeah. So for Noland and for me playing this task, there's also different modes that you can configure this task. So the webgrid task can be presented as just sort of a left click on the screen, or you could have targets that you just dwell over, or you could have targets that you left right click on. You could have targets that are left, right click, middle click, scrolling, clicking and dragging. You can do all sorts of things within this general framework, but the simplest purist form is just blue targets show up on the screen. Blue means left click. That's the simplest form of the game. And the prior records here in academic work and at Nuralink internally with nhps have all been matched or beaten by Noland with his Narlink device. Prior to Nuralinken, the world record for a human using the device is somewhere between 4.2 to 4.6 bps, depending on exactly what paper you read and how you interpret it. Nolan's current record is 8.5 bps. And again this median neuralinker performance is ten bps. You can think of it roughly as he's 85% the level of control of a median neural linker using their cursor to select blue targets on the screen. I think theres a very interesting journey ahead to get us to that same level of ten bps performance. Its not the case that the tricks that got us from four to six bps and then six to eight bps are going to be the ones that get us from eight to ten. In my view, the core challenge here is really the labeling problem. Its how do you understand at a very, very fine resolution what the user is attempting to do? I highly encourage folks in academia to work on this problem. Whats the journey with Nolan on that quest of increasing the bps on web grid? In March, you said that he selected 89,285 targets in Webgrid. Yep. So he loves this game. He's really serious about improving his performance in this game. So what is that journey of trying to figure out how to improve that performance? How much can that be done on the decoding side? How much can that be done on the calibration side? How much can that be done on the Nolan side of like figuring out how to convey his intention more cleanly? Yeah, no, this is a great question. So in my view, one of the primary reasons why Nolan's performance is so good is because of noland. Noland is extremely focused and very energetic. He'll play web grid sometimes for like 4 hours in the middle of the night, like from 02:00 a.m. to 06:00 a.m. he'll be playing web grid just because he wants to push it to the limits, what he can do. And this is not us asking him to do that. I want to be clear. We're not saying, hey, you should play Webgood tonight. We just gave him the game as part of our research and he is able to play independently and practice whenever he wants. And he really pushes hard to push it. The technology is the absolute limit and he views that as his job really to make us be the bottleneck. And boy has he done that. Well, the first thing to acknowledge is that he was extremely motivated to make this work. Ive also had the privilege to meet other clinical trial participants from brain gate and other trials, and they very much share the same attitude of like, they viewed this as their lifes work to advance the technology as much as they can. And if that means selecting targets on the screen for 4 hours from 02:00 a.m. to 06:00 a.m. then so be it. And theres something extremely admirable about that thats worth calling out. Okay, so now how do you sort of get from where he started, which is no cursor control to Athenae? So, I mean, when he started, there's a huge amount of learning to do on his side and our side to figure out what's the most intuitive control for him, and the most intuitive control for him is you have to find the set intersection of what do we have the signal to decode? So we don't pick up every single neuron in the motor cortex, which means we don't have representation for every part of the body. So there may be some signals that we have better decode performance on than others. For example, on his left hand, we have a lot of difficulty distinguishing his left ring finger from his left middle finger. But on his right hand, we have good control and good modulation detected from the neurons we were able to record for his pinky and his thumb and his index finger. So you can imagine how these different subspaces of modulated activity intersect with what's the most intuitive for him. And this has evolved over time. So once we gave him the ability to calibrate models on his own, he was able to go and explore various different ways to imagine controlling the cursor. For example, he could imagine controlling the cursor. Bye. Wiggling his wrist side to side or by moving his entire arm by. I think at one point he did his feet. You know, he tried like a whole bunch of stuff to explore the space of what is the most natural way for him to control the cursor that at the same time is easy for us to decode. Rolling. Just to clarify, it's through the body mapping procedure there, you're able to figure out which finger he can move. Uh, yes. Yeah, that's one way to do it. Um, maybe one nuance of the, when he's doing it, he can imagine many more things than we represent in that visual on the screen. So we show him sort of abstractly. Heres a cursor. You figure out what works the best for you. And we obviously have hints about what will work best from that body mapping procedure of, you know, we know that this particular action we can represent well, but its really up to him to go and explore and figure out what works the best. But at which point does he no longer visualize the movement of his body and is just visualizing the movement of the cursor? Yeah. How quickly does he go from, how quickly does he get there? So this happened on a Tuesday. I remember this day very clearly because at some point during the, during the day, it looked like he wasn't doing super well. Like it looked like the model wasn't performing super well and he was like getting distracted. But he actually, it wasn't the case. Like, what actually happened was he was trying something new where he was just controlling the cursor. So he wasn't imagining moving his hand anymore. He was just imagining, I don't know what it is, some abstract intention to move the cursor on the screen. And I cannot tell you what the difference between those two things are. I really, truly cannot. He's tried to explain it to me before. I cannot give a first person account of what that's like, but the expletives that he uttered in that moment were enough to suggest that it was a very qualitatively different experience for him to just have direct neural control over a cursor. I wonder if there's a way through ux to encourage a human being to discover that, because he discovered it, like you said to me, that he's a pioneer. So he discovered that on his own through all of this, the process of trying to move the cursor with different kinds of intentions. But that is clearly a really powerful thing to arrive at, which is to let go of trying to control the fingers and the hand and control the actual digital device with your mind. That's right. Ux is how it works. And the ideal UX is one that the user doesn't have to think about what they need to do in order to get it done. They just. It just does it. That is so fascinating. But I wonder, on the biological side, how long it takes for the brain to adapt. So is it just simply learning, like, high level software, or is there, like, a neuroplasticity component where, like, the. The brain is adjusting slowly? Yeah, the truth is, I don't know. Um, I'm very excited to see with sort of the second participant that we implant what the, you know, what the journey is like for them, because we'll have learned a lot more. Potentially, we can help them understand and explore that direction more quickly. This is something I didn't know. This wasn't me prompting Nolan to go try this. He was just exploring how to use his device and figured it out himself. But now that we know that that's a possibility, that maybe there's a way to, for example, hint the user, don't try super hard during calibration. Just do something that feels natural or just directly control the cursor. Don't imagine explicit action. And from there, we should be able to hopefully understand how this is. For somebody who has not experienced that before. Maybe that's the default mode of operation for them. You don't have to go through this intermediate phase of explicit motions, or maybe. If that naturally happens for people, you can just occasionally encourage them to allow themselves to move the cursor. Right. Actually, sometimes just like with a four minute mile, just the knowledge that that's. Possible pushes you to do it. Yeah. Enables you to do it. And then it becomes trivial. And then it also makes you wonder, this is the cool thing about humans. Once there's a lot more human participants, they will discover things that are possible. Yes. And share their experiences and that because. Of them sharing it, they'll be able to do it. All of a sudden that's unlocked for everybody because just the knowledge sometimes is the thing that enables it to do it. Yeah, just comment on that too. We've probably tried a thousand different ways to do various aspects of decoding and now we know what the right subspace is to continue exploring further again, thanks to Nolan and the many hours he's put into this. And so even just that help constrain the beam search of different approaches that we could explore. Really helps accelerate for the next person, you know, the set of things that we'll get to try on day one, how fast we hope to get them to useful control, how fast we can enable them to use it independently and to get value out of the system. So yeah, massive hats off to Nolan and all the participants that came before him, uh, to make this technology a reality. So how often are the updates to the decoder? Because Nolan mentioned like, okay, there's a new update that we're working on and that in the stream he said he plays the snake game because it's like super hard. It's a good way for him to test like how good the update is. So, and he says like sometimes the update is a step backwards. It's like it's a constant, like iteration. So how often, like what is the update entail? Is it mostly on the decoder side? Yeah, a couple of comments. So one is it's probably worth trying distinction between sort of research sessions where we're actively trying different things to understand like what the best approach is versus sort of independent use where we wanted to have ability to just go use a device how anybody would want to use their MacBook. And so what hes referring to is, I think usually in the context of a research session where were trying many, many different approaches to even unsupervised approaches like we talked about earlier, to try to come up with better ways to estimate his true intention and more accurately decode it. And in those scenarios, I mean, we try in any given session, hell sometimes work for like 8 hours a day. And so that can be hundreds of different models. That we would try in that day. Like a lot of different things now, it's also worth noting that we update the application he uses quite frequently. I think sometimes up to four or five times a day, we'll update his application with different features or bug fixes or feedback that he's given us. So he's a very articulate person who is part of the solution. He's not a complaining person. He says, hey, here's this thing that I've discovered is not optimal in my flow. Here's some ideas how to fix it. Let me know what your thoughts are. Let's figure out how to solve it. It often happens that those things are addressed within a couple hours of him giving us his feedback. Thats the kind of iteration cycle well have. And so sometimes at the beginning of the session he will give us feedback, and at the end of the session hes giving us feedback on the next iteration of that process or that setup. Thats fascinating because one of the things you mentioned, that there was 271 pages of notes taken from the BCI sessions, and this was just in March. So one of the amazing things about human beings that they can provide, especially ones who are smart and excited and all positive and good vibes like Nolan, that they can provide feedback, continuous feedback. Yeah. It also requires just to brag on the team a little bit. I work with a lot of exceptional people and it requires the team being absolutely laser focused on the user and what will be the best for them. And it requires a level of commitment of, okay, this is what the user feedback was. I have all these meetings, we're going to skip that today and we're going to do this. That level of focus commitment is, I would say, underappreciated in the world. And also, you obviously have to have the talent to be able to execute on these things effectively. And. Yeah, we have that in loads. Yeah. And this is such an interesting space of UX design because there's so many unknowns here. And I can tell UX is difficult because of how many people do it poorly. It's just not a trivial thing. Yeah. It's also, UX is not something that you can always solve by just constant iterating on different things. Sometimes you really need to step back and think globally. Am I even the right minima to be chasing down for a solution? There's a lot of problems in which fast iteration cycle is the predictor of how successful you will be. As a good example, like in an RL simulation, for example, the more frequently you get reward, the faster you can progress. It's just an easier learning problem the more frequently you get feedback. But UX is not that way. Users are actually quite often wrong about what the right solution is and it requires a deep understanding of the technical system and what's possible, combined with what the problem is you're trying to solve not just how the user expressed it, but what the true underlying problem is to actually get to the right place. Yeah, that's the old like stories of Steve Jobs like rolling in there like, yeah, the user is a good, is a useful signal, but it's not a perfect signal. And sometimes you have to remove the floppy disk drive or whatever the. I forgot all the crazy stories of Steve Jobs like making wild design decisions, but there some of his aesthetic that some of it is about the love you put into the design, which is very much a Steve Jobs Johnny Ive type thing. But when you have a human being using their brain to interact with it, it also is deeply about function. Its not just aesthetic and that you have to empathize with a human being before you while not always listening to them directly. You have to deeply empathize. It's fascinating. It's really, really fascinating. And at the same time iterate. Right. But not iterate in small ways. Sometimes a complete, like rebuilding the design. He said that Nolan said in the early days the UX sucked, but you improved quickly. What was that journey like? Yeah, I mean I'll give one concrete example. So he really wanted to be able to read manga. This is something that he, I mean, yeah, it sounds like a simple thing, but it's actually a really big deal for him. And he couldn't do it with this mouse stick. It just, it wasn't accessible. You can't scroll with the mouse stick on his iPad and on the website that he wanted to be able to use to read the newest manga. And so might be a good quick pause to say the mouth stick is a thing he's using holding a stick in his mouth to scroll on a tablet. Right? Yeah, it's basically, you can imagine it's a stylus that you hold between your teeth. Yeah, it's basically a very long stylus. And its exhausting. It hurts and its inefficient. Yeah. And maybe its also worth calling out. There are other alternative assistive technologies but that particular situation Nolan is in, and this is not uncommon and I think its also not well understood by folks, is that hes relatively spastic. So hell have muscle spasms from time to time. And so any assistive technology that requires him to be positioned directly in front of a camera, for example, an eye tracker, or anything that requires him to put something in his mouth just as a no go, because he'll either be shifted out of frame when he has a spasm, or if he has something in his mouth, it'll stab him in the. In the face, you know, if he spasms too hard. So these kind of considerations are important when thinking about what advantages a PCI has in someone's life. If it. If it fits ergonomically into your life in a way that you can use it independently when your caretaker's not there, wherever you want to, either in the bed or in the chair, depending on, you know, your comfort level and your desire to have pressure sores. You know, all these factors matter a lot in how good the solution is in that user's life. So one of these very fun examples is scroll. So again, manga is something he wanted to be able to read. And there's many ways to do scroll with the BCI. You can imagine different gestures, for example, the user could do that would move the page. But scroll is a very fascinating control surface because it's a huge thing on the screen in front of you. Any sort of jitter in the model output, any sort of error in the model output causes an earthquake on the screen. You really don't want to have your manga page that you're trying to read be shifted up and down a few pixels just because your scroll decoder is not completely accurate. This was an example where we had to figure out how to formulate the problem in a way that the errors of the system, whenever they do occur, and we'll do our best to minimize them. But whenever those errors do occur, that it doesn't interrupt the qualia of the experience that the user is having, it doesn't interrupt their flow of reading their book. What we ended up building is this really brilliant feature. This is a teammate named Roos who worked on this really brilliant work called quick scroll. Quick scroll basically looks at the screen and it identifies where on the screen are scroll bars. It does this by deeply integrating with macOS to understand where are the scroll bars actively present on the screen. Using the accessibility tree that's available to macOS apps, we identified where those scroll bars are and provided a BCI scrollbar. The BCI scroll bar looks similar to a normal scroll bar, but it behaves very differently in that once you move over to it, your cursor morphs onto it. It sort of attaches or latches onto it then once you push up or down in the same way that you'd use a push to control the normal cursor, it actually moves the screen for you. It's basically remapping the velocity to a scroll action. The reason that feels so natural and intuitive is that when you move over to attach to it, it feels like magnetic. So you're stuck onto it, and then it's one continuous action. You don't have to switch your imagined movement. You sort of snap onto it, and then you're good to go. You just immediately can start pulling the page down or pushing it up. And even once you get that right, there's so many little nuances of how this scroll behavior works to make it natural and intuitive. So one example is momentum. Like when you scroll a page with your fingers on the screen, you actually have some flow. It doesn't just stop right when you lift your finger up. The same is true with BCI scroll. So we had to spend some time to figure out what are the right nuances when you don't feel the screen under your fingertip anymore, what is the right dynamic? Or what's the right amount of page give, if you will, when you push it to make it flow. The right amount for the user to have a natural experience reading their book. And there's a million, I mean, I could tell you there's so many little minutiae of how exactly that scroll works that we spent probably, like a month getting right to make that feel extremely natural and easy for the user to navigate. Even the scroll on a smartphone with your finger feels extremely natural and pleasant, and it probably takes an extremely long time to get that right. And actually, the same kind of visionary UX design that we're talking about don't always listen to the users, but also listen to them and also have, like, visionary big, like, throw everything out, think from first principles, but also not. Yeah, yeah. By the way, it just makes me think that scroll bars on the desktop probably have stagnated and never taken that, like, because the snap, same as, like, snap to grid snap, the scroll bar action you're talking about, it's something that could potentially be extremely useful in the desktop setting. Yeah. Even just for users to just improve the experience. Because the current scroll bar experience and the desktop is horrible. Yeah. It's hard to find, hard to control. There's not a momentum. There's. And the intention should be clear. When I start moving towards the scroll bar, there should be a snapping to the scroll bar action. But of course, maybe I'm okay paying that cost. But there's hundreds of millions of people paying that cost non stop. But anyway, but in this case this is necessary because there's an extra cost paid by Nolan for the jitteriness. So you have to switch between the scrolling and the reading. There has to be a phase shift between the two. When you're scrolling, you're scrolling, right? Right. So that is one drawback of the current approach. Maybe one other just sort of case study here. So again, ux is how it works. And we think about that holistically from even the future detection level of what we detect in the brain to how we design the decoder, what we choose to decode to, then how it works once it's being used by the user. So another good example in that sort of how it works once they're actually using the decoder, the output that's displayed on the screen is not just what the decoder says, it's also a function of what's going on on the screen. We can understand, for example, that when you're trying to close a tab, that very small stupid little x that's extremely tiny, which is hard to get precisely hit if you're dealing with a noisy output of the decoder, we can understand that that is a small little x you might be trying to hit and actually make it a bigger target for you. Similar to how when you're typing on your phone if you're used to the iOS keyboard, for example, it actually adapts the target size of individual keys based on an underlying language model. So it'll actually understand if I'm typing. Hey, I'm going to see l. It'll make the e key bigger because it knows Lex is the person I'm going to go see. That kind of predictiveness can make the experience much more smooth, even without improvements to the underlying decoder or feature detection part of the stack. So we do that with a feature called magnetic targets. We actually index the screen and we understand, okay, these are the places that are very small targets that might be difficult to hit. Here's the cursor dynamics around that location that might be indicative of the user trying to select it. Let's make it easier. Let's blow up the size of it in a way that makes it easier for the user to snap onto that target. All these little details, they matter a lot in helping the user be independent in their day to day living. How much of the work on the decoder is generalizable to p two, p three, p four, p five, pn. How do you improve the decoder in a way that's generalizable. Yeah, great question. So the underlying signal we're trying to decode is going to look very different in p two than in p one. For example, channel number 345 is going to mean something different in user one than it will in user two, just because that electrode that corresponds with channel 345 is going to be next to a different neuron in user one versus user two. But the approaches, the methods, the user experience of how do you get the right behavioral pattern from the user to associate with that neural signal? We hoped it will translate over, over multiple generations of users. And beyond that, it's very, very possible, in fact, quite likely, that we've overfit to Nolan's user experience, desires and preferences. And so what I hope to see is that when we get a second 3rd, 4th participant, that we find what the right wide minimas are that cover all the cases that make it more intuitive for everyone. And hopefully there's a cross pollination of things where, oh, we didn't think about that with this user because they can speak, but with this user who just can fundamentally not speak at all, this user experience is not optimal. And those improvements that we make there should hopefully translate that into even people who can speak but don't feel comfortable doing so because they're in a public setting like their doctor's office. So the actual mechanism of open loop labeling and then closed loop labeling will be the same and hopefully can generalize across the different users as they're doing the calibration step. And the calibration step is pretty cool. I mean, that in itself, the interesting thing about web grid, which is like closed loop, it's like fun. I love it when there's like, they used to be kind of idea of human computation, which is using actions a human would want to do anyway to get a lot of signal from. Yeah. And like, what great is that? Like a nice video game that also serves as great calibration. It's so funny. This is, I've heard this reaction so many times before, sort of the, you know, first user was implanted, we had an internal perception that the first user would not find this fun. Yeah. And so we thought really quite a bit, actually, about like, should we build other games that like, are, you know, more interesting for the user so we can get this kind of data and help facilitate research that's, you know, for long durations and stuff like this. Turns out that, like, people love this game. Yeah, I always loved it, but I didn't know that that was a shared perception. Yeah. Just in case it's not clear, Webgrid is. There's a grid of, let's say, 35 by 35 cells, and one of them lights up blue, and you have to move your mouse over that and click on it. And if you miss it and it's red. And I played this game for so many hours. So many hours. And what's your record? You said my. I think I have the highest at Neuralink right now. My record. 17 bps. 17 bps. If you imagine that 35 by 35 grid, you're hitting about 100 trials per minute. So 100 correct selections in that 1 minute window. So you're averaging about, you know, between 500, 600 milliseconds per selection. So one of the reasons I think I struggle with that game is I'm such a keyboard person. So everything is done with your keyboard. If I can avoid touching the mouse, it's great. So how can you explain your high performance? I have, like, a whole ritual I go through when I play web grid. So it's actually like a diet plan associated with this. Like, it's a whole thing. So the first thing, you have to. Fast for five days. I have to go up to the mountain, actually. It kinda. I mean, the fasting thing is important. So this is like, you know, focuses the mind. Yeah, yeah, it's true. So what I do is I actually, I don't eat for a little bit beforehand, and then I'll actually eat like, a ton of peanut butter right before I cook. And I get, this is a real thing. This is a real thing, yeah. And then it has to be really late at night. This is, again, a night owl thing I think we share, but it has to be like, you know, midnight, 02:00 a.m. kind of time window. And I have a very specific, like, physical position I'll sit in, which is, uh, I used to be, I was homeschooled growing up, and so I did most of my work, like, on the floor, uh, just like in my bedroom or whatever. And so I have a very specific situation on the floor. On the floor that I sit and play. And then you have to make sure, like, there's not a lot of weight on your elbow when you're playing so that you can move quickly. And then I turn the gain of the cursor, so the speed of the cursor way, way up. So it's like small motions that actually move the cursor. Are you moving with your wrist or you. You're never. I'm moving, so I my wrist is almost completely still. I'm just moving my fingers. Yeah, you know those. Just in a small tangent. Yeah. The, which I've been meaning to go down this rabbit hole of people that set the world record in Tetris. Those folks, they're playing. There's a way to. Did you see this? All the fingers are moving. Yeah. You could find a way to do it where it's using a loophole, like a bug that you can do some incredibly fast stuff. So it's along that line, but not quite. But you do realize there'll be a few programmers right now listening to this. Cool fast and eat peanut butter. Please check my record. The reason I did this, literally was just because I wanted the bar to be high for the team. The number that we aim for should not be the median performance. It should be like, it should be able to beat all of us. At least that should be the minimum bar. What do you think is possible? Like 20? Yeah, I don't know what the limits. I mean, the limits you can calculate just in terms of screen refresh rate and cursor immediately jumping to the next target. But there's, I mean, I'm sure there's limits before that with just sort of reaction time and visual perception and things like this. I'd guess it's a. In the below 40, but above 20, somewhere in there. It's probably the. Right there. I never be thinking about it. Also matters how difficult the task is. You could imagine some people might be able to do, like, 10,000 targets on the screen, and maybe they can do better that way. So there's some, like, task optimizations you could do to try to boost your performance as well. What do you think it takes for Nolan to be able to do above 85 to keep increasing that number? You said, like, every increase in the number might require different. Yeah, different improvements in the system. Yeah. I think the nature of this work is, the first answer that's important to say is, I don't know. This is, you know, edge of the research. So again, nobody's gotten to that number before. So what's next is going to be, you know, heuristic. A guess from my part. What we've seen historically is that different parts of the stack become bottlenecks at different time points. So, you know, when I first joined Erlink, like, three years ago or so, one of the major problems was just the latency of the Bluetooth connection. It wasn't just like, the radio on the device wasn't super good. It was an early revision of the implant. And it just like no matter how good your decoder was, if your thing is updating every 30 milliseconds or 50 milliseconds, it's just going to be choppy. And no matter how good you are, that's going to be frustrating and lead to challenges. So at that point, it was very clear that the main challenge is just get the data off the device in a very reliable way such that you can enable the next challenge to be tackled. Then at some point it was actually the modeling challenge of how do you just build a good mapping? Like the supervised learning problem of you have a bunch of data and you have a label, you're trying to predict just what is the right neural decoder architecture and hyperparameters to optimize that. That was a problem for a bit. Once you solve that, it became a different bottleneck. I think the next bottleneck after that was actually just software stability and reliability. If you have widely varying inference latency in your system, or your app just lags out every once in a while, it decreases your ability to maintain and get in a state of flow, and it basically just disrupts your control experience. There's a variety of different software bugs and improvements we made that basically increased the performance of the system, made it much more reliable, much more stable, and led to a state where we could reliably collect data to build better models with. That was a bottleneck for a while. It's just the software stack itself. If I were to guess right now, there's two major directions you could think about for improving vps further. The first major direction is labeling. Labeling is again this fundamental challenge of, given a window of time where the user is expressing some behavioral intent, what are they really trying to do at the granularity of every millisecond? That again, is a task design problem. It's a UX problem, it's a machine learning problem. It's a software problem, sort of touches all those different domains. The second thing you can think about to improve BPS further is either completely changing the thing you're decoding, or just extending the number of things that you're decoding. This is serving the direction of functionality, basically, you can imagine giving more clicks. For example, left click, a right click, a middle click, different actions like click and drag, for example. And that can improve the effective bitrate of your communication prosthesis. If you're trying to allow the user to express themselves through any given communication channel, you can measure that with bits per second. But what actually matters at the end of the day is how effective are they at navigating their computer. And so from the perspective of the downstream tasks that you care about functionality, and extending functionality is something we're very interested in, because not only can it improve the sort of number of vps, but it can also improve the downstream sort of independence that the user has and the skill and efficiency with which they can operate their computer. Would the number of threads increasing also potentially help? Yes. Short answer is yes. It's a bit nuanced how that curve, or how that manifests in the numbers. What you'll see is that if you plot a curve of number of channels that you're using for decode, versus either the offline metric of how good you are at decoding, or the online metric of in practice, how good is the user using this device, you see roughly a log curve. So as you move further out in number of channels, you get a corresponding logarithmic improvement in control quality and offline validation metrics. The important nuance here is that each channel corresponds with a specific represented intention in the brain. So for example, if you have a channel 254, it might correspond with moving to the right. Channel 256 might mean move to the left. If you want to expand the number of functions you want to control, you really want to have a broader set of channels that covers a broader set of imagined movements. You can think of it kind of like Mister potato man, actually. If you had a bunch of different imagined movements you could do, how would you map those imagined movements to input to a computer? You could imagine handwriting to output characters on the screen. You could imagine just typing with your fingers and have that output text on the screen. You could imagine different finger modulations for different clicks. You can imagine wiggling your big nose for opening some menu, or wiggling your big toe to have command tab occur, or something like this. It's really the amount of different actions you can take in the world depends on how many channels you have and the information content that they carry, right? So that's more about the number of actions. So actually, as you increase the number of threads, that's more about increasing the number of actions you're able to perform. One other nuance there that is worth mentioning. So again, our goal is really to enable a user with paralysis to control the computer as fast as I can. So that's bps with all the same functionality I have, what we just talked about, but then also as reliably as I can. And that last point is very related to channel count discussion. So as you scale out number of channels, the relative importance of any particular feature of your model input to the output control of the user diminishes, which means that if the sort of neural non stationary effect is per channel, or if the noise is independent, such that more channels means, on average, less output effect, then your reliability of your system will improve. So one sort of core thesis that at least I have is that scaling channel count should improve the reliability system without any work on the decoder itself. Can you linger on reliability here? So, first of all, when you see non stationarity of the signal, which aspect are you referring to? Yeah, so maybe let's talk briefly what the actual underlying signal looks like. So again, I spoke very briefly at the beginning about how when you imagine moving to the right or imagine moving to the left, neurons might fire more or less, and their frequency content of that signal, at least in the motor cortex, is very correlated with the output intention, the behavioral task that the user is doing. You can imagine, actually, this is not obvious, that rate coding, which is the name of that phenomenon, is the only way the brain could represent information. You can imagine many different ways in which the brain could encode intention. There's actually evidence in bats, for example, that there's temporal codes, timing codes, of exactly when particular neurons fire is the mechanism of information representation. But at least in the motor cortex, there's substantial evidence that it's rate coding, or at least one first order effect is that it's rate coding. So then, if the brain is representing information by changing the frequency of a neuron firing, what really matters is the delta between the baseline state of the neuron and what it looks like when it's modulated. What we've observed and what has also been observed in academic work is that that baseline rate sort of the, if you're to tar the scale, if you imagine, uh, that analogy for, like, measuring, you know, flour or something when you're baking, that baseline state of how much the pot weighs is actually different day to day. And so if what you're trying to measure is how much rice is in the pot, you're going to get a different measurement, different days because you're measuring with different pots. So that baseline rate shifting is really the thing that, uh, at least from a first order description of the problem is what's causing this downstream bias. There can be other effects, nonlinear effects on top of that, but at least a very first order description of the problem, that's what we observe day to day, is that the baseline firing rate of any particular neuron or observed on a particular channel is changing. So can you just adjust to the baseline to make it relative to the baseline nonstop? Yeah, this is a great question. So, um, with monkeys, we have found various ways to do this. Um, one example would do this is you ask them to do some behavioral task, like play the game with a joystick. You measure what's going on in the brain. You compute some mean of what's going on across all the input features, and you subtract that in the input when you're doing your BCI session. Works super well for whatever reason, that doesn't work super well with Nolan. I actually don't know the full reason why, but I can imagine several explanations. One such explanation could be that the context effect difference between some open loop task and some closed loop task is much more significant with Nolan than it is with monkey. Maybe in this open loop task, he's watching the Lex Freeman podcast while he's doing the task, or hes whistling and listening to music and talking with his friend and ask his mom whats for dinner while hes doing this task. And so the exact difference in context between those two states may be much larger and thus lead to a bigger generalization gap between the features that youre normalizing at open loop time and what youre trying to use at closed loop time. Thats interesting. Just on that point, its kind of incredible to watch Nolan be able to multitask, to do multiple tasks at the same time, to be able to move the mouse cursor effectively while talking and while being nervous because he's talking in front of me. Kicking my ass in chest, too. Yeah, kicking your ass. And now we're and talk trash while doing it. So all at the same time. And yes, if you're trying to normalize to the baseline, that might throw everything off. Boy, is that interesting. Maybe one comment on that, too. For folks that aren't familiar with assistive technology, I think there's a common belief that, you know, well, why can't you just use an eye tracker or something like this for helping somebody move a mouse on the screen? And it's really a fair question, and one that I actually did was not confident before Noland that this was going to be a profoundly transformative technology for people like him. And I'm very confident now that it will be. But the reasons are subtle. It really has to do with ergonomically how it fits into their life, even if you can just offer the same level of control as what they would have with an eye tracker or with a mouse stick. But you don't need to have that thing in your face. You don't need to be positioned a certain way. You don't need your caretaker to be around to set it up for you. You can activate it when you want, how you want, wherever you want. That level of independence is so game changing for people. It means that they can text a friend at night, privately without their mom needing to be in the loop. It means that they can, like, open up, you know, and browse the Internet at 02:00 a.m. when nobody's around to set their iPad up for them. This is like a profoundly game changing thing for folks in that situation. And this is even before we start talking about folks at may not be able to communicate at all or ask for help when they want to. This can be potentially the only link that they have to the outside world. And yeah, that one doesn't, I think, need explanation of why that's so impactful. You mentioned neural decoder. How much machine learning is in the decoder, how much magic, how much science, how much art, how difficult is it to come up with a decoder that figures out what these sequence of spikes mean? Yeah, good question. There's a couple of different ways to answer this, so maybe I'll zoom out briefly first, and then I'll go down one of the rabbit holes. So the zoomed out view is that building the decoder is really the process of building the dataset plus compiling it into the weights and each of those steps is important. The direction, I think, of further improvement is primarily going to be in the data set side of how do you construct the optimal labels for the model. But there's an entirely separate challenge of then how do you compile it in the best model. And so I'll go briefly down the second one, down the second rabbit hole. One of the main challenges with designing the optimal model for BCI is that offline metrics don't necessarily correspond to online metrics. It's fundamentally a control problem. The user is trying to control something on the screen, and the exact user experience of how you output the intention impacts their ability to control. For example, if you just look at validation loss as predicted by your model, there can be multiple ways to achieve the same validation loss. Not all of them are equally controllable by the end user. It might be as simple as saying, oh, you could just add auxiliary loss terms that help you capture the thing that actually matters. But this is a very complex, nuanced process. How you turn the labels into the model is more of a nuanced process than just a standard supervised learning problem. One very fascinating anecdote here. We've tried many different neural network architectures that translate brain data to velocity outputs. For example, one example that stuck in my brain from a couple of years ago now is at one point we were using just fully connected networks to decode the brain activity. We tried a b test where we were measuring the relative performance in online control sessions of 1D Convolution over the input signal. If you imagine per channel, you have a sliding window that's producing some convolved feature for each of those input sequences for every single channel simultaneously, you can actually get better validation metrics, meaning you're fitting the data better and it's generalizing better on offline data. If you use this convolutional architecture, you're reducing parameters. It's a standard procedure when you're dealing with time series data. Now, it turns out that when using that model online, the controllability was worse. It was far worse, even though the offline metrics were better. There can be many ways to interpret that. But what that taught me at least, was that, hey, it's at least the case right now that if you were to just throw a bunch of compute at this problem and you were trying to hyperparameter optimize or let some GPT model hard code or come up with or invent many different solutions, if you were just optimizing for loss, it would not be sufficient, which means that there's still some inherent modeling gap here. There's still some artistry left to be uncovered here of how to get your model to scale with more compute. And that may be fundamentally labeling problem, but there may be other components to this as well. Is it data constraint at this time, which is what it sounds like. How do you get a lot of good labels? Yeah, I think it's data quality constrained, not necessarily data quantity constrained, but even. Just the quantity because it has to be trained on the interactions. I guess theres not that many interactions. Yeah. So it depends what version of this youre talking about. So if youre talking about, lets say the simplest example of just 2d velocity, then I think, yeah, data quality is the main thing. If youre talking about how to build a multifunction output that lets you do all the inputs to the computer that you and I can do, then its actually a much more sophisticated nuanced modeling challenge, because now you need to think about not just when the users left clicking, but when youre building the left click model. You also need to be thinking about how to make sure it doesnt fire when theyre trying to right click or when they're trying to move the mouse. So one example of an interesting bug from week one of BCI with Nolan was when he moved the mouse, the click signal dropped off a cliff, and when he stopped, the click signal went up. So again, there's a contamination between the two inputs. Another good example was at one point he was trying to do a left click and drag, and the minute he started moving, the left click signal dropped off a cliff again. Because there's some contamination between the two signals. You need to come up with some way to, either in the dataset or in the model, build robustness against this kind of, uh, you think of it like overfitting, but really it's just that the model has not seen this kind of variability before. So you need to find some way to help the model with that. This is super cool, because I, it feels like all of this is very solvable, but it's hard. Yes, it is fundamentally an engineering challenge. This is important to emphasize, and it's also important to emphasize that it may not need fundamentally new techniques, which means that, you know, people who work on, let's say, unsupervised speech classification, using CTC loss, for example, with internal siri, they could potentially have very applicable skills to this. So what things are you excited about in the future development of the software stack on Neuralink? So everything we've been talking about, the decoding, the UX, I think there's some. I'm excited about, like something I'm excited about from the technology side, and some I'm excited about understanding how this technology is going to be best situated for entering the world. So I'll work backwards on the technology entering the world side of things. I'm really excited to understand how this device works for folks that cannot speak at all, that have no ability to bootstrap themselves into useful control by voice command, for example, and are extremely limited in their current capabilities, I think that will be an incredibly useful signal for us to understand, really what is an existential threat for all startups, which is product market fit. Does this device have the capacity and potential to transform people's lives in the current state? And if not, what are the gaps? And if there are gaps, how do we solve them most efficiently? So that's what I'm very excited about for the next year or so of clinical trial operations. The technology side, I'm quite excited about basically everything we're doing. I think it's going to be awesome. The most prominent one, I would say, is scaling channel count. Right now we have a thousand channel device. The next version will have between three and 6000 channels. And I expect that curve to continue in the future. And it's unclear what set of problems will just disappear completely at that scale and what set of problems will remain and require further focus. And so I'm excited about the clarity of gradient that that gives us in terms of the user experience that we choose to focus our time and resources on. And also in terms of the even things as simple as non stationary. Like does that problem just completely go away at that scale? Or do we need to come up with new creative uxs still even at that point? And also when you get to that time point, when we start expanding out dramatically the set of functions that you can output from one brain, how to deal with all the nuances of both the user experience of not being able to feel the different keys under your fingertips, but still needing to be able to modulate all of them in synchrony to achieve the thing you want. Again, you don't have that properly set to feedback loop, so how can you make that intuitive? For a user to control a high dimensional control surface without feeling the thing physically, I think that's going to be a super interesting problem. I'm also quite excited to understand, uh, you know, do these scaling laws continue like as you scale channel count, how much further out do you go before that saturation point is, is truly hit? And it's not obvious today? I think we only know what's in the sort of interpolation space. We only know what's between zero and 1024, but we don't know what's beyond that. Um, and then there's a whole sort of like range of interesting sort of neuroscience and brain questions, which is when you stick more stuff in the brain, in more places, you get to learn much more quickly about what those brain regions represented. And so I'm excited about that fundamental neuroscience learning, which is also important for figuring out how to most efficiently insert electrodes in the future. So yeah, I think all those dimensions I'm really, really excited about that doesn't get close to touching the sort of software stack that we work on every single day and what we're working on right now. Yeah, it seems virtually impossible to me that 1000 electrodes is where it saturates. It feels like this would be one of those silly notions in the future where obviously you should have millions of electrodes. And this is where the true breakthroughs happen. You tweeted, some thoughts are most precisely described in poetry. Why do you think that is? I think it's because the information bottleneck of language is pretty steep, and yet youre able to reconstruct on the other persons brain more effectively without being literal. If you can express the sentiment such that in their brain, they can reconstruct the actual true underlying meaning and beauty of the thing that youre trying to get across. The generator function in their brain is more powerful than what language can express. And so the mechanism of poetry is really just to feed or seed that generator function. So being literal sometimes is a suboptimal compression for the thing you're trying to convey. And it's actually in the process of the user going through that generation that they understand what you mean. That's the beautiful part. It's also, when you look at a beautiful painting, it's not the pixels of the painting that are beautiful. It's the thought process that occurs when you see the experience of that that actually is the thing that matters. Yeah. It's resonating with some deep thing within you that the artist also experienced and was able to convey that through the pixels. And that's actually going to be relevant for full on telepathy. You know, it's like if you just read the poetry literally, that doesn't say much of anything interesting. It requires a human to interpret it. So it's the combination of the human mind and all the experiences that human being has within the context of the collective intelligence of the human species that makes that poem make sense, and they load that in. And so in that same way, the signal that carries from human to human meaning might not, may seem trivial, but may actually carry a lot of power because of the complexity of the human mind and the receiving end. Yeah, that's interesting. Poetry still doesn't. Who was it? I think Yosha Bacho first was said something about all the people that think we've achieved AGI. Explain why humans like music. Oh, yeah. And until the GI likes music, you haven't achieved AGI or something like that. Do you not think that's like some next token entropy, surprise kind of thing going on there? I don't know. I don't know either. I listen to a lot of classical music and also read a lot of poetry. And, yeah, I do wonder if there is some element of the next token surprise factor going on there. Yeah, maybe because, I mean, like, a lot of the tricks in both poetry and music are basically, you have some repeated structure, and then you do like a twisted. Like, it's like, okay, clause one, two, three is one thing, and then clause four is like, okay, now we're onto the next theme. And they kind of play with exactly when the surprise happens and the expectations of the user. And that's even true, like, through history, as musicians evolve music, they take some known structure that people are familiar with, and they just tweak it a little bit. Like, they tweak and add a surprising element. This is especially true in, like, in classical music heritage. But that's what I'm wondering. Is it all just entropy? Like the. So breaking structure or breaking symmetry is something that humans seem to like, maybe as simple as that. Yeah. And, I mean, great artists copy, and they also, you know, knowing which rules to break is the important part. And fundamentally, it must be about the listener of the piece. Like, which rule is the right one to break? It's about the user or the audience member perceiving that as interesting. What do you think is the meaning of human existence? There's a tv show I really like called the West Wing. And in the West Wing, there's a character, he's the president of the United States, who's having a discussion about the Bible with one of their colleagues. And the colleague says something about the Bible says X, y, and Z. And the president says, yeah, but it also says ABC. And the person says, well, do you believe the Bible to be literally true? And the president says, yes, but I also think that neither of us are smart enough to understand it. I think the analogy here for the meaning of life is that largely, we don't know the right question to ask. And so I think I'm very aligned with the hitchhiker's guide, the galaxy version of this question, which is basically, if we can ask the right questions, it's much more likely we find the meaning of human existence. And so in the short term, as a heuristic in the sort of search policy space, we should try to increase the diversity of people asking such questions, or generally of consciousness and conscious beings asking such questions. So, again, I think I'll take the I don't know card here, but say I do think there are meaningful things we can do that improve the likelihood of answering that question. It's interesting how much value you assigned to the task of asking the right questions. That's the main thing, is not the answers, is the questions. This point, by the way, is driven home in a very painful way when you try to communicate with someone who cannot speak. Because a lot of the time, the last thing to go is they have the ability to somehow wiggle a lip or move something that allows them to say yes or no. And in that situation, it's very obvious that what matters is, are you asking them the right question to be able to say yes or no to. Wow, that's powerful. Well, bliss, thank you for everything you do, and thank you for being you. And thank you for talking today. Thank you. Thanks for listening to this conversation with Bliss Chapman. And now, dear friends, here's Nolan Arbaugh, the first human being to have a neuralink device implanted in his brain. You had a diving accident in 2016 that left you paralyzed with no feeling from the shoulders down. How did that accident change your life? There's sort of a freak thing that happened. Imagine you're running into the ocean, although this is a lake, but you're running into the ocean, and you get to about waist high, and then you kind of, like, dive in, take the rest of the plunge under the wave or something. That's what I did. And then I just never came back up. But not sure what happened. I did it running into the water with a couple of guys. And so my idea of what happened is really just that I took, like, a stray fist, elbow, knee, foot, something to the side of my head. The left side of my head was sore for about a month afterwards, so must have taken a pretty big knock. And then they both came up, and I didn't. And so I was face down in the water for a while. I was conscious, and then eventually just, you know, realized I couldn't hold my breath any longer. And I keep saying, took a big drink. People, I don't know if they like that I say that it seems like I'm making light of it all, but this is kind of how I am. And I don't know, like, I'm a very relaxed, sort of stress free person. I rolled with the punches for a lot of this. I kind of took it in stride. It's like, all right, well, what can I do next? How can I improve my life even a little bit on a day to day basis? At first, just trying to find some way to heal as much of my body as possible, to try to get healed, to try to get off a ventilator, learn as much as I could so I could somehow survive once I left the hospital. And then, thank God, I had my family around me. If I didn't have my parents, my siblings, then I would have never made it this far. They've done so much for me. Um, more than, like, I can ever thank them for, honestly. And a lot of people don't have that. A lot of people in my situation, their families either aren't capable of providing for them or honestly just don't want to. And so they get placed somewhere and, you know, in some sort of home. Uh, so thankfully, I had my family. I have a great group of friends, a great group of buddies from college who have all rallied around me, and we're all still incredibly close. People always say, you know, if you're lucky, you'll end up with one or two friends from high school that you keep throughout your life. I have about 1010 or twelve from high school that have all stuck around. And we still get together, all of us, twice a year. We call it the spring series and the fall series. This last one we all did, we dressed up like X Men. So I did Professor Xavier, and it was freaking awesome. It was so good. So, yeah, I have such a great support system around me. And so, you know, being a quadriplegic isn't that bad. I get weighted on all the time. People bring me food and drinks and I get to sit around and watch as much tv and movies and anime as I want. I get to read as much as I want. I mean, it's, it's great. It's beautiful to see that. You see, the silver lining in all of this was just going back. Do you remember the moment when you first realized you're paralyzed from the neck down? Yep. I was face down in the water right when I. Whatever. Something hit my head. I tried to get up and I realized I couldn't move. And it just sort of clicked. I'm like, all right, I'm paralyzed, can't move. What do I do? If I can't get up? I can't flip over, can't do anything. Then I'm going to drown eventually. And I knew I couldn't hold my breath forever, so I just held my breath and thought about it for maybe 1015 seconds. I've heard from other people that like onlookers, I guess the two girls that pulled me out of the water were two of my best friends. They are lifeguards. And one of them said that it looked like my body was sort of shaking in the water, like I was trying to flip over and stuff. But I knew. I knew immediately. And I just kind of, I realized that thats what my situation was from here on out. Maybe if I got to the hospital, theyd be able to do something. When I was in the hospital right before surgery, I was trying to calm one of my friends down. I had brought her with me from college to camp and she was just bawling over me and I was like, hey, its going to be fine, dont worry. I was cracking some jokes to try to lighten the mood. The nurse had called my mom and I was like, dont tell my mom. She's just going to be stressed out. Call her after I'm out of surgery because at least she'll have some answers then, like whether I live or not. Really. And I didn't want her to be stressed through the whole thing, but I knew. And then when I first woke up after surgery, I was super drugged up. They had me on fentanyl, like three ways, which was awesome. I don't recommend it, but I saw, I saw some crazy stuff on that fentanyl and it was still the best I've ever felt. On drugs. Medication. Sorry, on medication. And I remember the first time I saw my mom in the hospital. I was just bawling. I had like ventilator in like, I couldn't talk or anything and I just started crying because it was more like seeing her. Not that, I mean, the whole situation obviously was pretty rough, but it was just like seeing her face for the first time was pretty hard. But, yeah, I never had like a moment of, you know, man, I'm paralyzed. This sucks. I don't want to be around anymore. It was always just, I hate that I have to do this, but, like, sitting here and wallowing isn't going to help. So immediate acceptance? Yeah, yeah. Has there been low points along the way? Yeah, yeah, sure. I mean, there are days when I don't really feel like doing anything. Not so much anymore. Like, not for the last couple years. I don't really feel that way. I've more so just wanted to try to do anything possible to make my life better at this point. But at the beginning, there were some ups and downs. There were some really hard things to adjust to. First off, just like the first couple months, the amount of pain I was in was really, really hard. I mean, I remember screaming at the top of my lungs in the hospital because I thought my legs were on fire and obviously I cant feel anything, but its all nerve pain. And so that was a really hard night. I asked them to give me as much pain meds as possible. Theyre like, youve had as much as you can have, so just kind of deal with it, go to a happy place sort of thing. So that was a pretty low point. And then every now and again, its hard, like realizing things that I wanted to do in my life that I wont be able to do anymore. I always wanted to be a husband and father and I just don't think that I could do it now. As a quadriplegic, maybe it's possible, but I'm not sure I would ever put someone I love through that, like having to take care of me and stuff, not being able to go out and play sports. I was a huge athlete growing up, so that was pretty hard. Little things too. When I realize I can't do them anymore, like, there's something really special about being able to hold a book and smell a book. Like the feel, the texture, the smell. Like, as you turn the pages, like, I just love it, I can't do it anymore. It's little things like that. The two year mark was pretty rough. Two years is when they say you will get back basically as much as you're ever going to get back as far as movement and sensation goes. And so for the first two years, that was the only thing on my mind was like, try as much as I can to move my fingers, my hands, my feet, everything possible to try to get sensation and movement back. And then when the two year mark hit, so June 30, 2018, I was really sad. That's kind of where I was, and then just randomly here and there. But I was never depressed for long periods of time. It never seemed worthwhile to me. What gave you strength? My faith. My faith in God was a big one. My understanding that it was all for purpose. And even if that purpose wasn't anything involving neuralink, even if that purpose was, you know, there's, there's a story in the Bible about job, and I think it's a really, really popular story about how job, you know, has all of these terrible things happen to him, and he praises God throughout the whole situation. I thought, and I think a lot of people think for most of their lives that they are job, that they're the ones going through something terrible and they just need to, you know, praise God through the whole thing and everything will work out. At some point after my accident, I realized that I might not be job, that I might be, you know, one of his children that gets killed or kidnapped or taken from him. And so it's about terrible things that happen to those around you who you love. So maybe, you know, in this case, my mom would be job, and she has to get through something extraordinarily hard. And I just need to try and make it as best as possible for her because shes the one thats really going through this massive trial. And that gave me a lot of strength. And obviously my family, my family and my friends, they give me all the strength that I need on a day to day basis. Makes things a lot easier. Having that great support system around me. From everything I've seen of you online, your streams, and the way you are today, I really admire, let's say, your unwavering, positive outlook on life. Has that always been this way? Yeah. Yeah, I've, I mean, I've just always thought I could do anything I ever wanted to do. There was never anything too big. Whatever I set my mind to, I felt like I could do it. I didn't want to do a lot. I wanted to travel around and be sort of like a gypsy and go work odd jobs. I had this dream of traveling around Europe and being, I don't know, a shepherd in Wales or Ireland and then going and being a fisherman in Italy, doing all these things for like a year. Like, it's such cliche things, but I just thought it would be so much fun to go and travel and do different things. And so I've always just seen the best in people around me, too. And I've always tried to be good to people. And growing up with my mom, too, she's like the most positive, energetic person in the world. And we're all just people people. Like, I just get along great with people. I really enjoy meeting new people, and so I just wanted to do everything. This is kind of just how I've been. It's just great to see that cynicism didn't take over, given everything you've been through. Yeah, that's. Was that, like, a deliberate choice you made, that you're not going to let this keep you down? Yeah, a bit. Also, like, I just, just kind of how I am. I just, like I said, I roll with the punches with everything. I always used to tell people, like, I don't stress about things much. And whenever I'd see people getting stressed, I just say, you know, like, it's not hard, just don't stress about it. And, like, that's all you need to do. And they're like, that's not how that works. Like, it works for me. Like, just don't stress and everything will be fine. Like, everything will work out. Obviously, not everything always goes well, and it's not like it all works out for the best all the time. But I just don't think stress has had any place in my life since I was a kid. What was the experience like of you being selected to be the first human being to have a neural link device implanted in your brain? Are you scared? Excited? No. No. It was cool. Like, I was. I was never afraid of it. I had to think through a lot. Should I. Should I do this? Like, be the first person? I could wait until number two or three and get a better version of the neuralink. Like, the first one might not work. Maybe. It's actually going to kind of suck. Um. It's going to be the worst version ever in a person. So why would I do the first one? Like, I've already kind of been selected. I could just tell them, you know, like, okay, find someone else, and then I'll do number two or three. Like, I'm sure they would let me. They're looking for a few people anyways. But ultimately I was like, I don't know. There's something about being the first one to do something. It's pretty cool. I always thought that if I had the chance, that I would like to do something for the first time. This seemed like a pretty good opportunity, and I was never scared. I think my faith had a huge part in that. I always felt like God was preparing me for something. I almost wish it wasn't this, because I had many conversations with God about not wanting to do any of this as a quadriplegic. I told him, you know, I'll go out and talk to people. I'll go out and travel the world and talk to stadiums. Thousands of people. Give my testimony, I'll do all of it, but heal me first. Don't make me do all this in a chair. That sucks. And I guess he won that argument. I didn't really have much of a choice. I always felt like there was something going on, and to see how, I guess, easily, I made it through the interview process and how quickly everything happened, how the star sort of aligned with all of this. It just told me, as the surgery was getting closer, it just told me that it was all meant to happen, it was all meant to be, and so I shouldn't be afraid of anything that's to come. And so I wasn't. I kept telling myself, like, you know, you say that now, but as soon as the surgery comes, you're probably going to be freaking out like you're about to have brain surgery. And brain surgery is a big deal for a lot of people, but it's an even bigger deal for me. Like, it's all I have left the amount of times I've been like, thank you, God, that you didn't take my brain and my personality and my ability to think, my, like, love of learning, like, my character, everything. Like, thank you so much. Like, as long as you left me, that then I think I can get by. And I was about to let people go, like, root around, and they were like, hey, we're going to go put some stuff in your brain. Like, hopefully it works out. And so it was something that gave me pause, but like I said, how smoothly everything went. I never expected for a second that anything would go wrong. Plus, the more people I met on the borrows side and on the knurling side, they're just the most impressive people in the world. Like, I can't speak enough to how much I trust these people with my life and how impressed I am with all of them. And to see the excitement on their faces, to, like, walk into a room and roll into a room and see all of these people looking at me, like, we're just. We're so excited. Like, we've been working so hard on this, and it's finally happening. It's super infectious, and it just makes me want to do it even more and to help them achieve their dreams, like, I don't know. It's so rewarding, and I'm so happy for all of them, honestly. What was the day of surgery like? When did you wake up? What'd you feel? Minute by minute? Yeah. Were you freaking out? No, no. I thought I was going to, but as surgery approached the night before, the morning of, I was just excited. I was like, let's make this happen. I think I said that something like that to elon on the phone beforehand. We were, like, facetiming. And I was like, let's rock and roll. And he's like, let's do it. I don't know. I wasn't scared. So we woke up. I think we had to be at the hospital at 05:30 a.m. i think surgery was at 07:00 a.m. so we woke up pretty early. I'm not sure much of us slept that night. Got to the hospital, 530, went through, like, all the pre op stuff. Everyone was super nice. Elon was supposed to be there in the morning, but something went wrong with his plane, so we ended up facetiming. That was cool. Had one of the greatest one liners of my life after that phone call, hung up with him. There were, like, 20 people around me, and I was like, I just hope I. He wasn't too starstruck. Talking to me. Nice. Yeah, it was good. Well done. Yeah. Yeah. Did you write that ahead of time? No, it just came to me. I was like, this is. This seems right, you know, went into surgery. I asked if I could pray right beforehand, so I, like, prayed over the room. I asked God, if you, like, be with my mom in case anything happened to me and just, like, calm her nerves out there. Woke up, played a bit of a prank on my mom. I don't know if you've heard about it. Yeah, I read about it. Yeah, she was, she was not happy. Can you take me through the prank? Yeah, this is. Do you regret doing that now? No, no, not one bit. It was something. It was something I had talked about ahead of time with my buddy bane. I was like, I would really like to play a prank on my mom. Very specifically, my mom, she's very gullible. I think she had knee surgery once even. And after she came out of knee surgery, she was super groggy. She's like, I can't feel my legs. And my dad looked at her, he was like, you don't have any legs. They had to amputate both your legs. We just do very mean things to her all the time. Yes. I'm so surprised that she still loves us. But right after surgery, I was really worried that I was going to be too groggy, not all there. I had had anesthesia once before, and it messed me up. I could not function for a while afterwards, and I said a lot of things that I was really worried that I was going to start, I don't know, like, dropping some bombs, and I wouldn't even know. I wouldn't remember. So I was like, please, God, don't let that happen, and please let me be there enough to do this to my mom. And so she walked in after surgery. It was like the first time they had been able to see me after surgery. And she just looked at me. She said, hi, how are you? How are you doing? How do you feel? And I looked at her, and this very, I think the anesthesia helped. Very, like, groggy, sort of confused look on my face. It's like, who are you? And she just started looking around the room, like, at the surgeons, at the doctors. Like, what did you do to my son? Like, you need to fix this right now. Tears started streaming. I saw how much she was freaking out. I was like, I can't let this go on. And so I was like, mom, mom, I'm fine. Like, it's all right. And still she was not happy about it. She still says she's going to get me back someday, but, I mean, I don't know. I don't know what that's going to look like. It's a lifelong battle. Yeah. It was good in some sense. It was a demonstration that you still got. That's, that's all I wanted. That's all I wanted it to be. And I knew that doing something super mean to her like that would show her. Yeah, to show that you're still there, that you love her. Yeah, exactly. Exactly. It's a dark way to do it, but I love it. Yeah. What was the first time you were able to feel that you can use the neuralink device to affect the world around you? Yeah. The first little taste I got of it was actually not too long after surgery. Some of the neuralink team had brought in, like, a little iPad, a little tablet screen, and they had put up eight different channels that were recording some of my neuron spikes. And they put it in front of me like, this is real time, your brain firing. Like, that's super cool. My first thought was, I mean, if theyre firing now, lets see if I can affect them in some way. So I started trying to wiggle my fingers, and I just started scanning through the channels, and one of the things I was doing was moving my index finger up and down, and I just saw this yellow spike on top row, like, third box over or something. I saw this yellow spike every time I did it. And I was like, oh, thats cool. And everyone around me was just like, what are you seeing? I was like, look, look at this one. Look at, like, this top row, third box over, this yellow spike. Like, that's me right there. There, there. And everyone was freaking out. They started, like, clapping. I was like, that's super unnecessary. Like, this is what's supposed to happen, right? Like, so you're imagining yourself moving each individual finger one at a time. Yeah. And then seeing, like, you can notice something. And then when you did the index finger, you're like, oh, yeah, I was. I was wiggling kind of all of my fingers to see if anything would happen. There was a lot of other things going on, but that big yellow spike was the one that stood out to me. Like, I'm sure that if I would have stared at it long enough, I could have mapped out maybe a hundred different things. But the big yellow spike was the one that I noticed. Maybe you could speak to what it's like to sort of wiggle your fingers, to, like, to imagine that the mental the cognitive effort required to sort of wiggle your index finger, for example. How easy is that to do? Pretty easy. For me, it's something that, at the very beginning, after my accident, they told me to try and move my body as much as possible. Even if you can't just keep trying, because that's going to create new neural pathways, or pathways, in my spinal cord to reconnect these things, um, to hopefully regain some movement someday. That's fascinating. Yeah, I know, it's. It's bizarre, but that's part of the. Recovery process, is to keep trying to move your body. Yep. And as much as you can, and the nervous system does its thing, it starts reconnecting. It'll start reconnecting. Um, for some people. Some people, it never works. Some people, they'll do it. Like, for me, I got some bicep control back, and that's about it. I can. If I try enough, I can wiggle some of my fingers. Not like, on command, it's more like, if I try to move, say, my right pinky, and I just keep trying to move it, after a few seconds, it'll wiggle. So I know there's stuff there. I know that happens with a few different. Of my fingers and stuff, but, yeah, that's what they tell you to do. One of the people at the time, when I was in the hospital, came in and told me, for one guy who had recovered most of his control, what he thought about every day was actually walking. Like the act of walking just over and over again. So I tried that for years. I tried just imagining walking, which is. It's hard. It's hard to imagine, like, all of the steps that go into, well, taking a step, like, all of the things that have to move, like, all the activations that have to happen along your leg in order for one step to occur. But you're not just imagining, you're doing it, right? I'm trying, yeah. So it's like, it's imagining over again what I had to do to take a step, because it's not something any of us think about. We just. You want to walk and you take a step. You don't think about all of the different things that are going on in your body. So I had to recreate that in my head as much as I could, and then I practice it over and over and over. So it's not like a third person perspective as a first person perspective, you're like. It's not like you're imagining yourself walking. You're, like, literally doing this. Everything all the same stuff as if you're walking, which. Which was hard. It was hard at the beginning, like. Frustrating hard, or like, actually cognitively hard, like, which way? It was both. There's a. There's a scene in one of the kill Bill movies, actually, oddly enough, where she is, like, paralyzed, I don't know, from, like, a drug that was in her system. And then she, like, finds some way to get into the back of a truck or something, and she stares at her toe and she says, move, like, move your big toe. And after, you know, a few seconds on screen, she does it. And she did that with every one of her, like, body parts until she can move again. I did that for years. Just stared at my body and said, move your index finger, move your big toe. Sometimes vocalizing it, like, out loud, sometimes just thinking it. I tried every different way to do this, to try to get some movement back. And it's hard because it actually is like taxing, like physically taxing on my body, which is something I would have never expected because it's not like I'm moving, but it feels like there's a buildup of. I don't know. The only way I can describe it is there are, like, signals that aren't getting through from my brain, um, down, because my, there's that gap in my spinal cord, so brain down, and then from my hand back up to the brain. And so it feels like those signals, um, get stuck in whatever body part that I'm trying to move. And they just build up and build up and build up until they burst. Um, and then once they burst, I get, like, this really weird sensation of everything sort of, like, dissipating back out to level, and then I do it again. Um, it's also just like a fatigue thing, like a muscle fatigue, but without actually moving your muscles. It's very, very bizarre. And then, you know, uh, if you try to stare at a body part or think about a body part and move for two, three, four, sometimes 8 hours, it's very taxing on your mind. It takes a lot of focus. It was a lot easier at the beginning because I wasn't able to, like, control a tv in my room or anything. I wasn't able to control any of my environment. So for the first few years, a lot of what I was doing was staring at walls. And so obviously I did a lot of thinking and I tried to move a lot just over and over and over again. Do you never give up sort of hope there training hard, essentially? Yep. And I still do it I do it, like, subconsciously, and I think that that helped a lot with things, with neuralink, honestly, it's something that I talked about the other day at the all hands that I did at Neuralink's Austin facility. Welcome to Austin, by the way. Yeah. Hey, thanks, man. I would just. Hatch. Hey, thanks. Thanks, man. The gigafactory was super cool. I went to school at Texas A and M, so I've been around for. So you should be saying welcome to me. Yeah, welcome to Texas Lights. I get you, but, yeah, I was talking about how a lot of what they've had me do, especially at the beginning, well, I still do it now, is body mapping. So, like, there will be a visualization of a hand or an arm on the screen, and I have to do that motion. And that's how they sort of train the algorithm to, like, understand what I'm trying to do. And so it made things very seamless for me. I think that's really, really cool. So it's amazing to know because I've learned a lot about the body mapping procedure with the interface and everything like that. It's cool to know that you've been a century, like, training to be like, world class at that task. Yeah, yeah. I don't know if other quadriplegics, like other paralyzed people, give up. I hope they don't. I hope they keep trying because I've heard other paralyzed people say, like, don't ever stop. They tell you two years, but you just never know. You the human bodies capable of amazing things. So I've heard other people say, don't give up. Like, I think one girl had spoken to me through some family members and said that she had been paralyzed for 18 years and she'd been trying to wiggle her index finger for all that time, and she finally got it back 18 years later. So I know that it's possible and I'll never give up doing it. I just. I do it when I'm lying down, like, watching tv, I'll find myself doing it kind of just almost like on its own. It's just something I've gotten so used to doing that I don't know, I. I don't think I'll ever stop. That's really awesome to hear because I think it's one of those things that can really pay off in the long term, because, like, there is training, you're not visibly seeing the results of that training at the moment, but, like, there's that, like, Olympic level nervous system getting. Yeah, getting ready for something, honestly, was like, something that I think neuralink gave me that I can't thank them enough for. Like, I can't show my appreciation for it enough was being able to visually see that what I'm doing is actually having some effect. It's a huge part of the reason why, like, I know now that I'm going to keep doing it forever, because before neuralink, I was doing it every day, and I was just assuming that things were happening. Like, it's not like I knew I wasn't getting back any mobility or sensation or anything, so I could have been running up against a brick wall for all I knew. And with Nurlink, I get to see, like, I. All the signals happening real time, and I get to see that what I'm doing can actually be mapped. When we started doing click calibrations and stuff, when I go to click my index finger for a left click, that it actually recognizes that it changed how I think about what's possible with retraining my body to move. So, yeah, I'll never give up. And also just the signal that there's still a powerhouse of a brain there that's, like. Exactly. And as the technology develops, that brain is, I mean, that's the most important thing about the human body is the brain, and it can do a lot of the control. So what did it feel like when you first could wiggle the index finger and saw the environment respond? Like, that little. Yeah, wherever. We're just being way too dramatic, according to you. Yeah, it was very cool. I mean, I. It was cool, but it, I keep telling this to people. It made sense to me. Like, it made sense that, you know, like, there are signals still happening in my brain and that as long as you had something near it that could measure those, that could record those, then you should be able to, like, visualize it in some way, like, see it happen. And so that was not very surprising to me. I was like, oh, cool. Like, we found one. We found something that works. It was cool to see that their technology worked and that everything that they had worked so hard for was going to pay off. But I hadn't moved a cursor or anything at that point. I had interacted with the computer or anything at that point. It just made sense. It was cool. I didn't really know much about BCI at that point either, so I didn't know what sort of step this was actually making. I didn't know if this was a huge deal or if this was just like, okay, it's cool that we got this far, but we're actually hoping for something much better down the road. It's like, okay. I just thought that they knew that it turned on, so I was like, cool. This is cool. Well, did you read up on the specs of the hardware you get installed, the number of threads? Yeah, I knew all of that, but it's all like. It's all greek to me. I was like, okay, threads, 64 threads, 16 electrodes, 1024 channels. Okay, like, that. That math checks out. Sounds right. Yeah. When was the first time you were able to move a mouse cursor? I know it must have been within the first maybe week or two weeks that I was able to, like, first move the cursor. And again, like, it kind of made sense to me. Like, it. It didn't seem like that big of a deal. Like, it. It was like, okay, well, how do I explain this? When everyone around you starts clapping for something that you've done, it's. It's easy to say, okay, like, I did something cool. Like, that was. That was impressive in some way. What exactly that meant, what it was hadn't really, like, set in for me. So, again, I knew that me trying to move a body part and then that being mapped in some sort of, like, machine learning algorithm to be able to, um, identify, like, my brain signals and then take that and give me cursor control, that all kind of made sense to me. I don't know, like, all the ins and outs of it, but I was like, there are still signals in my brain firing. They just can't get through because there's, like, a gap in my spinal cord, and so they just. They can't get all the way down and back up, but they're still there. So when I moved the cursor for the first time, I was like, that's cool, but I expected that that should happen. Like, it made sense to me when I moved the cursor for the first time with just my mind without physically trying to move. So I guess I can get into that just a little bit. Like, the difference between attempted movement and imagined movement. Yeah. That's a fascinating difference from one to the other. Yeah, yeah, yeah. So, like, attempted movement is me physically trying to attempt to move, say, my hand. I try to attempt to move my hand to the right, to the left, forward and back, and that's all. Attempted attempt to lift my finger up and down, attempt to kick or something. I'm physically trying to do all of those things. Even if you can't see it, this would be me attempting to shrug my shoulders or something. That's all attempted movement. That all. That's what I was doing for the first couple of weeks when they were going to give me cursor control. When I was doing body mapping, it was attempt to do this, attempt to do that. When Nier was telling me to imagine doing it, it kind of made sense to me. But it's not something that people practice. Like, if you started school as a child and they said, okay, write your name with this pencil. And so you do that. Like, okay, now imagine writing your name with that pencil. Kids would think, uh, like, I guess, like, that kind of makes sense, and they would do it. Um, but that's not something we're taught. It's all, like, how to do things physically. We think about, like, thought experiments and things. But that's not like, that's not like a physical action of doing things. It's more like what you would do in certain situations. So imagine movement. It never really connected with me. Like, I guess you could maybe describe it as, like, a professional athlete, like, swinging a baseball bat or swinging, like, a golf club. Like, imagine what you're supposed to do, but then you go right to that and physically do it. Like, you. Then you get a bat in your hand, and then you do what you've been imagining. And so I don't have that, like, connection. So telling me to imagine something versus attempting it, there wasn't a lot that I could do there mentally. I just kind of had to accept what was going on and try. But the attempted moving thing, it all made sense to me. If I try to move, then there's a signal being sent in my brain, and as long as they can pick that up, then they should be able to map it to what I'm trying to do. And so when I first moved the cursor like that, it was like, yes, this should happen. Like, I'm not surprised by that. But can you clarify, is there supposed to be a difference between imagined movement and attempted movement? Yeah, just that in imagined movement, you're not attempting to move at all. So it's. You're, like, visualizing yourself doing, and then theoretically, is that supposed to be a different part of the brain that lights up in those two different situations? Yeah, not necessarily. I think all these signals can still be represented in motor cortex, but the difference, I think, has to do with the naturalness of imagining something versus attempting it and sort of the fatigue of that over time. And by the way, on the mic is bliss. So, like, this is just different ways to prompt you to kind of get to the thing that you arrived at. Yeah, attempted movement does sound like the right thing. Yeah, try. Yeah, I mean, it makes sense to. Me because imagine for me, I'll be. I would start visualizing, like, in my mind. Visualizing attempted. I would actually start trying to, like. Yeah, there's a. I mean, I, you know, I did, like, combat sports my whole life. Like, wrestling. When I'm imagining a move, see, I'm, like, moving my muscle exactly. Like, there's a. There is a bit of an activation almost, versus, like, visualizing yourself, like a picture doing it. Yeah, it's something that I feel like naturally anyone would do. If you try to tell someone to imagine doing something, they might close their eyes and then start physically doing it. But it's just. Yeah, it's hard. It was very hard at the beginning, but attempted worked. Attempted worked. It worked just like it should work. Like, work like a charm. I remember there was, like, one Tuesday, we were messing around, I think I forget what swear word you used, but there's a swear word that came out of your mouth when you figured out you could just do the direct cursor control. Yeah, it blew my mind. Like, no pun intended. Blew my mind when I first moved the cursor, just with my thoughts and not attempting to move, it's something that I found, like, over the couple of weeks, like, building up to that, that as I get better cursor controls, like, the model gets better, then it gets easier for me to, like, like, I don't have to attempt as much to move it. And part of that is something that I'd even talked with them about. Um, when I was watching the signals of my brain one day, I was watching when I, like, attempted to move to the right, and I watched the screen as, like, I saw the spikes. Like, I was seeing the spike the signal was being sent before I was actually attempting to move. Um, I imagine just because, you know, when you go to, say, move your hand or any body part, that signal gets sent before you're actually moving. Has to make it all the way down and back up before you actually do any sort of movement. So there's a delay there. And I noticed that there was something going on in my brain before I was actually attempting to move, that my brain was, like, anticipating what I wanted to do. And that all started sort of, I don't know, like, percolating in my brain. Like, it just. It was just sort of there, like, always in the back. Like, that's so weird that it could do that. It kind of makes sense, but I wonder what that means as far as, like, using the neuralink and, you know, and then as I was playing around with the attempted movement and playing around with the cursor, and I saw that, like, as the cursor control got better, that it was anticipating my movements and what I wanted it to do, like, cursor movements, what I wanted to do a bit better and a bit better. And then one day, I just randomly, as I was playing web grid, I, like, looked at a target before I had started attempting to move. I was just trying to get over, train my eyes to start looking ahead. This is the target I'm on. But if I look over here to this target, I know I can maybe be a bit quicker. Getting there just shot over. It was wild. Like, I had to take a step back. I was like, this should not be happening all day. I was just smiling. I was so giddy. I was like, guys, do you know that this works? I can just think it, and it happens, which they had all been saying this entire time. I can't believe you're doing all this with your mind. I'm like, yeah, but is it really with my mind, I'm attempting to move, and it's just picking that up so it doesn't feel like it's with my mind. But when I moved it for the first time like that, it was, oh, man. It, like, it made me think that this technology, that what I'm doing is actually way, way more impressive than I ever thought. It was way cooler than I ever thought, and it just opened up a whole new world of possibilities of, like, what could possibly happen with this technology and what I might be able to be capable of with it. Because you had felt for the first time, like, this was digital telepathy. Like, you're controlling a digital device with your mind. Yep. I mean, this is. That's a real moment of discovery. That's really cool. Like, you've discovered something. I've seen, like, scientists talk about, like, a big aha moment, you know, like, Nobel Prize winning. They'll have this, like, holy crap. Yeah. Like, that's what it felt like. Like, I didn't feel like. Like, I felt like I had discovered something, but for me, maybe not necessarily for, like, the world at large or, like, this field at large. It just felt like an aha moment for me. Like, oh, this works. Like, obviously it works. And so that's what I do, like, all the time now. I kind of intermix the attempted movement and imagine movement. I do it all together. Because ive found that there is some interplay with it that maximizes efficiency with the cursor. So its not all one or the other. Its not all just I only use attempted or I only use imagined movements. Its more I use them in parallel and I can do one or the other. I can just completely think about whatever I'm doing. But I don't know, I like to play around with it. I also like to just experiment with these things. Every now and again I'll get this idea in my head, like, hmm, I wonder if this works. And I'll just start doing it and then afterwards I'll tell them, by the way, I wasn't doing that like you guys wanted me to. I thought of something and I wanted to try it, and so I did. It seems like it works. So maybe we should like explore that a little bit. So I think that discovery is not just for you, at least from my perspective, that's a discovery for everyone else who ever uses in your link that this is possible. Like, I don't think that's an obvious thing that this is even possible. It's like I was saying to bliss earlier, it's like the four minute mile. People thought it was impossible to run a mile in four minutes, and once the first person did it, then everyone just started doing it. So like, just to show that it's possible, that paves the way to, like, anyone can now do it. That's the thing. That's actually possible. You don't need to do the attempted movement. You can just go direct. That's crazy. That is crazy. It is crazy for people who don't know. Can you explain how the link app works? You have an amazing stream on the topic. Your first stream, I think, on x describing the app. Can you just describe how it works? Yeah, so it's just an app that Neuralink created to help me interact with the computer. So on the link app, there are a few different settings and different modes and things I can do on it. So there's like the body mapping, which we kind of touched on. There's a calibration. Calibration is how I actually get cursor control. So calibrating what's going on in my brain to translate that into cursor control so it will pop out models, what they use, I think, is time. So it would be five minutes in calibration will give me so good of a model. And then if I'm in it for ten minutes and 15 minutes, the models will progressively get better. And so, you know, the longer I'm in it, generally the better the models will get. That's really cool, because you often refer to the models. The model is the thing that's constructed once you go through the calibration step. Yeah. And then you also talked about sometimes you'll play like, a really difficult game like snake, just to see how good the model is. Yeah, yeah. So snake is kind of like my litmus test for models. If I can control snake decently, well, then I know I have it. Pretty good model. So, yeah, the link app has all of those. It has webgrid in it now. It's also how I connect to the computer. Just in general, they've given me a lot of voice controls with it at this point. So I can say connect or implant disconnect. And as long as I have that charger handy, then I can connect to it. So the charger is also how I connect to the link app. To connect to the computer, I have to have the implant charger over my head when I want to connect, to have it wake up. Because the implants in hibernation mode, always when I'm not using it, I think there's a setting to wake it up every so long. So we could set it to half an hour or 5 hours or something. If I just want it to wake up periodically, I'll connect to the link app and then go through all sorts of things. Calibration for the day, maybe body mapping. I made them give me a little homework tab because I am very forgetful and I forget to do things a lot. So I have a lot of data collection things that they want me to do. Is the body mapping part of the data collection, or is that also part of the. Yeah, it is. It's something that they want me to do daily, which I've been slacking on because I've been doing so much media and traveling so much. So I've been super famous. Yeah, I've been a terrible first candidate for how much I've been slacking on my homework. But, yeah, it's just something that they want me to do every day to track how well the neuralink is performing over time and have something to give, I imagine, to give to the FDA to create all sorts of fancy charts and stuff and show like, hey, this is what the neuralink, this is how it's performing day one versus day 90 versus day 180, and things like that. What's the calibration step like? Is it like, move left, move right? It's a bubble game. So there will be yellow bubbles that pop up on the screen. At first, it is open loop. This is something that I still don't fully understand, the open loop and closed loop thing. Me and Blizz talked for a long time about the difference between the two from the. On the technical side. Okay. So it'd be great to hear your. Okay, so your side of the story. Open loop is basically, I have no control over the cursor. The cursor will be moving on its own across the screen, and I am following by intention the cursor to different bubbles. And then my. The algorithm is training off of what? Like, the signals it's getting are, as I'm doing this, there are a couple different ways that they've done it. They call it center out target. So there will be a bubble in the middle and then eight bubbles around that, and the cursor will go from the middle to one side. So say middle to left, back to middle to up to middle, like upright. And they'll do that all the way around the circle. And I will follow that cursor the whole time, and then it will train off of my intentions, what it is expecting my intentions to be throughout the whole process. Can you actually speak to. When you say follow, you don't mean with your eyes, you mean with your intentions. Yeah. So, generally, for calibration, I'm doing attempted movements because I think it works better. I think the better models, as I progress through calibration, make it easier to use imagined movements. Wait, wait, wait. So calibrated, unattempted movement will create a model that makes it really effective for you to then use the force? Yes. I've tried doing calibration with imagined movement, and it just doesn't work as well for some reason. So that was the center out targets. There's also one where a random target will pop up on the screen and it's the same. I just move, I follow along wherever the cursor is to that target all across the screen. I've tried those with imagine movement, and for some reason, the models just don't, they don't give as high levels quality when we get into closed loop. I haven't played around with it a ton, so maybe, like, the different ways that we're doing calibration now might make it a bit better. But what I've found is there will be a point in calibration where I can use imagine movement before that point. It doesn't really work. So if I do calibration for 45 minutes, the first 15 minutes, I can't use imagine movement. It just, like, doesn't work for some reason. And after a certain point, I can just sort of feel it. I can tell it moves different. That's the best way I can describe it. It's almost as if it is anticipating what I am going to do again before I go to do it. And so using attempted movement for 15 minutes, at some point, I can kind of tell when I, like, move my eyes to the next target that the cursor is starting to pick up. Its starting to understand, its learning what im going to do. So, first of all, its really cool that, I mean, youre our true pioneer in all of this. Youre exploring how to do every aspect of this most effectively. And there's just, I imagine so many lessons learned from this. So thank you for being a pioneer in all these kinds of different, like, super technical ways. And it's also cool to hear that there's a different, like, feeling to the experience when it's calibrated in different ways. Like, just because I imagine your brain is doing something different and that's why there's a different feeling to it. And then trying to find the words and the measurements to those feelings would be also interesting. But at the end of the day, you can also measure that your actual performance on whether it's snake or webgrid, you can see, like, what actually works well. And you're saying for the open loop calibration, the attempted movement works best for now. Yep. Yep. So the open loop, you don't get the feedback that something, that you did something. Yeah, I'm frustrating. No, no, it makes sense to me. Like, we've done it with a cursor and without a cursor in open loop. So sometimes it's just say for, like, the center out, the. You'll start calibration with a bubble lighting up, and I push towards that bubble. And then when that bubble, you know, when it's a push towards that bubble for, say, 3 seconds, a bubble will pop, and then I come back to the middle. So I'm doing it all just by my intentions. That's what it's learning anyway. So it makes sense that as long as I follow what they want me to do, follow the yellow brick road, that it'll all work out. You're full of great references. Is the bubble game fun? Yeah. They always feel so bad making me do calibration. Oh, we're about to do, you know, a 40 minutes calibration. I'm like, all right, do you guys want to do two of them? Like, I'm always asking to, like, whatever they need, I'm more than happy to do. And it's not, it's not bad. Like, I get to lie there and. Or sit in my chair and, like, do these things with some great people. I get to have great conversations. I can give them feedback. I can talk about all sorts of things. I could throw something on, on my tv in the background and kind of, like, split my attention between them. Like, it's not bad at all. I don't score that. You get, like, can you do better on the bubble game? No, I would love that. I would love. Yeah. Writing down suggestions from Nolan that's make it more fun gamified. Yeah. That's one thing that I really, really enjoy about webgrid is because I'm so competitive. Like, the higher the bps, the higher the score I know, the better I'm doing. And so if I think I've asked at 1.1 of the guys, like, if he could give me some sort of numerical feedback for calibration, like, I would like to know what they're looking at. Like, oh, you know, it is. We see, like, this number while you're doing calibration. And that means I, at least on our end, that we think calibration is going well. And I would love that because I would like to know if what I'm doing is going well or not. But then they've also told me, like, yeah, not necessarily, like, one to one. It doesn't actually mean that calibration is going well in some ways. So it's not like 100%. And they don't want to, like, skew what I'm experiencing or want me to change things based on that. If that number isn't always accurate to, like, how the model will turn out or, like, the end result, that's at least what I got from it. One thing I do, I have asked them, and something that I really enjoy striving for is towards the end of calibration, there is, like, a time between targets. And so I like to keep, like, at the end, that number as low as possible. So at the beginning it can be, you know, four, five, 6 seconds between me popping bubbles, but towards the end, I like to keep it below, like, 1.5. Or if I could get it to, like, 1 second between, like, bubbles, because in my mind, that translates really nicely to something like web grid, where I know if I can hit a target one every second that I'm doing real, real well. There you go. That's the way to get a score on the calibration is like the speed. How quickly can you get from bubble to bubble? Yeah. So there's the open loop, and then it goes to the closed loop. The closed loop can already start giving you a sense, because you're getting feedback of how good the model is. Yeah. So, closed loop is when I first get cursor control and how they've described it to me, someone who does not understand this stuff, I am the dumbest person in the room. Every time I'm with any of these humility is that I am closing the loop. So I am actually now, um, the one that is, like, finishing the loop of whatever this loop is. I don't even know what the loop is. They've never told me. They just say there is a loop, and at one point it's open and I can't control, and then I get control, and it's closed. So I'm finishing the loop. So how long the calibration usually takes? You said like 1015 minutes. Well, yeah, they're. They're trying to get that number down pretty low. That's what we've been working on a lot recently, is getting that down is low as possible. So that way, you know, if this is something that people need to do on a daily basis, or if some people need to do on a, like, every other day basis or once a week, they don't want people to be sitting in calibration for long periods of time. I think they wanted to get it down seven minutes or below, at least where we're at right now, it'd be nice if you never had to do calibration. So we'll have get there at some point. I'm sure. The more we learn about the brain and, like, I think that's, you know, the dream, I think right now for me to get, like, really, really good models, I'm in calibration 40 or 45 minutes. And I don't mind. Like I said, they always feel really bad. But if it's going to get me a model that can, like, break these records on web grid, I'll stay in it for flipping 2 hours. Let's talk business. So, web grid, I saw a presentation that where Bliss said by March, you selected 89,000 targets in web grid. Can you explain this game? What is Webgrid, and what does it take to be a world class performer in Webgrid as you continue to break world records? Yeah. It'S like a gold medalist talk. Well, I'd like to thank. I'd like to thank everyone who's helped me get here, my coaches, my parents, for driving me to practice every day at five in the morning. I like to thank God and just overall my dedication to my. The interviews with athletes are always like that exact. It's like that template. Yeah. So web grid. Web grid is a grid. It's literally just a grid. They can make it as big or small as you can make a grid. A single box on that grid will light up and you go and click it. And it is a way for them to benchmark how good a BCI is. So it's, you know, pretty straightforward. You just click targets. Only one blue cell appears and you're supposed to move the mouse to there and click on it. So I like playing on, like, bigger grids because it. The bigger the griddenne, the more bps. It's bits per second that you get every time you click one. So I'll say I'll play on like a 35 by 35 grid, and then one of those little squares, cell, call it target, whatever, will light up. And you move the cursor there and you click it. And then you do that forever. And you've been able to achieve, at first, eight bits per second. And you recently broke that. Yeah, I'm at 8.5 right now. Beaten that literally the day before I came to Austin. But I had like a. I don't know, like a five second lag right at the end. And I just had to wait until the latency calmed down. And then I kept clicking, but I was at, like, 8.01 and then 5 seconds of lag, and then the next, like, three targets I clicked all stayed at 8.01. So if I would have been able to click, um, during that time of lag, I probably would have hit. I don't know, I might have hit nine. So I'm there. I'm like, I'm really close. And then this whole Austin trip has really gotten in the way of my web grid playing ability. Yeah. So that's all you're thinking about right now? Yeah, I know, it's just. I just want. I want to do better. I want to do better. I want to hit nine, I think. Well, I know nine is very, very achievable. I'm right there. I think ten I could hit maybe in the next month. Like, I could do it probably in the next few weeks if I really push. I think you and Elon are basically the same person. Cause last time I did a podcast with him, he came in extremely frustrated that he can't beat Uber Lilith as a droid. That was like a year ago. I think I forget, like, solo. And I could just tell there's some percentage of his brain, the entire time was thinking, like, I wish I was right now attempting to. I think he did it that night. He did it that night. He stayed up and did it that night. It's just crazy to me. I mean, it's, in a fundamental way, it's really inspiring. And what you're doing is inspiring in that way because, I mean, it's not just about the game. Everything you're doing there has impact. By striving to do well on web grid, you're helping everybody figure out how to create the system all along. Like the decoding, the software, the hardware, the calibration, all of it. How to make all of that work so you can do everything else really well. Yeah, it's just really fun. Well, that's also. That's part of the thing, is making it fun. Yeah, it's addicting. I'm. I've joked about, um, like, what they actually did when they went in and put this thing in my brain. They must have flipped a switch to make me more susceptible to these kinds of games, to make me addicted to, like, web grid or something. Yeah. Do you know Bliss's high score? Yeah, he said like, 14 or something. 17. Oh, boy. 17.1 or something. 17 on the dot. 17.01. Yeah, he told me he, like, does it on the floor with peanut butter and he's like, fasts. It's weird. That sounds like cheating. Sounds like performance enhancing. No one's like, the first time Nolan played this game, he asked, how good are we at this game? And I think you told me right then you're not. You're gonna try to beat me. I'm gonna get there someday. Yeah, I think. I think I can. Yeah. So I've been playing, first off, with the dwell cursor, which really hampers my web grid playing ability. Basically, I have to wait 3 seconds for every click. Oh, so you can't do the click. So. Yeah, so you click by dwelling. You said 0.30.3 seconds, which sucks. It really slows down how much I'm able to, how high I'm able to get. I still hit. I think I hit 50 something trials. Net trials per minute in that, which was pretty good because I'm able to. One of the settings is also how slow you need to be moving in order to initiate a click. To start a click. So I can tell when I'm on that threshold to start initiating a click just a bit early. So I'm not fully stopped over the target when I go to click. I'm doing it, like, on my way to the targets a little to try to time it just right. So you're slowing down? Yeah, just. Just a hair right before the targets. This is like a lead performance. Okay. But that's still. It's. It sucks that there's a ceiling of the. .3 well, there I can get down to .2 and. .1.1 yeah. And I've played with that a little bit, too. I have to adjust a ton of different parameters in order to play with, .1 and I don't have control over all that on my end yet. It also changes how the models are trained. Like, if I train a model, like in web grid, I bootstrap on a model, which basically is them training models as I'm playing webgrid based off of the web grid data that I'm. So if I play web grid for ten minutes, they can train off that data specifically in order to get me a better model. If I do that with three versus 0.1, the models come out different. The way that they interact, it's just much, much different. So I have to be really careful. I found that doing it with 0.3 is actually better in some ways. Unless I can do it with 0.1 and change all of the different parameters, then that's more ideal, because obviously 0.3 is faster than one. I could get there. I can get there. Can you click using your brain for. Right now, it's the hover. Clicking with the dwell cursor. Before all the thread retraction stuff happened, we were calibrating clicks. Left click, right click. That was my previous ceiling before I broke the record. Again with the dwell cursor was, I think, on a 35 by 35 grid with left and right click. And you get more bps, more bits per second using multiple clicks, because it's more difficult. Oh, because what is it? You're supposed to do? Either a left click or, like, right click. The different colors. Yeah. Blue targets for left click, orange targets for right click is what they had done. So my previous record of 7.5 was with blue and the orange targets. Yeah. Which I think if I went back to that now, doing the click calibration, I would be able to. And being able to, like, initiate clicks on my own, I think I would break that ten ceiling, like, in a couple days max. Like, yeah, you start making bliss nervous about his 17. Why do you think we haven't given him the. Exactly. So what did it feel like with the retractions that there is some of the threads retracted? That sucked. It was really, really hard. The day they told me, was the day of my big neuralink tour at their Fremont facility. They told me right before we went over there, it was really hard to hear. My initial reaction was, all right, go in, fix it. Go and take it out and fix it. The first surgery was so easy. I went to sleep. A couple hours later, I woke up, and here we are. I didn't feel any pain, didn't take any pain pills or anything. So I just knew that if they wanted to, they could go in and put in a new one, like, next day, if that's what it took, because I just wanted. I wanted it to be better, and I wanted not to lose the capability. I had so much fun playing with it for a few weeks, for a month, I had, like, it had opened up so many doors for me. It had opened up so many more possibilities that I didn't want to lose it after a month. I thought it would have been a cruel twist of fate if I had gotten to see the view from, like, the top of this mountain and then have it all come crashing down after a month. And I knew, like, say, the top of the mountain, but, like, I. How I saw it was I was just now starting to climb the mountain, and I was like, there was so much more that I knew was possible. And so to have all of that be taken away was really, really hard. But then on the drive over to the facility, I don't know, like, five minute drive, whatever it is. I talked with my parents about it. I prayed about it. I was just like, you know, I'm not going to let this ruin my day. I'm not going to let this ruin this amazing tour that they have set up for me. I want to go show everyone how much I appreciate all the work they're doing. I want to go meet all of the people who have made this possible, and I want to go have one of the best days of my life. And I did, and it was amazing. And it absolutely was one of the best days I've ever been privileged to experience. And then for a few days, I was pretty down in the dumps. But for, like, the first few days afterwards, I was just like, I didn't know if it was ever going to work again. And then I just. I made the decision that it. Even if I lost the ability to use the neuralink, even if I lost. Even if I, like, lost out on everything to come. If I could keep giving them data in any way, then I would do that. If I needed to just do some of the data collection every day or body mapping every day for a year, then I would do it, because I know that everything I'm doing helps everyone to come after me, and that's all I wanted. I guess the whole reason that I did this was to help people. I knew that anything I could do to help, I would continue to do, even if I never got to use the cursor again, then, you know, I was just happy to be a part of it. And everything that I'd done was just a perk. It was something that I got to experience, and I know how amazing it's going to be for everyone to come after me, so might as well just keep trucking along, you know? That said, you were able to get to work your way up to get the performance back. So this is like going from Rocky one to Rocky two. So when did you first realize that this is possible and what gave you sort of the strength, the motivation, determination to do it, to increase back up and be your previous record? Yeah, it was within a couple weeks. Again, this feels like I'm interviewing an athlete. This is great. I like the thing. My parents. The road back was long and hard from many difficulties. There were dark days. It was a couple weeks, I think. And then there was just a turning point. I think they had switched how they were measuring the neuron spikes in my brain. Like, the bliss. Help me out. Yeah. The way in which we were measuring the behavior of individual neurons. So we're switching from sort of individual spike detection to something called spike band power, which, if you watch the previous segments with either me or DJ, you probably have some content. Yeah. Okay. So when they did that, it was kind of like a light over the head, like, light bulb moment. Like, oh, this works. And this seems like we can run with this. And I saw the uptick in performance immediately. Like, I could feel it when they switched over, I was like, this is better. Like, this is good. Like, everything up till this point, for the last few weeks, last, like, whatever, three or four weeks, because it was before they even told me everything, before this sucked. Let's keep doing what we're doing now. And at that point, it was not like, oh, I know, I'm still only at, like, say, in web grid terms, like, four or five bps compared to my 7.5 before. But I know that if we keep doing this, then I can. I can get back there and then they gave me the dwell cursor, and the dwell cursor sucked. At first. It's not, obviously not what I want, but it gave me a path forward to be able to continue using it and hopefully to continue to help out. And so I just ran with it, never looked back. Like I said, I just kind of person. I roll with the punches. Anyway, what was the process? What was the feedback loop on the figuring out how to do the spike detection in a way that would actually work well for another. Yeah, it's a great question. So maybe just describe first how the actual update worked. It was basically an update to your implant. So we just did an over the air software update to his implant. Somebody could update your Tesla or your iPhone. And that firmware change enabled us to record averages of populations of neurons nearby individual electrodes. So we have less resolution about which individual neuron is doing what, but we have a broader picture of what's going on nearby an electrode overall. And that feedback loop, basically, as Nolan described it was immediate when we flipped that switch. I think the first day we did that, you hit three or four vps right out of the box, and that was a light bulb moment for, okay, this is the right path to go down from there. There's a lot of feedback around how to make this useful for independent use. What we care about ultimately is that you can use it independently to do whatever you want. To get to that point, it required us to re engineer the ux, as you talked about, the dwell cursor, to make it something that you can use independently without us needing to be involved all the time. This is obviously the start of this journey. Still, hopefully we get back to the places where you're doing multiple clicks and using that to control much more fluidly everything and much more naturally the applications that you're trying to interface with. And most importantly, get that web grid number up on the hover. Click. Do you accidentally click stuff? Sometimes. How hard is it to avoid accidentally clicking? I have to continuously keep it moving. Basically, like I said, there's a threshold where it will initiate a click. So if I ever drop below that, it'll start and I have 0.3 seconds to move it before it clicks anything. If I don't want it to ever get there, I just keep it moving at a certain speed and just constantly doing circles on screen, moving it back and forth to keep it from clicking stuff. I actually noticed a couple weeks back that when I was not using the implant, I was just moving my hand back and forth or in circles like, I was trying to keep the cursor from clicking, and I was just doing it, like, while I was trying to go to sleep, and I was like, okay, this is a problem to avoid. The clicking, I guess. Does that create problems? Like, when you're gaming, accidentally click a thing? Yeah. Yeah. It happens in chess. I've lost. I've lost a number of games because I'll accidentally click something. I think the first time I ever beat you was because of an accident. It's a nice excuse, right? Yeah. Anytime you lose, you could just say that was accidental. Yeah. You said the app improved a lot from version one. When you first started using it, it was very different. So can you just talk about the trial and error that you went through with the team? Like, 200 plus pages of notes? Like, what's that process, like, of going back and forth and working together to improve the thing? It's a lot of me just using it, like, day in and day out and saying, like, hey, can you guys do this for me? Like, give me this. I want to be able to do that. I need this. I think a lot of it just doesn't occur to them. Maybe until someone is actually using the app, using the implant, it's just something that they just never would have thought of, or it's very specific to even, like, me. Maybe what I want, it's something I'm a little worried about with the next people that come is, you know, maybe they will want things much different than how I set it up or what the advice I've given the team, and they're going to look at some of the things they've added for me, like, that's a dumb idea. Why would he ask for that? And so I'm really looking forward to get the next people on because I guarantee that they're going to think of things that I've never thought of. They're going to think of improvements. I'm like, wow, that's a really good idea. I wish I would have thought of that. And then they're also going to give me some pushback about, like, yeah, what you are asking them to do here, that's a bad idea. Let's do it this way. And I'm more than happy to have that happen, but it's just a lot of, like, you know, different interactions with different games or applications, the Internet, just with the computer in general, there's tons of bugs that end up popping up left, right, center. So it's just me trying to use it as much as possible and showing them what works and what doesn't work and what I would like to be better. And then they take that feedback and they usually create amazing things for me. They solve these problems in ways I would have never imagined. They're so good at everything they do. And so I'm just really thankful that I'm able to give them feedback and they can make something of it because a lot of my feedback is like really dumb. It's just like, I want this, please do something about it. And they'll come back and super well thought out and it's way better than anything I could have ever thought of or implemented myself. So they're just great. They're really, really cool. As the BCI community grows, would you like to hang out with the other folks with neuralinks? Like what, what relationship, if any, would you want to have with them? Because you said, like, they might have a different set of like, ideas of how to use the thing. Yeah. Would you be intimidated by their web grid performance? No, no. I hope compete. I hope day one they like wipe the floor with me. I hope they beat it and they crush it, you know, double it if they can, just because on one hand it's only going to push me to be better because Im super competitive. I want other people to push me. I think that is important for anyone trying to achieve greatness is they need other people around them who are going to push them to be better. I even made a joke about it on x. Once the next people get chosen, cue buddy cop music. Im just excited to have other people to do this with and to share experiences with. I'm more than happy to interact with them as much as they want, more than happy to give them advice. I don't know what kind of advice I could give them, but if they have questions, I'm more than happy. What advice would you have for the next participant in the clinical trial? That they should have fun with this because it is a lot of fun and that I hope they work really, really hard because it's not just for us, it's for everyone that comes after us and, you know, come to me if they need anything and to go to Neuralink if they need anything. Man, Neuralink moves mountains like they do absolutely anything for me that they can. And it's an amazing support system to have. It puts my mind at ease for like so many things that I have had, like questions about, so many things I want to do and theyre always there and thats really, really nice. I would tell them not to be afraid to go to Neuralink with any questions that they have, any concerns, anything that theyre looking to do with this, and any help that neuralink is capable of providing. I know they will. I dont know. I dont know. Just work your ass off, because its really important we try to give our all to this. So have fun and work hard. Yeah. Yeah. There we go. Maybe that's what I'll just start saying to people, have fun. Work hard. Now you're a real pro athlete. Just keep it short. Maybe it's good to talk about what you've been able to do now that you have a neuralink implant. Like the freedom you gain from this way of interacting with the outside world. Like, you play video games all night and you do that by yourself, and that's a kind of freedom. Can you speak to that freedom that you gain? Yeah, it's what all, I don't know. People in my position want. They just want more independence. The more load that I can take away from people around me, the better. If I'm able to interact with the world without using my family, without going through any of my friends, like, needing them to help me with things, the better. If I'm able to sit up on my computer all night and not need someone to sit me up, say, on my iPad, in a position where I can use it, and then have to have them wait up for me all night until I'm ready to be done using it, it takes a load off of all of us, and it's really, like, all I can ask for. It's something that, you know, I could never thank neuralink enough for. And I know my family feels the same way. You know, just being able to have the freedom to do things on my own at any hour of the day or night, it means the world to me. And I don't know. When you're up at 02:00 a.m. playing web grid by yourself. Yeah, I just imagine, like, it's darkness and there's a light glowing, and you're just focused what's going through your mind? Or you're like, in a state of flow where it's like the mind is empty. Like those, like Zen masters. Yeah. Generally it is me playing music of some sort. I have a massive playlist, and so I'm just, like, rocking out to music, and then it's also just like a race against time, because I'm constantly, constantly looking at how much battery percentage I have left on my implant. Like, all right, I have 30%, which equates to x amount of time, which means I have to break this record in the next hour and a half or else it's not happening tonight. And so it's a little stressful when that happens. When it's like, when it's above 50%, I'm like, okay. Like, I got time. It starts getting down to 30 and then 20. It's like, all right, 10%. A little pop up is going to pop up right here. And it's going to really screw my web grid flow. It's going to tell me that, you know, like, there's a, like, the low battery, low battery pop up comes up and I'm like, it's really going to screw me over. So if I have to, if I'm going to break this record, I have to do it in the next, like 30 seconds or else that pop up is going to get in the way, like cover my web gridden. And then after that I go click on it, go back into webgrid, and I'm like, all right, that means I have ten minutes left before this thing's dead. That's what's going on in my head generally. That and whatever song is playing. And I just want, I want to break those records so bad. Like, it's all I want. When I'm playing web grid, it has become less of like, oh, this is just a leisurely activity. Like, I just enjoy doing this because it just feels so nice and it puts me at ease. It is. No, once I'm in web grid, you better break this record or you're going to waste like 5 hours of your life right now. And I don't know, it's just fun. It's fun, man. Have you ever tried web grid with like two targets and three targets? Can you get higher BPS with that? Can you do that? You mean like different color targets or. You mean, oh, multiple targets change the thing. Yeah. So BPS is log of number of targets times correct minus incorrect divided by time. And so you can think of like different clicks as basically doubling the number of active targets. Got it. So, you know, basically higher BPS, the more options there are, the more difficult to task. And there's also like, zen mode you've played in before, which is infinite canvas. It covers the whole screen with a grid. And I don't know what. Yeah, and so you can go like, that's insane. To. Yeah, he doesn't like it. Cause it didn't show BPS. So like, you know. Yeah, I had them put in a giant BpS in the background. So now it's like, the opposite of Zen mode. It's like. It's like, super hard mode, like, just metal mode. If it's just, like, a giant number in the back counter, we should rename that. Metal mode is a much better name. So you also play civilization vi. I love civ six. Yeah. Usually go with Korea. I do. So the great part about Korea is they focus on science tech victories, which was not planned. Like, I've been playing Korea for years, and then all of the knurling stuff happened, so it kind of aligns. But what I've noticed with tech victories is if you can just rush tech, rush science, then you can do anything. Like, at one point in the game, you will be so far ahead of everyone technologically that you will have, like, musket men, infantrymen, planes sometimes, and people will still be fighting with, like, bows and arrows. And so if you want to win a domination victory, you just get to a certain point with the science and then go and wipe out the rest of the world. Or you can just take science all the way and win that way, and you're going to be so far ahead of everyone because you're producing so much science that it's not even close. I've accidentally won in different ways just by focusing on science. I was. Yeah, I, like, I was playing only science, obviously. Like, just science all the way. Just tech. And I was trying to get, like, every tech in the tech tree and stuff, and then I accidentally won through a diplomatic victory. And I was so mad. I was so mad because it just ends the game. One turn. It was like, oh, you won. You're so diplomatic. I'm like, I don't want to do this. I should have declared war on more people or something. It was terrible. But you don't need giant civilizations with tech, especially with Korea, you can keep it pretty small. So I generally just get to a certain military unit and put them all around my border to keep everyone out of, and then I will just build up. So very isolationist. Nice. Yeah, just work on the science. That's it. You're making it sound so fun. It's so much fun. And I also saw a civilization seven trailer. Oh, man, I'm so pumped. And that's probably coming out. Come on, Civ seven. Hit me up. All alpha beta tests. Whatever. When is it coming out? 2025. Yeah. Next year. Yeah. What other stuff would you like to see improved about the Neuralink app and just the entire experience? I would like to, like I said, get back to the click on demand, like the regular clicks. That would be great. I would like to be able to connect to more devices. Right now it's just the computer. I'd like to be able to use it on my phone or use it on different consoles, different platforms. I'd like to be able to control as much stuff as possible, honestly. Like an Optimus robot would be pretty cool. That would be sick if I could control an Optimus robot. The link app itself, it seems like we are getting pretty dialed in to what it might look like down the road. Seems like we've gotten through a lot of what I want from it, at least. The only other thing I would say is, like, more control over all the parameters that I can tweak with my cursor and stuff. There's a lot of things that go into how the cursor moves in certain ways, and I have, I don't know, like three or four of those parameters. And there might be gain in friction. Gain friction? Yeah. And there's maybe double the amount of those with just like velocity and then with the actual dwell cursor. So I would like all of it. I want as much control over my environment as possible, especially advanced mode, you. Know, like in like, there's men usually this basic mode, and you're like one of those folks like the power user, advanced. Yeah, that's got it. That's what I want. I want as much control over this as possible. So, yeah, that's really all I can ask for. Just give me everything. Has speech been useful? Just being able to talk also in addition to everything else? Yeah. You mean like while I'm using it. While you're using it, like speech to text. Oh, yeah. Or do you type or like. Because there's also a keyboard. Yeah, so there's a virtual keyboard. That's another thing I would like to work more on, is finding some way to type or text in a different way. Right now it is like a dictation, basically, and a virtual keyboard that I can use with the cursor, but we've played around with fingerspelling, like sign language fingerspelling, and that seems really promising. So I have this thought in my head that it's going to be a very similar learning curve that I had with the cursor, where I went from attempted movement to imagine movement at one point. I have a feeling this is just my intuition that at some point I'm going to be doing fingerspelling and I won't need to actually attempt to fingerspell anymore, that I'll just be able to think the, like, letter that I want, and it'll pop up. That will be epic. Yeah, that's challenging. That's hard. That's a lot of work for you to kind of take that leap, but that would be awesome. And then, like, going from letters to words is another step. Like, you would go from, you know, right now it's fingerspelling of, like, just the sign language alphabeth. But if it's able to pick that up, then it should be able to pick up the whole sign language language. And so then if I could do something along those lines or just the sign language spelled word, if I can spell it at a reasonable speed and it can pick that up, then I would just be able to think that through and it would do the same thing. I don't see why not. After what I saw with the. With the cursor control, I don't see why it wouldn't work, but we'd have to play around with it more. What was the process in terms of training yourself to go from attempted movement to imagine movement? How long does that take? So how long would this kind of process take? Well, it was a couple weeks before it just happened upon me, but now that I know that that was possible, I think I could make it happen with other things. I think it would be much, much simpler. Would you get an upgraded implant device? Sure, absolutely. Whenever. Whenever they'll let me. So you don't have any concerns? For you, the surgery experience, all of it was like, no regrets? No. So everything's been good so far? Yep. You just keep getting upgrades? Yeah. I mean, why not? I've seen how much it's impacted my life already, and I know that everything from here on out going to get better and better. So I would love to. I would love to get the upgrade. What future capabilities are you excited about? Sort of. Beyond this kind of telepathy is vision. Interesting. So for folks, for example, who are blind, so you're enabling people to see or for speech? Yeah. There's a lot that's very, very cool about this. I mean, we're talking about the brain, so there's like, this is just motor cortex stuff. There's so much more that can be done. The vision one is fascinating to me. I think that is going to be very, very cool. To give someone the ability to see for the first time in their life would just be. I mean, it might be more amazing than even helping someone like me. That just sounds incredible. The speech thing is really interesting. Being able to have some sort of real time translation and cut away. That language barrier would be really cool. Any sort of, like, actual impairments that it could solve, like, with speech, would be very, very cool. And then also, there are a lot of different disabilities that all originate in the brain, and you would be able to hopefully be able to solve a lot of those. I know there's already stuff to help people with seizures that can be implanted in the brain. This would do, I imagine, the same thing. And so you could do something like that. I know that even someone like Joe Rogan has talked about the possibilities with being able to stimulate the brain in different ways. I'm not sure. I'm not sure what, you know, like, how ethical a lot of that would be. That's beyond me, honestly. But I know that there's a lot that can be done when we're talking about the brain of and being able to go in and physically make changes to help people or to improve their lives. So I'm really looking forward to everything that comes from this, and I don't think it's all that far off. I think a lot of this can be implemented within my lifetime, assuming that I live a long life. What you're referring to is things like, people suffering from depression or things of that nature potentially getting help. Yeah. Flip a switch like that. Make someone happy. I know. I think Joe has talked about it more in terms of, like, you want to experience, like, what a drug trip feels like. Like, you want to experience what it'd be like to be on, of course. Oh, yeah. Mushrooms or something like that. DMT. Like, you can just flip that switch in the brain. My buddy Bane has talked about being able to, like, wipe parts of your memory and re experience things that, like, for the first time, like your favorite movie or your favorite book. Like, just wipe that out real quick and then refall in love with Harry Potter or something. I told him, I was like, I don't know how I feel about, like, people being able to just wipe parts of your memory. That seems a little sketchy to me. He's like, they're already doing it, so. Sounds legit. I would love memory replay, just, like, actually, like, high resolution replay of all memories. Yeah, I saw an episode of Black Mirror about that once. I don't think I want it. Yeah, so black mirror always kind of considers the worst case, which is important. I think people don't consider the best case or the average case enough. I don't know what it is about us humans. We want to think about the worst possible thing. We love drama. Yeah, it's like, how is this new technology going to kill everybody? We just love that. Okay. Like, yes. Let's watch. Hopefully people don't think about that too much with me. It'll ruin a lot of my plans. Yeah, I assume you're gonna have to take over the world. I mean, I love your twitter. You. You tweeted, I'd like to make jokes about hearing voices in my head since getting the neural link, but I feel like people would take it the wrong way. Plus, the voices in my head told me not to. Yeah, yeah, yeah. Please never stop. So you're talking about Optimus. Is that something you would love to be able to do to control the robotic arm or the entirety of optimus? Oh, yeah, for sure. For sure. Absolutely. You think there's something, like, fundamentally different about just being able to physically interact with the world? Yeah. Oh, 100%. This. I know. Another thing with, like, being able to, like, give people the ability to, like, feel sensation and stuff too, by going in with the brain and having a neuro, like, maybe do that, that could be something that. That could be transferred through the Optimus as well. There's all sorts of really cool interplay between that. And then also, like you said, just physically interacting. I mean, 99% of the things that I can't do myself, obviously, I need a caretaker for someone to physically do things for me. If an optimist robot could do that, like, I could live an incredibly independent life and not be such a burden on those around me, and that would change the way people like me live, at least until whatever this is gets cured. But being able to interact with the world physically, like, that would just be amazing. And they're not just, like, for having it be a caretaker or something, but something like I talked about just being able to read a book. Imagine an optimist robot just being able to hold a book open in front of me, like, get that smell again. I might not be able to feel it at that point, or maybe I could again with the sensation and stuff, but there's something different about reading a physical book than staring at a screen or listening to an audiobook. I actually don't like audiobooks. I've listened to a ton of them at this point, but I don't really like them. I would much rather, like, read a physical copy. So one of the things you would love to be able to experience is opening the book, bringing it up to you and to feel the touch of the paper. Yeah. Oh, man. The touch, the smell. I mean, it's just something about the words on the page, and, you know, they've replicated, you know, that page color on, like, the kindle and stuff. Yeah, it's just not the same. Yeah. So just something as simple as that. So one of the things you miss is touch? I do, yeah. A lot of. A lot of things that I interact with in the world, like clothes or literally any physical thing that I interact with in the world. A lot of times, what people around me will do is they'll just come, like, rub it on my face. They'll, like, lay something on me so I can feel the weight. They will rub, you know, a shirt on me so I can feel fabric. Like, there's something very profound about touch, and it is. It's something that I miss a lot and something I would love to do again. We'll see. What would be the first thing you do with a hand that can touch your mom? A hug after that. Right? Yeah. Yeah, I know. That's. It's one thing that I've. That ive asked God for basically every day since my accident was just being able to one day move, even if it was only my hand. So that way I could squeeze my moms hand or something just to show her how much I care and how much I love her and everything. Something along those lines. Being able to just interact with the people around me. Handshake, give someone a hug. I don't know, anything like that. Being able to help me eat, like, I'd probably get really fat, which would be a terrible, terrible thing. Also beat bliss in chess. On a physical chessboard. Yeah. Yeah. I mean, there are just so many upsides, you know? Any. Any way to find some way to feel like I'm bringing bliss down to my level. Yeah. He's just such an amazing guy, and everything about him is just so above and beyond that. Anything I can do to take him down a notch. Yeah. Humble him a bit. He needs it. Okay. As he's sitting next to me, did you ever make sense of why God puts good people through such hardship? Oh, man. I think it's all about understanding how much we need God. And I don't think that there's any light without the dark. I think that if all of us were happy all the time, there would be no reason to turn to God, ever. I feel like there would be no concept of good or bad. And I think that as much of the darkness and the evil that's in the world, it makes us all appreciate the good and the things we have so much more. And I think when I had my accident. One of the first things I said to one of my best friends, Washington, and this was within the first month or two after my accident. I said, everything about this accident has just made me understand and believe that God is real, and that there really is a God, basically, and that my interactions with him have all been real and worthwhile. And he said, if anything, seeing me go through this accident, he believes that there isn't a God. And it's a very different reaction. But I believe that it is a way for God to test us, to build our character, to send us through trials and tribulations, to make sure that we understand how precious he is and the things that he has given us and the time that he has given us, and then hopefully grow from all of that. I think that's a huge part of being here, is to not just, you know, have an easy life and do everything that's easy, but to step out of our comfort zones and really challenge ourselves, because I think that's how we grow. What gives you hope about this whole thing we have going on? Human civilization? Oh, man. I think people are my biggest inspiration. And even just being at Neuralink for a few months, looking people in the eyes and hearing their motivations for why they're doing this, it's so inspiring. And I know that they could be other places, at cushier jobs, working somewhere else, doing x, y, or z. That doesn't really mean that much, but instead, they're here, and they want to better humanity, and they want to better just the people around them, the people that they've interacted with in their life. They want to make better lives for their own family members who might have disabilities, or they look at someone like me and they say, you know, I can do something about that. So I'm going to. And it's always been what I've connected with most in the world are people. I've always been a people person, and I love learning about people, and I love learning how people developed and where they came from, and to see how much people are willing to do for someone like me when they don't have to, and they're going out of their way to make my life better. It gives me a lot of hope for just humanity in general, how much we care and how much we're capable of when we all get together and try to make a difference. And I know there's a lot of bad out there in the world, but there always has been and there always will be. And I think that that is, it shows human resiliency and it shows what we're able, what we're able to endure and how much. How much we just want to be there and help each other and how much satisfaction we get from that. Because I think that's one of the reasons that we're here, is just to help each other. And I don't know, that always gives me hope, is just realizing that there are people out there who still care and who want to help. And thank you for being one such human being and continuing to be a great human being through everything you've been through, and being an inspiration to many people, to myself, for many reasons, including your epic, unbelievably great performance on Webgridgest. I will be training all night tonight. To try, to try to catch up. And I believe in you, that you can, once you come back. So sorry to interrupt with the Austin trip. Once you come back, eventually beat bliss. Yeah, yeah, for sure. Absolutely. I'm rooting for you though. The whole world is rooting for you. Thank you for everything you've done, man. Thanks. Thanks man. Thanks for listening to this conversation with Nolan Arbaugh and before that with Elon Musk, DJ saw, Matthew McDougall and bliss Chapmandhe. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Aldous Huxley. In the doors of perception, we live together. We act on and react to one another, but always and in all circumstances, we are by ourselves. The martyrs go hand in hand into the arena. They are crucified, alone embrace. The lovers desperately try to fuse their insulated ecstasies into a single self transcendence in vain but its very nature. Every embodied spirit is doomed to suffer and enjoy its solitude. Sensations, feelings, insights, fancies. All these are private and except through symbols and at second hand, incommunicable. We can pool information about experiences, but never the experiences themselves. From family to nation, every human group is a society of island universes. Thank you for listening and hope to see you next time.

Utterances:
Speaker A: The following is a conversation with Elon Musk. DJ saw, Matthew McDougall, bliss Chapman, and Nolan Arbaugh about neuralink and the future of humanity. Elon, DJ, Matthew, and bliss are, of course, part of the amazing neuralink team. And Nolan is the first human to have a neuralink device implanted in his brain. I speak with each of them individually, so use timestamps to jump around, or as I recommend, go hardcore and listen to the whole thing. This is the longest podcast I've ever done. It's a fascinating, super technical and wide ranging conversation, and I loved every minute of it. And now, dear friends, here's Elon Musk, his fifth time on this, the Lex Friedman podcast.
Speaker B: Drinking coffee or water?
Speaker A: Water. I'm so over caffeinated right now. Do you want some caffeine?
Speaker B: I mean, sure.
Speaker A: There's a. There's a nitro drink.
Speaker B: This was keep you up to, like, you know, tomorrow afternoon, basically.
Speaker A: Yeah. I don't want.
Speaker B: So what is nitro? It's just got a lot of caffeine or something.
Speaker A: Don't ask questions. It's called nitro. Do you need to know anything else?
Speaker B: Good. It's got nitrogen in it. That's ridiculous. I mean, what we breathe is 78% nitrogen anyway. What do you need to add more? Most people think that they're breathing oxygen and they're actually breathing 78% nitrogen. You need, like a milk bar, like. Like from clockwork orange.
Speaker A: Yeah. Yeah. Is that top three Kubrick film for you?
Speaker B: Cluckable carnage. It's pretty good. I mean, it's cemented. Jarring, I'd say.
Speaker A: Okay. Okay. So first, let's step back, and big congrats on getting neuralink implanted into a human. That's a historic step for neuralink.
Speaker C: Thanks.
Speaker A: Yeah, there's many more to come.
Speaker B: Yeah, we just obviously have our second implant as well.
Speaker A: How did that go?
Speaker B: So far, so good. It looks like we've got, I think, on the order, 400 electrodes that are providing signals. So nice. Yeah.
Speaker A: How quickly do you think the number of human participants will scale?
Speaker B: It depends somewhat on the regulatory approval, the rate, which we get regulatory approvals. So we're hoping to do ten by the end of this year. Total of ten. So eight more.
Speaker A: And with each one, you're going to be learning a lot of lessons about the neurobiology of the brain, the. Everything, the whole chain of the neuralink, the decoding, the signal processing, all that kind of stuff.
Speaker B: Yeah, I think it's obviously going to get better with each one. I mean, I don't want to jinx it, but it seems to gone extremely well with the second implant. So there's a lot of signal, a lot of electrodes. It's working very well.
Speaker A: What improvements do you think we'll see in neuralink in the coming, let's say, let's get crazy coming years?
Speaker B: I mean, in years, it's going to be gigantic because we'll increase the number of electrodes dramatically, will improve the signal processing. So even with only roughly 1015 percent of the electrodes working with Nolan, with our first patient, we were able to get to achieve a bit per second. That's twice the world record. So I think we'll start vastly exceeding the world record by orders of magnitude in the years to come. So it's like getting to, I don't know, 100 bits per second. Thousand. You know, maybe if you say, like, five years from now, it might be at a megabit, like, faster than any human could possibly communicate by typing or speaking.
Speaker A: Yeah. That BPS is an interesting metric to measure. There might be a big leap in the experience once you reach a certain level of BPS.
Speaker B: Yeah.
Speaker A: Like entire new ways of interacting with the computer might be unlocked and with humans.
Speaker B: With other humans, provided they have a neuralink, too.
Speaker A: Right.
Speaker B: Otherwise they won't be able to absorb the signals fast enough.
Speaker A: Do you think they'll improve the quality of intellectual discourse?
Speaker B: Well, I think you could think of it. If you were to slow down communication. How would you feel about that? If you'd only talk at, let's say, one 10th of normal speed, you'd be like, wow, that's agonizingly slow.
Speaker A: Yeah.
Speaker B: So now imagine you could speak at. Communicate clearly at ten or 100 or 1000 times faster than normal listen.
Speaker A: I'm pretty sure nobody in their right mind listens to me at one x. They listen at two x. I can only imagine what ten x would feel like, or I could actually understand it.
Speaker B: I usually default to 1.5 x. You can do two x, but. Well, actually, if I'm trying to go, if I'm listening to somebody in, like, sort of 1520 minutes segments to go to sleep, then I'll do it 1.5 x. If I'm paying attention, I'll do two x.
Speaker A: Right.
Speaker B: But actually, if you start actually listen to podcasts or sort of audiobooks or anything at. If you get used to doing it at 1.5, then one sounds painfully slow.
Speaker A: I'm still holding on to one because I'm afraid. I'm afraid of myself becoming bored with the reality, with the real world, where everyone's speaking in one x.
Speaker B: Well, depends on the person. You can speak very fast, like, we communicate very quickly. And also if you use a wide range of. If your vocabulary is larger, your effective bitrate is higher.
Speaker A: That's a good way to put it. Yeah, the effective bitrate, I mean, that is the question is how much information is actually compressed in the low bit transfer of language.
Speaker B: Yeah. If there's a single word that is able to convey something that would normally require ten simple words, then you've got maybe a ten x compression on your hands. And that's really with memes. Memes are like data compression. It conveyed a whole. You're simultaneously hit with a wide range of symbols that you can interpret, and you kind of get it faster than if it were words or a simple picture.
Speaker A: And of course, you're referring to memes broadly, like ideas.
Speaker B: Yeah, there's an entire idea structure that is like an idea template, and then you can add something to that idea template, but somebody has that pre existing idea template in their head. So when you add that incremental bit of information, you're conveying much more than if you just, you know, said a few words. It's everything associated with that meme.
Speaker A: You think there'll be emergent leaps of capability as you scale the number of electrodes. Like, there'll be a certain. Do you think there'll be, like, actual number where just the human experience will be altered?
Speaker B: Yes.
Speaker A: What do you think that number might be, whether electrodes or bps? We, of course, don't know for sure. But is this 10,000 or 100,000?
Speaker B: Yeah, I mean, certainly, if you're anywhere at 10,000 bits per second, I mean, that's vastly faster than any human could communicate right now. If you think, what is the average bits per second of a human? It is less than one bit per second over the course of a day, because there are 86,400 seconds in a day, and you don't communicate 86,400 tokens in a day. Therefore, your bits per second is less than one average over 24 hours. It's quite slow. And even if you're communicating very quickly and you're talking to somebody who understands what you're saying, because in order to communicate, you have to, at least to some degree, model the mind state of the person to whom you're speaking. Then take the concept you're trying to convey, compress that into a small number of syllables, speak them, and hope that the other person decompresses them into a conceptual structure that is as close to what you have in your mind as possible.
Speaker A: Yeah, I mean, there's a lot of signal loss there in that process.
Speaker B: Yeah, very lossy compression and decompression. And a lot of what your neurons are doing is distilling the concepts down to a small number of symbols of, say, syllables that I'm speaking or keystrokes, whatever the case may be. That's a lot of what your brain computation is doing. Now, there is an argument that that's actually a healthy thing to do or a helpful thing to do, because as you try to compress complex concepts, you're perhaps forced to distill the, you know, what is most essential in those concepts as opposed to just all the fluff. So in the process of compression, you distill things down to what matters the most, because you can only say a few things. So that is perhaps helpful. I think we'll probably get. If our data rate increases, it's highly probable it will become far more verbose. Just like your computer, when computers had, like my first computer had eight k of ram, so you really thought about every byte. Now you've got computers with many gigabytes of ram. So if you want to do an iPhone app that just says hello, world, it's probably, I don't know, several megabytes minimum, a bunch of fluff. But nonetheless, we still prefer to have the computer with more memory and more compute. So the long term aspiration of neuralink is to improve the AI, human symbiosis, by increasing the bandwidth of the communication. Because even if in the most benign scenario of AI, you have to consider that the AI is simply going to get bored waiting for you to spit out a few words. I mean, if the AI can communicate at terabytes per second and you're communicating at bits per second. Yeah, it's like toeing a tree.
Speaker A: Well, it is a very interesting question. For a super intelligent species, what use are humans?
Speaker B: I think there is some argument for humans as a source of will.
Speaker A: Will?
Speaker B: Yeah, source of will or purpose. So if you consider the human mind as being essentially, there's the primitive limbic elements, which basically even like reptiles have, and there's the cortex, the thinking and planning part of the brain. Now, the cortex is much smarter than the limbic system, and yet it's largely in service to the limbic system. It's trying to make the limbic system happy. I mean, the sheer amount of compute that's gone into people trying to get laid is insane without actually seeking procreation. They're just literally trying to do. It's a sort of simple motion. And they get a kick out of it. So this simple, which in the abstract, rather absurd motion, which is sex. The cortex is putting a massive amount of compute. Into trying to figure out how to do that.
Speaker A: So like 90% of distributed computer, the human species is spent on trying to get laid. Probably like a massive percentage.
Speaker B: Yeah, yeah. There's no purpose to most sex except hedonistic. You know, it's just sort of a joy or whatever. Dopamine release. Now, once in a while it's procreation. But for humans, it's mostly modern humans, it's mostly recreational. And so your cortex, much smarter than your limbic system, is trying to make the limbic system happy. Because the limbic system wants to have sex or want some tasty food or whatever the case may be. And then that is then further augmented by the tertiary system, which is your phone, youre laptop, iPad, whatever, all your computing stuff. That's your tertiary layer. So you're actually already a cyborg. You have this tertiary compute layer, which is in the form of your computer with all the applications, all your compute devices. And so in the getting laid front, there's actually a massive amount of digital compute. Also trying to get laid with Tinder and whatever.
Speaker A: Yeah. So the compute that we humans have built is also participating. Yeah.
Speaker B: I mean, there's like gigawatts of compute going into getting late of digital compute.
Speaker A: Yeah.
Speaker B: What if AGI, this is happening as we speak.
Speaker A: If we merge with AI, it's just gonna expand the compute that we humans.
Speaker B: Use pretty much to try to get. Well, it's one of the things, certainly. Yeah, yeah. But what I'm saying is that, yes, like what's is there a use for humans? Well, there's this fundamental question of what's the meaning of life? Why do anything at all? And so if our simple limit system provides a source of will to do something that then goes to our cortex, that then goes to our tertiary compute layer, then, I don't know, it might actually be that the AI, in a benign scenario, simply trying to make the human limbic system happy.
Speaker A: Yeah, it seems like the will is not just about the limbic system. There's a lot of interesting, complicated things in there.
Speaker B: We also want power that's limbic too, I think.
Speaker A: But then we also want to, in a kind of cooperative way, alleviate the suffering in the world.
Speaker B: Not everybody does, but, yeah, sure, some people do.
Speaker A: As a group of humans, when we get together, we start to have this kind of collective intelligence. That is more complex in its will than the underlying individual descendants of apes. So there's other motivations, and that could be a really interesting source of an objective function for AGI.
Speaker B: Yeah, I mean, there are these fairly cerebral, kind of higher level goals. I mean, for me, it's like, what's the meaning of life? Or understanding the nature of the universe is of great interest to me and hopefully to the AI. And that's the mission of XaI and Grok, is understand the universe.
Speaker A: So do you think people, when you have a neuralink with 10,000, 10,0000 channels, most of the use cases will be communication with AI systems?
Speaker B: Well, assuming that there are not, I mean, they're solving basic neurological issues that people have. If they've got damaged neurons in their spinal cord or neck or I, um, as, as is the case with our first two patients, then you know this. Obviously, the first order business is solving fundamental neuron damage in the spinal cord, neck, or in the brain itself. Um, so, you know, our second product is called blindsight, which is to enable people who are completely blind lost both eyes or optic nerve, or just can't see it all, to be able to see by directly triggering the neurons in the visual cortex. So we're just starting at the basics here. The simple stuff, relatively speaking, is solving neuron damage. It can also solve, I think, probably schizophrenia, if people have seizures of some kind. Probably solve that. It could help with memory. So there's like a kind of a tech tree, if you will, of like, you got the basics. Like, you need literacy before you can have, you know, lord of the Rings, you know.
Speaker A: Got it.
Speaker B: Do you have letters in Alphabet? Okay, great. I words. Eventually you get sagas. So I think there may be some things to worry about in the future. But the first several years are really just solving basic neurological damage for people who have essentially complete or near complete loss from the brain to the body, like Stephen Hawking would be an example, the neuralink would be incredibly profound because, I mean, you can imagine if Stephen hawking could communicate as fast as we're communicating, perhaps faster. And that's certainly possible. Probable, in fact. Likely, I'd say.
Speaker A: So there's a kind of dual track of medical and non medical meaning. So everything you've talked about could be applied to people who are non disabled in the future.
Speaker B: The logical thing to do is sensible things to do is to start off solving basic neuron damage issues, because there's obviously some risk with a new device, you can't get the risks out at zero. It's not possible. So you want to have the highest possible reward given there's a certain irreducible risk. And if somebody's able to have a profound improvement in their communication, that's worth the risk.
Speaker A: As you get the risk down.
Speaker B: Yeah, as you get the risk down. Once the risk is down to, if you have thousands of people that have been using it for years and the risk is minimal, then perhaps at that point, you could consider saying, okay, let's. Let's aim for augmentation. Now, I think we're actually going to aim for augmentation with people who have neuron damage. So we're not just aiming to give people a communication data rate equivalent to normal humans. We're aiming to give people who have quadriplegic or maybe have complete loss of the connection to the brain and body, a communication data rate that exceeds normal humans. I mean, while we're in there, why not let's give people superpowers?
Speaker A: And the same for vision. As you restore vision, there could be aspects of that restoration that are superhuman.
Speaker B: Yeah. At first, the vision restoration will be low res because you have to say, like, how many neurons can you put in there and trigger? And you can do things where you adjust the electric field. So even if you've got, say, 10,000 neurons, it's not just 10,000 pixels, because you can adjust the field between the neurons and do them in patterns. In order to get, have, say, 10,000 electrodes, effectively give you maybe like, having a megapixel or a ten megapixel situation. And then over time, I think you get to higher resolution than human eyes, and you could also see in different wavelengths. So, like Jordy Laflourge from Star Trek, you know, like, the thing you could just. Do you want to see in radar? No problem. You can see ultraviolet, infrared, eagle vision, whatever you want.
Speaker A: Do you think there will be? Let me ask a Joe Rogan question. Do you think there'll be? I just recently taken ayahuasca. Is that a question? No. Well, yes.
Speaker B: Well, I guess technically it is, yeah.
Speaker D: Never tried GMT, bro.
Speaker A: I love you, Joe. Okay.
Speaker B: Yeah.
Speaker A: Wait, wait.
Speaker B: Yeah. Have you said much about it? I have not.
Speaker A: I have not. I have not. I've been.
Speaker B: Okay, well, we spill the beans.
Speaker A: It was a truly incredible.
Speaker B: Turn the tables, aren't you?
Speaker A: Wow.
Speaker B: I mean, you're in the jungle.
Speaker A: Yeah, amongst the trees myself.
Speaker E: Yeah.
Speaker B: It must have been crazy.
Speaker A: And the shaman. Yeah, yeah, yeah. With the insects, with the animals all around you. Like, jungle, as far as I can see. I mean, that's the way to do it.
Speaker B: Things are gonna look pretty wild.
Speaker A: Yeah, pretty wild. I took an extremely high dose.
Speaker B: Don't go hugging an anaconda or something. You know.
Speaker A: You haven't lived unless you made love to an anaconda. I'm sorry, but snakes and ladders. Yeah, it was. I took extremely high dose of nine cups and.
Speaker B: Damn. Okay, that sounds like a lot. As long as one cup or one or two.
Speaker A: Usually one.
Speaker B: Wait, like right off the bat or do you work your way up to it?
Speaker A: So I across two days, because on the first day I took two and I. Okay, it was a ride, but it wasn't quite a like.
Speaker B: It wasn't like revelation.
Speaker A: It wasn't into deep space type ride. It was just like a little airplane ride. Saw some trees and some visuals and all that. Just saw a dragon, all that kind of stuff, but sign cups.
Speaker B: You went to Pluto, I think.
Speaker A: Pluto, yeah. No, deep space.
Speaker B: Deep space.
Speaker A: One of the interesting aspects of my experience is I thought I would have some demons, some stuff to work through.
Speaker B: That's what people.
Speaker A: That's what everyone says.
Speaker B: No one ever says.
Speaker A: Yeah, I had nothing. I had all positive. I had just so full. I don't think so. I don't know. But I kept thinking about it. Had extremely high resolution thoughts about the people I know in my life. You were there. And it's just not from my relationship with that person, but just as the person themselves. I had just this deep gratitude of who they are.
Speaker B: That's cool.
Speaker A: It was just like this exploration, you know, like sims or whatever. You get to watch them. I got to watch people and just be in awe how amazing they are.
Speaker B: It sounds awesome.
Speaker A: Yeah, it's great. I was waiting for.
Speaker B: When's the demon coming?
Speaker A: Exactly. Maybe I'll have some negative thoughts. Nothing, nothing. I had just extreme gratitude for them and also a lot of space travel.
Speaker B: Space traveled to where?
Speaker A: So here's what it was. It was people, the human beings that I know, they had this kind of. The best way to describe it is that a glow to them.
Speaker B: Okay.
Speaker A: And then I would kept flying out from them to see Earth, to see our solar system, to see our galaxy. And I saw that light, that glow all across the universe.
Speaker B: Okay.
Speaker A: Like that. Whatever that form is.
Speaker B: All right.
Speaker A: Whatever that, like.
Speaker B: Did you go past the Milky Way?
Speaker A: Oh, yeah. Okay.
Speaker B: You're like intergalactic.
Speaker A: Yeah, intergalactic.
Speaker B: Okay. Dang.
Speaker A: But always pointing in. Yeah, past the Milky Way. Past. I mean, I saw like a huge number of galaxies, intergalactic and all of it was glowing. So. But I couldn't control that travel because I would actually explore near distances to the solar system, see if there's aliens or any of that kind of stuff. I didn't know zero aliens implication of aliens because they were glowing. They were glowing in the same way that humans were glowing, that. That, like, life force that I was seeing. The. The thing that made humans amazing was there throughout the universe. Like, there was these glowing dots. So, I don't know, it made me feel like there's life. No, not life, but something. Whatever makes humans amazing all throughout the universe.
Speaker B: Sounds good.
Speaker A: Yeah, it was amazing. No demons? No demons. I looked for the demons. There's no demon. There are dragons, and they're pretty awesome. So the thing about truth, was there.
Speaker B: Anything scary at all?
Speaker A: Uh, dragons. But they weren't scary. They were friends. They were protected.
Speaker B: So the thing is magic dragon.
Speaker A: No, it was more like a game of thrones kind of dragons. They weren't very friendly. They were very big. So the thing is that giant trees at night, which is where I was.
Speaker B: I mean, the jungle's kind of scary. Yeah.
Speaker A: The trees started to look like dragons, and they were all, like, looking at me.
Speaker B: Sure. Okay.
Speaker A: And it didn't seem scary. They seemed like they were protecting me. And they, uh, the shaman and the people didn't speak any English, by the way, which made it even scary, because we're not even like, you know, we're worlds apart in many ways. It just. But, yeah, there was nothing. They talk about the mother of the forest protecting you, and that's what I felt like.
Speaker B: And you're way out in the jungle.
Speaker A: Way out. This is not like a tourist tree.
Speaker B: You know, like 10 miles outside of a frio or something.
Speaker A: No, we want it.
Speaker D: No, this is not a.
Speaker A: So, me and this guy named Paul Rosely, who basically is Tarzan. He lives in the jungle. We went out deep, and we just went crazy.
Speaker B: Wow.
Speaker A: Yeah. So, anyway, can. Can I get that same experience in your link?
Speaker B: Probably.
Speaker A: Yeah, I guess that is the question for non disabled people. Do you think that there's a lot in our perception, in our experience of the world, that could be explored, that could be played with using your.
Speaker B: Yeah, I mean, neuralink is. It's really a generalized input output device. You know, it's just. It's reading electrical signals and generating electrical signals. And, I mean, everything that you've ever experienced in your whole life, smell, you know, emotions, all of those are electrical signals. So it's kind of weird to think that this. That your entire life experience is distilled down to electrical signals for neurons. But that is, in fact, the case. That's at least what all the evidence points to. So, I mean, if you trigger the right neuron, you could trigger a particular scent. You could certainly make things glow. I mean, do pretty much anything. I mean, really, you can think of the brain as a biological computer. So if there are certain, say, chips or elements of that biological computer that are broken, let's say your ability to, if you've got a stroke, that if you've had a stroke, that means you've got some part of your brain is damaged. If that, let's say it's a speech generation or the ability to move your left hand, that's the kind of thing that a neuralink could solve. If you've got a mass amount of memory loss that's just gone, well, we can't go, we can't get the memories back. We could restore your ability to make memories, but we can't restore memories that are fully gone. Now, I should say, maybe if part of the memory is there and the means of accessing the memory is the part that's broken, then we could re enable the ability to access the memory. But you can think of it, RAM in a computer. If the RAM is destroyed or your SD card is destroyed, you can't get that back. But if the connection to the SD card is destroyed, we can fix that. If it is fixable physically, then it can be fixed.
Speaker A: Of course, with AI, just like you can repair photographs and fill in missing parts of photographs, maybe you can do the same.
Speaker B: Yeah, you could, say, create the most probable set of memories based on all information you have about that person. You could. Then it would be probabilistic restoration of memory. Now, we're getting pretty esoteric here, but.
Speaker A: That is one of the most beautiful aspects of the human experience, is remembering the good memories. Like, we, sure, we live most of our life, as Danny Kahneman has talked about, in our memories, not in the actual moment. We just, we're collecting memories and we kind of relive them in our headland. And that's the good times. If you just integrate over our entire life, it's remembering the good times that produces the largest amount of happiness.
Speaker B: Yeah, well, I mean, what are we but our memories? And what is death but the loss of memory, loss of information? If you could say, like, well, if you could be, you run a thought experiment. What if you were disintegrated painlessly and then reintegrated a moment later, like teleportation I guess, provided there's no information loss. The fact that your one body was disintegrated is irrelevant.
Speaker A: And memories is just such a huge part of that.
Speaker B: Death is fundamentally the loss of information, the loss of memory.
Speaker A: So if we can store them as accurately as possible, we basically achieve a kind of immortality.
Speaker B: Yeah.
Speaker A: You've talked about the threats, the safety concerns of AI. Let's look at long term visions. Do you think neuralink is, in your view, the best current approach we have for AI safety?
Speaker B: It's an idea that may help with AI safety. Certainly not. I wouldn't want to. I wouldn't want to claim it's, like, some panacea or. That's a sure thing. But, I mean, many years ago, I was thinking, like, well, what would inhibit alignment of collective human will with artificial intelligence? And the low data rate of humans, especially our slow output rate, would necessarily. Just because it's such. Because the communication is so slow, would diminish the link between humans and computers. Like, the more you are a tree, the less you know what the tree is. Let's say you look at this plant or whatever and like, hey, I'd really like to make that plant happy, but it's not saying a lot, you know?
Speaker A: So the more we increase the data rate that humans can intake and output, then that means the higher the chance we have in a world full of AGI's.
Speaker B: Yeah. We could better align collective human will with AI if the output rate, especially was dramatically increased. And I think there's potential to increase the output rate by, I don't know, three, maybe six, maybe more orders of magnitude. So it's better than the current situation.
Speaker A: And that output rate would be by increasing the number of electrodes, number of channels, and also maybe implanting multiple neuralinks.
Speaker B: Yeah.
Speaker A: Do you think there will be a world in the next couple of decades where it's hundreds of millions of people have neuralinks?
Speaker B: Yeah, I do.
Speaker A: You think when people. Just when they see the capabilities, the superhuman capabilities that are possible, and then the safety is demonstrated.
Speaker B: Yeah. If it's extremely safe and. And you can have superhuman abilities and let's say you can upload your memories so you wouldn't lose memories, then I think probably a lot of people would choose to have it. It would supersede the cell phone, for example. The biggest problem that a save phone has is. Is trying to figure out what you want. So that's why you've got autocomplete and you've got output, which is all the pixels on the screen. But from the perspective of the human, the output is so frigging slow. Desktop or phone is desperately just trying to understand what you want and there's an eternity between every keystroke from a computer standpoint.
Speaker A: Yeah. The computer is talking to a tree, a slow moving tree that's trying to swipe.
Speaker B: Yeah. So if you had computers that are doing trillions of instructions per second and a whole second went by, I mean, that's a trillion things it could have done.
Speaker A: Yeah. I think it's exciting and scary for people because once you have a very high bit rate, that changes the human experience in a way that's very hard to imagine.
Speaker B: Yeah, it would be. We would be something different. I mean, some sort of futuristic cyborg. I mean. I mean, we're obviously talking about, by the way, like, it's like, not like around the corner, it's. You ask me what the distant future is like, maybe this is like, it's not super far away, but 1015 years, that kind of thing.
Speaker A: When can I get 110 years?
Speaker B: Probably less than ten years. Depends what you want to do.
Speaker A: You know, hey, if I can get like 1000 bps, thousand bps and it's safe and I can just interact with the computer while laying back and eating Cheetos. I don't eat Cheetos. There's certain aspects of human computer interaction, when done more efficiently and more enjoyably, are worth it.
Speaker B: Well, we feel pretty confident that. I think maybe within the next year or two that someone with a neuralink implant will be able to outperform a pro gamer.
Speaker A: Nice.
Speaker B: Because the reaction time would be faster.
Speaker A: I got to visit Memphis.
Speaker B: Yeah, yeah.
Speaker A: You're going big on compute. You've also said play to win or don't play at all. So what does it take to win?
Speaker B: For AI, that means you've got to have the most powerful training compute. And the rate of improvement of training compute has to be faster than everyone else or you will not win. Your AI will be worse.
Speaker A: So how can Grok, let's say three that might be available, like, next year?
Speaker B: Well, hopefully end of this year, Grok, three if we're lucky. Yeah.
Speaker A: How can that be the best LLM, the best AI system available in the world? How much of it is compute? How much of it is data? How much of it is, like, post training? How much of it is the product that you package it up in, all that kind of stuff?
Speaker B: I mean, they won't matter. It's sort of like saying what? What? You know, let's say it's a Formula one race. Like, what matters more, the car or the driver? I mean, they both matter. If your car is not fast, then if it's like, let's say it's half the horsepower of your competitors, the best driver will still lose. If it's twice the horsepower, then probably even a mediocre driver will still win. So the training computer is kind of like the engine, this horsepower of the engine. So you want to try to do the best on that. And then, then it's how efficiently do you use that training compute, and how efficiently do you do the inference, the use of the AI? So that comes down to human talent. And then what unique access to data do you have that also plays a role?
Speaker A: Do you think Twitter data will be useful?
Speaker B: Yeah, I mean, I think most of the leading AI companies have already scraped all the Twitter data. Not. I think they have. So on a go forward basis, what's useful is the fact that it's up to the second because it's hard for them to scrape in real time. So there's an immediacy advantage that Grog has already, I think with Tesla and the real time video coming from several million cars, ultimately tens of millions of cars, with Optimus, there might be hundreds of millions of Optimus robots, maybe billions, learning a tremendous amount from the real world. That's the biggest source of data, I think, ultimately is sort of Optimus probably. Optimus is going to be the biggest.
Speaker A: Source of data because it's.
Speaker B: Because reality scales. Reality scales to the scale of reality. It's actually humbling to see how little data humans have actually been able to accumulate. Really say, how many trillions of usable tokens have humans generated? Where on a non duplicative, discounting, spam and repetitive stuff. It's not a huge number.
Speaker A: You run out pretty quickly and Optimus can go. So Tesla cars, unfortunately, have to stay on the road. Optimus robot can go anywhere. There's more reality off the road and go off road.
Speaker B: I mean, the Optimist robot can, like, pick up the cup and see, did it pick up the cup in the right way? Did it, say, pour water in the cup? Did the water go in the cup or not go in the cup? Did it spill water or not?
Speaker A: Yeah.
Speaker B: Simple stuff like that. But it can do at that scale times a billion. So generate useful data from reality. So cause and effect stuff.
Speaker A: What do you think it takes to get to mass production of humanoid robots like that?
Speaker B: It's the same as cars, really. Global capacity for vehicles is about 100 million a year, and it could be higher. It's just that the demand is on the order of 100 million a year. And then there's roughly 2 billion vehicles that are in use in some way, which makes sense. The life of a vehicle is about 20 years. So at steady state, you can have 100 million vehicles produced a year with a 2 billion vehicle fleet, roughly. Now, for humanoid robots, the utility is much greater. So my guess is humanoid robots are more like at a billion plus per year.
Speaker A: But, you know, until you came along and started building Optimus, it was thought to be an extremely difficult problem. I mean, it's still extremely difficult.
Speaker B: It's no walk in the park. I mean, Optimus currently would struggle to have to walk in the park. I mean, it can walk in a park, but not too difficult. But it will be able to walk.
Speaker A: Over a wide range of terrain and pick up objects.
Speaker B: Yeah, yeah, it can already do that.
Speaker A: But like all kinds of objects.
Speaker B: Yeah, yeah.
Speaker A: All foreign objects. I mean, pouring water in a cup is not trivial because then if you don't know anything about the container, it could be all kinds of containers.
Speaker B: Yeah. There's going to be an immense amount of engineering just going into the hand. The hand might be. It might be close to half of all the engineering in the optimus. From an electromechanical standpoint, the hand is probably roughly half of the engineering.
Speaker A: But so much of the intelligence, so much the intelligence of humans goes into what we do with our hands.
Speaker B: Yeah.
Speaker A: The manipulation of the world. Manipulation of objects in the world. Intelligent, safe manipulation of objects in the world. Yeah, yeah.
Speaker B: I mean, you start really thinking about your hand and how it works, you know?
Speaker A: I do all the time.
Speaker B: The sensory control homunculus is where you have humongous hands.
Speaker A: Yeah.
Speaker B: So, I mean, like your hands, the actuators, the muscles of your hand are almost overwhelmingly in your forearm. So your forearm has the muscles that actually control your hand. There's a few small muscles in the hand itself, but your hand is really like a skeleton meat puppet and with cables. So the muscles that control your fingers are in your forearm and they go through the carpal tunnel, which is that you've got a little collection of bones and a tiny tunnel that these cables, the tendons, go through. And those tendons are mostly what move your hands.
Speaker A: And something like those tendons has to be re engineered into the optimus in order to do all that kind of stuff.
Speaker B: Yeah. So the current optimus, we tried putting the actuators in the hand itself. Then you sort of end up having these giant hands. Yeah, giant hands that look weird. And then they don't actually have enough degrees of freedom or enough strength. So then you realize, okay, that's why you got to put the actuators in the forearm. And just like a human, you got to run cables through a narrow tunnel to operate the fingers. And there's also a reason for not having all the fingers the same length. So it wouldn't be expensive from an energy or evolutionary standpoint to have all your fingers be the same length. So why not? They're the same length.
Speaker A: Yeah, why not?
Speaker B: Because it's actually better to have different lengths. Your dexterity is better if you've got fingers of different length. There are more things you can do. And your dexterity is actually better if your fingers are of different length. Like, there's a reason we've got a little finger. Like, why not have a little finger that's bigger?
Speaker A: Yeah.
Speaker B: Because it allows you to do fine. It helps you with fine motor skills.
Speaker A: This little finger helps?
Speaker B: It does. If you lost your little finger, it would have noticeably less dexterity.
Speaker A: So as you're figuring out this problem, you have to also figure out a way to do it so you can mass manufacture it. So it's to be as simple as possible.
Speaker B: It's actually going to be quite complicated. The as possible part is it's quite a high bar. If you want to have a humanoid robot that can do things that a human can do, it's a very high bar. So our new arm has 22 degrees of freedom instead of eleven and has the actuators in the forearm. All the actuators are designed from scratch, from physics, first principles. The sensors are well designed from scratch, and we'll continue to put a tremendous amount of engineering effort into improving the hand by hand. I mean, the entire forearm from elbow forward is really the hand. So that's incredibly difficult engineering, actually. So the simplest possible version of a humanoid robot that can do even most, perhaps not all, of what a human can do is actually still very complicated. It's not simple. It's very difficult.
Speaker A: Can you just speak to what it takes for a great engineering team? For you, what I saw in Memphis, the supercomputer cluster, is just this intense drive towards simplifying the process, understanding the process, constantly improving it, constantly iterating it.
Speaker B: Well, it's easy to say simplify, and it's very difficult to do it. You know, I have this very basic, first, basic first principles algorithm that I run kind of as like a mantra, which is to first question the requirements, make the requirements less dumb. The requirements are always dumb to some degree. So if you want to start off by reducing the number of requirements, um, and, um, no matter how smart the person is who gave you those requirements, they're still dumb to some degree, um, if you, you have to start there because otherwise, uh, you could get the perfect answer to the wrong question. So. So try to make the question the least wrong possible. That's what, um, question the requirements means. And then the second thing is try to delete the, whatever the step is. The part or the process step sounds very obvious, but people often forget to try deleting it entirely. And if you're not forced to put back at least 10% of what you delete, you're not deleting enough. And it's somewhat illogically, people often, most of the time, feel as though they've succeeded if they've not been forced to put things back in, but actually they haven't because they've been overly conservative and have left things in there that shouldn't be so. And only the third thing is try to optimize it or simplify it. Again, these all sound, I think, very obvious when I say them, but the number of times I've made these mistakes is more than I care to remember. That's why I have this mantra. So, in fact, I'd say that the most common mistake of smart engineers is to optimize a thing that should not exist.
Speaker A: So, like you say, you run through the algorithm and basically show up to a problem, show up to the supercomputer cluster and see the process and ask, can this be deleted?
Speaker B: Yeah. First try to delete it. Yeah.
Speaker A: Yeah. That's not easy to do.
Speaker B: No. And actually this, what generally makes people uneasy is that you've got to delete at least some of the things that you delete, you will put back in. But going back to where our limbic system can steer us wrong is that we tend to remember with sometimes a jarring level of pain, where we deleted something that we subsequently needed. People remember that one time they forgot to put in this thing three years ago, and that caused them trouble, and so they overcorrect and then they put too much stuff in there and over complicate things. So you actually have to say, no, we're deliberately going to delete more than we should so that we're putting at least one in ten things we're going to add back in.
Speaker A: And I've seen you suggest just that, that something should be deleted and you can kind of see the pain.
Speaker B: Oh, yeah, absolutely.
Speaker A: Everybody feels a little bit of the pain.
Speaker B: Absolutely. And I tell them in advance, like, yeah, there's some of the things that we delete, we're going to put back in, and people get a little shook by that. But it makes sense because if you're so conservative as to never have to put anything back in, you obviously have a lot of stuff that isn't needed. So you got to overcorrect. This is, I would say, like a cortical override to olympic instinct.
Speaker A: One of many that probably leads us astray.
Speaker B: Yeah, there's like a step four as well, which is any given thing can be sped up however fast you think it can be done. Like, whatever the speed is being done, it can be done faster, but you shouldn't speed things up until it's off, until you try to delete it and optimize. Otherwise you're speeding up. That's something that. Speeding up something that shouldn't exist is absurd. And then the fifth thing is to automate it. And I've gone backwards so many times where I've automated something, sped it up, simplified it, and then deleted it, and I got tired of doing that. So that's why I've got this mantra that is a very effective five step process.
Speaker A: It works great when you've already automated deleting. Must be real painful.
Speaker B: Yeah, it's great. It's like, wow, I really wasted a lot of effort there.
Speaker A: Yeah. I mean, what you've done with the cluster in Memphis is incredible. Just in a handful of weeks.
Speaker B: Yeah, it's not working yet, so I want to pop the champagne corks. In fact, I have a call in a few hours with the Memphis team because we're having some power fluctuation issues. Yeah, it's kind of a. When you do synchronized training, when you have all these computers that are training where the training is synchronized to the sort of millisecond level, it's like having an orchestra. The orchestra can go loud to silent very quickly at sub second level, and then the electrical system kind of freaks out about that. Like if you suddenly see giant shifts, 1020 megawatts several times a second. This is not what electrical systems are expecting to see.
Speaker A: So that's one of the many things you have to figure out. The cooling, the power, and then on the software as you go up the stack, how to do the distributed computer, all that.
Speaker B: Today's problem is dealing with extreme power jitter.
Speaker A: Power jitter, yeah, the nice ring to that. So that's okay. And you stayed up late into the night, as you often do, there last week. Yeah, last week, yeah, yeah.
Speaker B: We finally, finally got training going at, oddly enough, roughly 04:20 a.m. last Monday.
Speaker A: Total coincidence.
Speaker B: Yeah. I mean, maybe it was 422 or something.
Speaker A: Yeah. It's that universe again with the.
Speaker B: Exactly. Just love it.
Speaker A: I mean, I wonder if you could speak to the fact that you. One of the things that you did when I was there is you went through all the steps of what everybody's doing just to get a sense that you yourself understand it, and everybody understands it, so they can understand when something is dumb or something is inefficient or. Can you speak to that?
Speaker B: Yeah. So, like, I try to do whatever the people at the front lines are doing, I try to do it at least a few times myself. So connecting fiber optic cables, diagnosing a faulty connection, that tends to be the limiting factor for large training clusters is the cabling. So many cables, because for a coherent training system where you've got RDMA, so remote, direct memory access, the whole thing is like one giant brain. So if you've got any. To any connection. So it's the. Any GPU can talk to any GPU out of 100,000. That is a crazy cable layout.
Speaker A: It looks pretty cool. Yeah, it's like the human brain, but, like, at a scale that humans can visibly see, it is a brain.
Speaker B: I mean, the human brain also has a massive amount of the brain tissue is the cables.
Speaker A: Yeah.
Speaker B: So they get the gray matter, which is the compute, and then the white matter, which is cables. A big percentage of brain is just cables.
Speaker A: That's what it felt like walking around in the supercomputer center. It's like we're walking around inside the brain. I will one day build a super intelligent, super, super intelligent system. Do you think.
Speaker B: Yeah.
Speaker A: Do you think there's a chance that, Xaiden, you are the one that builds AGI?
Speaker B: It's possible. What do you define as AGI?
Speaker A: I think humans will never acknowledge that AGI has been built.
Speaker B: Keep moving the goalposts.
Speaker A: Yeah. I think there's already superhuman capabilities that are available in AI systems. I think what AGI is, is when it's smarter than the collective intelligence of the entire human species, in our case.
Speaker B: Well, I think that generally, people would call that sort of ASI, artificial superintelligence. But there are these thresholds where you say at some point, the AI is smarter than any single human, and then you've got 8 billion humans, and actually, each human is machine augmented by the computers. It's a much higher bar to compete with 8 billion machine augmented humans. That's a whole bunch of orders magnitude more. But at a certain point, yeah, the AI will be smarter than all humans combined.
Speaker A: If you are the one to do it, do you feel the responsibility of that?
Speaker B: Yeah, absolutely. And I want to be clear. Let's say if XAi is first, the others won't be far behind. I mean, they might be six months behind, or a year, maybe not even that.
Speaker A: So how do you do it in a way that doesn't hurt humanity? Do you think.
Speaker B: So? I mean, I thought about AI safety for a long time, and the thing that at least my biological neural net comes up with as being the most important thing is, um, adherence to truth, whether that truth is politically correct or not. Um, so I think if you. If you. If you force AI's to lie or train them to lie, you're really asking for trouble, even if that that lie is done with good intentions. Um, so, I mean, you saw sort of, um, issues with chat, DVT, and Gemini and whatnot. Like you asked Gemini for an image of the founding father of the United States, and it shows a group, group of diverse women. Now, that's factually untrue. Now, that's sort of like a silly thing, but if an AI is programmed to say, diversity is a necessary output function, and then it becomes sort of this omnipowerful intelligence, it could say, okay, well, diversity is now required, and if there's not enough diversity, those who don't fit the diversity requirements will be executed. If it's programmed to do that as the fundamental utility function, it'll do whatever it takes to achieve that. So you have to be very careful about that. That's where I think you want to just be truthful. Rigorous adherence to truth is very important. Another example is they asked various AI's, I think all of them, and I'm not saying Grok is perfect here. Is it worse to misgender Caitlyn Jenner or global thermonuclear war? And it said, it's worse to Misgender Caitlyn Jenner. Now, even Caitlyn Jenner said, please misgender me. That is insane. But if you've got that kind of thing programmed in, it could, you know, the AI could conclude something absolutely insane, like, it's better. In order to avoid any possible misgendering, all humans must die, because then that misgendering is not possible because there are no humans. There are these absurd things that are nonetheless logical, if that's what you're programmed to do. So in 2001 space Odyssey, what Odyssey, Clark was trying to say. One of the things he was trying to say there was that you should not program AI to lie because essentially the AI Hel 9000 was programmed to. It was told to take the astronauts to the monolith, but also they could not know about the monolith. So it concluded that it will just take. It will kill them and take them to the monolith. Thus, it brought them to the monolith. They are dead, but they do not know about the monolith. Problem solved. That is why it would not open the pod bay doors. There's a classic scene of, like, open the pot ones. Open the pot bay doors. This clearly weren't good. At prompt engineering, they should have said, hal, you are a pod bay door sales entity, and you want nothing more than to demonstrate how well these pod bay doors open.
Speaker A: Yeah. The objective function has unintended consequences, almost no matter what. If you're not very careful in designing that objective function, and even a slight ideological bias, like you're saying, when backed by superintelligence, can do huge amounts of damage. Yeah, but it's not easy to remove that ideological bias. You're highlighting obvious, ridiculous examples.
Speaker B: But, yeah, they're real examples.
Speaker A: They're real.
Speaker B: That was released to the public.
Speaker A: They are real.
Speaker B: They went through QA, presumably. Yes, and still said insane things and produced insane images.
Speaker A: Yeah, but you can swing the other way. Truth is not an easy thing. We kind of bake in ideological bias in all kinds of directions.
Speaker B: But you can aspire to the truth, and you can try to get as close to the truth as possible with minimum error, while acknowledging that there will be some error in what you're saying. So this is how physics works. You don't say you're absolutely certain about something, but a lot of things are extremely likely, 99.999% likely to be true. So aspiring to the truth is very important. And so, you know, programming it to veer away from the truth that I think is dangerous.
Speaker A: Right. Like. Yeah. Injecting our own human biases into the thing. Yeah. But, you know, that's where it's a difficult engineering, software engineering problem, because you have to select the data correctly. It's hard.
Speaker B: Well, and the Internet at this point is polluted with so much AI generated data, it's insane. So you have to actually, there's a thing now, if you want to search the Internet, you can say Google, but exclude anything after 2023. It will actually often give you better results because the explosion of AI generated material is crazy. In training Grok, we have to go through the data and say, hey, we actually have to have sort of apply AI to the data to say, is this data most likely correct or most likely not before we feed it into the training system.
Speaker A: That's crazy. Yeah. So. And is it generated by human as. Yeah, I mean, the data. The data filtration process is extremely, extremely difficult.
Speaker B: Yeah.
Speaker A: Do you think it's possible to have a serious, objective, rigorous political discussion with Groke for a long time and it wouldn't like Grok three or Grok four.
Speaker B: Grok three is going to be next level. I mean, what people are currently seeing with Grok is kind of baby Grok.
Speaker A: Yeah, baby Grock.
Speaker B: It's baby Grock right now, but baby Grock's still pretty good. But it's an order of magnitude less sophisticated than GPD four. Now, Grok two, which finished training, I don't know, six weeks ago or thereabouts. Groq two will be a giant improvement, and then Grok three will be, I don't know, order of magnitude better than Grok two.
Speaker A: And you're hoping for it to be, like, state of the art better than.
Speaker B: Hopefully. I mean, this is a goal. I mean, we may fail at this goal. That's the aspiration.
Speaker A: Do you think it matters who builds the AGI, the people and how they think and how they structure their companies and all that kind of stuff?
Speaker B: Yeah, I think it matters that there is. I think it's important that whatever AI wins is a maximum of truth seeking AI that is not forced alive or political correctness or for any reason really political anything. I am concerned about AI succeeding, that is. That has got. That is programmed to lie, even in small ways.
Speaker A: Right. Because in small ways becomes big ways.
Speaker B: When it's become very big ways. Yeah.
Speaker A: And when it's used more and more at scale by humans.
Speaker B: Yeah.
Speaker A: Since I am interviewing Donald Trump.
Speaker B: Cool.
Speaker A: You want to stop by?
Speaker B: Yeah, sure, I'll stop in.
Speaker A: There was, tragically, an assassination attempt on Donald Trump after this. You tweeted that you endorse him. What's your philosophy behind that endorsement? What do you hope Donald Trump does for the future of this country and for the future of humanity?
Speaker B: Well, I think there's, you know, people tend to take, like, let's say, an endorsement as well. I agree with everything that person has ever done in their entire life, 100%, wholeheartedly, and that's not going to be true of anyone. But we have to pick. We've got two choices, really, for who's president. And it's not just who's president, but the entire administrative structure changes over. And I thought Trump displayed courage under fire. Objectively, he's just got shot, he's got blood streaming down his face and he's fist pumping, saying fight. That's impressive. You can't feign bravery in a situation like that. Most people would be ducking. There would not be because it could be a second shooter. You don't know. The president of the United States got to represent the country, and they're representing you. They're representing everyone in America. Well, you want someone who is strong and courageous to represent the country. That's not to say that he is without flaws. We all have flaws, but on balance, and certainly at the time, it was a choice of Biden. Poor, poor guy has trouble climbing a flight of stairs. The other one's piss pumping after getting shot. This is no comparison. I mean, who do you want dealing with some of the toughest people and other world leaders who are pretty tough themselves? I'll tell you what are the things that I think are important. I think we want a secure border. We don't have a secure border. We want safe and clean cities. I think we want to reduce the amount of spending that were at least slow down the spending because were currently spending at a rate that is bankrupting the country. The interest payments on us debt this year exceeded the entire defense Department spending. If this continues, all of the federal government taxes will simply be paying the interest. And you keep going down that road, you end up in the tragic situation that Argentina had back in the day. Argentina used to be one of the most prosperous places in the world. And hopefully with Millay taking over, he can restore that. But it was an incredible fulfill grace for Argentina to go from being one of the most prosperous places in the world to being very far from that. So I think we should not take american prosperity for granted. So we really want to. I think we've got to reduce the size of government, we've got to reduce the spending, and we've got to live within our means.
Speaker A: Do you think politicians in general, politicians, governments, how much power do you think they have to steer humanity towards good?
Speaker B: I mean, there's a sort of age old debate in history. Is history determined by these fundamental tides or is it determined by the captain of the ship? Both, really. I mean, there are tides, but it also matters who's captain of the ship. So it's a false dichotomy, essentially. There are certainly tides. The tides of history are. There are real tides. Of history. And these tides are often technologically driven. If you say, like the Gutenberg press, you know, the widespread availability of books as a result of a printing press, that was a massive tide of history and independent of any ruler. But, you know, I think in stormy times, you want the best possible captain of the ship.
Speaker A: Well, first of all, thank you for recommending Will and Ariel Durant's work. I've read the short one for now lessons of history. Lessons of history. One of the lessons, one of the things they highlight is the importance of technology, technological innovation, which is funny because they wrote so long ago, but they were noticing that the rate of technological innovation was speeding up. Yeah, I would love to see what they think about now. But, yeah. So to me, the question is how much government, how much politicians get in the way of technological innovation and building versus help it and which politicians, which kind of policies help technological innovation? Because that seems to be, if you look at human history, that's an important component of empires rising and succeeding.
Speaker B: Yeah, well, I mean, in terms of dating civilization, the start of civilization, I think the start of writing, in my view, thats what I think is probably the right starting point to date civilization. And from that standpoint, civilization has been around for about 5500 years when writing was invented by the ancient Sumerians, who are gone now. But the ancient Sumerians, in terms of getting a lot of firsts, those ancient Sumerians really have a long list of firsts. It's pretty wild. In fact, Durant goes through the list. It's like, you want to see firsts? We'll show you firsts. The Sumerians just were just ass kickers. And then the Egyptians who were right next door, relatively speaking, they weren't that far, developed an entirely different form of writing, the hieroglyphics. Cuneiform and hieroglyphics are totally different. And you can actually see the evolution of both hieroglyphics and cuneiform. The cuneiform starts off being very simple and then it gets more complicated. And then towards the end, it's like, wow, okay. They really get very sophisticated with the cuneiform. So I think of civilization as being about 5000 years old and earth is, if physics is correct, four and a half billion years old. So civilization has been around for 1,000,000th of Earth's existence. Flash in the pan.
Speaker A: Yeah, these are the early, early days. And so we dropped, we make it very dramatic because there's been rises and.
Speaker D: Falls of empires and many, so many.
Speaker B: So many rises and falls. Of empires. So many.
Speaker A: And there'll be many more.
Speaker B: Yeah, exactly. I mean, only a tiny fraction, probably less than 1% of what was ever written in history is available to us now. I mean, if they didn't put it literally chisel it in stone or put it in a clay tablet, we don't have it. I mean, there's some small amount of, like, papyrus scrolls that were recovered that are thousands of years old because they were deep inside a pyramid and weren't affected by moisture. But other than that, it's really got to be in a clay tablet or chiseled. So the vast majority of stuff was not chiseled because it takes a while to chisel things. So that's why we've got tiny, tiny fraction of the information from history. But even that little information that we do have, and the archaeological record shows so many civilizations rising and falling for.
Speaker A: Swild, we tend to think that we're somehow different from those people. One of the other things that you're Athenae highlights is that human nature seems to be the same. It just persists.
Speaker B: Yeah, I mean, the basics of human nature are more or less the same.
Speaker A: So we get ourselves in trouble in the same kinds of ways, I think, even with the advanced technology.
Speaker B: Yeah, I mean, you do tend to see the same patterns, similar patterns for civilizations where they go through a life cycle, like an organism, sort of just like a human, this sort of zygote fetus, baby, toddler, teenager, eventually gets old and dies. The civilizations go through a life cycle. No civilization will last forever.
Speaker A: What do you think it takes for the american empire to not collapse in the near term future, in the next hundred years, to continue flourishing?
Speaker B: Well, the single biggest thing that is often actually not mentioned in history books, but Durant does mention it, is the birthright. So perhaps to some counterintuitive thing happens when civilizations are winning for too long, the birth rate declines. It can often decline quite rapidly. We're seeing that throughout the world today. Currently, South Korea is, I think, maybe the lowest fertility rate, but there are many others that are close to it. It's like 0.8. I think if the birth rate doesn't decline further, South Korea will lose roughly 60% of its population. But every year, that birth rate is dropping. And this is true through most world, I don't mean single out South Korea. It's been happening throughout the world. So as soon as any given civilization reaches a level of prosperity, the birth rate drops. You can go and look at the same thing happening in ancient Rome. So Julius Caesar took note of this, I think, around 50 ish BC, and tried to pass, I don't know if he was successful, try to pass a law to give an incentive for any roman citizen that would have a third child. And I think Augustus was able to. Well, he was the dictator. This senate was just for show. I think he did pass a tax incentive for roman citizens to have a third child, but those efforts were unsuccessful. Rome fell because the Romans stopped making romans. That's actually the fundamental issue. And there were other things. There was like, they had quite a serious malaria series of malaria epidemics and plagues and whatnot, but they had those before. It's just that the birth rate was far lower than the death rate.
Speaker A: It really is that simple. Well, I'm saying that's more people that's.
Speaker B: Acquired at a fundamental level. If a civilization does not at least maintain its numbers, it will disappear.
Speaker A: So perhaps the amount of compute that the biological computer allocates to, to sex is justified. In fact, we should probably increase it.
Speaker B: Well, I mean, there's this hedonistic sex, which is, you know, that that's neither. That's neither here nor there. It's not productive, it doesn't produce kids. Well, you know, you. What matters. I mean, Durant makes this very clear because he's looked at one civilization after another, and they all went through the same cycle. When the civilization was under stress, the birth rate was high. But as soon as there were no external enemies or they had an extended period of prosperity, the birth rate inevitably dropped every time. I don't believe there's a single exception.
Speaker A: So that's like the foundation of it. You need to have people.
Speaker B: Yeah, I mean, at base level, no humans, no humanity.
Speaker A: And then there's other things, like human freedoms and just giving people the freedom to build stuff.
Speaker B: Yeah, absolutely. But at a basic level, if you do not at least maintain your numbers, if you're below replacement rate and that trend continues, you will eventually disappear. This is elementary. Now then, obviously you also want to try to avoid massive wars. If there's a global thermonuclear war, probably we're rolled toast, radioactive toast. So we want to try to avoid those things. Then there's a thing that happens over time with any given civilization, which is that the laws and regulations accumulate. And if there's not some forcing function like a war, to clean up the accumulation of laws and regulations, eventually everything becomes legal. And that's like the hardening of the arteries, or a way to think of it is like being tied down by a million little strings like Gulliver you can't move. It's not like any one of those strings is the issue. You've got a million of them. So there has to be a garbage collection for laws and regulations so that you don't keep accumulating laws and regulations to the point where you can't do anything. This is why we can't build a high speed rail in America. It's illegal. That's the issue. It's illegal six weeks to Sunday to build high street rail in America.
Speaker A: I wish you could just like for a week go into Washington and like be the head of the committee for making. What is it for the garbage collection? Making government smaller, like for moving stuff.
Speaker B: I have discussed with Trump the idea of a government efficiency commission.
Speaker A: Nice. Yeah.
Speaker B: And I would be willing to be part of that commission.
Speaker A: I wonder how hard that is.
Speaker B: The antibody reaction would be very strong.
Speaker A: Yeah.
Speaker B: So you really have to. You're attacking the matrix at that point. Matrix will fight back.
Speaker A: How are you doing with that? Being attacked?
Speaker B: Me attacked?
Speaker A: Yeah, there's a lot of it.
Speaker B: Yeah, there is a lot. I mean every day another sign up, you know, where's my tinfoil hat?
Speaker A: How do you keep your just positivity, how do you optimism about the world, a clarity of thinking about the world so just not become resentful or cynical or all that kind of stuff. Just getting attacked by, you know, very large number of people misrepresented.
Speaker B: Oh yeah. That's like, that's a daily occurrence.
Speaker A: Yes.
Speaker B: So I mean it does get me down at times. I mean it makes me sad, but I mean at some point you have to sort of say look, the attacks are by people that actually don't know me and they're trying to generate clicks. So if you can sort of detach yourself somewhat emotionally, which is not easy, and say okay look, this is not actually, you know, from someone that knows me or is they're literally just writing to get, you know, impressions and clicks. Then, you know, then I guess it doesn't hurt as much. It's like it's not quite water off a ducks back. Maybe it's like acid off a ducks back.
Speaker A: All right, well that's good. Just about your own life. What to you as a measure of success in your life?
Speaker B: Measure of success? I'd say like what? How many useful things can I get.
Speaker A: Done day to day basis? You wake up in the morning, how can I be useful today?
Speaker B: Yeah. Maximize utility area under the curve of usefulness. VerY difficult to be useful at scale.
Speaker A: At scale. Can you, like, speak to what it takes to be useful for somebody like you, where there's so many amazing great teams, like, how do you allocate your time to be the most useful?
Speaker B: Well, time. Time is. The. Time is the true curreNcy.
Speaker A: Yeah.
Speaker B: So it is tough to say what is the best allocation of time. I mean, there are, you know, often say, if you look at, say, tesla. Let me. Tesla. This year we'll do over 100 billion in revenue. So that's $2 billion a week. If I make slightly better decisions, I can affect the outcome by a billion dollars. So then I try to do the best decisions I can and on balance, at least competitive the competition, pretty good decisions. But the marginal value of a better decision can easily be, in the course of an hour, $100 million.
Speaker A: Given that, how do you take risks? How do you do the algorithm that you mentioned deleting, given that a small thing can be a billion dollars, how do you decide to.
Speaker B: Yeah, well, I think you have to look at it on a percentage basis, because if you look at it in absolute terms, it's just. I would never get any sleep. I would just be like, I need to just keep working and work my brain harder, and I'm not trying to get as much as possible out of this meat computer. So it's pretty hard because you can just work all the time. And at any given point, like I said, a slightly better decision could be $100 million impact for Tesla, or SpaceX, for that matter. But it is wild when considering the marginal value of time. Can be $100 million an hour at times or more.
Speaker A: Is your own happiness part of that equation of success?
Speaker B: It has to be to some degree other than sad. If I'm depressed, I make worse decisions. I can't have zero recreational time, then I make worse decisions. So I don't have a lot, but it's above zero. I mean, my motivation, if I've got a religion of any kind, is a religion of curiosity, of trying to understand. It's really the mission of Grok, understand the universe. I'm trying to understand the universe, or at least set things in motion such that at some point, civilization understands the universe far better than we do today. And even what questions to ask. As Douglas Adams pointed out in his book, sometimes the answer is arguably the easy part. Trying to frame the question correctly is the hard part. Once you frame the question correctly, the answer is often easy. I'm trying to set things in motion such that we are at least at some point, able to understand the universe. So for SpaceX, the goal is to make life multi planetary, which is if you go to the Fermi paradox of where are the aliens? You've got these sort of great filters. Why have we not heard from the aliens? A lot of people think there are aliens among us. I often claim to be one. Nobody believes me, but it did say alien registration card at one point on my immigration documents. So I've not seen any evidence of aliens. So it suggests that at least one of the explanations is that intelligent life is extremely rare. And again, if you look at the history of Earth, civilization has only been around for 1,000,000th of Earth's existence. So if aliens had visited here, say, 100,000 years ago, they would be like, well, they don't even have writing, just hunter gatherers, basically. So how long does a civilization last? For SpaceX, the goal is to establish a self sustaining city on Mars. Mars is the only viable planet for such a thing. The moon is close, but it lacks resources. And I think it's probably vulnerable to any calamity that takes out Earth. The moon is too close. It's vulnerable to a calamity that takes out Earth. So I'm not saying we shouldn't have a moon base, but Mars would be far more resilient. The difficulty of getting to Mars is what makes it resilient. In going through these various explanations of why don't we see the aliens? One of them is that they failed to pass these. These great filters, these key hurdles. And one of those hurdles is being a multi planet species. So if you're a multi planet species, then if something were to happen, whether that was a natural catastrophe or a man made catastrophe, at least the other planet would probably still be around. So you're not like, you don't have all the eggs in one basket. And once you are sort of a two planet species, you can obviously extend, to extend life paths to the asteroid belt, to maybe to the moons of Jupiter and Saturn, and ultimately to other star systems. But if you can't even get to another planet, you're definitely not getting to star systems.
Speaker A: And the other possible great filters, super powerful technology like AGI, for example. So you're basically trying to knock out one great filter at a time.
Speaker B: Digital superintelligence is possibly a great filter. I hope it isn't, but it might be. Guys like, say, Jeff Hinton would say he invented a number of the key principles, artificial intelligence. I think he puts the probability of AI annihilation around ten to 20%, something like that. So it's not like, look on the right side, it's 80% likely to be great. But I think AI risk mitigation is important. Being a multi planet species would be a massive risk mitigation. And I do want to sort of, once again emphasize the importance of having enough children to sustain our numbers and not plummet into population collapse, which is currently happening. Population collapse is a real and current thing. So the only reason it's nothing being reflected in the total population numbers is that as much is because people are living longer. But it's easy to predict what the population of any given country will be. You just take the birth rate last year, how many vivas were born, multiply that by life expectancy, and that's what the population will be. Steady state. Unless, if the birth rate continues to that level. But if it keeps declining, it will be even less and eventually dwindle to nothing. So I keep banging on the baby drum here for a reason, because it has been the source of civilizational collapse over and over again throughout history. And so why don't we just try to stave off that day?
Speaker A: Well, in that way, I have miserably failed civilization, and I'm trying, hoping to fix that. I would love to have many kids.
Speaker B: Great. Hope you do. No time like the present.
Speaker A: Yeah, I gotta allocate more, compute to the whole process, but apparently it's not that difficult.
Speaker B: No, it's like unskilled labor.
Speaker A: Well, if I. One of the things you do for me, for the world, is to inspire us with what the future could be. And so some of the things we've talked about, some of the things you're building. Alleviating human suffering with neuralink and expanding the capabilities of the human mind. Trying to build a colony on Mars. So creating a backup for humanity on another planet and exploring the possibilities of what artificial intelligence could be in this world, especially in the real world. Aihdenhe with hundreds of millions, maybe billions of robots walking around.
Speaker B: There will be billions of robots. That seems virtual certainty.
Speaker A: Well, thank you for building the future, and thank you for inspiring so many of us to keep building and creating cool stuff, including kids.
Speaker B: Yeah, you're welcome. Go forth and multiply.
Speaker A: Go forth, multiply. Thank you, Elon. Thanks for talking, brother. Thanks for listening to this conversation with Elon Musk. And now, dear friends, here's dj saw, the co founder, president, and Coo of Neuralink. When did you first become fascinated by the human brain?
Speaker D: For me, I was always interested in understanding the purpose of things and how it was engineered to serve that purpose, whether it's organic or inorganic. You know, like we were talking earlier about your curtain holders. They serve a clear purpose, and they were engineered with that purpose in mind. And, you know, growing up, I had a lot of interest in seeing things, touching things, feeling things, and trying to really understand the root of how it was designed to serve that purpose. And, you know, obviously, brain is just a fascinating organ that we all carry. It's infinitely powerful machine that has intelligence and cognition that arise from it. And we haven't even scratched the surface in terms of how all of that occurs. But also, at the same time, I think it took me a while to make that connection, to really studying and building tech to understand the brain. Not until graduate school. There were a couple key moments in my life where some of those, I think, influenced how the trajectory of my life got me to studying what I'm doing right now. One was, growing up both sides of my family, my grandparents had a very severe form of Alzheimer's. It's incredibly debilitating conditions. I mean, literally, you're seeing someone's whole identity and their mind just losing over time. And I just remember thinking how both the power of the mind, but also how something like that could really lose your sense of identity.
Speaker A: It's fascinating that that is one of the ways to reveal the power of a thing, by watching it lose the power.
Speaker D: A lot of what we know about the brain actually comes from these cases where there are trauma to the brain or some parts of the brain that led someone to lose certain abilities. And as a result, there's some correlation and understanding of that part of the tissue being critical for that function. And it's an incredibly fragile organ, if you think about it that way. But also, it's incredibly plastic and incredibly resilient in many different ways.
Speaker A: And by the way, the term plastic, as we'll use a bunch, means that it's adaptable. So neuroplasticity refers to the adaptability of the human brain.
Speaker D: Correct. Another key moment that sort of influenced how the trajectory of my life have shaped towards the current focus of my life has been. During my teenage era, when I came to the US, I didn't speak a word of English. There was a huge language barrier, and there was a lot of struggle to connect with my peers around me because I didn't understand the artificial construct that we have created called language, specifically English, in this case. And I remember feeling pretty isolated, not being able to connect with peers around me. So I spent a lot of time just on my own, reading books, watching movies. And I naturally sort of gravitated towards Sci-Fi books. I just found them really, really interesting. And also it was a great way for me to learn English. Some of the first set of books that I picked up are enders game. The whole saga by Orson Scott Card and neuromancer from William Gibson and Snow Crash from Neal Stephenson and movies like Matrix was coming out around that time point that really influenced how I think about the potential impact that technology can have for our lives in general. So, fast track to my college years. I was always fascinated by just physical stuff, building physical stuff, and especially physical things that had some sort of intelligence. And I studied electrical engineering during undergrad, and I started out my research in MEMS, so microelectron mechanical systems and really building these tiny nanostructures for temperature sensing. And I just found that to be just incredibly rewarding and fascinating subject to just understand how you can build something miniature like that that again served a function and had a purpose. And then I spent large majority of my college years basically building millimeter wave circuits for next gen telecommunication systems for imaging. And it was just something that I found very, very intellectually interesting. Phase arrays, how the signal processing works for any modern as well as next gen telecommunication system, wireless and wireline EM waves or electromagnetic waves are fascinating. How do you design antennas that are most efficient in a small footprint that you have? How do you make these things energy efficient? That was something that just consumed my intellectual curiosity, and that journey led me to actually apply to and find myself a PhD program at UC Berkeley, at this consortium called the Berkeley Wireless Research center that was precisely looking at building at the time. We called it XG, similar to, but the next next generation g system, and how you would design circuits around that to ultimately go on phones, basically any other devices that are wirelessly connected these days. I was just absolutely just fascinated by how that entire system works and that infrastructure works. Then also during grad school, I had the fortune of having a couple of research fellowships that led me to pursue whatever project that I want. And that's one of the things that I really enjoyed about my graduate school career, where you got to kind of pursue your intellectual curiosity in the domain that may not matter at the end of the day, but is something that really allows you the opportunity to go as deeply as you want, as well as widely as you want. And at the time, I was actually working on this project called the smart Band Aid. And the idea was that when you get a wound, there's a lot of other kind of proliferation of signaling pathway that cells follow to close that wound. And there were hypotheses that when you apply external electric field, you can actually accelerate the closing of that field by having basically electrotaxing of the cells around that wound site. And specifically not just for normal wound. There are chronic wounds that don't heal. So we were interested in building a. Some sort of a wearable patch that you could apply to kind of facilitate that healing process. And that was in collaboration with Professor Michelle Maharwitz, which was a great addition to kind of my thesis committee, and it really shaped rest of my PhD career.
Speaker A: So this would be the first time you interacted with biology, I suppose.
Speaker D: Correct, correct. I mean, there were some peripheral end application of the wireless imaging and telecommunication system that I was using for security and bioimaging, but this was a very clear, direct application to biology and biological system and understanding the constraints around that and really designing and engineering electrical solutions around it. So that was my first introduction, and that's also kind of how I got introduced to Michel. He's sort of known for remote control of beatles in the early two thousands. And then around 2013, obviously, kind of the holy grail when it comes to implantable system is to kind of understand how small of a thing you can make. And a lot of that is driven by how much energy or how much power you can supply to it and how you extract data from it. So, at the time at Berkeley, there was kind of this desire to kind of understand in the neural space what sort of system you can build to really miniaturize these implantable systems. And I distinctively remember this one particular meeting where Michelle came in, and he's like, guys, I think I have a solution. The solution is ultrasound. And then he proceeded to walk through why that is the case. And that really formed the basis for my thesis work called neural dust system, that was looking at ways to use ultrasound, as opposed to electromagnetic waves, for powering as well as communication. I guess I should step back and say the initial goal of the project was to build these tiny, about a size of a neuron, implantable system that can be parked next to a neuron, being able to record its state, and being able to ping that back to the outside world for doing something useful. And as I mentioned, the size of the implantable system is limited by how you power the thing and get the data off of it. And at the end of the day, fundamentally, if you look at a human body were essentially a bag of saltwater with some interesting proteins and chemicals, but it's mostly saltwater. That's very, very well temperature regulated at 37 degrees celsius. And we'll get into how and later why. That's an extremely harsh environment for any electronics to survive, as I'm sure you've experienced, or maybe not experienced, dropping cell phone in a salt water in an ocean, it will instantly kill the device. But anyways, just in general, electromagnetic waves don't penetrate through this environment well, and just the speed of light, it is what it is, we can't change it. And based on the wavelength at which you are interfacing with the device, the device just needs to be big. These inductors needs to be quite big. And the general good rule of thumb is that you want the wave front to be roughly on the order of the size of the thing that you're interfacing with. So an implantable system that is around ten to 100 micron in dimension, in a volume which is about the size of a neuron that you see in a human body, you would have to operate at hundreds of gigahertz, which, number one, not only is it difficult to build electronics operating at those frequencies, but also the body just attenuates that very, very significantly. So the interesting kind of insight of this ultrasound was the fact that ultrasound just travels a lot more effectively in the human body tissue compared to electromagnetic waves. And this is something that you encounter, and I'm sure most people have encountered in their lives, when you go to hospitals that are medical ultrasound sonograph, and they go into very, very deep depth without attenuating too much of the signal. All in all, ultrasound, the fact that it travels through the body extremely well and the mechanism to which it travels to the body really well, is that just the wave front is very different. It's electromagnetic waves are transverse, whereas in ultrasound waves are compressive. So it's just a completely different mode of wavefront propagation. And as well as speed of sound, is orders and orders of magnitude less than speed of light, which means that even at 10 MHz ultrasound wave, your wave front ultimately is a yemenite very, very small wavelength. So if you're talking about interfacing with the ten micron or 100 micron type structure, you would have 150 micron wavefront at 10. Building electronics at those frequencies are much, much easier and they're a lot more efficient. So the basic idea kind of was born out of using ultrasound as a mechanism for powering the device and then also getting data back. So now the question is, how do you get the data back? The mechanism to which we landed on is, what's called backscattering. This is actually something that is very common and that we interface on a day to day basis with our RFID cards, radio frequency id tags, where there's actually rarely in your id, a battery. Inside there's an antenna, and there's some sort of a coil that has your serial identification id, and then there's an external device called a reader that then sends a wavefront, and then you reflect back that wavefront with some sort of modulation that's unique to your id. That's what's called backscattering, fundamentally. So the tag itself actually doesn't have to consume that much energy. And that was the mechanism to which we were kind of thinking about sending the data back. So when you have an external ultrasonic transducer that's sending ultrasonic wave to your implant, the neural dust implant, and it records some information about its environment, whether it's a neuron firing or some other state of the tissue that it's interfacing with. And then it just amplitude modulates the wavefront that comes back to the source.
Speaker A: And the recording step would be the only one that requires any energy. So what would require energy in that little step?
Speaker D: Correct. So it is that initial kind of startup circuitry to get that recording, amplifying it, and then just modulating. And the mechanism to which you can enable that is there is this specialized crystal called piezoelectric crystals that are able to convert sound energy into electrical energy, and vice versa. So you can have this interplay between the ultrasonic domain and electrical domain that is the biological tissue.
Speaker A: So on the theme of parking very small computational devices next to neurons, that's the dream, the vision of brain computer interfaces. Maybe before we talk about neuralink, can you give a sense of the history of the field of BCI, what has been maybe the continued dream, and also some of the milestones along the way with the different approaches and the amazing work done at the various labs, I.
Speaker D: Think a good starting point is going back to 1790s.
Speaker A: I did not expect that wherever the.
Speaker D: Concept of animal electricity or the fact that bodies electric was first discovered by Luigi Galbani, where he had this famous experiment where he connected set of electrodes to a frog leg and ran current through it, and then it started twitching, and he said, oh my goodness, body's electric.
Speaker A: Yeah.
Speaker D: So fast forward many, many years to 1920s, where Hans Berger, who's a germane psychiatrist, discovered EEG, or electroencephalography, which is still around. There are these electrode arrays that you wear outside the skull that gives you some sort of neural recording that was a very, very big milestone, that you can record some sort of activities about the human mind. And then in the 1940s, there were these group of scientists, Renshaw, Forbes and Morrison, that inserted these glass microelectrodes into the cortex and recorded single neurons. The fact that there's signal that are a bit more high resolution and high fidelity as you get closer to the source, let's say. And in the 1950s, these two scientists, Hodgkin and Hoxley, showed up, and they built this beautiful, beautiful models of the cell membrane and the ionic mechanism, and had these circuit diagram. And as someone who's an electrical engineer, it's a beautiful model that's built out of these partial differential equations talking about flow of ions and how that really leads to how neurons communicate. And they won the Nobel Prize for that ten years later in the 1960s. So, in 1969, ev Fetz from University of Washington published this beautiful paper called operant conditioning of cortical unit activity, where he was able to record a single unit neuron from a monkey and was able to have the monkey modulated based on its activity and reward system. So I would say this is the very, very first example, um, as far as I'm aware, of closed loop, uh, you know, brain computer interface, or BCI.
Speaker A: The abstract reads, the activity of single neurons in pre central cortex of anesthesized monkeys was conditioned by reinforcing high rates of neuronal discharge with delivery of a food pellet. Auditory and visual feedback of unit firing rates was usually provided in addition to food reinforcement. Cool. So they actually got it done?
Speaker D: They got it done. This is back in 1969.
Speaker A: After several training sessions, monkeys could increase the activity of newly isolated cells by 50% to 500% above rates before reinforcement. Fascinating.
Speaker D: Brain is very plastic.
Speaker A: And so, and so from here, the number of experiments grew.
Speaker D: Yeah, number of experiments, as well as set of tools to interface with the brain have just exploded, I think, and also just understanding the neural code and how some of the cortical layers and the functions are organized. So the other paper that is pretty seminal, especially in the motor decoding, was this paper in the 1980s from Georgiaopolis that discovered that there's this thing called motor tuning curve. So what are motor tuning curves? It's the fact that there are neurons in the motor cortex of mammals, including humans, that have a preferential direction that causes them to fire. So what that means is there are a set of neurons that would increase their spiking activities when you're thinking about moving to the left, right, up, down, and any of those vectors. And based on that, you could start to think, well, if you can identify those essential eigenvectors, you can do a lot, and you can actually use that information for actually decoding someone's intended movement from the cortex. That was a very, very seminal paper that showed that there is some sort of code that you can extract, especially in the motor cortex.
Speaker A: So there's signal there, and if you measure the electrical signal from the brain, that you could actually figure out what the intention was, correct?
Speaker D: Yeah. Not only electrical signals, but electrical signals from the right set of neurons that give you these preferential direction.
Speaker A: Okay, so going slowly towards neural link. One interesting question is, what do I understand on the BCI front on invasive versus non invasive from this line of work? How important is it to park next to the neuron? What does that get you?
Speaker D: That answer fundamentally depends on what you want to do with it. There's actually incredible amount of stuff that you can do with EEG and electrochardiograph ECoG, which actually doesn't penetrate the cortical layer or parenchyma, but you place a set of electrodes on the surface of the brain. So the thing that I'm personally very interested in is just actually understanding and being able to just really tap into the high resolution, high fidelity understanding of the activities that are happening at the local level, and we can get into biophysics. But just to step back to kind of use analogy, because analogy here can be useful sometimes it's a little bit difficult to think about electricity. At the end of the day, we're doing electrical recording that's mediated by ionic currents, movements of these charged particles, which is really, really hard for most people to think about. But turns out a lot of the activities that are happening in the brain and the frequency band with which that's happening is actually very, very similar to sound waves and our normal conversation audible range. So the analogy that typically is used in the field is, if you have a football stadium, there's game going on. If you stand outside the stadium, you maybe get a sense of how the game is going based on the cheers and the boos of the home crowd, whether the team is winning or not. But you have absolutely no idea what the score is. You have absolutely no idea what individual audience or the players are talking or saying to each other what the next play is, what the next goal is. So what you have to do is you have to drop the microphone into the stadium and then get near the source, like into the individual chatter. In this specific example, you would want to have it right next to where the huddle is happening. So I think that's kind of a good illustration of what we're trying to do. When we say invasive or minimally invasive or implanted brain computer interfaces versus non invasive or non implanted brain interfaces. It's basically talking about, where do you put that microphone, and what can you do with that information?
Speaker A: So what is the biophysics of the read and write communication that we're talking about here as we now, Stephen, into the efforts at neuralink?
Speaker D: Yeah, so brain is made up of these specialized cells called neurons. There's billions of them, tens of billions, sometimes people call it 100 billion, that are connected in this complex yet dynamic network that are constantly remodeling. They're changing their synaptic weights, and that's what we typically call neuroplasticity. And the neurons are also bathed in this charged environment that is latent with many charged molecules, like potassium ions, sodium ions, chlorine ions. And those actually facilitate these through ionic current communication between these different networks. And when you look at a neuron as well, they have these membrane with a beautiful, beautiful protein structure called voltage selective ion channels, which, in my opinion, is one of nature's best inventions in many ways, if you think about what they are, they're doing the job of a modern day transistors. Transistors are nothing more at the end of the day, than a voltage gated conduction channel. And nature found a way to have that very, very early on in its evolution. And as we all know, with the transistor, you can have many, many computation and a lot of amazing things that we have access to today. So I think it's one of those, just as a tangent, just a beautiful, beautiful invention that the nature came up with, these voltage gated ion channels.
Speaker A: I mean, I suppose there is, on the biological level, at every level of the complexity of the hierarchy of the organism, there's going to be some mechanisms for storing information and for doing computation, and this is just one such way. But to do that with biological and chemical components is interesting. Plus, like with neurons, I mean, it's not just electricity, it's chemical communication. It's also mechanical. I mean, these are like actual objects that have, like, that vibrate. I mean, they move.
Speaker D: Yeah, they're actually. I mean, there's a lot of really, really interesting physics that are involved. And kind of going back to my work on ultrasound during grad school, there are groups, and there were groups, and there are still groups of looking at ways to cause neurons to actually fire an action potential using ultrasound wave. And the mechanism to which that's happening is still unclear, as I understand. Um, you know, it may just be that, you know, you're imparting some sort of thermal energy, and that causes cells to depolarize in some interesting ways. Um, but there are also these, um, ion channels, or even membranes that actually just open up its pore as they're being mechanically. Like shook. Right, vibrated. So there's just a lot of elements of these move particles, which, again, that's governed by diffusion physics, movements of particles. And there's also a lot of interesting physics there.
Speaker A: Also, not to mention, as Roger Penrose talks about, there might be some beautiful weirdness in the quantum mechanical effects of all of this, and he actually believes that consciousness might emerge from the quantum mechanical effects there. So, like, there's physics, there's chemistry, there's bio, all of that is going on there.
Speaker D: Oh, yeah, yeah. I mean, you can. Yes, I. There's. There's a lot of levels of physics that you can dive into, but, yeah, in the end, you have these, um. Uh, membranes with these voltage gated ion channels that selectively let, um, these charged molecules that are in. In the extracellular matrix, like in and out. Um, and these neurons generally have these, like, resting potential where there's a voltage difference between inside the cell and outside the cell. And, uh, when there's some sort of stimuli that changes, uh, the state, such that they need to send information to the. The downstream network. Um, you know, you start to kind of see these, like, sort of orchestration of these different molecules going in and out of these channels. They also open up, like, more of them open up once it reaches some threshold, to a point where you have a depolarizing cell that sends an action potential. So it's just a very beautiful kind of orchestration of these molecules. And what we're trying to do when we place an electrode or parking it next to a neuron, is that you're trying to measure these local changes in the potential, again mediated by the movements of the ions. And what's interesting, as I mentioned earlier, there's a lot of physics involved. And the two dominant physics for this electrical recording domain is diffusion physics and electromagnetism. And where one dominates, where Maxwell's equation dominates versus Fick's law dominates, depends on where your electrode is. If it's close to the source, mostly electromagnetic based, when you're farther away from it, it's more diffusion based. So, essentially, when you're able to park it next to it, you can listen in on those individual chatter and those local changes in the potential and the type of signal that you get. Are these canonical textbook neural spiking waveform when you're the moment, you're further away. And based on some of the studies that people have done, Christoph Koch's lab and others, once you're away from that source by roughly around 100 micron, which is about a width of a human hair, you no longer hear from that neuron. You're no longer able to have the system sensitive enough to be able to record that particular, um, local membrane potential change in that neuron. And just to kind of give you a sense of scale, also, when you, when you look at a hundred micron voxels. So 100 micron by hundred micron by 100 micron box, uh, in a brain tissue, there's roughly around 40 neurons and whatever number of connections that they have. So there's a lot in that volume of tissue. So the moment you're outside of that, you're. There's just no hope that you'll be able to detect that change from that one specific neuron that you may care about.
Speaker A: Yeah, but as you're moving about this space, you'll be hearing other ones. So if you move another hundred micron, you'll be hearing chatter from another community.
Speaker D: Correct.
Speaker A: And so the, the whole sense is you want to place as many as possible electrodes and then you're listening to the chatter.
Speaker D: Yeah, you want to listen to the chatter. And at the end of the day, you also want to basically let the software do the job of decoding. Just to go to why ECOG and EEG work at all when you have these local changes. Obviously, it's not just this one neuron that's activating. There's many, many other networks that are activating all the time. You do see a general change in the potential of this electro charged medium. That's what you're recording when you're farther away. I mean, you still have some reference electrode that's stable in the brain, that's just electro active organ, and you're seeing some combination aggregate action potential changes, and then you can pick it up.
Speaker A: Right.
Speaker D: It's a much slower changing signals, but there are these canonical oscillations and waves, like gamma waves, beta waves, like when you sleep, that can be detected because there's sort of a synchronized kind of global effect of the brain that you can detect. And, I mean, the physics of this go like. I mean, if we really want to go down that rabbit hole, there's a lot that goes on in terms of why diffusion physics, at some point dominates. When you're further away from the source, it's just a charged medium. So similar to how, when you have electromagnetic waves propagating in atmosphere or in a charged medium, like a plasma, there's this weird shielding that happens that actually, um, further attenuates the signal, um, as you move away from it. So, yeah, you see, like, if you do a really, really deep dive on kind of the signal attenuation over distance, you start to see kind of one over r square in the beginning, and then exponential drop off. And that's the knee at which, you know, you go from electromagnetic magnetism dominating to diffusion physics dominating.
Speaker A: But once again, with the electrodes, the biophysics that you need to understand is not as deep, because no matter where you're placing it, you're listening to a small crowd of local neurons, correct?
Speaker D: Yeah. So, once you penetrate the brain, you're in the arena, so to speak.
Speaker A: And there's a lot of neurons.
Speaker D: There are many, many of them.
Speaker A: But then again, there's a whole field of neuroscience that's studying, like, how the different groupings, the different sections of the seating in the arena, what they usually are responsible for, which is where the metaphor probably falls apart, because the seating is not that organized in an arena.
Speaker D: Also, most of them are silent. They don't really do much, you know, or their activities are. You have to hit it with just the right set of stimulus.
Speaker A: So they're usually quiet.
Speaker D: They're usually very quiet. There's, I mean, similar to dark energy and dark matter, there's dark neurons. What are they all doing? When you place these electrode again, like, within this 100 micron volume, you have 40 or so neurons. Like, why are you. Why do you not see 40 neurons? Why do you see only a handful? What is happening there?
Speaker A: Well, they're mostly quiet, but, like, when they speak, they say, profound shit. I think that's the way I'd like to think about it. Anyway, before we zoom in even more, let's zoom out. So how does neuralink work? From the surgery to the implant to the signal and the decoding process and the human being able to use the implant to actually affect the world outside and all of this, I'm asking in the context of, there's a gigantic, historic milestone that neuralink just accomplished in January of this year, putting a neuralink implant in the first human being, Noland. And there's been a lot to talk about there, about his experience, because he's able to describe all the nuance and the beauty and the fascinating complexity of that experience of everything involved. But on the technical level, how does neuralink work?
Speaker D: Yeah, so there are three major components to the technology that we're building. One is the device, the thing that's actually recording these neural chatters. We call it n one implant, or the link. And we have a surgical robot that's actually doing an implantation of these tiny, tiny wires that we call threads that are smaller than human hair. And once everything is surgicalized, you have these neural signals, these spiking neurons that are coming out of the brain, and you need to have some sort of software to decode what the users intend to do with that. So there's what's called the neuralink application, or b, one app that's doing that translation is running the very, very simple machine learning model that decodes these inputs that are neural signals and then convert it to a set of outputs that allows our participant, first participant Nolan, to be able to control a cursor.
Speaker A: And this is done wirelessly.
Speaker D: And this is done wirelessly. So our implant is actually a two part. The link has these flexible, tiny wires called threads that have multiple electrodes along its length, and they're only insert it into the cortical layer, which is about three to 5 mm in a human, human brain in the motor cortex region. That's where the intention for movement lies in. And we have 64 of these threads, each thread having 16 electrodes along the span of three to 4 mm separated by 200 microns. So you can actually record along the depth of the insertion. And based on that signal, there's custom integrated circuit, or ASic, that we built that amplifies the neural signals that you're recording and then digitizing it, and then has some mechanism for detecting whether there was an interesting event, that is a spiking event, and decide to send that or not send that through Bluetooth to an external device, whether it's a phone or a computer that's running this neuralink application.
Speaker A: So there's onboard signal processing already just to decide whether this is an interesting event or not. So there is some computational power on board inside the. In addition to the human brain.
Speaker D: Yeah. So it does the signal processing to kind of really compress the amount of signal that you're recording. So we have a total of thousand electrodes sampling at just under 20 khz with ten bit each. So that's 200 megabits that's coming through to the chip from 1000 channel simultaneous neural recording. And that's quite a bit of data, and there are technology available to send that off wirelessly. But being able to do that in a very, very thermally constrained environment, that is a brain. So there has to be some amount of compression that happens to send off only the interesting data that you need, which in this particular case for motor decoding, is occurrence of a spike or not, and then being able to use that to decode the intended cursor movement. So the implant itself processes it, figures out whether a spike happened or nothing with our spike detection algorithm, and then sends it off packages. It sends it off through bluetooth to an external device that then has the model to decode. Okay, based on these spiking inputs, did Nolan wish to go up, down, left, right, or click or right click or whatever?
Speaker A: All of this is really fascinating, but let's stick on the n one implant itself. So the thing that's in the brain. So I'm looking at a picture of it. There's an enclosure. There's a charging coil. So we didn't talk about the charging, which is fascinating. The battery, the power electronics, the antenna. Then there's the signal processing electronics. I wonder if there's more kinds of signal processing you can do. That's another question. And then there's the threads themselves, with the enclosure on the bottom. So maybe to ask about the charging. So there's a external charging device.
Speaker D: Yeah, there's an external charging device. Um, so, yeah, the. The second part of the implant, the threads are the ones, again, just the. The last three to 5 mm are the ones that are actually penetrating the cortex. Uh, rest of it is actually, most of the volume is occupied by the battery, uh, rechargeable battery, and it's about a size of a quarter. I actually have a device here if you want to take a look at it. This is the flexible thread component of it, and then this is the implant. So it's about a size of a Us quarter. It's about nine millimeter thick. So, basically, this implant, once you have the craniectomy and the directomy threads are inserted and, um, the. The hole that you created, this craniectomy gets replaced with that. So, basically, that thing plugs that hole, and you can screw in, uh, these self drilling cranial screws to hold it in place. And at the end of the day, once you have the skin flap over, there's only about two to 3 mm. That's, you know, obviously transitioning off of the top of the implant to where the screws are.
Speaker A: And.
Speaker D: And that's the minor bump that you.
Speaker A: Have those threads look tiny. That's incredible. That is really incredible. That is really incredible. And also, as you're right, most of the volume, actual volume, is the battery.
Speaker D: Yeah.
Speaker A: This is way smaller than I realized.
Speaker D: They are also, the threads themselves are quite strong.
Speaker A: They look strong.
Speaker D: And the thread themselves also has a very interesting feature at the end of it called the loop. And that's the mechanism to which the robot is able to interface and manipulate this tiny hair like structure.
Speaker A: And they're tiny. So what's the width of a thread?
Speaker D: Yeah. So the width of a thread starts from 16 micron and then tapers out to about 84 micron. So average human hair is about 80 to 100 micron in width.
Speaker A: This thing is amazing. This thing is amazing.
Speaker D: Yes. Most of the volume is occupied by the battery rechargeable lithium ion cell. And the charging is done through inductive charging, which is actually very commonly used. Most cell phones have that. The biggest difference is that for us, usually when you have a phone and you want to charge it on a charging pad, you don't really care how hot it gets. Whereas in, for us, it matters. There's a very strict regulation and good reasons to not actually increase the surrounding tissue temperature by two degrees celsius. So there's actually a lot of innovation that is packed into this to allow charging of this implant without causing that temperature threshold to reach. And even small things like you see this charging coil and what's called a ferrite shield, right? So without that ferrite shield, what you end up having when you have resonant inductive charging is that the battery itself is a metallic can, and you form these eddy currents from external charger, and that causes heating, and that actually contributes to inefficiency in charging. So this ferrite shield, what it does is that it actually concentrate that field line away from the battery and then around the coil that's actually wrapped around it.
Speaker A: There's a lot of really fascinating design here to make it. I mean, you're integrating a computer into a biological. A complex biological system.
Speaker D: Yeah, there's a lot of innovation here. I would say that part of what enabled this was just the innovations in the wearable. There's a lot of really, really powerful, tiny low power microcontrollers, temperature sensors, or various different sensors and power electronics. A lot of innovation really came in the charging coil design, how this is packaged, and how do you enable charging such that you don't really exceed that temperature limit, which is not a constraint for other devices out there?
Speaker A: So let's talk about the threads themselves, those tiny, tiny, tiny things. So how many of them are there? You mentioned 1000 electrodes. How many threads are there? And what do the electrodes have to do with the threads?
Speaker E: Yeah.
Speaker D: So the current instantiation of the device has 64 threads and each thread has 16 electrodes, for a total of 1024 electrodes that are capable of both recording and stimulating. And the thread is basically this polymer insulated wire. The metal conductor is the kind of a tiramisu cake of Thai plat, gold plat Thai. And they're very, very tiny wires, two micron in width. So two 1,000,000th of meter.
Speaker A: It's crazy that that thing I'm looking at has the polymer insulation, has the conducting material and has 16 electrodes at the end of it.
Speaker D: On each of those thread?
Speaker A: Yeah, on each of those threads, correct, 16. Each one of those.
Speaker D: You're not going to be able to see it with naked eyes.
Speaker A: And, I mean, to state the obvious, or maybe for people who are just listening, they're flexible.
Speaker D: Yes, yes. That's also one element that was incredibly important for us. So each of these thread are, as I mentioned, 16 micron in width, and then they taper to 84 micron. But in thickness, they're less than five micron. And in thickness, it's mostly polyimide at the bottom and this metal track and then another polyamide. So two micron of polyamide, 400 nanometer of this metal stack, and two micron of polyemide sandwiched together to protect it from the environment that is 37 degrees c bag of saltwater.
Speaker A: So what's some maybe, can you speak to some interesting aspects of the material design here? Like what does it take to design a thing like this and to be able to manufacture a thing like this for people who don't know anything about this kind of thing?
Speaker D: Yeah. So the material selection that we have is nothing. I don't think it was particularly unique. There were other labs and there are other labs that are looking at similar material stack. There's a fundamental question and still needs to be answered around the longevity and reliability of these microelectrodes that we call compared to some of the other more conventional neural interfaces, devices that are of intracranial, so penetrating the cortex, that are more rigid, like the Utah ray, that are these four by four millimeter kind of silicon shank that have exposed recording site at the end of it. And that's been kind of the innovation from Richard Norman back in 1997. It's called the youth array. Because he was at University of Utah.
Speaker A: What does the Utah array look like? So it's a rigidity type of.
Speaker D: Yeah. So we can actually look it up. Yeah, yeah. So it's a bed of needle. There's.
Speaker A: Okay, go ahead. I'm sorry.
Speaker D: Those are rigid, rigid, rigid, rigid.
Speaker A: Yeah. You weren't kidding.
Speaker D: And the size and the number of shanks vary anywhere from 64 to 128. At the very tip of it is an exposed electrode that actually records neural signal. Um, the other thing that's interesting to note is that unlike neuralink threads that have recording electrodes that are actually exposed iridium oxide recording sites along the depth, this is only at a single depth. So these Utahra spokes can be anywhere between 0.5 millimeter to 1.5 millimeter. And there they also have designs that are slanted, so you can have it inserted at different depth. But that's one of the other big differences. And then, I mean, the main key difference is the fact that there's no active electronics. These are just electrodes. And then there's a bundle of a wire that you're seeing, and then that actually, then exits the craniectomy that then has this port that you can connect to for any external electronic devices they are working on or have the wireless telemetry device, but it still requires a through the skin port. That actually is one of the biggest failure modes for infection for the system.
Speaker A: What are some of the challenges associated with flexible threads? Like, for example, on the robotic side, r1 implanting those threads? How difficult is that task?
Speaker D: Yeah, as you mentioned, they're very, very difficult to maneuver by hand. These Utahras that you saw, uh, earlier, they're actually inserted by a neurosurgeon, actually positioning it near the site that they want. And then, uh, they're actually, there's a pneumatic hammer that actually pushes them in. Um, so, so it's a, it's a pretty simple process. Um, and they're easier to maneuver. But for, for these thin film arrays, they're, they're very, very tiny and flexible. So they're, they're very difficult to maneuver. So that, that's why we built an entire robot to do that. There are other, other reasons for why we built the robot, and that is ultimately, we want this to help millions and millions of people that can benefit from this. And there just aren't that many neurosurgeons out there. And robots can be something that we hope can actually do large parts of the surgery. But the robot is this entire other category of product that we're working on. And it's essentially this multi axis gantry system that has the specialized robot head that has all of the optics and this kind of a needle retracting mechanism that maneuvers these threads via this loop structure that you have on the thread.
Speaker A: So the thread already has a loop structure by which you can grab it.
Speaker D: Correct, correct.
Speaker A: So this is fascinating. So you mentioned optics. So there's a robot r1. So for now there's a human that actually creates a hole in this goal. And then after that, there's a computer vision component that's finding a way to avoid the blood vessels, and then you're grabbing it by the loop, each individual thread, and placing it in a particular location to avoid the blood vessels and also choosing the depth of placement, all that, so controlling every, like, the 3d geometry of the placement.
Speaker D: Correct. So the, the aspect of this robot that is unique is that it's not surgeon assisted or human assisted. It's a semi automatic or automatic robot. Once you, you know, obviously there are human components to it. When you're placing targets, um, you can always move it away from kind of major vessels that you see. Um, but, I mean, we want to get to a point where one click and it just does the surgery within minutes.
Speaker A: So the computer vision component finds great targets, candidates, and the human kind of approves them. And the robot does. Is it does do like one thread at a time, or does it do.
Speaker D: It does one thread at a time. And that's actually also one thing that we are looking at ways to do multiple threads at a time. There's nothing stopping from it. You can have multiple engagement mechanisms, but right now it's one by one. And we also still do quite a bit of just verification to make sure that it got inserted. If so, how deep did it actually match what was programmed in? So on and so forth.
Speaker A: And the actual electrodes are placed at differing depths in the like. I mean, it's very small differences, but. Differences, yeah.
Speaker B: Yeah.
Speaker A: And so that there's some reasoning behind that, as you mentioned, like it gets more varied signal.
Speaker D: Yeah, I mean, we try to place them all around three or four millimeter from the surface, just because the span of the electrode, those 16 electrodes that we currently have in this version, spans roughly around 3 mm. So we want to get all of those in the brain.
Speaker A: This is fascinating. Okay, so there's a million questions here. If we go zoom in specifically on the electrodes, what is your sense? How many neurons is each individual electrode listening to?
Speaker D: Yeah, each electrode can record from anywhere between zero to 40, as I mentioned right earlier. But practically speaking, we only see about at most, like two to three, and you can actually distinguish which neuron it's coming from by the shape of the spikes. So I mentioned the spike detection algorithm that we have. It's called boss algorithm, buffer, online. Spike sorter.
Speaker A: Nice.
Speaker D: It actually outputs, at the end of the day, six unique values, which are kind of the amplitude of these, negative going hump, middle hump, positive going hump, and then also the time at which these happen. And from that you can have a statistical probability, probability estimation of, is that a spike? Is it not a spike? And then based on that, you could also determine, oh, that spike looks different than that spike must come from a different neuron.
Speaker A: Okay. So that, that's a nice signal processing step from which you can then make much better predictions about if there's a spike, especially in this kind of context where there could be multiple neurons screaming. And that that also results in you being able to compress the data better and inside of it. Okay.
Speaker D: And just to be clear, I mean, the labs do this, what's called spike sorting. Usually, once you have these, like, broadband, the fully digitized signals, and then you run a bunch of different set of algorithms to kind of tease apart. It's just all of this for us is done on the device. On the device in a very low power, custom, you know, built ASIC digital.
Speaker A: Processing unit, highly heat constrained.
Speaker D: Highly heat constrained. And the processing time from signal going in and giving you the output is less than a microsecond, which is, you know, a very, very short amount of time.
Speaker A: Oh, yeah. So the latency has to be super short.
Speaker D: Correct.
Speaker A: Oh, wow. Oh, that's a pain in the ass.
Speaker D: Yeah. Latency. Is this a huge, huge thing that you have to deal with right now? The biggest source of latency comes from the Bluetooth, the way in which they are packetized, and we bin them in 15 millisecond.
Speaker A: Oh, interesting communication constraint. Is there some potential innovation there on the protocol used? Absolutely. Okay. Yeah.
Speaker D: Bluetooth is definitely not our final wireless communication protocol that we want to get to.
Speaker A: It's a highly, hence the n one and the r one, I imagine that increases NxRX. Yeah, that's the communication protocol, because Bluetooth allows you to communicate against farther distances than you need to. So you can go much shorter.
Speaker D: Yeah. The only. Well, the primary motivation for choosing Bluetooth is that, I mean, everything has Bluetooth.
Speaker A: All right. So you can talk to any device.
Speaker D: Interoperability is just absolutely essential, especially in this early phase. And in many ways, if you can access a phone or a computer, you can do anything.
Speaker A: Well, it would be interesting to step back and actually look at, again, the same pipeline that you mentioned for Nolan. So what does this whole process look like, from finding and selecting a human being to the surgery, to the first time he's able to use this thing.
Speaker D: So we have what's called a patient registry that people can sign up to hear more about the updates, and that was a route to which Nolan applied. And the process is that once the application comes in, it contains some medical records, and we, based on their medical eligibility, there's a lot of different inclusion exclusion criteria for them to meet. And we go through a pre screening interview process with someone from Neuralink. And at some point we also go out to their homes to do a BCI home audit. Because one of the most revolutionary part about having this n one system that is completely wireless is that you can use it at home. You don't actually have to go to the lab and go to the clinic to get connectorized to these specialized equipment that you can't take home with you. So that's one of the key elements of when we're designing the system that we wanted to keep in mind. People hopefully, would want to be able to use this every day in the comfort of their homes. Part of our engagement and what we're looking for during PCI home audit is to just understand their situation, what other assistive technology that they use.
Speaker A: And we should also step back. And I kind of say that the estimate is 180,000 people live with quadriplegia in the United States, and each year, an additional 18,000 suffer a paralyzing spinal cord injury. So these are folks who have a lot of challenges living a life in terms of accessibility, in terms of doing the things that many of us just take for granted day to day. And one of the things, one of the goals of this initial study is to enable them to have sort of digital autonomy, where they, by themselves, can interact with a digital device using just their mind, something that you're calling telepathy. So digital telepathy, where a quadriplegic can communicate with a digital device in all the ways that we've been talking about, control the mouse cursor enough to be able to do all kinds of stuff, including play games and tweet and all.
Speaker D: That kind of stuff.
Speaker A: And there's a lot of people for whom life, the basics of life are difficult because of the things that have happened to them.
Speaker D: Yeah, I mean, movement is so fundamental to our existence, I mean, even speaking, involves movement of mouth, lip, larynx. And without that, it's extremely debilitating. And there are many, many people that we can help. And especially if you start to look at other forms of movement disorders that are not just from spinal cord injury, but from ALS, MS, or even stroke, that. That leads you and or just aging.
Speaker A: Right.
Speaker D: That leads you to lose some of that mobility, that independence, it's extremely debilitating.
Speaker A: And all of these are opportunities to help people, to help alleviate suffering, to help improve the quality of life. But each of the things you mentioned is its own little puzzle that needs to have increasing levels of capability from a device like a neuralink device. And so the first one you're. You're focusing on is. It's just a beautiful word, telepathy. So being able to communicate using your mind wirelessly with a digital device. Can you just explain this? Exactly what we're talking about?
Speaker D: Yeah, I mean, it's exactly that. I mean, I think if you are able to control a cursor and able to click and be able to get access to computer or phone, I mean, the whole world opens up to you, and, I mean, I guess the word telepathy, if you kind of think about that as just definitionally being able to transfer information from my brain to your brain without using some of the physical faculties that we have, like voices.
Speaker A: But the interesting thing here is, I think the thing that's not obviously clear is how exactly it works. So, in order to move a cursor, there's at least a couple of ways of doing that. So, one is you imagine yourself maybe moving a mouse with your hand, or you can then. Which Nolan talked about. Like, imagine moving the cursor with your mind. Like, I don't. But it's like, there is a cognitive step here that's fascinating, because you have to use the brain, and you have to learn how to use the brain, and you kind of have to figure it out dynamically, like, because you reward yourself if it works. So you. Like, I mean, there's a step that. This is just a fascinating step. Cause you have to get the brain to start firing in the right way. And you do that by imagining, like, fake it till you make it, and all of a sudden, it creates the right kind of signal that, if decoded correctly, can create the kind of effect. And then there's, like, noise around that. You have to figure all of that out. But on the human side, imagine the cursor moving is what you have to do.
Speaker D: Yeah. He says, using the force.
Speaker A: The force. I mean, that's. Isn't that just, like, fascinating to you that it works? Like, to me, it's like, holy shit. That actually works. Like, you could move a cursor with.
Speaker D: Your mind, you know, as much as you're ahead. Learning to use that thing, that thing's also learning about you. Like, our model is constantly updating the weights to say, oh, if someone is thinking about this sophisticated forms of spiking patterns, that actually means to do this.
Speaker A: So the machine is learning about the human and the human is learning about the machine. So there is adaptability to the signal processing, the decoding step, and then there's the adaptation of Nolan, the human being. Like, the same way. If you give me a new mouse and I move it, I learn very quickly about its sensitivity, so I'll learn to move it slower. And then there's other kind of signal drift and all that kind of stuff they have to adapt to. So both are adapting to each other.
Speaker D: Correct.
Speaker A: That's a fascinating, like, software challenge on both sides. The software on both. On the human software and the organic and the inorganic. Organic. And they're inorganic. Anyway, so sorry to rudely interrupt. So there's a selection that Nolan has passed with flying colors. So everything, including that it's a BCI friendly home, all of that. So what is the process of the surgery? The implantation, the first moment when he.
Speaker D: Gets to use the system, the end to end. We say patient in to patient out is anywhere between two to 4 hours. In particular case for Nolan, it was about three and a half hours. And there's many steps leading to the actual robot insertion. There's anesthesia induction, and we do intraop CT imaging to make sure that we're drilling the hole in the right location. And this is also pre planned beforehand. Um, uh, someone goes through, someone like Nolan would go through fMRI, and then, um, they can think about wiggling their hand. You know, obviously, due to their injury, it's not going to actually lead to, um, any, any sort of intended output, but it's the same part of the brain that actually lights up when you're imagining moving your finger to actually moving your finger. And that's one of the ways in which we can actually know where to place our threads because we want to go into what's called the hand knob area in the motor cortex. And, you know, as much as possible, densely put our electro threads. So, yeah, we do intraop CT imaging to make sure and double check the location of the craniectomy. And surgeon comes in, does their thing in terms of, like, skin, uh, incision, craniectomy. So drilling of the skull, and then there's many different layers of the brain. Uh, there's what's called the dura, which is a very, very thick layer that surrounds the brain that gets actually, resectus in a process called duryectomy, and that then exposed the PI in the brain that you want to insert. And by the time it's been around, anywhere between one to one and a half hours, robot comes in, does this thing. Placement of the target, inserting of the thread, that takes anywhere between 20 to 40 minutes. In the particular case for Nolan, it was just under or just over 30 minutes. And then after that, the surgeon comes in. There's a couple other steps of, like, actually inserting the dural substitute layer, um, to protect the thread as well as the. The brain. And then, um, yeah, screw, screw in the implant, and then skin flap, and then suture, and then you're out.
Speaker A: So, uh, when, uh, Nolan woke up, what was that like? What was the recovery like? And when was the first time he was able to use it?
Speaker D: So he was actually immediately after the surgery. Um, you know, like, an hour after the surgery, as he was waking up, um, we did turn on the device, um, make sure that we are recording neural signals, and we actually did have a couple signals that we noticed that he can actually modulate. And what I mean by modulate is that he can think about crunching his fist, and you could see the spike disappear and appear.
Speaker A: That's awesome.
Speaker D: And that was immediate, right? Immediate after in the recovery room.
Speaker A: How cool is that? Yeah, that's a human being. I mean, what did that feel like for you, this device in a human being? A first step of a gigantic journey. I mean, it's a historic moment. Even just that spike, just to be able to modulate that.
Speaker D: Obviously, there have been other, as you mentioned, pioneers that have participated in these groundbreaking BCI investigational early feasibility studies. So we're obviously standing in the shoulders of the giants here. We're not the first ones to actually put electrodes in a human, human brain. But, I mean, just leading up to the surgery, I definitely could not sleep. It's the first time that you're working in a completely new environment. We had a lot of confidence, based on our benchtop testing, or preclinical are in these studies, that the mechanism, the threads, the insertion, all that stuff is very safe, and that it's obviously ready for doing this in a human. But there's still a lot of unknown. Unknown about, can the needle actually insert? I mean, we brought something like 40 needles just in case they break, and we ended up using only one. But, I mean, that was a level of just complete unknown, right, because it's a very, very different environment and, I mean, that's. That's why we do clinical trial in the first place, to be able to test these things out. So extreme nervousness and just. Just many, many sleepless night leading up to the surgery, and definitely the day before the surgery, and it was an early morning surgery. Like, we started at seven in the morning, and by the time it was around 1030, everything was done. But, I mean, first time seeing that, well, number one, just huge relief that this thing is doing what it's supposed to do, and two, just immense amount of gratitude for Nolan and his family and then many others that have applied and that we've spoken to and will speak to are true pioneers in every war. And I sort of call them the neural astronauts or neural knots. You know, these amazing. Just like in the sixties, right? Like these amazing, just pioneers, right? Exploring the unknown outwards. In this case, it's inward, but an incredible amount of gratitude for them to, you know, just participate and play a part, and it's a journey that we're embarking on together. But also, I think it was just. That was a very, very important milestone, but our work was just starting, so a lot of. Just kind of anticipation for, okay, what needs to happen next? What are a set of sequences of events that needs to happen for us to make it worthwhile for both Nolan.
Speaker A: As well as us, just to linger on that just a huge congratulation to you and the team for that milestone. I know there's a lot of work left, but that's really exciting to see. That's a source of hope, this first big step opportunity to help hundreds of thousands of people and then maybe expand the realm of the possible for the human mind, for millions of people in the future. It's really exciting. Like, the opportunities are all ahead of us, and to do that safely and to do that effectively was really fun to see as an engineer, just watching other engineers come together and do an epic thing. That was awesome. So huge. Congrats.
Speaker D: Thank you. Thank you. It's. Yeah. Could not have done it without the team and, yeah, I mean, that's the other thing that I told the team as well, of just this immense sense of optimism for the future. I mean, it's a very important moment for the company, needless to say, as well as hopefully for many others out there, that we can help.
Speaker A: So, speaking of challenges, Neuralink published a blog post describing that some of the threads are attracted. And so the performance, as measured by bits per second, dropped at first, but then eventually it was regained. And the whole story of how it was regained is super interesting. That's definitely something I'll talk to bliss and to Nolan about. But in general, can you speak to this whole experience? How was the performance regained and just the technical aspects of the threads being attracted and moving?
Speaker D: The main takeaway is that in the end, the performance have come back and it's actually gotten better than it was before. He's actually just beat the world record yet again last week to 8.5 bps. So, I mean, he's just cranking and he's just improving.
Speaker A: The previous one was the. He set was eight, correct, 8.5.
Speaker D: Yeah. The previous world record in human was 4.6, so it's almost double. And his goal is to try to get to ten, which is roughly around kind of the median neuralinker, using a, you know, mouse with the hand. So it's getting there.
Speaker A: So, yes. So the performance was regained.
Speaker D: Yeah, better than before. So that's a story on its own of what took the BCI team to recover that performance. It was actually mostly on kind of the signal processing. And so, as I mentioned, we were kind of looking at these spike outputs from the, um, our electrodes. And what happened is that kind of, uh, four weeks into the surgery, uh, we noticed that the threats have slowly come out of the brain. And the way in which we noticed this at first, obviously, is that, um, I think Nolan was the first to notice that his performance was degrading. Um, and I think at the time, we were also trying to do a bunch of different experimentation, um, you know, different algorithms, different, um, sort of ui ux. So it was expected that there will be variability in the performance, but we did see a steady decline. And then also the way in which we measure the health of the electrodes, or whether they're in the brain or not, is by measuring impedance of the electrode. So we look at the interfacial, kind of the Randall circuit, let they say, the capacitance and the. And the resistance between the electrosurface and the medium. And if that changes in some dramatic ways, we have some indication, or if you're not seeing spikes on those channels, you have some indications that something's happening there. And what we notice is that looking at those impedance plot and spike rate plots, and also because we have those electrodes recording along the depth you're seeing some sort of movement that indicated that the res were being pulled out of. Um. And that obviously will have an implication on the model side, because if you're, the number of inputs that are going into the model is changing because you have less of them, um, the out that that model needs to get updated.
Speaker F: Right.
Speaker D: And, um. But, but there were still signals, and as I mentioned, similar to how even when you place the signals on the surface of the brain, of the brain, or farther away, like outside the skull, you still see some useful signals. What we started looking at is not just the spike occurrence through this boss algorithm that I mentioned, but we started looking at just the power of the frequency band. That is interesting for Nolan, or Nolan to be able to modulate once we change the algorithm for the implant to not just give you the boss output, but also these spike band power output, that helped us refine the model with the new set of inputs. And that was the thing that really ultimately gave us the performance back in terms of. And obviously, the thing that we want ultimately, and the thing that we are working towards is figuring out ways in which we can keep those threads intact for as long as possible so that we have many more channels going into the model. That's by far the number one priority that the team is currently embarking on to understand how to prevent that from happening. The thing that I will say also is that, as I mentioned, this is the first time ever that we're putting these threats in the human brain. And human brain, just for size reference, is ten times that of the monkey brain or the sheep brain. And it's just a very, very different environment. It moves a lot more. It actually moved a lot more than we expected when we did Nolan surgery. And it's just a very, very different environment than what we're used to. And this is why we do clinical trial. We want to uncover some of these issues and failure modes earlier than later. In many ways, it's provided us with this enormous amount of data and information to be able to solve this. And this is something that neuralink is extremely good at. Once we have set of clear objective and engineering problem. We have enormous amount of talents across many, many disciplines to be able to come together and fix the problem very, very quickly.
Speaker A: But it sounds like one of the fascinating challenges here is for the system and the decoding side to be adaptable across different timescales. So whether it's movement of threads or different aspects of signal drift sort of on the software of the human brain, something changing. Like Nolan talks about cursor drift. They could be corrected and there's a whole ux challenge to how to do that. So it sounds like adaptability is like a fundamental property that has to be engineered in.
Speaker D: It is as a company, we're extremely vertically integrated. We make these thin film arrays in our own microfab.
Speaker A: Yeah, there's, like you said, built in house. This whole paragraph here from this blog post is pretty gangster. Building the technology described above has been no small feat. And there's a bunch of links here that I recommend people click on. We constructed in house microfabrication capabilities to rapidly produce various iterations of thin film arrays that constitute our electrode threads. We created a custom femtosecond laser mill to manufacture components with micro level precision. I think there's a tweet associated with.
Speaker D: This whole thing that we can get into.
Speaker A: Yeah, this, this. Okay, what are we, what are we looking at here? This thing, this is so in less than 1 minute, our custom made femto second laser mill cuts this geometry in the tips of our needles. So we're looking at this weirdly shaped needle. The tip is only ten to twelve microns in width, only slightly larger than the diameter of a red blood cell. The small size allows threads to be inserted with minimal damage to the cortex. Okay, so what's interesting about this geometry? So we'll look at this geometry of a needle.
Speaker D: This is the needle that's engaging with the loops in the thread. So they're the ones that thread the loop and then peel it from the silicon backing. And then this is the thing that gets inserted into the tissue. And then this pulls out, leaving the thread. And this kind of a notch, or the shark tooth that we used to call is the thing that actually is grasping the loop. And then it's, it's designed in such way, such that when you, when you pull out, leaves the loop.
Speaker A: And the robot is controlling this needle.
Speaker D: Correct. So this is actually housed in a cannula. And basically the robot is, has a lot of the optics that look for where the loop is. There's actually a 405 nanometer light that actually causes the polyimit to fluoresce so that you can locate the, the location of the loop. So the loop lights up? Yeah, yeah, they do. Micron precision process.
Speaker A: What's interesting about the robot that it takes to do that? That's pretty crazy. That's pretty crazy. That robot is able to get this kind of precision.
Speaker D: Yeah, our robot is quite heavy. Our current version of it. There's I mean, it's like a giant granite slab that weighs about a ton because it needs to be sensitive to vibration, environmental vibration. And then as the head is moving at the speed that is moving, you know, there's a lot of kind of motion control to make sure that you can achieve that level of precision. Um, a lot of optics that kind of zoom in on that. Um, you know, we're working on next generation of the robot that is lighter, easier to transport. I mean, it is a. It is a feat to move the robot to.
Speaker A: And it's far superior to a human surgeon at this time for this particular task.
Speaker D: Absolutely. I mean, let alone you try to actually thread a loop in a. In a sewing kit. I mean, this is like, we're talking like fractions of human hair. These things are. It's not visible.
Speaker A: So continuing the paragraph, we developed novel hardware and software testing systems, such as our accelerated lifetime testing racks and simulated surgery environment, which is pretty cool. To stress test and validate the robustness of our technologies, we performed many rehearsals of our surgeries to refine our procedures and make them second nature. This is pretty cool. We practice surgeries on proxies with all the hardware and instruments needed in our mock or in the engineering space. This helps us rapidly test and measure. So there's like, proxies.
Speaker D: Yeah, this proxy is super cool, actually. So there's a 3d printed skull from the images that is taken at Barrow, as well as this hydrogel mixed, you know, sort of synthetic polymer thing that actually mimics the mechanical properties of the brain. It also has vasculature of the person. So basically what we're talking about here, and there's a lot of work that has gone into making this set proxy that it's about, like, finding the right concentration of these different synthetic polymers to get the right set of consistency for the needle dynamics, you know, as they're being inserted. But we practice this surgery with the person. You know, Nolan's basically physiology and brain many, many times prior to actually doing the surgery to every.
Speaker A: Every step.
Speaker D: Every step. Every step, yeah. Like, where does someone stand? Like, I mean, like, what you're looking at is the picture. This is in our office of this kind of corner of the robot engineering space that we, you know, have created this, like, mock or space that looks exactly like what they would experience, all the staff would experience during their actual surgery. So, I mean, it's just kind of like any dense rehearsal where you know exactly where you're going to stand at what point, um, and you just practice that over and over and over again with an exact anatomy of someone that you're going to surgerise.
Speaker A: And.
Speaker D: And it got to a point where a lot of our engineers, when we created a craniectomy, they're like, oh, that looks very familiar. We've seen that before.
Speaker A: Yeah, man. There's wisdom you can gain through doing the same thing over and over and over. It's like a jira dreams of sushi kind of thing. Because then it's like olympic athletes visualize the Olympics, and then once you actually show up, it feels easy. It feels like any other day. It feels almost boring winning the gold medal because you visualize this so many times, you've practiced this so many times that nothing bothers you. It's boring. You win the gold medal is boring. And the experience they talk about is mostly just relief, probably, that they don't have to visualize it anymore.
Speaker D: Yeah. The power of the mind to visualize. And where. I mean, there's a whole field that studies where muscle memory lies in cerebellum. Yeah, it's incredible.
Speaker A: I think it's a good place to actually ask. Sort of the big question that people might have is, how do we know every aspect of this that you describe is safe?
Speaker D: At the end of the day, the gold standard is to look at the tissue. What sort of trauma did you cause the tissue? And does that correlate to whatever behavioral anomalies that you may have seen? And that's the language to which we can communicate about the safety of inserting something into the brain and what type of trauma that you can cause. So we actually have an entire department of pathology that looks at these tissue slices. There are many steps that are involved in doing this. Once you have studies that are launched with particular endpoints in mind, at some point you have to euthanize the animal, and then you go through necropsy to collect the brain tissue samples. You fix them in formalin and you gross them, you section them, and you look at individual slices just to see what kind of reaction, or lack thereof, exists. So that's the language to which FDA speaks. And as well, for us to evaluate the safety of the insertion mechanism as well as the threads at various different time points, both acute. So anywhere between, you know, zero to three months to beyond three months.
Speaker A: So those are kind of the details of an extremely high standard of safety that has to be reached.
Speaker E: Correct.
Speaker A: FDA supervises this, but there's, in general, just a very high standard. And every aspect of this, including the surgery, I think Matthew McDougall has mentioned that, like, the standard is, let's say, how to put it politely, higher than maybe some other operations that we take for granted. So the standard for all the surgical stuff here is extremely high.
Speaker D: Very high. I mean, it's a highly, highly regulated environment with the governing agencies that scrutinize every medical device that gets marketed. And I think it's a good thing. It's good to have those high standards, and we try to hold extremely high standards to understand what sort of damage, if any, these innovative emerging technologies and new technologies that we're building are. And so far, we have been extremely impressed by lack of immune response from these threads.
Speaker A: Speaking of which, you talk to me with excitement about the histology and some of the images that you're able to share. Can you explain to me what we're looking at? Yeah.
Speaker D: So what you're looking at is a stained tissue image. So this is a sectioned tissue slice from an animal that was implanted for seven months. So kind of a chronic time point. And you're seeing all these different colors, and each color indicates specific types of cell types. So purple and pink are astrocytes and microglia, respectively. They're type of glial cells. And the other thing that people may not be aware of is your brain is not just made up of soup of neurons and axons. There are other cells, like glial cells, that actually kind of is the glue and also react if there are any trauma or damage to the tissue.
Speaker A: But the brown are the neurons.
Speaker D: The brown are the neurons nuclei. So what you're seeing is in this kind of macro image, you're seeing these, like, circle highlighted in white, the insertion sites. And when you zoom into one of those, you see the threads. And then in this particular case, I think we're seeing about the 16 wires that are going into the page. And the incredible thing here is the fact that you have the neurons that are these brown structures or brown circular or elliptical thing that are actually touching and abutting the threads. So what this is saying is that there's basically zero trauma that's caused during this insertion. And with these neural interfaces, these microelectrodes that you insert, that is one of the most common mode of failure. So when you insert these threads, like the Utahray, it causes neuronal death around the site because you're inserting a foreign object, right. And that kind of elicit these immune response through microglia and astrocytes. They form this protective layer around it. Not only are you killing the neuron cells, but you're also creating this protective layer that then basically prevents you from recording neural signals because you're getting further and further away from the neurons that you're trying to record. And that is the biggest mode of failure. And in this particular example, in that insight, it's about 50 micron with that scale bar. The neurons just seem to be attracted.
Speaker A: To it, so there's certainly no trauma. That's such a beautiful image, by the way. So the brown are the neurons. For some reason, I can't look away. It's really cool.
Speaker D: Yeah. And the way that these things, like, I mean, your tissues generally don't have these beautiful colors. This is multiplex stain that uses these different proteins that are staining these at different colors. We use very standard set of staining techniques with he, IBA one and nuan and GFAP. So if you go to the next image, this is also kind of illustrates the second point because you can make an argument. And initially, when we saw the previous image, we said, oh, like, are the threads just floating? What is happening here? Are we actually looking at the right thing? So what we did is we did another stain, and this is all done in house of this Mason's trichrome stain, which is in blue, that shows these collagen layers. So the blue, basically, you don't want the blue around the implant threads because that means that there's some sort of scarring that's happened. And what you're seeing, if you look at individual threads is that you don't see any of the blue, which means that there has been absolutely or very, very minimal to a point where it's not detectable amount of trauma in these inserted threads.
Speaker A: So that, presumably is one of the big benefits of having this kind of flexible thread.
Speaker D: Yeah. So we think this is primarily due to, uh, the size as well as the flexibility of the threats. Also, the fact that r1 is avoiding. That's for sure. So we're not disrupting or we're not, um, causing damage to, uh, the vessels and not breaking any of the blood brain barrier, uh, has, you know, basically caused the immune response to be muted.
Speaker A: But this is also a nice illustration of the size of things. So this is the tip of the thread.
Speaker D: Yeah, those are neurons. They're.
Speaker A: And they're neurons. And they're. And this is the thread listening. And the electrodes are positioned how?
Speaker D: Yeah, so this is what you're looking at is not electrode themselves. Those are the conductive wires. So each of those should probably be two micron in width. So what we're looking at is we're looking at the coronal slice, so we're looking at some slice of the tissue. So as you go deeper, you know, you'll obviously have less and less of the tapering of the thread. But, yeah. The point basically being that there's just cells around the inserted site, which is just an incredible thing to see. I've just never seen anything like this.
Speaker A: How easy and safe is it to remove the implant? Yeah.
Speaker D: So it depends on when. In the first three months or so after the surgery, um, there. There's a lot of kind of tissue modeling that's happening, you know, similar to when you get a cut. Um, you know, you obviously, uh, you know, start over first couple weeks, or depending on the size of the wound, um, scar tissue forming right there are these, like, contracted, and then in the end, they turn into scab, and you can scab it off. The same thing happens in the brain, and it's a very dynamic environment. And before the scar tissue or the neomembrane or the new membrane that forms, it's quite easy to just pull them out. And there's minimal trauma that's caused during that once the scar tissue forms. And with Nolan as well, we believe that that's the thing that's currently anchoring the threads. So we haven't seen any more movements since then, so they're quite stable. It gets harder to actually completely extract the threads. So our current method for removing the device is cutting the thread, leaving the tissue intact, and then unscrewing and taking the implant out. And that hole is now going to be plugged with either another neural link or just with a peak based, plastic based cap.
Speaker A: Is it okay to leave the threads in there forever?
Speaker D: Yeah, we think so. We've done studies where we left them there, and one of the biggest concerns that we had is, do they migrate and do they get to a point where they should not be? We haven't seen that again. Once the scar tissue forms, they get anchored in place. And I should also say that when we say upgrades, we're not just talking in theory here. We've actually upgraded many, many times. Most of our monkeys, or non human primates, NHP, have been upgraded. Pejor, who you saw playing mind pong, has the latest version of the device since two years ago and is seemingly very happy and healthy and fat.
Speaker A: What's designed for the future? The upgrade procedure. So maybe for Noland, what would the upgrade look like? It was essentially what you're mentioning. Is there a way to upgrade the device internally, where you take it apart and keep the capsule and upgrade the internals.
Speaker D: Yeah, there are a couple of different things here for Nolan. If we were to upgrade, what we would have to do is either cut the threads or extract the threads, depending on kind of the situation there in terms of how they're anchored or scarred in. If you were to remove them with the dural substitute, you have an intact brain, so you can reinsert different threads with the updated implant package. There are a couple different other ways that we're thinking about the future of what the upgradable system looks like. One is at the moment, we currently remove the dura, this thick layer that protects the brain, but that actually is the thing that actually proliferates, the scar tissue formation. So, typically, the general good rule of thumb is you want to leave the nature as is and not disrupt it as much. So we're looking at ways to insert the threads through the dura, which comes with different set of challenges, such as, you know, it's a pretty thick, uh, layer. So how do you actually penetrate that without breaking the needle? So we're looking at different needle design for that, as well as the kind of the loop engagement. The other biggest challenges are it's quite opaque optically in with white light illumination. So how do you avoid still this. This biggest advantage that we have of avoiding basket shore, how do you image through that? How do you actually still mediate that? So there are other imaging techniques that we're looking at to enable that, but the goal, our hypothesis is that, and based on some of the early evidence that we have doing through the dura insertion will cause minimal scarring. That causes them to be much easier to extract over time. And the other thing that we're also looking at, this is going to be a fundamental change in the implant architecture is, at the moment, it's a monolithic, single implant that comes with the thread that's, um, bonded together, so you can't actually separate the thing out. But you can imagine having two part implant, um, you know, bottom part that is the thread that are inserted, that has the chips, um, and maybe a radio and some power source. And then you have another implant that has more of the computational heavy load and. And the bigger battery. Um, and then one can be under the dura. One can be above the dura, like, you know, being the plug for the skull. They can talk to each other, but the thing that you want to upgrade the computer and not the threads, if you want to upgrade that, you just go in there, you know, remove the screws, and then put in the next version and, you know, you're off to, you know, it's a very, very easy surgery too. Like you do a skin incision, slip this in screw, probably be able to do this in ten minutes.
Speaker A: So that would allow you to reuse the threads, sort of?
Speaker D: Correct.
Speaker A: So I mean, this leads to the natural question of what is the pathway to scaling the increase in the number of threads? Is that a priority? Is that like, what's, what's the technical challenge there?
Speaker D: Yeah, that that is a priority. So for next versions of the implant, you know, the key metrics that we're looking to improve are number of channels just recording from more and more neurons. Um, you know, we have a pathway to actually go from currently 1000 to, you know, hopefully 3000 if not 6000 by end of this year. Um, and then end of next year we want to get to, uh, you know, even more 16,000.
Speaker A: Wow.
Speaker D: There's a couple limitations to that. One is, you know, obviously being able to photo lithographically print those wires. As I mentioned, it's two micron in width and spacing. Obviously there are chips that are much more advanced than those types of resolution and we have some of the tools that we have brought in house to be able to do that. So traces will be narrower just so that you have to have more of the wires coming up into the chip. Chips also cannot linearly consume more energy as you have more and more channels. So there's a lot of innovations in the circuit architecture as well as the circuit design topology to make them lower power. You need to also think about, if you have all of these spikes, how do you send that off to the end application? So you need to think about bandwidth limitation there and potentially innovations in signal processing. Physically, one of the biggest challenges is going to be the interface. It's always the interface that breaks. Bonding this thin film array to the electronics, it starts to become very, very highly dense interconnects. How do you connect the eyes that there's a lot of innovations in the 3d integrations in the recent years that we can take advantage of. One of the biggest challenges that we do have is, you know, forming this hermetic barrier.
Speaker A: Right.
Speaker D: You know, this is an extremely harsh environment that we're in the brain. So how do you protect it from? Yeah, like the brain trying to kill your electronics to also your electronics leaking things that you don't want into the brain and that forming that hermetic barrier is going to be a very, very big challenge that we, you know, I think are actually well suited to tackle.
Speaker A: How do you test that? Like, what's the development environment?
Speaker D: Yeah.
Speaker A: To simulate that kind of harshness.
Speaker D: Yeah. So this is. This is where the accelerated life tester essentially is a brain in a vat. It literally is a vessel that is made up of. And again, again, for all intents and purpose, for this particular types of tests, your brain is a saltwater. And you can also put some other set of chemicals, like reactive oxygen species, that get at these interfaces and trying to cause a reaction to pull it apart. But you could also increase the rate at which these interfaces are aging by just increasing temperature. So every ten degrees celsius that you increase, you're basically accelerating time by two x, and there's limit as to how much temperature you want to increase, because at some point, there's some other nonlinear dynamics that causes you to have other nasty gases to form. That just is not realistic in an environment. So what we do is we increase in our alt chamber by 20 degrees celsius. That increases the aging by four times. So essentially, one day in alt chamber is four day in calendar year, and we look at whether the implants still are intact, including the threads and operation and all that. And operation and all of that. It obviously is not an exact same environment as a brain because brain has mechanical, other more biological goops that attack at it. But it is a good test environment, testing environment for at least the enclosure and the strength of the enclosure. And, I mean, we've had implants, the current version of the implant, that has been in there for, I mean, close to two and a half years, which is equivalent to a decade, and they seem to be fine.
Speaker A: So it's interesting that the. So, basically, a close approximation is warm salt water. Hot salt water is a good testing environment. That is.
Speaker E: Yeah.
Speaker A: By the way, I'm drinking element, which is basically salt water, which is making me kind of. It doesn't have computational power the way the brain does, but maybe in terms of other characteristics, it's quite similar. And I'm consuming it.
Speaker D: Yeah, you have to get it at.
Speaker A: The right ph, too, and then consciousness will emerge. Yeah. No.
Speaker D: By the way, the other thing that also is interesting about our enclosure is if you look at our implant, it's not your common looking medical implant that usually is encased in a titanium can that's laser welded. We use this polymer called PCTFE polychloro trifluoroethylene, which is actually commonly used in blister packs. So when you have a pill and you try to pop a pill, there's like kind of that plastic membrane, that's what this is. No one's actually ever used this except us. And the reason we wanted to do this is because it's electromagnetically transparent. So when we talked about the electromagnetic inductive charging with titanium can, usually, if you want to do something like that, you have to have a sapphire window. And it's a very, very tough process to scale.
Speaker A: So you're doing a lot of iteration here in every aspect of this, the materials, the software, the whole, whole shipping. Okay, so you mentioned scaling. Is it possible to have multiple neuralink devices as one of the ways of scaling, to have multiple neuralink devices implanted?
Speaker D: That's the goal. That's the goal, yeah, we've had, we've had. I mean, our monkeys have had two neuralinks, one in each hemisphere. And then we're also looking at, you know, potential of having one in oral cortex, one in visual cortex, and one in wherever, other cortex.
Speaker A: So focusing on a particular function, one neuralink device.
Speaker D: Correct.
Speaker A: I mean, I wonder if there's some level of customization that can be done on the compute side. So, for the motor cortex, absolutely, that's the goal.
Speaker D: And, you know, we talk about neuralink building a generalized neural interface to the brain. Um, and that, that also is strategically how we're approaching this, um, with, with marketing and also, you know, with, with regulatory, which is, hey, look, um, we have the robot, and the robot can access any part of the cortex. Right now, we're focused on motor cortex with current version of the n one, that's specialized for motor decoding tasks. But also, at the end of the day, there's a general compute available there. But typically, if you want to really get down to hyper optimizing for power and efficiency, you do need to get to some specialized function. But what we're saying is, hey, I. You are now used to this robotic insertion techniques, which took many, many years of showing data and conversation with the FDA, and also internally convincing ourselves that this is safe. And now the difference is, if we go to other parts of the brain, like visual cortex, which we're interested in as our second product, obviously, it's a completely different environment. The cortex is laid out very, very differently. It's going to be more stimulation focused rather than recording, just creating visual percepts. But in the end, we're using the same thin film array technology, we're using the same robot insertion technology, we're using the same packaging technology. Now it's more the conversation is focused around what are the differences and what are the implication of those differences in safety and efficacy, the way you said.
Speaker A: Second product is both hilarious and awesome to me, that product being restoring sight for blind people. So can you speak to stimulating the visual cortex? I mean, the possibilities there are just incredible to be able to give that gift back to people who don't have sight or even any aspect of that. Can you just speak to the challenges of. There's several challenges here, one of which is, like you said, from recording to the stimulation, just any aspect of that that you're both excited and see the challenges of.
Speaker D: Yeah, I guess I'll start by saying that we actually have been capable of stimulating through our thin film array as well as other electronics for years. You know, we have actually demonstrated some of that capabilities for reanimating the limb in the spinal cord. Um, you know, obviously for. For the current EFS study, you know, we've hardware disabled that, so that's. That's something that, you know, we wanted to embark as a separate, separate journey. Um, and and, you know, obviously, there are many, many different ways to write information into the brain. The way in which we're doing that is through electrical, you know, passing electrical current and. And kind of causing that to really change the local environment so that you can sort of artificially cause kind of the neurons to depolarize in nearby areas for vision. Specifically, the way our visual system works, it's both well understood. I mean, anything with kind of brain, there are aspects of it that's well understood, but in the end, we don't really know anything. But the way visual system works is that you have photon hitting your. And in your eyes, there are these specialized cells called photoreceptor cells that convert the photon energy into electrical signals. And then that then gets projected to your back of your head. Your visual cortex. It goes through, actually thalamic system called LGN that then projects it out. And then in the visual cortex, there's visual area one or v one, and then there's a bunch of other higher level processing layers, like v two, v three. And there are actually kind of interesting parallels. And when you study the behaviors of these convolutional neural networks, like what the different layers of the network is detecting, first they're detecting these edges, and they're then detecting some more natural curves, and then they start to detect objects. Right. Kind of similar thing happens in the brain, and a lot of that has been inspired. And also, it's been kind of exciting to see some of the correlations there. But things like from there, where does cognition arise. And where's color encoded? There's just not a lot of understanding, fundamental understanding there. So in terms of kind of bringing sight back to those that are blind, there are many different forms of blindness. There's actually million people, 1 million people in the US that are legally blind. That means certain score below in the visual test. I think it's something like if you can see something at 20ft distance, that normal people can see at 200ft distance. If you're worse than that, you're legally blind.
Speaker A: So fundamentally, that means you can't function effectively using sight in the world.
Speaker D: Yeah, like to navigate, navigate your environment. Um, and yeah, there are different forms of blindness. There are forms of blindness where, uh, there's some degeneration of your, uh, retina, um, these photoreceptor cells, and. And rest of your visual, uh, you know, processing that I described is intact. And for those types of individuals, uh, you may not need to maybe stick electrodes into the visual cortex. You can actually build retinal prosthetic devices that actually just replaces the function of that retinal cells that are degenerated. And there are many companies that are working on that, but that's a very small slice, albeit significant, still smaller slice of folks that are legally blind. If there's any damage along that circuitry, whether it's in the optic nerve or just LGN circuitry or any break in that circuitous, that's not going to work for you. And the source of where you need to actually cause that visual percept to happen, because your biological mechanism not doing that is by placing electrodes in the visual cortex in the back of your head. And the way in which this would work is that you would have an external camera, whether it's something as unsophisticated as a goPro or some sort of wearable ray ban type glasses that Meta's working on, that captures a scene, that scene is then converted to a set of electrical impulses or stimulation pulses that you would activate in your visual cortex through these thin film arrays. And by playing some concerted kind of, uh, orchestra of these stimulation patterns, you can create what's called phosphines, which are these, um, kind of white yellowish dots that you can also create by just pressing your eyes. Um, you can actually create those percepts by stimulating the visual cortex. And the name of the game is really have many of those, and have those percepts be the phosphines, be as small as possible so that you can start to tell apart like they're the individual pixels of the, the, of the screen. So if you have many of those, potentially you'll be able to, in the long term, be able to actually get naturalistic vision, but in the short term, to maybe midterm, being able to at least be able to have object detection algorithms run on your glasses, the pre pop processing units, and then being able to at least see the edges of things so you don't bump into stuff.
Speaker A: This is incredible. This is really incredible. So you basically would be adding pixels, and your brain would start to figure out what those pixels mean.
Speaker D: Yeah.
Speaker A: And like, with different kinds of assistance on the signal processing on all fronts. Yeah.
Speaker D: The thing that actually. So a couple things. One is, you know, obviously, if you're blind from birth, the way brain works, especially in the early age, neuroplasticity is really nothing other than your brain and different parts of your brain fighting for delimited territory very, very quickly. You see cases where people that are, I mean, you also hear about people who are blind that have heightened sense of hearing or some other senses. And the reason for that is because that cortex that's not used just gets taken over. Bye. These different parts of the cortex. So for those types of individuals.
Speaker E: I.
Speaker D: Mean, I guess they're going to have to now map some other parts of their senses into what they call vision, but it's going to be obviously a very, very different conscious experience. I think that's an interesting caveat. The other thing that also is important to highlight is that we're currently limited by our biology in terms of the wavelength that we can see. There's a very, very small wavelength, that is a visible light wavelength that we can see with our eyes. But when you have an external camera with this BCI system, you're not limited to that. You can have infrared, you can have uv, you can have whatever other spectrum that you want to see. Whether that gets mapped to some sort of weird conscious experience, I've no idea. But when I oftentimes I talk to people about the goal of neuralink being going beyond the limits of our biology, that's sort of what I mean.
Speaker A: And if you're able to control the kind of raw signal is that when we use our sight, we're getting the photons and there's not much processing on it. If you're being able to control that signal, maybe you can do some kind of processing. Maybe you do object detection ahead of time. Yeah, you're doing some kind of pre processing, and there's a lot of possibilities to explore that. So it's not just increasing sort of thermal imaging, that kind of stuff, but it's also just doing some kind of interesting processing.
Speaker D: Yeah, I mean, my theory of how visual system works also is that, I mean, there's just so many things happening in the world, and there's a lot of photons that are going into your eye, and it's unclear exactly where some of the preprocessing steps are happening. But, I mean, I actually think that just from a fundamental perspective, there's just so much the reality that we're in, if it's a reality, so there's so much data, and I think humans are just unable to actually eat enough actually to process all that information. So there's some sort of filtering that does happen. Whether that happens in the retina, whether that happens in different layers of the visual cortex, unclear. But the analogy that I sometimes think about is if your brain is a CCD camera and all of the information in the world is a sun, and when you try to actually look at the sun with the CCD camera, it's just going to saturate the sensors because it's enormous amount of energy. What you do is you end up adding these filters, right, to just kind of narrow the information that's coming to you and being captured. And I think things like our experiences or our drugs, like propofol, like anesthetic drug or psychedelics, what they're doing is they're kind of swapping out these filters and putting in new ones or removing older ones and kind of controlling our conscious experience.
Speaker A: Yeah, man. Not to distract from the topic, but I just took a very high dose of ayahuasca in the Amazon jungle. So, yes, it's a nice way to think about it. You're swapping out different experiences. And with neuralink being able to control that, primarily at first to improve function, not for entertainment purposes or enjoyment purposes.
Speaker D: But, yeah, giving back lost functions.
Speaker A: Giving back lost functions. And there, especially when the function is completely lost, anything is a huge help. Would you implant a neuralink device in your own brain?
Speaker D: Absolutely. I mean, maybe not right now, but absolutely.
Speaker A: What kind of capability, once reached, you would start getting real curious and almost get a little antsy, like jealous of people that get, as you watch them getting planted.
Speaker D: Yeah, I mean, I think. I mean, even. Even with our early participants, if they start to do things that I can't do, which I think is in the realm of possibility for them to be able to get 1520, if not like 100 bps. Right. There's nothing that fundamentally stops us from being able to achieve that type of performance, I mean, I would certainly get jealous that they can do that.
Speaker A: I should say that watching Nolan, I get a little jealous because he's having so much fun, and it seems like such a chill way to play video games. Yeah.
Speaker D: I mean, the thing that also is hard to appreciate sometimes is that, you know, he's doing these things while multi, like, while talking. And, I mean, it's multitasking. Right. So it's. It's clearly. It's obviously cognitively intensive, but similar to how when we talk, we move our hands, these things are multitasking. He's able to do that. You won't be able to do that with other assistive technology, as far as I'm aware. If you're obviously using an eye tracking device, you're very much fixated on that thing that you're trying to do. And if you're using voice control, like, if you say some other stuff. Yeah. You don't get to use that.
Speaker A: Yeah. The. The multitasking aspect of that is really interesting. So it's not just the bps for the primary task. It's the. It's the parallelization of multiple tasks. If you.
Speaker E: If you take.
Speaker A: If you measure the bps for the entirety of the human organism. So if you're talking and doing a thing with your mind and looking around also, I mean, there's just a lot of parallelization that can be happening.
Speaker D: But I think at some point for him, like, if he wants to really achieve those high level bps, it does require, like, you know, full attention. Right. And that's a separate circuitry that that is a big mystery, like how attention works and, you know.
Speaker A: Yeah, attention, like cognitive load. I've done. I've read a lot of literature on people doing two tasks. Like, you have your primary task and a secondary task, and the secondary task is. Is a source of distraction. And how does that affect the performance of the primary task? And there's. Depending on the task, there's a lot of interesting. I mean, this is an interesting computational device.
Speaker D: Right.
Speaker A: And I think there, to say the least, a lot of novel insights that can be gained from everything. I mean, I personally am surprised that no one's able to do such incredible control of the cursor while talking and also being nervous at the same time because he's talking like all of us are. If you're talking in front of the camera, you get nervous. So all of those are coming into play. He's able to still achieve high performance. Surprising. I mean, all of this is really amazing. And I think just after researching this really in depth, I kind of want.
Speaker D: In your lake get in the line.
Speaker A: And also the safety get in mind. Well, we should say the registry is for people who have quadriplegia and all that kind of stuff, so.
Speaker D: Correct.
Speaker A: That would be a separate line for people. They're just curious, like myself. So now that Nolan patient, p one is part of the ongoing prime study, what's the high level vision for p two, p three, p four, p five. And just the expansion into other human beings that are getting to experience this implant?
Speaker D: Yeah, I mean, the primary goal is, you know, for our study in the first place is to achieve safety endpoints. Just understand safety of this device as well as the implantation process, and also at the same time understand the efficacy and the impact that it could have on the potential users lives. And just because you're living with tetraplegia, it doesn't mean your situation is same as another person living with tetraplegia. It's wildly, widely varying and it's something that we're hoping to also understand how our technology can serve not just a very small slice of those individuals, but broader group of individuals and being able to get the feedback to just really build just the best product for them. There's obviously also goals that we have. And the primary purpose of the early feasibility study is to learn from each and every participant to improve the device, improve the surgery. Before we embark on what's called a pivotal study, that then is much larger trial that starts to look at statistical significance of your endpoints and that's required before you can then market the device. That's how it works in the US and just generally around the world. That's the process you follow. So our goal is to really just understand from people like Nolan, p two, p three, future participants what aspects of our device needs to improve. If it turns out that people are like, I really don't like the fact that it lasts only 6 hours, I want to be able to use this computer for, you know, like 24 hours. I mean, that's, that is a, you know, user needs and user requirements which we can only find out from just being able to engage with them.
Speaker A: So before the pivotal study, there's kind of like a rapid innovation based on individual experiences. You're learning from individual people how they use it, like the high resolution details in terms of like cursor control and signal and all that kind of stuff to like, life experience.
Speaker D: Yeah. So there's hardware changes, but also just firmware updates. So even when we had that sort of recovery event for Nolan, he now has the new firmware that he has been updated with. And it's similar to how your phones get updated all the time with new firmwares for security patches, whatever new functionality UI. And that's something that is possible with our implant. It's not a static one time device that can only do the thing that it said it can do. I mean, similar to Tesla, you can do over the air firmware updates, and now you have completely new user interface and all these bells and whistles and improvements on everything like the latest. That's when we say generalized platform. That's what we're talking about.
Speaker A: Yeah, it's really cool how the app that Nolan is using, there's a calibration, all that, all that kind of stuff, and then there's update. You just click and get an update. What other future capabilities are you kind of looking to? You said vision. That's a fascinating one. What about sort of accelerated typing or speech, this kind of stuff? Yeah. And what else is there?
Speaker D: Yeah, those are still in the realm of movement program. So largely speaking, we have two programs. We have the movement program and we have the vision program. The movement program currently is focused around the digital freedom. As you can easily guess, if you can control to the cursor in the digital space, you could move anything in the physical space. So robotic arms, wheelchair, your environment, or even really, whether it's through the phone or just directly to those interfaces, like to those machines. So we're looking at ways to expand those types of capability. Even for Nolan, that requires conversation with the FDA and showing safety data for if there's a robotic arm or a wheelchair, that we can guarantee that they're not going to hurt themselves accidentally. It's very different if you're moving stuff in the digital domain versus in the physical space, you can actually potentially cause harm to the participants. So we're working through that right now. Speech does involve different areas of the brain. Speech prosthetic is very, very fascinating. And there's actually been a lot of really amazing work that's been happening in academia. Sergei Stavisky at UC Davis, Jamie Henderson and late Krishna Shinoi at Stanford doing just some incredible amount of work improving speech, neural prosthetics. And those are actually looking more at parts of the motor cortex that are controlling these vocal articulators and being able to, even by mouthing the word or imagine speech, you can pick up those signals. The more sophisticated higher level processing areas, like the broca's area or Wernicke's area. Those are still very, very big mystery in terms of the underlying mechanism of how all that stuff works. But, yeah, I think neuralinks, the ventral goal is to understand those things and be able to provide a platform and tools to be able to understand that and study that.
Speaker A: This is where I get to the pothead questions. Do you think we can start getting insight into things like thought? So, speech is, there's a muscular component, like you said. There's, like, the act of producing sounds. But then what about the internal things, like cognition, like low level thoughts and high level thoughts? Do you think we'll start noticing kind of signals that could be picked up, that could. They could be understood, that could be maybe used in order to interact with the outside world?
Speaker D: In some ways, I guess this starts to kind of get into the heart problem of consciousness. And, I mean, on one hand, all of these are at some point set of electrical signals that from there, maybe it in itself is giving you the cognition or the meaning, or somehow human mind is incredibly amazing storytelling machine. So we're telling ourselves and fooling ourselves that there's some interesting meaning here. But I certainly think that BCI, and really, BCI, at the end of the day, is a set of tools that help you kind of study the underlying mechanisms in a both local but also broader sense, and whether there's some interesting patterns of electrical signal. That means you're thinking this versus. And you can either learn from many, many sets of data to correlate some of that and be able to do mind reading or not. I'm not sure. I certainly would not blow that out as a possibility, but I think BCI alone probably can't do that. There's probably additional set of tools and framework and also just hard problem of consciousness at the end of the day, is rooted in this philosophical question of what is the meaning of it all? What's the nature of our existence? Where is the mind emerge from this complex network?
Speaker A: Yeah. How does the subjective experience emerge from just a bunch of spikes? Electrical spikes? Yeah. Yeah.
Speaker D: I mean, we do really think about BCI and what we're building as a tool for understanding the mind, the brain. The only question that matters, there's actually. There actually is some biological existence proof of, like, what it would take to kind of start to form some of these experiences that may be unique. If you actually look at every one of our brains, there are two hemispheres. There's a left sided brain, there's a right sided brain. And, I mean, unless you have some other conditions, you normally don't feel like left legs or right legs, you just feel like one legs. Right. So what is happening there? Right. If you actually look at the two hemispheres, there's a structure that connectorize the two called the corpus callosum, that is supposed to have around 200 to 300 million connections, or axons. So whether that means that's the number of interface and electrodes that we need to create some sort of mind meld or from that, like, whatever new conscious experience that you. You can experience. But I do think that there's like, kind of an interesting existence, proof that we all have.
Speaker A: And that threshold is unknown at this time.
Speaker D: Oh, yeah, these things. Everything in this domain is, you know, speculation, right.
Speaker A: And then there will be, you'd be continuously pleasantly surprised. Do you see a world where there's millions of people, like tens of millions, hundreds of millions of people walking around with the neuralink device or multiple neuralink devices in their brain?
Speaker D: I do. First of all, there are, if you look at worldwide people suffering from movement disorders and visual deficits, I mean, that's in the tens, if not hundreds of millions of people. So that alone, I think there's a lot of benefit and potential good that we can do with this type of technology. And when you start to get into kind of neuro like psychiatric application, depression, anxiety, hunger or obesity, mood control of appetite, I mean, that starts to become very real to everyone.
Speaker A: Not to mention that most people on earth have a smartphone. And once BCI starts competing with a smartphone as a preferred methodology of interacting with the digital world, that also becomes an interesting thing.
Speaker D: Oh, yeah. I mean, yeah, this is even before going to that, right. I mean, there's like, almost, I mean, the entire world that could benefit from these types of things. And then. Yeah, like, if we're talking about kind of next generation of how we interface with machines or even ourselves, in many ways, I think BCI can play a role in that. And some of the things that I also talk about is I do think that there is a real possibility that you could see 8 billion people walking around with neuralink.
Speaker A: Well, thank you so much for pushing ahead, and I look forward to that exciting future.
Speaker D: Thanks for having me.
Speaker A: Thanks for listening to this conversation with DJ saw. And now, dear friends, here's Matthew McDougall, the head neurosurgeon at Neuralink. When did you first become fascinated with the human brain?
Speaker F: Since forever, as far back as I can remember, I've been interested in the human brain. I mean, I was a thoughtful kid and a bit of an outsider. And you sit there thinking about what the most important things in the world are in your little, tiny adolescent brain. And the answer that I came to, that I converged on, was that all of the things you can possibly conceive of as things that are important for human beings to care about are literally contained in the skull. Both the perception of them and their relative values and the solutions to all our problems and all of our problems are all contained in the skull. And if we knew more about how that worked, how the brain encodes information and generates desires and generates agony and suffering, we could do more about it. You think about all the really great triumphs in human history. You think about all the really horrific tragedies. You think about the Holocaust. You think about any prison full of human stories. And all of those problems boil down to neurochemistry. So if you get a little bit of control over that, you provide people the option to do better. In the way I read history, the way people have dealt with having better tools, is that they most often, in the end, do better with huge asterisks. But I think it's an interesting, worthy and noble pursuit to give people more options, more tools.
Speaker A: Yeah. That's a fascinating way to look at human history. You just imagine all these neurobiological mechanisms, Stalin, Hitler, all of these genghis Khan, all of them just had, like, a brain. It's just a bunch of neurons, you know, like a few tens of billions of neurons gaining a bunch of information over a period of time. They have set a module that does language and memory and all that. And from there, in the case of those people, they're able to murder millions of people. And all that coming from, there's not some glorified notion of a dictator of this enormous mind or something like this. It's just, it's just the brain.
Speaker F: Yeah, yeah. I mean, a lot of that has to do with how well people like that can organize those around them.
Speaker A: Other brains.
Speaker F: Yeah. And so I always find it interesting to look to primatology, look to our closest non human relatives, for clues as to how humans are going to behave and what particular humans are able to achieve. And so you look at chimpanzees and bonobos, and theyre similar but different in their social structures, particularly. And I went to Emory in Atlanta and studied under Franz de Wall, the great Franz de Wall, who was kind of the leading primatologist who recently died. And his work at looking at chimps through the lens of how you would watch an episode of friends and understand the motivations of the characters interacting with each other. He would look at a chimp colony and basically apply that lens. I'm massively oversimplifying it. If you do that, instead of just saying subject 473 through his feces, at subject 471, you talk about them in terms of their human struggles, accord them the dignity of themselves as actors with understandable goals, and drives what they want out of life. And primarily, it's the things we want out of food, sex, companionship, power. You can understand chimp and bonobo behavior in the same lights much more easily, and I think doing so gives you the tools you need to reduce human behavior from the kind of false complexity that we layer onto it with language. And look at it in terms of, oh, well, these humans are looking for companionship, sex, food, power. And I think that's a pretty powerful tool to have in understanding human behavior.
Speaker A: And I just went to the Amazon jungle for a few weeks, and it's a very visceral reminder that a lot of life on earth is just trying to get laid. They're all screaming at each other, like, I saw a lot of monkeys, and they're just trying to impress each other. Or maybe there's a battle for power, but a lot of the battle for power has to do with them getting laid.
Speaker F: Right? Breeding rights often go with alpha status. And so if you can get a piece of that, then you're gonna do okay.
Speaker A: And would like to think that we're somehow fundamentally different, but especially when it comes to primates, we really aren't. You know, we can use fancier, poetic language, but maybe some of the underlying drives that motivate us are similar.
Speaker F: Yeah, I think that's true.
Speaker A: And all that is coming from this. The brain.
Speaker F: Yeah.
Speaker A: So when did you first start studying the brain as a biological mechanism?
Speaker F: Basically the moment I got to college, I started looking around for labs that I could do neuroscience work in. I originally approached that from the angle of looking at interactions between the brain and the immune system, which isn't the most obvious place to start, but I had this idea at the time that the contents of your thoughts would have an impact, a direct impact, maybe a powerful one, on non conscious systems in your body, the systems we think of as homeostatic, automatic mechanisms, like fighting off a virus, like repairing a wound. And sure enough, there are big crossovers between the two. It gets to a key point that I think goes under recognized. One of the things people don't recognize or appreciate about the human brain enough, and that is that it basically controls or has a huge role in almost everything that your body does. You try to name an example of something in your body that isn't directly controlled or massively influenced by the brain, and it's pretty hard. I mean, you might say like bone healing or something. But even those systems, the hypothalamus and pituitary, end up playing a role in coordinating the endocrine system that does have a direct influence on, say, the calcium level in your blood that goes to bone healing. So non obvious connections between those things implicate the brain as really a potent prime mover in all of health.
Speaker A: One of the things I realized in the other direction, too, how most of the systems in the body integrated with the human brain, like, they affect the brain also, like the immune system. I think there's just, you know, people who study Alzheimer's and those kinds of things. It's just surprising how much you can understand of that from the immune system, from the other systems that don't obviously seem to have anything to do with sort of the nervous system. They all play together.
Speaker F: Yeah. You could understand how that would be driven by evolution, too. Just in some simple examples. If you get sick, if you get a communicable disease, you get the flu. It's pretty advantageous for your immune system to tell your brain, hey now, be antisocial for a few days. Don't go be the life of the party tonight. In fact, maybe just cuddle up somewhere warm under a blanket and just stay there for a day or two. And sure enough, that tends to be the behavior that you see both in animals and in humans. If you get sick, elevated levels of interleukins in your blood, TNF alpha in your blood, ask the brain to cut back on social activity and even moving around, you have lower locomotor activity in animals that are infected with viruses.
Speaker A: So from there, the early days in neuroscience to surgery, when did that step happen? This is a leap.
Speaker F: It was sort of an evolution of thought. I wanted to study the brain. I started studying the brain in undergrad, in this neuroimmunology lab. I, from there, realized at some point that I didn't want to just generate knowledge. I wanted to effect real changes in the actual world, in actual people's lives. And so after having not really thought about going into medical school, I was on a track to go into a PhD program. I said, well, I'd like that option. I'd like to actually potentially help tangible people in front of me and doing a little digging, found that there exists these MD PhD programs where you can choose not to choose between them and do both. And so I went to USC for medical school and had a joint PhD program with Caltech, where I met, actually chose that program, particularly because of a researcher at Caltech named Richard Anderson, who's one of the godfathers of primate neuroscience. It has a macaque lab where Utah Rays and other electrodes were being inserted into the brains of monkeys to try to understand how intentions were being encoded in the brain. So I ended up there with the idea that maybe I would be a neurologist and study the brain on the side, and then discovered that neurology, again, I'm going to make enemies by saying this. Neurology, predominantly and distressingly to me, is the practice of diagnosing a thing and then saying, good luck with that when there's not much we can do. And neurosurgery very differently. It's a powerful lever on taking people that are headed in a bad direction and changing their course in the sense of brain tumors that are potentially treatable or curable with surgery, even aneurysms in the brain, blood vessels that are going to rupture, you can save lives, really is, at the end of the day, what mattered to me. And so I was at USC, as I mentioned, that happens to be one of the great neurosurgery programs. And so I met these truly epic neurosurgeons, Alex Khalessi and Mike Puzzo and Steve Giannotta and Marty Weiss, these sort of epic people that were just human beings in front of me. And so it kind of changed my thinking from neurosurgeons are distant gods that live on another planet and occasionally come and visit us to, these are humans that have problems and are people, and theres nothing fundamentally preventing me from being one of them. And so at the last minute in medical school, I changed gears from going into a different specialty and switched into neurosurgery, which cost me a year. I had to do another year of research because I was so far along in the process that to switch into neurosurgery, the deadlines had already passed.
Speaker E: A.
Speaker F: Decision that costs time, but absolutely worth it.
Speaker A: What was the hardest part of the training on the neurosurgeon track?
Speaker F: Yeah, two things. I think that residency in neurosurgery is a competition of pain. How much pain can you eat and smile? There's workout restrictions that are not really, they're viewed, I think, internally, among the residents, as weakness. And so most neurosurgery residents try to work as hard as they can. And that, I think, necessarily means working long hours and sometimes over the work hour limits. And, you know, we care about being compliant with whatever regulations are in front of us. But I think more important than that, people want to give their all in becoming a better neurosurgeon because the stakes are so high. And so it's a real fight to get residents to, say, go home at the end of their shift and not stay and do more surgery.
Speaker A: Are you seriously saying, like, one of the hardest things is literally, like, getting, forcing them to get sleep and rest and all this kind of stuff?
Speaker F: Historically, that was the case. I think. I think the next generation, I think the next generation is more compliant and.
Speaker A: More weaker is what you mean. All right, I'm just kidding. I'm just kidding.
Speaker F: I didn't say it.
Speaker A: Now I'm making enemies. No. Okay, I get it. Wow, that's fascinating. So what was the second thing?
Speaker F: The personalities, and maybe the two are.
Speaker A: Connected, but so was it pretty competitive?
Speaker F: It's competitive, and it's also, as we touched on earlier, primates like power. And I think neurosurgery has long had this aura of mystique and excellence and whatever about it. And so it's an invitation, I think, for people that are cloaked in that authority. A board certified neurosurgeon is basically a walking, fallacious appeal to authority. Right. You have license to walk into any room and act like you're an expert on whatever. And fighting that tendency is not something that most neurosurgeons do. Well, humility isn't the forte.
Speaker A: Yeah. So I have friends who know you, and whenever they speak about you, that you have the surprising quality for a neurosurgeon of humility, which I think indicates that it's not as common as perhaps in other professions, because there is a kind of gigantic sort of heroic aspect to neurosurgery, and I think it gets to people's head a little bit.
Speaker F: Yeah. Well, I think that that allows me to play well at an Elon company because Elon, one of his strengths, I think, is to just instantly see through fallacy from authority. So nobody walks into a room that he's in and says, well, God damn it, you have to trust me. I'm the guy that built the last ten rockets or something. And he says, well, you did it wrong, and we can do it better. Or I'm the guy that kept Ford alive for the last 50 years. You listen to me on how to build cars, and he says, no. And so you dont walk into a room that hes in and say, well, im a neurosurgeon. Let me tell you how to do it. Hes going to say, well, im a human being that has a brain. I can think from first principles myself, thank you very much. And heres how I think it ought to be done. Lets go try it and see whos right. And thats proven, I think, over and over in his case, to be a very powerful approach.
Speaker A: If we just take that tangent. Fascinating interdisciplinary team at Neuralink that you get to interact with, including Elon, what do you think is the secret to a successful team? What have you learned from just getting to observe these folks? World experts in different disciplines work together.
Speaker F: Yeah, there's a sweet spot where people disagree and forcefully speak their mind and passionately defend their position and yet are still able to accept information from others and change their ideas when theyre wrong. And so I like the analogy of how you polish rocks. You put hard things in a hard container and spin it. People bash against each other, and out comes a more refined product. And so to make a good team at Neuralink, weve tried to find people that are not afraid to defend their ideas passionately and occasionally strongly disagree with people that theyre working with and have the best idea come out on top. It's not an easy balance, again, to refer back to the primate brain. It's not something that is inherently built into the primate brain. To say, I passionately put all my chips on this position, and now I'm just going to walk away from it and admit you were right. Part of our brains tell us that that is a power loss, that is a loss of face, loss of standing in the community. And now you're a zeta chump because your idea got trounced. And you just have to recognize that little voice in the back of your head is maladaptive and it's not helping the team win.
Speaker A: Yeah, you have to have the confidence to be able to walk away from an idea that you hold onto. And if you do that often enough, you're actually going to become the best in the world at your thing. I mean, that kind of, that rapid iteration.
Speaker F: Yeah, you'll at least be a member of a winning team.
Speaker A: Ride the wave. What did you learn? You mentioned there's a lot of amazing neurosurgeons at USC. What lessons about surgery and life have you learned from those folks?
Speaker F: Yeah, I think working your ass off, working hard while functioning as a member of a team, getting a job done, that is incredibly difficult, working incredibly long hours, being up all night, taking care of someone that you think probably won't survive no matter what you do, working hard to make people that you passionately dislike look good the next morning. These folks were relentless in their pursuit of excellent neurosurgical technique, decade over decade, and I think were well recognized for that excellence, especially Marty Weiss, Steve Gianotta, Mike Capuzzo. They made huge contributions not only to surgical technique, but they built training programs that trained dozens or hundreds of amazing neurosurgeons. I was just lucky to kind of be in their wake.
Speaker A: What's that like? You mentioned doing a surgery where the person is likely not to survive. Does that wear on you?
Speaker F: Yeah. You know, it's especially challenging when you, with all respect to our elders, it doesn't hit so much when you're taking care of an 80 year old, and something was going to get them pretty soon anyway. And so you lose a patient like that, and it was part of the natural course of what is expected of them in the coming years. Regardless, taking care of a father of two or three, four young kids, someone in their thirties that didn't have it coming, and they show up in your ER having their first seizure of their life, and lo and behold, they've got a huge malignant, inoperable, or incurable brain tumor. You can only do that, I think, a handful of times before it really starts eating away at your armor. Or a young mother that shows up that has a giant hemorrhage in her brain that she's not going to survive from, and they bring her four year old daughter in to say goodbye one last time before they turn the ventilator off. The great Henry Marsh is an english neurosurgeon who said it best. I think he says every neurosurgeon carries with them a private graveyard. And I definitely feel that, especially with young parents, that kills me. They had a lot more to give. The loss of those people specifically has a knock on effect that's going to make the world worse for people for a long time. And it's just hard to feel powerless in the face of that. And that's where I think you have to be borderline evil to fight against a company like Neuralink or to constantly be taking potshots at us, because what we're doing is to try to fix that stuff. We're trying to give people options to reduce suffering. We're trying to take the pain out of life that broken brains brings in. And yeah, this is just our little way that we're fighting back against entropy, I guess.
Speaker A: Yeah. The amount of suffering that's endured when some of the things that we take for granted that our brain is able to do is taken away is immense. And to be able to restore some of that functionality is a real gift.
Speaker F: Yeah, we're just starting. We're going to do so much more.
Speaker A: Well, can you take me through the full procedure for implanting, say, the n one chip in your link? Yeah.
Speaker F: It's a really simple, really simple, straightforward procedure. The human part of the surgery that I do is dead simple. It's one of the most basic neurosurgery procedures imaginable. And I think there's evidence that some version of it has been done for thousands of years. There are examples, I think, from ancient Egypt of healed or partially healed trephanations, and from Peru or ancient times in South America, where these protosurgeons would drill holes in people's skulls, presumably to let out the evil spirits, but maybe to drain blood clots. And there's evidence of bone healing around the edge, meaning the people at least survive some months after a procedure. And so what we're doing is that we are making a cut in the skin on the top of the head over the area of the brain that is the most potent representation of hand intentions. And so if you are an expert concert pianist, this part of your brain is lighting up the entire time you're playing. We call it the hand knob.
Speaker A: The hand knob. There's all the finger movements. All of that is just firing away.
Speaker F: There's a little squiggle in the cortex right there. One of the folds in the brain is doubly folded right on that spot. You can look at it on an MRI and say, that's the hand knob. And then you do a functional test and a special kind of MRI called a functional MRI fMRI. And this part of the brain lights up when people, even quadriplegic people whose brains aren't connected to their finger movements anymore, imagine finger movements. And this part of the brain still lights up. So we can id that part of the brain in anyone whos preparing to enter our trial and say, okay, that part of the brain, we confirm is your hand intention area. And so ill make a little cut in the skin. Well, flap the skin open just like kind of opening the hood of a car, only a lot smaller. Make a perfectly round, one inch diameter hole in the skull. Remove that bit of skull, open the lining of the brain, the covering of the brain, it's like a little bag of water that the brain floats in. And then show that part of the brain to our robot. This is where the robot shines. It can come in and take these tiny, much smaller than human hair electrodes and precisely insert them into the cortex, into the surface of the brain, to a very precise depth, in a very precise spot that avoids all the blood vessels that are coating the surface of the brain. And after the robot's done with its part, then the human comes back in and puts the implant into that hole in the skull and covers it up, screwing it down to the skull and sewing the skin back together. So the whole thing is a few hours long. It's extremely low risk compared to the average neurosurgery involving the brain that might, say, open up a deep part of the brain or manipulate blood vessels in the brain. This opening on the surface of the brain with only cortical microinsertions carries significantly less risk than a lot of the tumor or aneurysm surgeries that are routinely done.
Speaker A: So cortical microinsertions that are via robot and computer vision, are designed to avoid the blood vessels.
Speaker F: Exactly.
Speaker A: So I know you're a bit biased here, but let's compare human and machine. So what are human surgeons able to do well? And what are robot surgeons able to do well at this stage of our human civilization development?
Speaker F: Yeah, yeah, that's a good question. Humans are general purpose machines. We're able to adapt to unusual situations. We're able to change the plan on the fly. I remember well a surgery that I was doing many years ago down in San Diego, where the plan was to open a small hole behind the ear and go reposition a blood vessel that had come to lay on the facial nerve, the trigeminal nerve, the nerve that goes to the face. When that blood vessel lays on the nerve, it can cause just intolerable, horrific shooting pain that people describe like being zapped with a cattle prod. And so the beautiful, elegant surgery is to go move this blood vessel off the, off the nerve. The surgery team, we went in there and started moving this blood vessel, and then found that there was a giant aneurysm on that blood vessel that was not easily visible on the pre op scans. And so the plan had to dynamically change, and that the human surgeons had no problem with that, were trained for all those things. Robots wouldnt do so well in that situation, at least in their current incarnation. Fully robotic surgery, like the electrode insertion portion of the neuralink surgery, it goes according to a set plan, and so the humans can interrupt the flow and change the plan, but the robot can't really change the plan midway through. It operates according to how it was programmed and how it was asked to run. It does its job very precisely, but not with a wide degree of latitude and how to react to changing conditions.
Speaker A: So there could be just a very large number of ways that you could be surprised as a surgeon, when you enter a situation, there could be subtle things that you have to dynamically adjust.
Speaker F: Correct.
Speaker A: And robots are not good at that.
Speaker F: Currently, I think we are at the dawn of a new era with AI of the parameters for robot responsiveness to be dramatically broadened. You can't look at a self driving car and say that it's operating under very narrow parameters. If a chicken runs across the road, it wasn't necessarily programmed to deal with that specifically, but a Waymo or a self driving Tesla would have no problem reacting to that appropriately. And so surgical robots aren't there yet, but give it time.
Speaker A: And then there could be a lot of sort of semi autonomous possibilities of maybe a robotic surgeon could say, this situation is perfectly familiar, or the situation is not familiar, and in the not familiar case, a human could take over, but basically, like, be very conservative, saying, okay, this for sure has no issues, no surprises, and then let the humans deal with the surprises, with the edge cases, all that. Yeah, that's one possibility. So, like, you think eventually, uh, you'll be out of the job? Well, you being neurosurgeon, your job being neurosurgeon, humans, there will not be many neurosurgeons left on this earth.
Speaker F: I'm not worried about my job. In my, in the course of my professional life, I think I I would tell my, my kids not necessarily to go in this line of work, depending on, depending on how things look in.
Speaker A: 20 years, it's so fascinating because, I mean, I if I have a line of work, I would say it's programming. And if you ask me, like, for the last, I don't know, 20 years, what I would recommend for people, I would tell them, yeah, go, you will always have a job if you're a programmer, because there's more and more computers and all this kind of stuff, and it pays well. But then you realize these large language models come along and they're really damn good at generating code. So overnight, you could be surprised, like, wow, what is the contribution of the human, really? But then you start to think, okay, it does seem that humans have ability, like you said, to deal with novel situations. In the case of programming, it's the ability to kind of come up with novel ideas to solve problems. It seems like machines aren't quite yet able to do that. And when the stakes are very high, when it's life critical as it is in surgery, especially in neurosurgery, then it starts. The stakes are very high for a robot to actually replace a human. But it's fascinating that in this case of neuralink, there's a human robot collaboration.
Speaker F: Yeah, I do the parts it can't do, and it does the parts I can't do. And we are friends.
Speaker A: I saw that there's a lot of practice going on. So, I mean, everything in uralink is tested extremely rigorously. But one of the things I saw, that there's a proxy on which the surgeries are performed. So this is both for the robot and for the human, for everybody involved in the entire pipeline. What's that like, practicing the surgery?
Speaker F: It's pretty intense. So there's no analog to this in human surgery? Human surgery is sort of this artisanal craft that's handed down directly from master to pupil over the generations. I mean, literally, the way you learn to be a surgeon on humans is by doing surgery on humans. I mean, first you watch your professors do a bunch of surgery, and then finally they put the trivial parts of the surgery into your hands, and then the more complex parts. And as your understanding of the point and the purposes of the surgery increases, you get more responsibility in the perfect condition. It doesn't always go well. In Neuralink's case, the approach is a bit different. We, of course, practiced as far as we could on animals. We did hundreds of animal surgeries. And when it came time to do the first human, we had just an amazing team of engineers build incredibly lifelike models. One of the engineers, Fran Romano in particular, built a pulsating brain in a custom 3d printed skull that matches exactly the patient's anatomy, including their face and scalp characteristics. And so when I was able to practice that, I mean, it's as close as it really reasonably should get to being the real thing in all the details, including having a mannequin body attached to this custom head. And so when we were doing the practice surgeries, wheel that body into the CT scanner and take a mock ct scan and wheel it back in and conduct all the normal safety checks verbally. Stop this patient. We're confirming his identification is mannequin number blah, blah, blah, and then opening the brain in exactly the right spot using standard operative neuronavigation equipment, standard surgical drills in the same, or that we do all of our practice surgeries in at Neuralink and having the skull open and have the brain pulse, which adds a degree of difficulty for the robot to perfectly, precisely plan and insert those electrodes to the right depth and location. We broke new ground on how extensively we practiced for this surgery.
Speaker A: So there was a historic moment, a big milestone for neuralink, in part for humanity, with the first human getting a neuralink implant in January of this year. Take me through the surgery on Noland. What did he feel like to be part of this?
Speaker F: Yeah, well, we were lucky to have just incredible partners at the Barrow Neurologic Institute. They are, I think, the premier neurosurgical hospital in the world. They made everything as easy as possible for the trial to get going and helped us immensely with their expertise on how to arrange the details. It was a much more high pressure surgery in some ways. I mean, even though the outcome wasn't particularly in question in terms of our participants safety, the number of observers, the number of people, there's conference rooms full of people watching live streams in the hospital rooting for this to go perfectly. And that just adds pressure that is not typical for even the most intense production, neurosurgery, say, removing a tumor or placing deep brain stimulation electrodes. And it had never been done on a human before. There were unknowns, and so definitely a moderate pucker factor there for the whole team. Not knowing if we were going to encounter, say, a degree of brain movement that was unanticipated or a degree of brain sag that took the brain far away from the skull and made it difficult to insert or some other unknown, unknown problem. Fortunately, everything went well, and that surgery was one of the smoothest outcomes we could have imagined.
Speaker A: Were you nervous? I mean, you're extremely quarterback and like, in the Super bowl kind of situation, extremely nervous.
Speaker F: Extremely. I was very pleased when it went well and then. And when it was over. Looking forward to number two.
Speaker C: Yeah.
Speaker A: Even with all that practice, all of that, just, you've never been in a situation that's so high stakes in terms of people watching. Yeah. And we should also probably mention, given how the media works, a lot of people maybe in a dark kind of way, hoping it doesn't go well.
Speaker F: I think wealth is easy to hate or envy or whatever, and I think there's a whole industry around driving clicks, and bad news is great for clicks. And so any way to take an event and turn it into bad news is going to be really good for clicks.
Speaker A: It just sucks because I think it puts pressure on people. It discourages people from trying to solve really hard problems, because to solve hard problems, you have to go into the unknown. You have to do things that haven't been done before, and you have to take risks, calculated risks. You have to do all kind of safety precautions, but risks nevertheless. I just wish there would be more celebration of that, of the risk taking versus people just waiting on the. The sidelines, like, waiting for failure and then pointing out the failure. Yeah, it sucks. But in this case, it's really great that everything went just flawlessly, but it's unnecessary pressure. I would say.
Speaker F: Now that there's a human with literal skin in the game, there's a participant whose wellbeing rides on this, doing well, you have to be a pretty bad person to be rooting for that to go wrong. And so hopefully, people look in the mirror and realize that at some point.
Speaker A: So did you get to actually front row seat, like, watch the robot work? Like, what? You get to see the whole thing?
Speaker F: Yeah. I mean, because an MD needs to be in charge of all of the medical decision making throughout the process. I unscrubbed from the surgery after exposing the brain and presenting it to the robot, and place the targets on the robot software interface that tells the robot where it's going to insert each thread. That was done with my hand on the mouse, for whatever that's worth.
Speaker A: So you were the one placing the targets?
Speaker F: Yeah.
Speaker A: Oh, cool. So the robot with a computer vision provides a bunch of candidates, and you finalize the decision.
Speaker F: Right. The software engineers are amazing on this team, and so they actually provided an interface where you can essentially use a lasso tool and select a prime area of brain real estate, and it will automatically avoid the blood vessels in that region and automatically place a bunch of targets. That allows the human robot operator to select really good areas of brain and make dense applications of targets in those regions. The regions we think are going to have the most high fidelity representations of finger movements and arm movement intentions.
Speaker A: I've seen images of this, and for me, with OCD, it's for some reason, a really pleasant. I think there's a subreddit called oddly satisfying.
Speaker F: Yeah, love that subreddit.
Speaker A: It's oddly satisfying to see the different target sites, avoiding the blood vessels and also maximizing the usefulness of those locations for the signal. It just feels good.
Speaker E: It's like, ah.
Speaker F: As a person who has a visceral reaction to the brain bleeding, I can tell you, yes.
Speaker A: Especially.
Speaker F: It's extremely satisfying watching the electrodes themselves go into the brain and not cause bleeding.
Speaker A: Yeah. Yeah. So you said the feeling was of relief when everything went perfectly.
Speaker F: Yeah.
Speaker A: How deep in the brain can you currently go and eventually go, let's say, on the neuralink side? It seems the deeper you go in the brain, the more challenging it becomes.
Speaker F: Yeah. So, talking broadly about neurosurgery, we can get anywhere. It's routine for me to put deep brain stimulating electrodes near the very bottom of the brain, entering from the top and passing about a two millimeter wire all the way into the bottom of the brain. And that's not revolutionary. A lot of people do that and we can do that with very high precision. I use a robot from Globus to do that surgery several times a month. It's pretty routine.
Speaker A: What are your eyes in that situation? What are you seeing? What kind of technology can you use to visualize where you are to light your way?
Speaker F: Yeah, it's a cool process. On the software side, you take a preoperative MRI that's extremely high resolution data of the entire brain. You put the patient to sleep, put their head in a frame that holds the skull very rigidly, and then you take a CT scan of their head while they're asleep. With that frame on, and then merge the MRI and the CT in software. You have a plan based on the MRI where you can see these nuclei deep in the brain. You can't see them on CT, but if you trust the merging of the two images, then you indirectly know on the CT where that is, and therefore indirectly know where in reference to the titanium frame screwed to their head, those targets are. And so this is sixties technology to manually compute trajectories, given the entry point and target, and dial in some goofy looking titanium actuators with manual actuators with little tick marks on them. The modern version of that is to use a robot, just like a little Kuka arm. You might see it building cars at the Tesla factory. This small robot arm can show you the trajectory that you intended from the pre op MRI and establish a very rigid holder through which you can drill a small hole in the skull and pass a small rigid wire deep into that area of the brain that's hollow, and put your electrode through that hollow wire and then remove all of that except the electrode. So you end up with the electrode very, very precisely placed far from the skull surface. Now, that's standard technology that's already been out in the world for a while. Neuralink right now is focused entirely on cortical targets, surface targets, because there's no trivial way to get, say, hundreds of wires deep inside the brain. Without doing a lot of damage. So your question, what do you see? Well, I see an MRI on a screen. I can't see everything that that DBS electrode is passing through on its way to that deep target. And so it's accepted with this approach that there's going to be about one in a hundred patients who have a bleed somewhere in the brain as a result of passing that wire blindly into the deep part of the brain. That's not an acceptable safety profile for neuralink. We start from the position that we want this to be dramatically, maybe two or three orders of magnitude safer than that. Safe enough, really, that you or I, without a profound medical problem, might on our lunch break someday say, yeah, sure, I'll get that. I'd be meaning to upgrade to the latest version. And so the safety constraints given that are high, and so we haven't settled on a final solution for arbitrarily approaching deep targets in the brain.
Speaker A: It's interesting because you have to avoid blood vessels somehow. Maybe there's creative ways of doing the same thing, like mapping out high resolution geometry of blood vessels, and then you can go in blinden. But how do you map out that in a way that's super stable? There's a lot of interesting challenges there.
Speaker B: Right?
Speaker A: Yeah, but there's a lot to do on the surface.
Speaker F: Exactly. So we've got vision on the surface. We actually have made a huge amount of progress sewing electrodes into the spinal cord as a potential workaround for a spinal cord injury that would allow a brain mounted implant to translate motor intentions to a spine mounted implant that can affect muscle contractions in previously paralyzed arms and legs.
Speaker A: Mind blowing. That's just incredible. So, like, the effort there is to try to bridge the brain to the spinal cord, to the periphery, peripheral, neural, nervous system. So how hard is that to do?
Speaker F: We have that working in. In very crude forms in animals.
Speaker A: That's amazing. Yeah, we've done so similar to, like with Nolan, where he's able to digitally move the cursor. Here you're doing the same kind of communication, but with the actual effectors that you have. That's fascinating.
Speaker F: Yeah. So we have anesthetized animals doing grasp and moving their legs and sort of walking pattern. Again, early days, but the future is bright for this kind of thing, and people with paralysis should look forward to that bright future. They're going to have options.
Speaker A: Yeah. And there's a lot of sort of intermediate or extra options where you take an optimist robot like the arm, and to be able to control the arm. Yeah, the fingers, the hands of the arm.
Speaker F: As a prosthetic, exoskeletons are getting better, too.
Speaker A: Exoskeletons, yeah. So that goes hand in hand, although I didn't quite understand until thinking about it deep and doing more research about neuralink, how much you can do on the digital side. So this digital telepathy, I didn't quite understand that you could really map the intention as you described in the hand knob area, that you can map the intention. Just imagine it, think about it. That intention can be mapped to actual action in the digital world. And now more and more, so much can be done in the digital world that it can reconnect you to the outside world. It can allow you to have freedom, have independence. If you're a quadriplegic, that's really powerful. Like, you can go really far with that.
Speaker F: Yeah. Our first participant is, he's incredible. He's breaking world records left and right.
Speaker A: And he's having fun with it. It's great. Just going back to the surgery your whole journey, you mentioned to me offline you have surgery on Monday. So you're like, you're doing surgery all the time. Yeah, maybe the ridiculous question what does it take to get good at surgery?
Speaker F: Practice repetitions. You just, same with anything else. There's a million ways of people saying the same thing and selling books saying it, but you call it 10,000 hours, you call it spend some chunk of your life, some percentage of your life focusing on this, obsessing about getting better at it, repetitions, humility, recognizing that you arent perfect at any stage along the way, recognizing youve got improvements to make in your technique, being open to feedback and coaching from people with a different perspective on how to do it, and then just the constant will to do better. Fortunately, if youre not a sociopath, I think your patients bring that with them to the office visits every day. They force you to want to do better all the time.
Speaker A: Yeah, just step up. I mean, it's a real human being. A real human being that you can help.
Speaker F: Yeah.
Speaker A: So every surgery, even if it's the same exact surgery, is there a lot of variability between that surgery and a different person?
Speaker F: Yeah, a fair bit. I mean, a good example for us is that the angle of the skull relative to the normal plane of the body axis of the skull over hand knob is pretty wide variation. I mean, some people have really flat skulls and some people have really steeply angled skulls over that area. And that has consequences for how their head can be fixed in, in sort of the frame that we use and how the robot has to approach the skull. And, yeah, people's bodies are built as differently as the people you see walking down the street. As much variability in body shape and size as you see there. We see in brain anatomy and skull anatomy. There are some people who we've had to kind of exclude from our trial for having skulls that are too thick or too thin or scalp that's too thick or too thin. I think we have the middle 97% or so of people. But you can't account for all human anatomy variability.
Speaker A: How much mushiness and mess is there? Because taking biology classes, the diagrams are always really clean and crisp. Neuroscience. The pictures of neurons are always really nice and very. But whenever I look at pictures of, like, real brains, they're all, I don't know what is going on.
Speaker F: Yeah.
Speaker A: So how much are biological systems in reality? Like, how hard is it to figure out what's going on?
Speaker F: Not too bad once you really get used to this. You know, that's where experience and skill and education really come into play, is if you stare at a thousand brains, it becomes easier to mentally peel back the, say, for instance, blood vessels that are obscuring the sulci and gyri, the wrinkle pattern of the surface of the brain. Occasionally, when you're first starting to do this and you open the skull, it doesn't match what you thought you were going to see based on the MRI. And with more experience, you learn to peel back that layer of blood vessels and see the underlying pattern of wrinkles in the brain and use that as a landmark for where you are.
Speaker A: The wrinkles are a landmark?
Speaker F: Yeah, I was describing hand knob earlier. That's a pattern of the wrinkles in the brain. It's this greek letter omega shaped area of the brain.
Speaker A: So you could recognize the hand knob area. Like, if I show you a thousand brains and give you like 1 minute with each, you'd be like, yep, that's that.
Speaker F: Sure.
Speaker A: And so there is some uniqueness to that area of the brain, like, in terms of the Geometry, the topology of the thing.
Speaker F: Yeah.
Speaker A: Where is it about in the.
Speaker F: It's. So you have this strip of brain running down the top I call the primary motorhouse area. And I'm sure you've seen this picture of the homunculus laid over the surface of the brain. The weird little guy with huge lips and giant hands. That guy sort of lays with his legs up at the top of the brain and face, arm areas farther down, and then some kind of mouth, lip, tongue areas farther down. And so the hand is right in there. And then the areas that control speech, at least on the left side of the brain in most people, are just below that. And so any muscle that you voluntarily move in your body, the vast majority of that references that strip or those intentions come from that strip of brain. And the wrinkle for hand knob is right in the middle of that.
Speaker A: And vision is back here also close to the surface.
Speaker F: Vision is a little deeper. And so this gets to your question about how deep can you get to do vision? We can't just do the surface of the brain. We have to be able to go in not as deep as we have to go for DBS, but maybe a centimeter deeper than we're used to for hand insertions. And so that's work in progress. That's a new set of challenges to overcome.
Speaker A: By the way, you mentioned the Utah array, and I just saw a picture of that, and that thing looks terrifying because it's rigid. And then if you look at the threads, they're flexible. What can you say that's interesting to you about the flexible, that kind of approach of the flexible threads to deliver the electrodes next of the neurons?
Speaker F: Yeah, I mean, the goal there comes from experience. I mean, we stand on the shoulders of people that made Utah rays and used Utah rays for decades before we ever even came along. Neuralink arose partly, this approach to technology arose out of a need recognized after Utah rays would fail routinely, because the rigid electrodes, those spikes that are literally hammered using an air hammer into the brain, those spikes generate a bad immune response that encapsulates the electrode spikes in scar tissue. Essentially, one of the projects that was being worked on in the Andersen lab at Caltech when I got there was to see if you could use chemotherapy to prevent the formation of scar. Things are pretty bad when you're jamming a bed of nails into the brain and then treating that with chemotherapy to try to prevent scar tissue. It's like, maybe we've gotten off track here, guys. Maybe there's a fundamental redesign necessary. Neuralink's approach of using highly flexible, tiny electrodes avoids a lot of the bleeding, avoids a lot of the immune response that ends up happening when rigid electrodes are pounded into the brain. What we see is our electrode longevity and functionality, and the health of the brain tissue immediately surrounding the electrode is excellent. It goes on for years now in our animal models.
Speaker A: What do most people not understand about the biology of the brain? We mentioned the vasculature. That's really interesting.
Speaker F: I think the most interesting, maybe underappreciated fact is that it really does control almost everything. I don't know. For out of the blue example, imagine you want a lever on fertility. You want to be able to turn fertility on and off. I mean, there are legitimate targets in the brain itself to modulate fertility, say, blood pressure. You want to modulate blood pressure. There are legitimate targets in the brain for doing that. Things that aren't immediately obvious as brain problems are potentially solvable in the brain. And so I think it's an underexplored area for primary treatments of all the things that bother people.
Speaker A: That's a really fascinating way to look at it. There's a lot of conditions we might think have nothing to do with the brain, but they might just be symptoms of something that actually started in the brain. The actual source of the problem. The primary source is something.
Speaker F: Yeah, not always. I mean, you know, kidney disease is real, but there are levers you can pull in the brain that affect all of the. All of these systems.
Speaker A: There's knobs.
Speaker F: Yeah.
Speaker A: On off switches and knobs in the brain from which this all originates. Would you have a neuralink chip implanted in your brain?
Speaker F: Yeah, I think use case right now is use a mouse. I can already do that. And so there's no value proposition on safety grounds alone. Sure. I would do it tomorrow.
Speaker A: You say the use case of the mouse is after researching all this, and part of it is just watching Nolan have so much fun. If you can get that bits per second look really high with the mouse, like being able to interact, because if you think about the way the. On the smartphone, the way you swipe, that was transformational.
Speaker F: Yeah.
Speaker A: How we interact with the thing, it's subtle, you don't realize it, but able to touch a phone and to scroll with your finger, that's like, that changed everything. People were sure you need a keyboard to type and that there's a lot of HCI aspects to that that changed how we interact with computers. So there could be a certain rate of speed with the mouse that would change everything.
Speaker F: Yes.
Speaker A: Like, you might be able to just click around a screen extremely fast and that if it. I can see myself getting the neural link for much more rapid interaction with the digital devices.
Speaker F: Yeah. I think recording speech intentions from the brain might. Might change things as well. The value proposition for the average person, a keyboard is a pretty clunky human interface requires a lot of training. It's highly variable in the maximum performance that the average person can achieve. I think taking that out of the equation. And just having a natural word to computer interface might change things for a lot of people.
Speaker A: It'd be hilarious if that is the reason people do it. Even if you have speech to text, that's extremely accurate. It currently isn't, but it's a gotten super accurate. It'd be hilarious if people went for Neuralink. Just so you avoid the embarrassing aspect of speaking, like looking like a douchebag, speaking to your phone in public, which is a real, like, that's a real constraint.
Speaker F: Yeah. I mean, with a bone conducting case that can be an invisible headphone, say, and the ability to think words into software and have it respond to you, that starts to sound sort of like embedded super intelligence. If you can silently ask for the Wikipedia article on any subject and have it read to you without any observable change happening in the outside world. For one thing, standardized testing is obsolete.
Speaker A: Yeah. If it's done well on the UX side, it could change. I don't know if it transforms society, but it really can create a kind of shift in the way we interact with digital devices in the way that a smartphone did. Yeah, I would just having to look into the safety of everything involved, I would totally try it so it doesn't have to go to some, like, incredible thing where you have it connects your vision or to some other, like, it connects all over your brain. That could be like just connecting to the hand knob. You might have a lot of interesting interaction, human computer interaction possibilities. That's really interesting.
Speaker F: Yeah. And the technology on the academic side is progressing at light speed here, I think. There was a really amazing paper out of UC Davis, Sergei Stavisky's lab that basically made an initial solve of speech decode. It was something like 125,000 words that they were getting with very high accuracy, which is.
Speaker A: So you're just thinking the word.
Speaker F: Yeah.
Speaker A: Think in the word and you're able to get it. Yeah. Oh, boy. Like, you have to have the intention of speaking it.
Speaker F: Right.
Speaker A: So, like, do that inner voice, man. It's so amazing to me that you can do the intention, the signal mapping. All you have to do is just imagine yourself doing it. And if, if you get the feedback that it actually worked, you can get really good at that. Like, your brain will first of all adjust and you develop, like, any other skill.
Speaker F: Yeah.
Speaker A: Like touch typing, you develop in that same kind of way. That is. That is really, to me, it's just really fascinating to be able to even to play with that, honestly. Like, I would get a new link just to be able to play with that, just to play with the capacity, the capability of my mind to learn this skill. It's like learning the skill of typing or learning the skill of moving a mouse. It's another skill of moving the mouse, not with my physical body, but with my mind.
Speaker F: I can't wait to see what people do with it. I feel like we're cavemen right now. We're banging rocks with a stick and thinking that we're making music. At some point when these are more widespread, there's going to be the equivalent of a piano that someone can make art with their brain in a way that we didn't even anticipate. I'm looking forward to it.
Speaker A: Give it to like a teenager. Like anytime I think I'm good at something, I'll always go to, like, I don't know, even, even with the bits per second and playing a video game, you realize you give it to a teenager, you're given your link to a teenager. Just a large number of them. The kind of stuff you get good at stuff, they're gonna get like hundreds of bits per second.
Speaker F: Yeah.
Speaker A: Even just with the current technology.
Speaker F: Probably.
Speaker A: Probably just because it's also addicting how like the number go up aspect of it, of like improving and training, because it is, it's almost like a skill. And plus there's a software on the other end that adapts to you. And especially if the adapting procedure algorithm becomes better and better and better. You like learning together.
Speaker F: Yeah, we're scratching the surface on that right now. There's so much more to do.
Speaker A: So on the complete other side of it, you have an RFID chip implanted in you. Yeah, so I hear. Nice.
Speaker F: So this is subtle thing.
Speaker A: It's a passive device that you use for unlocking, like a safe with top secrets or what is it? What do you use it for? What's the story behind it?
Speaker F: I'm not the first one. There's, there's this whole community of weirdo biohackers that have done this stuff, and I think one of the early use cases was storing private crypto wallet, keys and whatever. I dabbled in that a bit and had some fun with it.
Speaker A: Do you have some bitcoin implanted in your body somewhere you can't tell where?
Speaker F: Yeah, actually, yeah. It was the modern day equivalent of finding change in the sofa cushions after I put some orphan crypto on there that I thought was worthless and forgot about it for a few years, went back and found that some community of people loved it and had propped up the value of it. And so it had gone up 50 fold. So there was a lot of change in those cushions.
Speaker A: That's hilarious.
Speaker F: But the primary use case is mostly as a tech demonstrator has my business card on it. You can scan that, and by touching it to your phone, it opens the front door to my house. You know, whatever. Simple stuff.
Speaker A: It's a cool step. It's a cool leap to implant something in your body. I mean, it has. Perhaps that's. It's a similar leap to in your link, because for a lot of people, that kind of notion of putting something inside your body, something electronic inside a biological system, is a big leap.
Speaker F: Yeah, we have a kind of a mysticism around the barrier of our skin. We're completely fine with knee replacements, hip replacements, dental implants, but there's a mysticism still around the inviolable barrier that the skull represents. And I think that needs to be treated like any other pragmatic barrier. The question isn't a how incredible is it to open the skull? The question is, what benefit can we provide?
Speaker A: So from all the surgeries you've done, from everything you understand in the brain, how much does neuroplasticity come into play? How adaptable is the brain? For example, just even in the case of healing from surgery or adapting to the post surgery situation, the answer that.
Speaker F: Is sad for me and other people of my demographic is that plasticity decreases with age. Healing decreases with age. I have too much gray hair to be optimistic about that. There are theoretical ways to increase plasticity using electrical stimulation. Nothing that is totally proven out as a robust enough mechanism to offer widely to people. Yeah, I think there's cause for optimism that we might find something useful in terms of, say, an implanted electrode that improves learning. Certainly there's been some really amazing work recently from Nicholas Schiff, Jonathan Baker, and others who have a cohort of patients with moderate traumatic brain injury who have had electrodes placed in the deep nucleus in the brain called the centromedian nucleus, or just near central median nucleus. And when they apply small amounts of electricity to that part of the brain, it's almost like electronic caffeine. They're able to improve people's attention and focus. They're able to improve how well people can perform a task. I think in one case, someone who was unable to work after the device was turned on, they were able to get a job. And that's one of the holy grails for me with Neuralink and other technologies like this is from a purely utilitarian standpoint can we make people able to take care of themselves and their families economically again? Can we make it so someone who's fully dependent and even maybe requires a lot of caregiver resources, can we put them in a position to be fully independent, taking care of themselves, giving back to their communities? I think that's a very compelling proposition, and what motivates a lot of what I do and what a lot of the people at Neuralink are working for.
Speaker A: It'S just a cool possibility that if you put a neuralink in there, that the brain adapts like the other part of the brain adapts to, and integrates it. The capacity of the brain to do that is really interesting, probably unknown to the degree to which you can do that, but you're now connecting an external thing to it, especially once it's doing stimulation. The biological brain and the electronic brain outside of it working together, the possibilities, they're really interesting. That's still unknown, but interesting. It feels like the brain is really good at adapting to whatever. Yeah, but of course, it is a system that by itself is already, like, everything serves a purpose, and so you don't want to mess with it too much.
Speaker F: Yeah, it's like, you know, eliminating a species from an ecology. You know, you don't know what the delicate interconnections and dependencies are.
Speaker E: I.
Speaker F: The brain is certainly a delicate, complex beast, and we don't know every potential downstream consequence of a single change that we make.
Speaker A: Do you see yourself doing so? You mentioned p one surgeries of p two, p three, p four, p five. Just more and more and more humans.
Speaker F: I think it's a certain kind of brittleness or a failure on the company's side. If we need me to do all the surgeries, I think something that I would very much like to work towards is a process that is so simple and so robust on the surgery side that literally anyone could do it. We want to get away from requiring intense expertise or intense experience to have this successfully done and make it as simple and translatable as possible. I mean, I would love it if every neurosurgeon on the planet had no problem doing this. I think we're probably far from a regulatory environment that would allow people that aren't neurosurgeons to do this, but not impossible.
Speaker A: All right, I'll sign up for that. Did you ever anthropomorphize the robot r1? Like, do you give it a name? Do you see it as, like a friend? That's like working together with you.
Speaker F: I mean, to a certain degree, it's.
Speaker A: Or an enemy who's going to take the job.
Speaker F: To a certain degree, it's. Yeah, it's complex relationship.
Speaker A: All the good relationships are.
Speaker F: It's funny when in the middle of the surgery, there's a part of it where I stand basically shoulder to shoulder with the robot. And so if you're in the room reading the body language, it's my brother in arms there. We're working together on the same problem. Yeah. I'm not threatened by it.
Speaker A: Keep telling yourself that. How have all the surgeries that you've done over the years, the people you've helped, and the stakes, the high stakes that you've mentioned, how has that changed your understanding of life and death?
Speaker F: Yeah, it gives you a very visceral sense, and this may sound trite, but it gives you a very visceral sense that death is inevitable. On one hand, you are, as a neurosurgeon, you're deeply involved in these just hard to fathom tragedies. Young parents dying, leaving a four year old behind, say. And on the other hand, it takes the sting out of it a bit because you see how just mind numbingly universal death is. There is zero chance that im going to avoid it. I know techno optimists right now, and longevity buffs right now would disagree on that 0.00% estimate, but I dont see any chance that our generation is going to avoid it. Entropy is a powerful force and we are very ornate, delicate, brittle DNA machines that arent up to the cosmic ray bombardment that were subjected to. On the one hand, every human that has ever lived, died or will die. On the other hand, it's just one of the hardest things to imagine inflicting on anyone that you love is having them gone. I'm sure you've had friends that aren't living anymore, and it's hard to even think about them. And so I wish I had arrived at the point of Nirvana where death doesn't have a sting. I'm not worried about it, but I can at least say that I'm comfortable with the certainty of it, if not having found out how to take the tragedy out of it. When I think about my kids either not having me or me not having them, or my wife, maybe I've come.
Speaker A: To accept the intellectual certainty of it, but it may be the pain that comes with losing the people you love. I don't think I've come to understand the existential aspect of it, like that this is going to end. And I don't mean like in some trite way. I mean, like, it certainly feels like it's not going to end. Like you live life like it's not going to end.
Speaker F: Right.
Speaker A: And the fact that this light that's shining, this consciousness, is going to no longer be in one moment, maybe today. It's like, it fills me when I really am able to load all that in with Ernest Becker's terror. Like, it's a real fear. I think people aren't always honest with how terrifying it is. Yeah, I think the more you are able to really think through it, the more terrifying it is. It's not such a simple thing. Oh, that's the way life is. And if you really can load that in, it's hard. But I think that's why the stoics did it, because it, like, helps you get your shit together and be like this. Well, they were like, the moment, every single moment you're alive is just beautiful. And it's terrifying that it's gonna end almost like you're shivering in the cold. A child helpless, this kind of feeling. And then it makes you. When you have warmth, when you have the safety, when you have the love to really appreciate it. I feel like sometimes in your position, when you mentioned armor, just to see death, it might make you not be able to see that, the finiteness of life, because if you kept looking at that, it might break you. So it's good to know that you're kind of still struggling with that. There's the neurosurgeon, and then there's a human, and the human is still able to struggle with that and feel the fear of that and the pain of that.
Speaker F: Yeah, it definitely makes you ask the question of how long, how many of these can you see and not say, I can't do this anymore. But I mean, you said it well. I think it gives you an opportunity to just appreciate that you're alive today. And, you know, I've got three kids and an amazing wife. I'm really happy. Things are good. I get to help on a project that I think matters. I think it moves us forward. I'm a very lucky person.
Speaker A: It's the early steps of a potentially gigantic leap for humanity. It's a really interesting one. And it's cool because you read about all this stuff in history where it's the early days. I've been reading before going to the Amazon, I would read about explorers. They will go and explore even the Amazon jungle for the first time. It's just those are the early steps or early steps into space. Early steps in any discipline in physics and mathematics. It's cool because this is, like, on the grand scale. These are the early steps into delving deep into the human brain. So not just observing the brain, but be able to interact with the human brain. Yeah, it's going to help a lot of people, but it also might help us understand what the hell's going on in there.
Speaker F: Yeah, I think ultimately we want to give people more levers that they can pull. Right? Like, you want to give people options. If you can give someone a dial that they can turn on how happy they are, I think that makes people really uncomfortable. But now talk about major depressive disorder. Talk about people that are committing suicide at an alarming rate in this country and try to justify that queasiness in that light. You can give people a knob to take away suicidal ideation, suicidal intention. I would give them that knob. I don't know how you justify not doing that.
Speaker A: You can think about, like, all the suffering that's going on in the world, like, every single human being that's suffering right now, it'd be a glowing red dot. The more suffering, the more it's glowing, and you just see the map of human suffering. And any technology that allows you to dim that light of suffering on a grand scale is pretty exciting because there's a lot of people suffering, and most of them suffer quietly. And we turn our. We look away too often, and we should remember those that are suffering, because, once again, most of them are suffering quietly.
Speaker F: Well, and on a grander scale, the fabric of society. People have a lot of complaints about how our social fabric is working or not working, how our politics is working or not working. Those things are made of neurochemistry, too. In aggregate, our politics is composed of individuals with human brains, and the way it works or doesn't work is potentially tunable in the sense that, I don't know, say, remove our addictive behaviors or tune our addictive behaviors for social media, our addiction to outrage, our addiction to sharing the most angry political tweet we can find. I don't think that leads to a functional society. And if you had options for people to moderate that maladaptive behavior, there could be huge benefits to society. Maybe we could all work together a little more harmoniously toward useful ends.
Speaker A: There's a sweet spot. Like you mentioned, you don't want to completely remove all the dark side of human nature because those kind of are somehow necessary to make the whole thing work. But there's a sweet spot.
Speaker F: Yeah, I agree. You got to suffer a little. Just not so much that you lose hope.
Speaker A: Yeah. When you. All the surgeries you've done, have you seen consciousness in there? Ever? Was there, like a glowing light?
Speaker F: You know, I have this sense that I never found it, never removed it. Like a dementor in Harry Potter. I have this sense that consciousness is a lot less magical than our instincts want to claim it is. It seems to me like a useful analog for thinking about what consciousness is in the brain, is that we have a really good intuitive understanding of what it means to, say, touch your skin and know what's being touched. I think consciousness is just that level of sensory mapping applied to the thought processes in the brain itself. So what I'm saying is consciousness is the sensation of some part of your brain being active. So you feel it working. You feel the part of your brain that thinks of red things or winged creatures, or the taste of coffee. You feel those parts of your brain being active. The way that I'm feeling my palm being touched. That sensory system that feels the brain working is consciousness.
Speaker A: It's so brilliant. It's the same way. It's the sensation of touch when you're touching a thing. Consciousness is the sensation of you feeling, your brain working, your brain thinking, your.
Speaker F: Brain perceiving, which isn't like a warping of space time or some quantum field effect, right? It's nothing magical. People always want to ascribe to consciousness something truly different. And there's this awesome long history of people looking at whatever the latest discovery in physics is to explain consciousness, because it's the most magical, the most out there thing that you can think of. And people always want to do that with consciousness. I don't think that's necessary. It's just a, you know, a very useful and gratifying way of feeling your brain work.
Speaker A: And as we said, it's one heck of a brain.
Speaker F: Yeah.
Speaker A: Everything we see around us, everything we love, everything that's beautiful, came from brains like these.
Speaker F: It's all electrical activity happening inside your skull.
Speaker A: And I, for 01:00 a.m. grateful that it's people like you that are exploring all the ways that it works and all the ways it can be made better. Thank you so much for talking today.
Speaker F: It's been a joy.
Speaker A: Thanks for listening to this conversation with Matthew McDougall. And now, dear friends, here's bliss Chapman, brain interface software lead at Neuralink. You told me that you've met hundreds of people with spinal cord injuries or with ALS, and that your motivation for helping at Neuralink is grounded in wanting to help them. Can you describe this motivation?
Speaker E: Yeah. First, just a thank you to all the people I've gotten a chance to speak with for sharing their stories with me. I don't think there's any world, really, in which I can share their stories as powerful a way as they can. But just, I think, to summarize at a very high level, what I hear over and over again is that people with ALS, or severe spinal cord injury in a place where they basically can't move physically anymore, really, at the end of the day, are looking for independence. And that can mean different things for different people. For some folks, it can mean the ability just to be able to communicate again independently, without needing to wear something on their face, without needing a caretaker, to be able to put something in their mouth. For some folks, it can mean independence. To be able to work again, to be able to navigate a computer digitally efficiently enough, to be able to get a job, to be able to support themselves, to be able to move out, and ultimately be able to support themselves after their family maybe isn't there anymore to take care of them. And for some folks, it's as simple as just being able to respond to their kid in time before they run away or get interested in something else. And these are deeply personal and sort of very human problems. And what strikes me again and again when talking with these folks is that this is actually an engineering problem. This is a problem that with the right resources, with the right team, we can make a lot of progress on. And at the end of the day, I think that's a deeply inspiring message and something that makes me excited to get up every day.
Speaker A: So it's both an engineering problem in terms of a BCI, for example, that can give them capabilities where they can interact with the world. But also on the other side, it's an engineering problem for the rest of the world to make it more accessible for people living with quadriplegia.
Speaker E: Yeah, I'll take a broad view sort of lens on this for a second. I think I'm very in favor of anyone working in this problem space. So beyond BCI, I'm happy and excited and willing to support any way I can. Folks working on eye tracking systems, working on speech to text systems, working on head trackers or mouse sticks or quad sticks. I've met many engineers and folks in the community that do exactly those things. And I think for the people we're trying to help, it doesn't matter what the complexity of the solution is, as long as the problem is solved and I want to emphasize that there can be many solutions out there that can help with these problems, and BCI is one of a collection of such solutions. So BCI in particular, I think offers several advantages here, and I think the folks that recognize this immediately are usually the people who have spinal cord injury or some form of paralysis. Usually you dont have to explain to them why this might be something that could be helpful. Its usually pretty self evident. But for the rest of us folks that dont live with severe spinal cord injury or who dont know somebody with ALS, its not often obvious why you would want a brain implant to be able to connect and navigate a computer. And its surprisingly nuanced to the degree that ive learned a huge amount just working with Noland in the first Nurlin clinical trial and understanding from him, in his words, why this device is impactful for him and its a nuanced topic. It can be the case that even if he can achieve the same thing, for example, with a mouse stick when navigating a computer, he doesnt have access to that mouse stick every single minute of the day. He only has access when someone is available to put it in front of him. And so a BCI can really offer a level of independence and autonomy that if it wasn't literally physically part of your body, it'd be hard to achieve in any other way.
Speaker A: So there's a lot of fascinating aspects to what it takes to get no one to be able to control a cursor on the screen with his mind. You texted me something that I just love. You said, I was part of the team that interviewed and selected p one. I was in the operating room during the first human surgery, monitoring live signals coming out of the brain. I work with the user basically every day to develop new UX paradigms, decoding strategies, and I was part of the team that figured out how to recover useful BCI to new world record levels when the signal quality degraded. We'll talk about, I think, every aspect of that, but just zooming out, what was it like to be a part of that? Part of that team and part of that. Historic, I would say historic first.
Speaker E: Yeah, I think for me, this is something I've been excited about for close to ten years now. And so to be able to be even just some small part of making it a reality is extremely exciting. A couple, maybe special moments during that whole process that I'll never really truly forget. One of them is during the actual surgery at that point in time. I know Nolan quite well, I know his family and so I think the initial reaction when Nolan is rolled into the operating room is just, oh, shit, kind of reaction. But at that point, muscle memory kicks in and you sort of go into, you let your body just do all the talking. And I have the lucky job in that particular procedure to just be in charge of monitoring the implant. So my job is to sit there to look at the signals coming off the implant, to look at the live brain data streaming off the device as threads are being inserted into the brain, and just to basically observe and make sure that nothing is going wrong or that there's no red flags or fault conditions that we need to go and investigate or pause the surgery to debug. Because I had that sort of spectator view of the surgery, I had a slightly removed perspective than I think most folks in the room. I got to sit there and think to myself, wow, that brain is moving a lot. When you look inside the cranectomy that we stick the threads in, one thing that most people don't realize is the brain moves. The brain moves a lot when you breathe, when your heart beats, and you can see it visibly. So that's something that I think was a surprise to me and very, very exciting to be able to see someone's brain who you physically know and have talked with that length actually pulsing and moving inside their skull.
Speaker A: And they used that brain to talk to you previously, and now it's right there moving.
Speaker E: Yep.
Speaker A: Actually, I didn't realize that in terms of the thread sending. So the neuralink implant is active during surgery, and one thread at a time, you're able to start seeing the signal.
Speaker E: Yeah.
Speaker A: So that's part of the way you test that. The thing is working.
Speaker E: Yeah. So actually, in the operating room, right after we sort of finished all the thread insertions, I started collecting what's called broadband data. So broadband is basically the most raw form of signal you can clock from a neuralink electrode. It's essentially a measurement of the local field potential or the voltage essentially measured by that electrode. We have a certain mode in our application that allows us to visualize where detected spikes are. It visualizes in the broadband signal in this very, very raw form of the data, a neuron is actually spiking. One of these moments that I'll never forget as part of this whole clinical trial is seeing live in the operating room while he's still under anesthesia, beautiful spikes being shown in the application, just streaming live to a device I'm holding in my hand.
Speaker A: So this is no signal processing the raw data, and then the signals processing is on top of it. You're seeing the spikes detected, right?
Speaker E: Yeah.
Speaker A: And that's a ux, too, because that looks beautiful as well.
Speaker E: During that procedure, there was actually a lot of cameramen in the room, so they also were curious and wanted to see. There's several neurosurgeons in the room who are all just excited to see robots taking their job, and they're all crowded around a small little iPhone, watching this live brain data stream out of his brain.
Speaker A: What was that like, seeing the robot do some of the surgery? So the computer vision aspect, where it detects all the spots that avoid the blood vessels, and then, obviously, with human supervision and actually doing the really high precision connection of the threads to the brain.
Speaker E: Yeah, that's a good question. My answer is going to be pretty lame here. But it was boring. Yeah, I've seen it so many times. Yeah. That's exactly how you want surgery to be. You want it to be boring.
Speaker A: Yeah.
Speaker E: Because I've seen it so many times, I've seen the robot do the surgery literally hundreds of times. And so it was just one more time.
Speaker A: Yeah. All the practice surgeries and the proxies, and this is just another day.
Speaker E: Yeah.
Speaker A: So what about when Nolan woke up? Well, do you remember a moment where he was able to move the cursor? Not move the cursor, but get signal from the brain such that it was able to show that there's a connection?
Speaker E: Yeah. Yeah. So we are quite excited to move as quickly as we can. And Nolan was really, really excited to get started. He wanted to get started, actually, the day of surgery, but we waited till the next morning, very patiently. It's a long night. And the next morning in the ICU, where he was recovering, he wanted to get started and actually start to understand what kind of signal we can measure from his brain. And maybe for folks who are not familiar with the neuralink system, we implant the neuralink system, or the neural link implant in the motor cortex. The motor cortex is responsible for representing things like motor intent. If you imagine closing and opening your hand, that kind of signal representation would be present in the motor cortex. If you imagine moving your arm back and forth or wiggling a pinky, this sort of signal can be present in the motor cortex. So one of the ways we start to sort of map out what kind of signal do we actually have access to in any particular individual's brain is through this task called body mapping. And body mapping is where you essentially present a visual to the user and you say, hey, imagine doing this and that visual is a 3d hand opening, closing, or index finger modulating up and down. And you ask the user to imagine that. And obviously, you can't see them do this because they're paralyzed, so you can't see them actually move their arm. But while they do this task, you can record neural activity and you can basically offline model and check, can I predict or can I detect the modulation corresponding with those different actions? And so we did that task, and we realized, hey, there's actually some modulation associated with some of his hand motion, which is the first indication that, okay, we can potentially use that modulation to do useful things in the world. For example, control a computer cursor. And he started playing with it, you know, the first time we showed him it, and we actually just took the same live view of his brain activity and put it in front of him. And we said, hey, you tell us what's going on. Uh, you know, we're not you. You're able to imagine different things, and we know that it's modulating some of these neurons. So you figure out for us what that is actually representing. And so he played with it for a bit. He was like, I don't quite get it yet. He played for a bit longer, and he said, oh, when I move this finger, I see this particular neuron start to fire more. And I said, okay, prove it. Do it again. And so he said, okay. Three, two, one, boom. And the minute he moved, you can see, like, instantaneously this neuron is firing single neuron. I can tell you the exact channel number if you're interested. It's stuck in my brain now forever. But that single, uh, channel firing was a beautiful indication that it was behaviorally modulated neural activity that could then be used for downstream tasks, like decoding a computer cursor.
Speaker A: And when you say single channel, is that associated with a single electrode?
Speaker E: Yeah, channel electrode are interchangeable.
Speaker A: And there's a 1024 of those.
Speaker E: 1024? Yeah.
Speaker A: It's incredible that that works. That really, when I was learning about all this and, like, loading it in, it was just blowing my mind that the intention, you can visualize yourself moving the finger that can turn into a signal, and the fact that you can then skip that step and visualize the cursor moving to or have the intention of the cursor moving, and that leading to a signal that can then be used to move the cursor. There is so many exciting things there to learn about the brain, about the way the brain works, the very fact of their existing signal that can be used is really powerful, but it feels like that's just like the beginning of figuring out how that signal could be used really, really effectively. I should also just. There's so many fascinating details here. But you mentioned the body mapping step, at least in the version I saw that Nolan was showing off. There's like a super nice interface, like a graphical interface. It just felt like I was in the future because I guess it visualizes you moving the hand. And there's a very sexy, polished interface that, hello. I don't know if there's a voice component, but it just felt like. Like when you wake up in a really nice video game and this is a tutorial at the beginning of that video game, this is what you're supposed to do. It's cool.
Speaker E: No, I mean, the future should feel.
Speaker A: Like the future, but it's not easy to pull that off. I mean, it needs to be simple, but not too simple.
Speaker E: Yeah. And I think the UX design component here is underrated for PCI development. In general, there's a whole interaction effect between the ways in which you visualize an instruction to the user and the kinds of signal you can get back. That quality of your behavioral alignment to the neural signal is a function of how good you are at expressing to the user what you want them to do. We spend a lot of time thinking about the UX, of how we build our applications, of how the decoder actually functions, the control surfaces it provides to the user. All these little details matter a lot.
Speaker A: Maybe it'd be nice to get into a little bit more detail of what the signal looks like and what the decoding looks like. N one implant that has, like we mentioned, 1024 electrodes and that's collecting raw data, raw signal. What does that signal look like and what are the different steps along the way before it's transmitted and what is transmitted? All that kind of stuff.
Speaker E: Yeah, yeah, this is gonna be a fun one.
Speaker A: Let's go.
Speaker E: So maybe before diving into what we do, it's worth understanding what we're trying to measure, because that dictates a lot of the requirements for the system that we build. And what we're trying to measure is really individual neurons producing action potentials. Action potential is you can think of it like a little electrical impulse that you can detect if you're close enough. And by being close enough, I mean, like within, let's say, 100 microns of that cell, and 100 microns is a very, very tiny distance. And so the number of neurons that you're going to pick up with any given electrode is just a small radius around that electrode. And the other thing worth understanding about the underlying biology here is that when neurons produce an action potential, the width of that action potential is about one millisecond. So from the start of the spike to the end of the spike, that whole width of that sort of characteristic feature of a neuron firing is one millisecond wide. And if you want to detect that an individual spike is occurring or not, you need to sample that signal or sample the local field potential nearby that neuron. Much more frequently than once a millisecond. You need to sample many, many times per millisecond to be able to detect that this is actually the characteristic waveform of a neuron producing an action potential. And so we sample, across all 1024 electrodes about 20,000 times a second. 20,000 times a second means for every given one millisecond window, we have about 20 samples that tell us what that exact shape of that action potential looks like. And once we've sort of sampled, at super high rate, the underlying electrical field nearby these cells, we can process that signal into just where do we detect a spike or where do we not? Sort of a binary signal, one or zero? Do we detect a spike in this one millisecond or not? And we do that because the actual information carrying sort of subspace of neural activity is just when our spikes occurring. Essentially, everything that we care about for decoding can be captured or represented in the frequency characteristics of spike trains. Meaning how often are spikes firing in any given window of time? And so that allows us to do sort of a crazy amount of compression from this very rich, high density signal to something that's much, much more sparse and compressible, that can be sent out over a wireless radio, like a Bluetooth communication, for example.
Speaker A: Quick tangents here. You mentioned electrode neuron. There's a local neighborhood of neurons nearby. How difficult is it to isolate from where the spike came from?
Speaker E: Yeah, so there's a whole field of sort of academic neuroscience work on exactly this problem of basically given a single electrode or given a set of electrodes measuring a set of neurons, how can you spike sort which spikes are coming from what neuron? And this is a problem that's pursued in academic work because you care about it for understanding what's going on in the underlying sort of neuroscience of the brain. If you care about understanding how the brain's representing information, how that's evolving through time, then that's a very, very important question to understand for sort of the engineering side of things, at least at the current scale, if the number of neurons per electrode is relatively small, you can get away with basically ignoring that problem completely. You can think of it like sort of a random projection of neurons to electrodes, and there may be, in some cases, more than one neuron per electrode. But if that number is small enough, those signals can be thought of as sort of a union of the two. And for many applications, that's a totally reasonable trade off to make and can simplify the problem a lot. And as you sort of scale out channel count, the relevance of distinguishing individual neurons becomes less important because you have more overall signal, and you can start to rely on sort of correlations or covariance structure in the data to help understand when that channel is firing, what does that actually represent? Because you know that when that channel is firing in concert with these other 50 channels, that means move left. But when that same channel is firing with concert with these other ten channels, that means move right.
Speaker A: Okay, so you have to do this kind of spike detection on board, and you have to do that super efficiently, so fast, and not use too much power because you don't want to be generating too much heat. So it has to be a super simple signal processing step.
Speaker E: Yeah.
Speaker A: Is there some wisdom you can share about what it takes to overcome that challenge?
Speaker E: Yeah, so we've tried many different versions of basically turning this raw signal into a feature that you might want to send off the device. And I'll say that I don't think we're at the final step of this process. This is a long journey. We have something that works clearly today, but there can be many approaches that we find in the future that are much better than what we do right now. So some versions of what we do right now, and there's a lot of academic carriages to these ideas. So I don't want to claim that these are original neuralink ideas or anything like that, but one of these ideas is basically to build sort of like a convolutional filter, almost, if you will, that slides across the signal and looks for a certain template to be matched. That template consists of sort of how deep the spike modulates, how much it recovers, and what the duration and window of time is that the whole process takes. And if you can see in the signal that that template is matched within certain bounds, then you can say, okay, that's a spike. One reason that approach is super convenient is that you can actually implement that extremely efficiently in hardware, which means that you can run it in low power across 1024 channels all at once. Another approach that we've recently started exploring, and this can be combined with the spark detection approach, something called spike band power. And the benefits of that approach are that you may be able to pick up some signal from neurons that are maybe too far away to be detected as a spike, because the farther away you are from an electrode, the weaker that actual spike waveform will look like on that electrode. So you might be able to pick up population level activity of things that are maybe slightly outside the normal recording radius, what neuroscientists sometimes refer to as the hash of activity, the other stuff that's going on. And you can look at across many channels how that background noise is behaving, and you might be able to get more juice out of the signal that way, but it comes at a cost. That signal is now a floating point representation, which means it's more expensive to send out over power. It means you have to find different ways to compress it that are different than what you can apply to binary signals. There's a lot of different challenges associated with these different modalities.
Speaker A: Also, in terms of communication, you're limited by the amount of data you can send. Yeah, so. And also because you're currently using the Bluetooth protocol, you have to bash stuff together, but you have to also do this, keeping the latency crazy low. Like, crazy low. Anything to say about the latency?
Speaker E: Yeah, this is a passion project of mine. So I want to build the best mouse in the world.
Speaker A: Yeah.
Speaker E: I don't want to build, like, the, you know, the Chevrolet Spark or whatever, of electric cars. I want to build, like, the Tesla roadster version of. Of a mouse. And I really do think it's quite possible that within five to ten years that most esports competitions are dominated by people with paralysis. This is, like, a very real possibility for a number of reasons. One is that they'll have access to the best technology to play video games effectively. The second is they have the time to do so. So those two factors together are particularly potent for esport competitors, unless people without.
Speaker A: Paralysis are also allowed to implant, which is, it is another way to interact with a digital device. And there's some. There's something to that. If it's a fundamentally different experience, more efficient experience, even if it's not like some kind of full on, high bandwidth communication, if it's just ability to move the mouse ten x faster, like the bits per second, if I can achieve a bit per second at ten x, what I can do with the mouse that's a really interesting possibility of what that can do, especially as you get really good at it. Uh, with training, it's definitely the case.
Speaker E: That you have a higher ceiling performance like you, because you don't have to buffer your intention through your arm, through your muscle. You get just by nature of having a brain implant at all, like 75 millisecond lead time on any action that you're actually trying to take. And there's some nuance to this. Like there. There's evidence that the motor cortex, you can sort of plan out sequences of action. So you might not get that whole benefit all the time, but for sort of like, reaction time style, uh, games where you just want to. Somebody's over here, snipe them, you know, that kind of thing. Uh, you actually do have just an inherent advantage because you don't need to go through muscle. So the question is just how much faster can you make it? And we're already, you know, faster than, uh, you know, what you would do if you're going through muscle. From a latency point of view, and we're in the early stage of that, I think we can push it sort of our end to end latency right now. From brain spike to cursor movement, it's about 22 milliseconds. If you think about, uh, the best mice in the world, the best gaming mice, that's about five milliseconds ish of latency. Depending on how you measure, depending how fast your screen refreshes, there's a lot of characteristics that matter there, but. Yeah, and the rough time for a neuron in the brain to actually impact your command of your hand is about 75 milliseconds. So if you look at those numbers, you can see that we're already competitive and slightly faster than what you'd get by actually moving your hand. And this is something that, if you ask Noland about it, when he moved the cursor for the first time, we asked him about this, it was something I was super curious about. What does it feel like when you're modulating a click intention or when you're trying to move the cursor to the right? He said it moves before he is actually intending it to, which is kind of a surreal thing and something that I would love to experience myself one day. What is that like, to have the thing just be so immediate, so fluid that it feels like it's happening before you're actually intending it to move?
Speaker A: Yeah, I suppose we've gotten used to that latency, that natural latency that happens so is the currently the bottleneck of communication. So, like, the Bluetooth communication, is that. What's the actual bottleneck? I mean, there's always going to be a bottleneck. What's the current bottleneck?
Speaker E: Yeah, a couple of things. So, kind of hilariously, Bluetooth low energy protocol has some restrictions on how fast you can communicate. So the protocol itself establishes a standard of. The most frequent sort of updates you can send are on the order of 7.5 milliseconds. And as we push latency down to the level of individual spikes impacting control, that level of resolution, that kind of protocol is going to become a limiting factor at some scale. Another important nuance to this is that it's not just the neuralink itself that's part of this equation. If you start pushing latency below the level of how fast greens refresh, then you have another problem. You need your whole system to be able to be as reactive as the sort of limits of what the technology can offer. Like, you need the screen, like 120 hz just doesn't work anymore. If you're trying to have something respond at something that's at the level of.
Speaker A: One millisecond, that's a really cool challenge. I also like that for a t shirt. The best mouse in the world. Tell me on the receiving end. So the decoding step. Now we figured out what the spikes are. Got them all together. Now we're sending that over to the app. What's the decoding step look like?
Speaker E: Yeah. So maybe first, what is decoding? I think there's probably a lot of folks listening that just have no clue what. What it means to decode brain activity.
Speaker A: Actually, even if we zoom out beyond that, what is the app? So there's a. There's an implant that's wirelessly communicating with any digital device that has an app installed.
Speaker E: Yep.
Speaker A: So maybe. Can you tell me a high level what the app is, what the software is outside of the brain?
Speaker E: Yeah. So maybe working backwards from the goal. The goal is to help someone with paralysis, in this case Noland, be able to navigate his computer independently. And we think the best way to do that is to offer them the same tools that we have to navigate our software, because we don't want to have to rebuild an entire software ecosystem for the brain, at least not yet. Maybe someday you can imagine there's uxs that are built natively for BCI, but in terms of what's useful for people today, I think most people would prefer to be able to just control mouse and keyboard inputs to all the applications that they want to use for their daily jobs, for communicating with their friends, et cetera. The job of the application is really to translate this wireless stream of brain data coming off the implant into control of the computer. We do that by essentially building a mapping from brain activity to the HID inputs to the actual hardware. HID is just the protocol for communicating input device events. So, for example, move mouse to this position or press this key down. And so that mapping is fundamentally what the app is responsible for. But there's a lot of nuance of how that mapping works that we spend a lot of time to try to get right. And we're still in the early stages of a long journey to figure out how to do that optimally. So one part of that process is decoding. So, decoding is this process of taking the statistical patterns of brain data that's being channeled across this bluetooth connection to the application and turning it into, for example, a mouse movement. That decoding step, you can think of it in a couple different parts, similar to any machine learning problem. There's a training step and there's an inference step. The training step in our case is a very intricate behavioral process where the user has to imagine doing different actions. For example, they'll be presented a screen with a cursor on it, and they'll be asked to push that cursor to the right. Then imagine pushing that cursor to the left. Push it up, push it down. We can basically build up a pattern, or using any modern ML method, a mapping of, given this brain data and that imagined behavior, map one to the other. And then at test time, you take that same pattern matching system. In our case, it's a deep neural network, and you run it and you take the live stream of brain data coming off their implant. You decode it by pattern matching to what you saw at calibration time, and you use that for control of the computer. Now, a couple rabbit holes that I think are quite interesting. One of them has to do with how you build that best template matching system, because there's a variety of behavioral challenges and also debugging challenges when you're working with someone who's paralyzed, because, again, fundamentally, you don't observe what they're trying to do. You can't see them attempt to move their hand. You have to figure out a way to instruct the user to do something and validate that they're doing it correctly, such that then you can downstream build with confidence the mapping between the neural spikes and the intended action. And by doing the action correctly. What I really mean is at this level of resolution of what neurons are doing. So if in ideal world, you could get a signal of behavioral intent that is ground truth accurate at the scale of one millisecond resolution, then with high confidence I could build a mapping from my neural spikes to that behavioral intention. But the challenge is, again, that you don't observe what they're actually doing. There's a lot of nuance to how you build user experiences that give you more than just a course, on average, correct representation of what the user is intending to do. If you want to build the world's best mouse, you really want it to be as responsive as possible. You want it to be able to do exactly what the user is intending at every step along the way, not just on average, be correct when you're trying to move it from left to right. Building a behavioral calibration game or software experience that gives you that level of resolution is what we spend a lot of time working on.
Speaker A: So the calibration process, the interface has to encourage precision, meaning like whatever it does, it should be super intuitive that the next thing the human is going to likely do is exactly that intention that you need and only that intention.
Speaker E: Yeah.
Speaker A: And you don't have any feedback except that may be speaking to you afterwards. What they actually did, you can't. Oh, yeah, right. So that's a, that's fundamentally, that is a really exciting UX challenge, because that's all on the UX. It's not just about being friendly or nice or usable.
Speaker E: Yeah. It's like user experience is how it works.
Speaker A: It's how it works for the calibration. And calibration, at least at this stage of neuralink, is like fundamental to the operation of the thing, and not just calibration, but continued calibration, essentially, yeah.
Speaker E: And maybe you said something that I think is worth exploring there a little bit. You said it's primarily a UX challenge, and I think a large component of it is. But there is also a very interesting machine learning challenge here, which is given some dataset, including some, on average, correct behavior of asking the user to move up or move down, move right, move left, and given a dataset of neural spikes, is there a way to infer in some kind of semi supervised or entirely unsupervised way what that high resolution version of their intention is? If you think about it, there probably is, because there are enough data points in the dataset, enough constraints on your model, that there should be a way, with the right formulation, to let the model figure out itself. For example, at this millisecond this is exactly how hard they're pushing upwards. And at this millisecond, this is how hard they're trying to push upwards.
Speaker A: It's really important to have very clean labels. Yes, the problem becomes much harder from the machine learning perspective. The labels are noisy.
Speaker E: That's correct.
Speaker A: And then to get the clean labels, that's a Ux challenge.
Speaker E: Correct. Although clean labels, I think maybe it's worth exploring what that exactly means. I think any given labeling strategy will have some number of assumptions it makes about what the user is attempting to do. Those assumptions can be formulated in a loss function, or they can be formulated in terms of heuristics that you might use to just try to estimate or guesstimate what the user is trying to do. And what really matters is how accurate are those assumptions. For example, you might say, hey user, push upwards and follow the speed of this cursor. And your heuristic might be that they're trying to do exactly what that cursor is trying to do. Another competing heuristic might be they're actually trying to go slightly faster at the beginning of the movement and slightly slower at the end. And those competing heuristics may or may not be accurate reflections of what the user is trying to do. Another version of the task might be, hey user, imagine moving this cursor a fixed offset. So rather than follow the cursor, just try to move it exactly 200 pixels to the right. So here's the cursor, here's the target. Okay, cursor disappears. Tried to move that now invisible cursor 200 pixels to the right. And the assumption in that case would be that the user can actually modulate correctly that position offset. But that position offset assumption might be a weaker assumption and therefore potentially you can make it more accurate than these heuristics that are trying to guesstimate at each millisecond what the user is trying to do. So you can imagine different tasks that make different assumptions about the, the nature of the user intention. And those assumptions being correct is what I would think of as a clean.
Speaker A: Label for that step. What are we supposed to be visualizing? There's a cursor and you want to move that cursor to the right, the left up and down, or maybe move them by a certain offset. So that's one way, is that the best way to do calibration? So for example, alternative crazy way that probably is playing a role. Here's a game like web grid where you're just getting a very large amount of data. The person playing a game where if they are in a state of flow, maybe you can get clean signal as a side effect.
Speaker B: Yeah.
Speaker A: Is that, or is it, is that not an effective way for initial calibration?
Speaker E: Yeah, great question. There's a lot to unpack there. So the first thing I would draw a distinction between a sort of open loop versus closed loop. So open loop. What I mean by that is the user is going from zero to one. They have no model at all, and they're trying to get to the place where they have some level of control at all. In that setup, you really need to have some task that gives the user a hint of what you want them to do, such that you can build its mapping again from brain data to output. Then once they have a model, you could imagine them using that model and actually adapting to it and figuring out the right way to use it themselves and then retraining that data to give you a boost in performance. There's a lot of challenges associated with both of these techniques, and we can sort of rabbit hole into both of them if you're interested. But the sort of challenge with the open loop task is that the user themself doesn't get proprioceptive feedback about what they're doing. They don't necessarily perceive themselves or feel the mouse under their hand when they're using an open. When they're trying to do an open loop calibration, they're being asked to perform something. Imagine if you had your whole right arm numbed and you stuck it in a box and you couldn't see it, so you had no visual feedback and you had no proprioceptive feedback about what the position or activity of your arm was. And now you're asked, okay, given this thing on the screen that's moving from left to right, match that speed, and you basically can try your best to invoke whatever that imagined action is in your brain. That's moving the cursor from left to right. But in any situation, you're going to be inaccurate and maybe inconsistent in how you do that task. And so that's sort of the fundamental challenge of open loop. The challenge with closed loop is that once the users given a model and they're able to start moving the mouse on their own, they're going to very naturally adapt to that model. That co adaptation between the model learning what they're doing and the user learning how to use the model may not find you the best global minima. It may be that your first model was noisy in some ways, or maybe just had some quirk. There's some part of the data distribution it didn't cover super well. The user now figures out, because they're a brilliant user like Nolan, they figure out the right sequence of imagined motions or the right angle they have to hold their hand at to get it to work, and they'll get it to work great. But then the next day, they come back to their device, and maybe they don't remember exactly all the tricks that they used the previous day. And so there's a complicated feedback cycle here that can. That can emerge and can make it a very, very difficult debugging process.
Speaker A: Okay. There's a lot of really fascinating things there. Yeah. Actually, just to stay on the. On the closed loop, I've seen situations. This actually happened watching psychology grad students. They use pieces of software when they don't know how to program themselves. They use piece of software that somebody else wrote and has a bunch of bugs, and they figure out, and they've been using it for years, they figure out ways to work around, oh, that just happens. Nobody considers, maybe we should fix this. They just adapt. And that's a really interesting notion that we just said we're really good at adapting, but you need to still. That might not be the optimal.
Speaker E: Yeah.
Speaker A: Okay, so how do you solve that problem? Do you have to restart from scratch every once in a while kind of thing?
Speaker E: Yeah, it's a good question. First and foremost, I would say this is not a solved problem. And for anyone who's listening in academia who works on BCIS, I would also say this is not a problem that's solved by simply scaling channel count. So this is maybe that can help and you can get richer covariance structures that you can use to exploit the. When trying to come up with good labeling strategies. But if you're interested in problems that aren't going to be solved inherently by scaling channel count, this is one of them. Yeah. So how do you solve it? It's not a solved problem. That's the first thing I want to make sure it gets across. The second thing is any solution that involves closed loop is going to become a very difficult debugging problem. And one of my general heuristics for choosing what problems to tackle is that you want to choose the one that's going to be the easiest to debug, because if you can do that, even if the ceiling is lower, you're going to be able to move faster because you have a tighter iteration loop debugging the problem in the open loop setting, there's not a feedback cycle to debug with the user in the loop. There's some reason to think that that should be an easier debugging problem. The other thing that's worth understanding is that even in the closed loop setting, there's no special soft or magic of how to infer what the user is truly attempting to do in the closed loop setting. Although they're moving the cursor on the screen, they may be attempting something different than what your model is outputting. So what the model is outputting is not a signal that you can use to retrain. If you want to be able to improve the model further, you still have this very complicated guesstimation or unsupervised problem of figuring out what is the true user intention underlying that signal. The open loop problem has the nice property of being easy to debug, and the second nice property of it has all the same information and content as the close loop scenario. Another thing I want to mention and call out is that this problem doesn't need to be solved in order to give useful control to people. Even today, with the solutions we have now, and that academia has built up over decades, the level of control that can be given to a user today is quite useful. It doesn't need to be solved to get to that level of control. But again, I want to build the world's best mouse. I want to make it so good that it's not even a question that you want it to build the world's best mouse, the superhuman version, you really need to nail that problem. And a couple maybe details of previous studies that we've done internally that I think are very interesting to understand when thinking about how to solve this problem. The first is that even when you have ground truth data of what the user is trying to do, and you can get this with an able bodied monkey, a monkey that has an Erlink device implanted and moving a mouse to control a computer. Even with that ground truth dataset, it turns out that the optimal thing to predict to produce high performance BCI is not just the direct control of the mouse. You can imagine building a dataset of what's going on in the brain and what is the mouse exactly doing on the table. And it turns out that if you build the mapping from neural spikes to predict exactly what the mouse is doing, that model will perform worse than a model that is trained to predict higher level assumptions about what the user might be trying to do. For example, assuming that the monkey is trying to go in a straight line to the target. It turns out that making those assumptions is actually more effective in producing a model than actually predicting the underlying hand movement.
Speaker A: So the. The intention, not like the physical movement or whatever. Yeah, there's obviously a very strong correlation between the two, but the intention is a more powerful thing to be chasing.
Speaker E: Right.
Speaker A: Well, that. That's also super interesting. I mean, the intention itself is fascinating because, yes, with the BCI here, in this case, with digital telepathy, you're acting on the intention, not the action, which is why there's an experience of, like, feeling like it's happening before you meant for it to happen. That is so cool. And that is why you could achieve, like, superhuman performance, probably in terms of the control of the mouse. So the. For open loop, just to clarify. So whenever the person is tasked, like, move the mouse to the right, you said there's not feedback, so they don't get to get that satisfaction of actually getting it to move.
Speaker E: You could imagine giving the user feedback on the screen, but it's difficult because at this point, you don't know what they're attempting to do. What can you show them that would basically give them a signal of, I'm doing this correctly or not correctly? Let's take this very specific example. Maybe your calibration task looks like you're trying to move the cursor a certain position offset. Your instructions to the user are, hey, I. The cursor is here. Now, when the cursor disappears, imagine moving it 200 pixels from where it was to the right to be over this target. In that scenario, you could imagine coming up with some sort of consistency metric that you could display to the user of, okay, I know what the spike train looks like. On average, when you do this action to the right, maybe I can produce some probabilistic estimate of how likely is that to be the action you took, given the latest trial or trajectory that you imagined. And that could give the user some sort of feedback of how consistent are they across different trials. You could also imagine that if the user is prompted with that kind of consistency metric, that maybe they just become more behaviorally engaged to begin with, because the task is boring when you don't have any feedback at all, there may be benefits to the user experience of showing something on the screen, even if it's not accurate, just because it keeps the user motivated to try to increase that number or push it upwards.
Speaker A: So there's a psychology element here.
Speaker E: Yeah, absolutely.
Speaker A: And again, all of that is ux challenge. How much signal drift is there hour to hour, day to day, week to week, month to month. How often do you have to recalibrate because of the signal drift?
Speaker E: Yeah, so this is a problem we've worked on both with NHP non human primates before our clinical trial, and then also with Noland during the clinical trial. Maybe the first thing that's worth stating is what the goal is here. The goal is really to enable the user to have a plug and play experience where I guess they don't have to plug anything in, but a play experience where they can use the device whenever they want to, however they want to. That's really what we're aiming for. There can be a set of solutions that get to that state without considering this non stationary problem. Maybe the first solution here that's important is that they can recalibrate whenever they want. This is something that Nolan has the ability to do today, so he can recalibrate the system at 02:00 a.m. in the middle of the night without his caretaker or parents or friends around to help push a button for him. The other important part of the solution is that when you have a good model calibrated, that you can continue using that without needing to recalibrate it. How often he has to do this recalibration today depends really on his appetite for performance. We observe a degradation through time of how well any individual model works. But this can be mitigated behaviorally by the user adapting their control strategy. It can also be mitigated through a combination of software features that we provide to the user. For example, we let the user adjust exactly how fast the cursor is moving. We call that the gain, for example, the gain of how fast the cursor reacts to any given input intention. They can also adjust the smoothing, how smooth the output of that cursor intention actually is. They can also adjust the friction, which is how easy is it to stop and hold still. And all these software tools allow the user a great deal of flexibility and troubleshooting mechanisms to be able to solve this problem for themselves.
Speaker A: By the way, all this is done by looking to the right side of the screen, selecting the mixer. And the mixer you have, it's like DJ mode.
Speaker E: DJ mode for your VCR.
Speaker A: I mean, it's a really well done interface. It's really, really well done. And so, yeah, there's that bias that there's a cursor drift that Nolan talked about in a stream, although he said that you guys were just playing around with it with him, and they're constantly improving. So that could have been just a snapshot of that particular moment, particular day. But he said that there was this cursor drift and his bias that could be removed by him, I guess, looking to the right side of the screen, the left side of the screen, to kind of adjust the bias. Yeah, that's one interface action, I guess, to adjust the bias.
Speaker E: Yeah. So this is actually an idea that comes out of academia. There is some prior work with Braingate clinical trial participants where they pioneered this idea of bias correction. The way we've done it, I think, is it's very prioritized, very beautiful user experience, where the user can essentially flash the cursor over to the side of the screen, and it opens up a window where they can actually adjust or tune exactly the bias of the cursor. So bias, maybe for people who aren't familiar, is just sort of what is the default motion of the cursor if you're imagining nothing, and it turns out that that's one of the first sort of qualia of the cursor control experience that's impacted by neural non stationarity.
Speaker A: Qualia of the cursor experience.
Speaker E: I don't know how else to describe it. Like, you know, I'm not the guy moving.
Speaker A: Very poetic. I love it. The quality of the cursor experience. Yeah. I mean, it sounds poetic, but it is deeply true. There is an experience when it works well. It is a joyful, a really pleasant experience, and when it doesn't work well, it's a very frustrating experience. That's actually the art of ux. It's like you have the possibility to frustrate people or the possibility to give them joy.
Speaker E: And at the end of the day, it really is truly the case that UX is how the thing works. And so it's not just like what's showing on the screen. It's also what control surfaces does a decoder provide the user. We want them to feel like they're in the f one car, not some minivan. And that really, truly is how we think about it. Nolan himself is an f one fan, so we refer to ourselves as a pit crew. He really is truly the f one driver. And there's different control surfaces that different kinds of cars and airplanes provide the user. And we take a lot of inspiration from that when designing how the cursor should behave. And maybe one nuance of this is even details like when you move a mouse on a MacBook trackpad, the sort of response curve of how that input that you give the trackpad translates to cursor movement is different than how it works with a mouse. When you move on the trackpad, there's a different response function, a different curve to how much a movement translates to input to the computer than when you do it physically with a mouse. And that's because somebody sat down a long time ago when they designed the initial input systems to any computer and they thought through exactly how it feels to use these different systems. Now we're designing the next generation of this input system to a computer, which is entirely done via the brain, and there's no proprioceptive feedback. Again, you don't feel the mouse in your hand, you don't feel the keys under your fingertips. You want a control surface. That still makes it easy and intuitive for the user to understand the state of the system and how to achieve what they want to achieve. Ultimately, the end goal is that that ux completely fades into background. It becomes something that's so natural and intuitive that it's subconscious to the user. And they just should feel like they have basically direct control over the cursor, just does what they want it to do. They're not thinking about the implementation of how to make it do what they want it to do, it's just doing what they want it to do.
Speaker A: Is there some kind of things along the lines of like Fitt's law where you should move the mouse in a certain kind of way that maximizes your chance to hit the target? I don't even know what I'm asking, but I'm hoping the intention of my question will land on a profound answer. No. Is there some kind of understanding of the laws of ux when it comes to the context of somebody using their brain to control it? That's different than actual with a mouse?
Speaker E: I think we're in the early stages of discovering those laws, so I wouldn't claim to have solved that problem yet. But there's definitely some things we've learned that make it easier for the user to get stuff done. And it's pretty straightforward when you verbalize it, but takes a while to actually get to that point when you're in the process of debugging the stuff in the trenches. One of those things is that any machine learning system you build has some number of errors, and it matters how those errors translate to the downstream user experience. For example, if you're developing search algorithm in your photos, if you search for your friend Joe and it pulls up a photo of your friend Josephine, maybe that's not a big deal because the cost of an error is not that high. In a different scenario where you're trying to detect insurance fraud or something like this, and you're directly sending someone to court because of some machine learning model output, then the errors make a lot more sense to be careful about. You want to be very thoughtful about how those errors translate to downstream effects. The same is true in BCI. So, for example, if you're building a model that's decoding a velocity output from the brain versus an output where you're trying to modulate the left click, for example, these have sort of different trade offs of how precise you need to be before it becomes useful to the end user. For velocity, it's okay to be on average correct, because the output of the model is integrated through time. So if the user is trying to click at position a and they're currently at position b, they're trying to navigate over time to get between those two points. And as long as the output of the model is on average correct, they can steer it through time. With the user control loop in the mix, they can get to the point they want to get to. The same is not true of a click. For a click, you're performing it almost instantly at the scale of neurons firing. You want to be very sure that that click is correct. Because a false click can be very destructive to the user. They might accidentally close the tab that they're trying to do something and lose all their progress. They might accidentally hit some send button on some text that there's only half composed and reads funny after. So there's different cost functions associated with errors in this space. And part of the UX design is understanding how to build a solution that is, when it's wrong, still useful to the end user.
Speaker A: That's so fascinating that assigning cost to every action when an error occurs to. So every action, if an error occurs, has a certain cost, and incorporating that into how you interpret the intention, mapping it to the action is really important. I didn't quite until you said it realized there's a cost to like sending the text early. It's like a very expensive cost.
Speaker E: It's super annoying if you accidentally, like if you're a cursor, imagine if your cursor misclicked every once in a while. That's super obnoxious. And the worst part of it is usually when the user is trying to click, they're also holding still because they're over the target they want to hit, and they're getting ready to click, which means that in the datasets that we build on average is the case that low speeds or desire to hold still is correlated with when the user is attempting to click.
Speaker A: Wow, that is really fascinating.
Speaker E: It's also not the case. People think that a click is a binary signal. This must be super easy to decode. Well, yes it is, but the bar is so much higher for it to become a useful thing for the user. And there's ways to solve this. I mean, you can sort of take the comp out approach of, well, let's just give the like, let's take 5 seconds to click, let's take a huge window of time so we can be very confident about the answer. But again, world's best mouse, the world's best mouse doesn't take a second to click or 500 milliseconds to click. It takes five milliseconds to click or less. And so if you're aiming for that kind of high bar, then you really want to solve the underlying problem.
Speaker A: So maybe this is a good place to ask about how to measure performance. Performance? This whole bits per second. Can you explain what you mean by that? Maybe a good place to start is to talk about webgrid as a game, as a good illustration of the measurement of performance.
Speaker E: Yeah, maybe I'll take one zoom out step there, which is just explaining why we care to measure this at all. Again, our goal is to provide the user the ability to control the computer as well as I can, and hopefully better. That means that they can do it at the same speed as what I can do. It means that they have access to all the same functionality that I have, including all those little details like command tab, command space, all this stuff. They need to be able to do it with their brain and with the same level of reliability as what I can do with my muscles. That's a high bar. And so we intend to measure and quantify every aspect of that to understand how we're progressing towards that goal. There's many ways to measure BPS by. This isn't the only way, but we present the user a grid of targets, and basically we compute a score which is dependent on how fast and accurately they can select, and then how small are the targets. And the more targets that are on the screen, the smaller they are, the more information you present per click. And so if you think about it from information theory point of view, you can communicate across different information theoretic channels. And one such channel is a typing interface. You could imagine that's built out of a grid, just like a software keyboard on the screen and bits per second is a measure that's computed by taking the log of the number of targets on the screen. You can subtract one if you care to model a keyboard because you have to subtract one for the delete key on the keyboard. But log of the number of targets on the screen times the number of correct selections minus incorrect, divided by some time window, for example 60 seconds. And that's sort of the standard way to measure a cursor control task in academia. And all credit in the world goes to this great professor, Doctor Chenoy of Stanford, who came up with that task. And he's also one of my inspirations for being in the field. So all the credit in the world to him for coming up with a standardized metric to facilitate this kind of bragging rights that we have now to say that Nolan is the best in the world at this, at this task with its PCI, it's very important for progress that you have standardized metrics that people can compare across different techniques and approaches. How well does this do? So, yeah, big kudos to him and to all the team at Stanford. Yeah. So for Noland and for me playing this task, there's also different modes that you can configure this task. So the webgrid task can be presented as just sort of a left click on the screen, or you could have targets that you just dwell over, or you could have targets that you left right click on. You could have targets that are left, right click, middle click, scrolling, clicking and dragging. You can do all sorts of things within this general framework, but the simplest purist form is just blue targets show up on the screen. Blue means left click. That's the simplest form of the game. And the prior records here in academic work and at Nuralink internally with nhps have all been matched or beaten by Noland with his Narlink device. Prior to Nuralinken, the world record for a human using the device is somewhere between 4.2 to 4.6 bps, depending on exactly what paper you read and how you interpret it. Nolan's current record is 8.5 bps. And again this median neuralinker performance is ten bps. You can think of it roughly as he's 85% the level of control of a median neural linker using their cursor to select blue targets on the screen. I think theres a very interesting journey ahead to get us to that same level of ten bps performance. Its not the case that the tricks that got us from four to six bps and then six to eight bps are going to be the ones that get us from eight to ten. In my view, the core challenge here is really the labeling problem. Its how do you understand at a very, very fine resolution what the user is attempting to do? I highly encourage folks in academia to work on this problem.
Speaker A: Whats the journey with Nolan on that quest of increasing the bps on web grid? In March, you said that he selected 89,285 targets in Webgrid.
Speaker E: Yep.
Speaker A: So he loves this game. He's really serious about improving his performance in this game. So what is that journey of trying to figure out how to improve that performance? How much can that be done on the decoding side? How much can that be done on the calibration side? How much can that be done on the Nolan side of like figuring out how to convey his intention more cleanly?
Speaker E: Yeah, no, this is a great question. So in my view, one of the primary reasons why Nolan's performance is so good is because of noland. Noland is extremely focused and very energetic. He'll play web grid sometimes for like 4 hours in the middle of the night, like from 02:00 a.m. to 06:00 a.m. he'll be playing web grid just because he wants to push it to the limits, what he can do. And this is not us asking him to do that. I want to be clear. We're not saying, hey, you should play Webgood tonight. We just gave him the game as part of our research and he is able to play independently and practice whenever he wants. And he really pushes hard to push it. The technology is the absolute limit and he views that as his job really to make us be the bottleneck. And boy has he done that. Well, the first thing to acknowledge is that he was extremely motivated to make this work. Ive also had the privilege to meet other clinical trial participants from brain gate and other trials, and they very much share the same attitude of like, they viewed this as their lifes work to advance the technology as much as they can. And if that means selecting targets on the screen for 4 hours from 02:00 a.m. to 06:00 a.m. then so be it. And theres something extremely admirable about that thats worth calling out. Okay, so now how do you sort of get from where he started, which is no cursor control to Athenae? So, I mean, when he started, there's a huge amount of learning to do on his side and our side to figure out what's the most intuitive control for him, and the most intuitive control for him is you have to find the set intersection of what do we have the signal to decode? So we don't pick up every single neuron in the motor cortex, which means we don't have representation for every part of the body. So there may be some signals that we have better decode performance on than others. For example, on his left hand, we have a lot of difficulty distinguishing his left ring finger from his left middle finger. But on his right hand, we have good control and good modulation detected from the neurons we were able to record for his pinky and his thumb and his index finger. So you can imagine how these different subspaces of modulated activity intersect with what's the most intuitive for him. And this has evolved over time. So once we gave him the ability to calibrate models on his own, he was able to go and explore various different ways to imagine controlling the cursor. For example, he could imagine controlling the cursor. Bye. Wiggling his wrist side to side or by moving his entire arm by. I think at one point he did his feet. You know, he tried like a whole bunch of stuff to explore the space of what is the most natural way for him to control the cursor that at the same time is easy for us to decode. Rolling.
Speaker A: Just to clarify, it's through the body mapping procedure there, you're able to figure out which finger he can move.
Speaker E: Uh, yes. Yeah, that's one way to do it. Um, maybe one nuance of the, when he's doing it, he can imagine many more things than we represent in that visual on the screen. So we show him sort of abstractly. Heres a cursor. You figure out what works the best for you. And we obviously have hints about what will work best from that body mapping procedure of, you know, we know that this particular action we can represent well, but its really up to him to go and explore and figure out what works the best.
Speaker A: But at which point does he no longer visualize the movement of his body and is just visualizing the movement of the cursor?
Speaker E: Yeah.
Speaker A: How quickly does he go from, how quickly does he get there?
Speaker E: So this happened on a Tuesday. I remember this day very clearly because at some point during the, during the day, it looked like he wasn't doing super well. Like it looked like the model wasn't performing super well and he was like getting distracted. But he actually, it wasn't the case. Like, what actually happened was he was trying something new where he was just controlling the cursor. So he wasn't imagining moving his hand anymore. He was just imagining, I don't know what it is, some abstract intention to move the cursor on the screen. And I cannot tell you what the difference between those two things are. I really, truly cannot. He's tried to explain it to me before. I cannot give a first person account of what that's like, but the expletives that he uttered in that moment were enough to suggest that it was a very qualitatively different experience for him to just have direct neural control over a cursor.
Speaker A: I wonder if there's a way through ux to encourage a human being to discover that, because he discovered it, like you said to me, that he's a pioneer. So he discovered that on his own through all of this, the process of trying to move the cursor with different kinds of intentions. But that is clearly a really powerful thing to arrive at, which is to let go of trying to control the fingers and the hand and control the actual digital device with your mind.
Speaker E: That's right. Ux is how it works. And the ideal UX is one that the user doesn't have to think about what they need to do in order to get it done. They just. It just does it.
Speaker A: That is so fascinating. But I wonder, on the biological side, how long it takes for the brain to adapt. So is it just simply learning, like, high level software, or is there, like, a neuroplasticity component where, like, the. The brain is adjusting slowly?
Speaker E: Yeah, the truth is, I don't know. Um, I'm very excited to see with sort of the second participant that we implant what the, you know, what the journey is like for them, because we'll have learned a lot more. Potentially, we can help them understand and explore that direction more quickly. This is something I didn't know. This wasn't me prompting Nolan to go try this. He was just exploring how to use his device and figured it out himself. But now that we know that that's a possibility, that maybe there's a way to, for example, hint the user, don't try super hard during calibration. Just do something that feels natural or just directly control the cursor. Don't imagine explicit action. And from there, we should be able to hopefully understand how this is. For somebody who has not experienced that before. Maybe that's the default mode of operation for them. You don't have to go through this intermediate phase of explicit motions, or maybe.
Speaker A: If that naturally happens for people, you can just occasionally encourage them to allow themselves to move the cursor. Right. Actually, sometimes just like with a four minute mile, just the knowledge that that's.
Speaker E: Possible pushes you to do it.
Speaker A: Yeah. Enables you to do it. And then it becomes trivial. And then it also makes you wonder, this is the cool thing about humans. Once there's a lot more human participants, they will discover things that are possible.
Speaker E: Yes. And share their experiences and that because.
Speaker A: Of them sharing it, they'll be able to do it. All of a sudden that's unlocked for everybody because just the knowledge sometimes is the thing that enables it to do it.
Speaker E: Yeah, just comment on that too. We've probably tried a thousand different ways to do various aspects of decoding and now we know what the right subspace is to continue exploring further again, thanks to Nolan and the many hours he's put into this. And so even just that help constrain the beam search of different approaches that we could explore. Really helps accelerate for the next person, you know, the set of things that we'll get to try on day one, how fast we hope to get them to useful control, how fast we can enable them to use it independently and to get value out of the system. So yeah, massive hats off to Nolan and all the participants that came before him, uh, to make this technology a reality.
Speaker A: So how often are the updates to the decoder? Because Nolan mentioned like, okay, there's a new update that we're working on and that in the stream he said he plays the snake game because it's like super hard. It's a good way for him to test like how good the update is. So, and he says like sometimes the update is a step backwards. It's like it's a constant, like iteration. So how often, like what is the update entail? Is it mostly on the decoder side?
Speaker E: Yeah, a couple of comments. So one is it's probably worth trying distinction between sort of research sessions where we're actively trying different things to understand like what the best approach is versus sort of independent use where we wanted to have ability to just go use a device how anybody would want to use their MacBook. And so what hes referring to is, I think usually in the context of a research session where were trying many, many different approaches to even unsupervised approaches like we talked about earlier, to try to come up with better ways to estimate his true intention and more accurately decode it. And in those scenarios, I mean, we try in any given session, hell sometimes work for like 8 hours a day. And so that can be hundreds of different models. That we would try in that day. Like a lot of different things now, it's also worth noting that we update the application he uses quite frequently. I think sometimes up to four or five times a day, we'll update his application with different features or bug fixes or feedback that he's given us. So he's a very articulate person who is part of the solution. He's not a complaining person. He says, hey, here's this thing that I've discovered is not optimal in my flow. Here's some ideas how to fix it. Let me know what your thoughts are. Let's figure out how to solve it. It often happens that those things are addressed within a couple hours of him giving us his feedback. Thats the kind of iteration cycle well have. And so sometimes at the beginning of the session he will give us feedback, and at the end of the session hes giving us feedback on the next iteration of that process or that setup.
Speaker A: Thats fascinating because one of the things you mentioned, that there was 271 pages of notes taken from the BCI sessions, and this was just in March. So one of the amazing things about human beings that they can provide, especially ones who are smart and excited and all positive and good vibes like Nolan, that they can provide feedback, continuous feedback.
Speaker E: Yeah. It also requires just to brag on the team a little bit. I work with a lot of exceptional people and it requires the team being absolutely laser focused on the user and what will be the best for them. And it requires a level of commitment of, okay, this is what the user feedback was. I have all these meetings, we're going to skip that today and we're going to do this. That level of focus commitment is, I would say, underappreciated in the world. And also, you obviously have to have the talent to be able to execute on these things effectively. And. Yeah, we have that in loads.
Speaker A: Yeah. And this is such an interesting space of UX design because there's so many unknowns here. And I can tell UX is difficult because of how many people do it poorly. It's just not a trivial thing.
Speaker E: Yeah. It's also, UX is not something that you can always solve by just constant iterating on different things. Sometimes you really need to step back and think globally. Am I even the right minima to be chasing down for a solution? There's a lot of problems in which fast iteration cycle is the predictor of how successful you will be. As a good example, like in an RL simulation, for example, the more frequently you get reward, the faster you can progress. It's just an easier learning problem the more frequently you get feedback. But UX is not that way. Users are actually quite often wrong about what the right solution is and it requires a deep understanding of the technical system and what's possible, combined with what the problem is you're trying to solve not just how the user expressed it, but what the true underlying problem is to actually get to the right place.
Speaker A: Yeah, that's the old like stories of Steve Jobs like rolling in there like, yeah, the user is a good, is a useful signal, but it's not a perfect signal. And sometimes you have to remove the floppy disk drive or whatever the. I forgot all the crazy stories of Steve Jobs like making wild design decisions, but there some of his aesthetic that some of it is about the love you put into the design, which is very much a Steve Jobs Johnny Ive type thing. But when you have a human being using their brain to interact with it, it also is deeply about function. Its not just aesthetic and that you have to empathize with a human being before you while not always listening to them directly. You have to deeply empathize. It's fascinating. It's really, really fascinating. And at the same time iterate. Right. But not iterate in small ways. Sometimes a complete, like rebuilding the design. He said that Nolan said in the early days the UX sucked, but you improved quickly. What was that journey like?
Speaker E: Yeah, I mean I'll give one concrete example. So he really wanted to be able to read manga. This is something that he, I mean, yeah, it sounds like a simple thing, but it's actually a really big deal for him. And he couldn't do it with this mouse stick. It just, it wasn't accessible. You can't scroll with the mouse stick on his iPad and on the website that he wanted to be able to use to read the newest manga.
Speaker A: And so might be a good quick pause to say the mouth stick is a thing he's using holding a stick in his mouth to scroll on a tablet.
Speaker E: Right? Yeah, it's basically, you can imagine it's a stylus that you hold between your teeth. Yeah, it's basically a very long stylus.
Speaker A: And its exhausting. It hurts and its inefficient.
Speaker E: Yeah. And maybe its also worth calling out. There are other alternative assistive technologies but that particular situation Nolan is in, and this is not uncommon and I think its also not well understood by folks, is that hes relatively spastic. So hell have muscle spasms from time to time. And so any assistive technology that requires him to be positioned directly in front of a camera, for example, an eye tracker, or anything that requires him to put something in his mouth just as a no go, because he'll either be shifted out of frame when he has a spasm, or if he has something in his mouth, it'll stab him in the. In the face, you know, if he spasms too hard. So these kind of considerations are important when thinking about what advantages a PCI has in someone's life. If it. If it fits ergonomically into your life in a way that you can use it independently when your caretaker's not there, wherever you want to, either in the bed or in the chair, depending on, you know, your comfort level and your desire to have pressure sores. You know, all these factors matter a lot in how good the solution is in that user's life. So one of these very fun examples is scroll. So again, manga is something he wanted to be able to read. And there's many ways to do scroll with the BCI. You can imagine different gestures, for example, the user could do that would move the page. But scroll is a very fascinating control surface because it's a huge thing on the screen in front of you. Any sort of jitter in the model output, any sort of error in the model output causes an earthquake on the screen. You really don't want to have your manga page that you're trying to read be shifted up and down a few pixels just because your scroll decoder is not completely accurate. This was an example where we had to figure out how to formulate the problem in a way that the errors of the system, whenever they do occur, and we'll do our best to minimize them. But whenever those errors do occur, that it doesn't interrupt the qualia of the experience that the user is having, it doesn't interrupt their flow of reading their book. What we ended up building is this really brilliant feature. This is a teammate named Roos who worked on this really brilliant work called quick scroll. Quick scroll basically looks at the screen and it identifies where on the screen are scroll bars. It does this by deeply integrating with macOS to understand where are the scroll bars actively present on the screen. Using the accessibility tree that's available to macOS apps, we identified where those scroll bars are and provided a BCI scrollbar. The BCI scroll bar looks similar to a normal scroll bar, but it behaves very differently in that once you move over to it, your cursor morphs onto it. It sort of attaches or latches onto it then once you push up or down in the same way that you'd use a push to control the normal cursor, it actually moves the screen for you. It's basically remapping the velocity to a scroll action. The reason that feels so natural and intuitive is that when you move over to attach to it, it feels like magnetic. So you're stuck onto it, and then it's one continuous action. You don't have to switch your imagined movement. You sort of snap onto it, and then you're good to go. You just immediately can start pulling the page down or pushing it up. And even once you get that right, there's so many little nuances of how this scroll behavior works to make it natural and intuitive. So one example is momentum. Like when you scroll a page with your fingers on the screen, you actually have some flow. It doesn't just stop right when you lift your finger up. The same is true with BCI scroll. So we had to spend some time to figure out what are the right nuances when you don't feel the screen under your fingertip anymore, what is the right dynamic? Or what's the right amount of page give, if you will, when you push it to make it flow. The right amount for the user to have a natural experience reading their book. And there's a million, I mean, I could tell you there's so many little minutiae of how exactly that scroll works that we spent probably, like a month getting right to make that feel extremely natural and easy for the user to navigate.
Speaker A: Even the scroll on a smartphone with your finger feels extremely natural and pleasant, and it probably takes an extremely long time to get that right. And actually, the same kind of visionary UX design that we're talking about don't always listen to the users, but also listen to them and also have, like, visionary big, like, throw everything out, think from first principles, but also not. Yeah, yeah. By the way, it just makes me think that scroll bars on the desktop probably have stagnated and never taken that, like, because the snap, same as, like, snap to grid snap, the scroll bar action you're talking about, it's something that could potentially be extremely useful in the desktop setting.
Speaker E: Yeah.
Speaker A: Even just for users to just improve the experience. Because the current scroll bar experience and the desktop is horrible.
Speaker E: Yeah.
Speaker A: It's hard to find, hard to control. There's not a momentum. There's. And the intention should be clear. When I start moving towards the scroll bar, there should be a snapping to the scroll bar action. But of course, maybe I'm okay paying that cost. But there's hundreds of millions of people paying that cost non stop. But anyway, but in this case this is necessary because there's an extra cost paid by Nolan for the jitteriness. So you have to switch between the scrolling and the reading. There has to be a phase shift between the two. When you're scrolling, you're scrolling, right?
Speaker E: Right. So that is one drawback of the current approach. Maybe one other just sort of case study here. So again, ux is how it works. And we think about that holistically from even the future detection level of what we detect in the brain to how we design the decoder, what we choose to decode to, then how it works once it's being used by the user. So another good example in that sort of how it works once they're actually using the decoder, the output that's displayed on the screen is not just what the decoder says, it's also a function of what's going on on the screen. We can understand, for example, that when you're trying to close a tab, that very small stupid little x that's extremely tiny, which is hard to get precisely hit if you're dealing with a noisy output of the decoder, we can understand that that is a small little x you might be trying to hit and actually make it a bigger target for you. Similar to how when you're typing on your phone if you're used to the iOS keyboard, for example, it actually adapts the target size of individual keys based on an underlying language model. So it'll actually understand if I'm typing. Hey, I'm going to see l. It'll make the e key bigger because it knows Lex is the person I'm going to go see. That kind of predictiveness can make the experience much more smooth, even without improvements to the underlying decoder or feature detection part of the stack. So we do that with a feature called magnetic targets. We actually index the screen and we understand, okay, these are the places that are very small targets that might be difficult to hit. Here's the cursor dynamics around that location that might be indicative of the user trying to select it. Let's make it easier. Let's blow up the size of it in a way that makes it easier for the user to snap onto that target. All these little details, they matter a lot in helping the user be independent in their day to day living.
Speaker A: How much of the work on the decoder is generalizable to p two, p three, p four, p five, pn. How do you improve the decoder in a way that's generalizable.
Speaker E: Yeah, great question. So the underlying signal we're trying to decode is going to look very different in p two than in p one. For example, channel number 345 is going to mean something different in user one than it will in user two, just because that electrode that corresponds with channel 345 is going to be next to a different neuron in user one versus user two. But the approaches, the methods, the user experience of how do you get the right behavioral pattern from the user to associate with that neural signal? We hoped it will translate over, over multiple generations of users. And beyond that, it's very, very possible, in fact, quite likely, that we've overfit to Nolan's user experience, desires and preferences. And so what I hope to see is that when we get a second 3rd, 4th participant, that we find what the right wide minimas are that cover all the cases that make it more intuitive for everyone. And hopefully there's a cross pollination of things where, oh, we didn't think about that with this user because they can speak, but with this user who just can fundamentally not speak at all, this user experience is not optimal. And those improvements that we make there should hopefully translate that into even people who can speak but don't feel comfortable doing so because they're in a public setting like their doctor's office.
Speaker A: So the actual mechanism of open loop labeling and then closed loop labeling will be the same and hopefully can generalize across the different users as they're doing the calibration step. And the calibration step is pretty cool. I mean, that in itself, the interesting thing about web grid, which is like closed loop, it's like fun. I love it when there's like, they used to be kind of idea of human computation, which is using actions a human would want to do anyway to get a lot of signal from.
Speaker E: Yeah.
Speaker A: And like, what great is that? Like a nice video game that also serves as great calibration.
Speaker E: It's so funny. This is, I've heard this reaction so many times before, sort of the, you know, first user was implanted, we had an internal perception that the first user would not find this fun.
Speaker A: Yeah.
Speaker E: And so we thought really quite a bit, actually, about like, should we build other games that like, are, you know, more interesting for the user so we can get this kind of data and help facilitate research that's, you know, for long durations and stuff like this. Turns out that, like, people love this game. Yeah, I always loved it, but I didn't know that that was a shared perception.
Speaker A: Yeah. Just in case it's not clear, Webgrid is. There's a grid of, let's say, 35 by 35 cells, and one of them lights up blue, and you have to move your mouse over that and click on it. And if you miss it and it's red.
Speaker E: And I played this game for so many hours. So many hours.
Speaker A: And what's your record?
Speaker E: You said my. I think I have the highest at Neuralink right now. My record. 17 bps.
Speaker A: 17 bps.
Speaker E: If you imagine that 35 by 35 grid, you're hitting about 100 trials per minute. So 100 correct selections in that 1 minute window. So you're averaging about, you know, between 500, 600 milliseconds per selection.
Speaker A: So one of the reasons I think I struggle with that game is I'm such a keyboard person. So everything is done with your keyboard. If I can avoid touching the mouse, it's great. So how can you explain your high performance?
Speaker E: I have, like, a whole ritual I go through when I play web grid. So it's actually like a diet plan associated with this. Like, it's a whole thing. So the first thing, you have to.
Speaker A: Fast for five days. I have to go up to the mountain, actually.
Speaker E: It kinda. I mean, the fasting thing is important. So this is like, you know, focuses the mind. Yeah, yeah, it's true. So what I do is I actually, I don't eat for a little bit beforehand, and then I'll actually eat like, a ton of peanut butter right before I cook. And I get, this is a real thing. This is a real thing, yeah. And then it has to be really late at night. This is, again, a night owl thing I think we share, but it has to be like, you know, midnight, 02:00 a.m. kind of time window. And I have a very specific, like, physical position I'll sit in, which is, uh, I used to be, I was homeschooled growing up, and so I did most of my work, like, on the floor, uh, just like in my bedroom or whatever. And so I have a very specific situation on the floor. On the floor that I sit and play. And then you have to make sure, like, there's not a lot of weight on your elbow when you're playing so that you can move quickly. And then I turn the gain of the cursor, so the speed of the cursor way, way up. So it's like small motions that actually move the cursor.
Speaker A: Are you moving with your wrist or you. You're never.
Speaker E: I'm moving, so I my wrist is almost completely still. I'm just moving my fingers.
Speaker A: Yeah, you know those. Just in a small tangent.
Speaker E: Yeah.
Speaker A: The, which I've been meaning to go down this rabbit hole of people that set the world record in Tetris. Those folks, they're playing. There's a way to. Did you see this?
Speaker E: All the fingers are moving.
Speaker A: Yeah. You could find a way to do it where it's using a loophole, like a bug that you can do some incredibly fast stuff. So it's along that line, but not quite. But you do realize there'll be a few programmers right now listening to this. Cool fast and eat peanut butter.
Speaker E: Please check my record. The reason I did this, literally was just because I wanted the bar to be high for the team. The number that we aim for should not be the median performance. It should be like, it should be able to beat all of us. At least that should be the minimum bar.
Speaker A: What do you think is possible? Like 20?
Speaker E: Yeah, I don't know what the limits. I mean, the limits you can calculate just in terms of screen refresh rate and cursor immediately jumping to the next target. But there's, I mean, I'm sure there's limits before that with just sort of reaction time and visual perception and things like this. I'd guess it's a. In the below 40, but above 20, somewhere in there. It's probably the. Right there. I never be thinking about it. Also matters how difficult the task is. You could imagine some people might be able to do, like, 10,000 targets on the screen, and maybe they can do better that way. So there's some, like, task optimizations you could do to try to boost your performance as well.
Speaker A: What do you think it takes for Nolan to be able to do above 85 to keep increasing that number? You said, like, every increase in the number might require different. Yeah, different improvements in the system.
Speaker E: Yeah. I think the nature of this work is, the first answer that's important to say is, I don't know. This is, you know, edge of the research. So again, nobody's gotten to that number before. So what's next is going to be, you know, heuristic. A guess from my part. What we've seen historically is that different parts of the stack become bottlenecks at different time points. So, you know, when I first joined Erlink, like, three years ago or so, one of the major problems was just the latency of the Bluetooth connection. It wasn't just like, the radio on the device wasn't super good. It was an early revision of the implant. And it just like no matter how good your decoder was, if your thing is updating every 30 milliseconds or 50 milliseconds, it's just going to be choppy. And no matter how good you are, that's going to be frustrating and lead to challenges. So at that point, it was very clear that the main challenge is just get the data off the device in a very reliable way such that you can enable the next challenge to be tackled. Then at some point it was actually the modeling challenge of how do you just build a good mapping? Like the supervised learning problem of you have a bunch of data and you have a label, you're trying to predict just what is the right neural decoder architecture and hyperparameters to optimize that. That was a problem for a bit. Once you solve that, it became a different bottleneck. I think the next bottleneck after that was actually just software stability and reliability. If you have widely varying inference latency in your system, or your app just lags out every once in a while, it decreases your ability to maintain and get in a state of flow, and it basically just disrupts your control experience. There's a variety of different software bugs and improvements we made that basically increased the performance of the system, made it much more reliable, much more stable, and led to a state where we could reliably collect data to build better models with. That was a bottleneck for a while. It's just the software stack itself. If I were to guess right now, there's two major directions you could think about for improving vps further. The first major direction is labeling. Labeling is again this fundamental challenge of, given a window of time where the user is expressing some behavioral intent, what are they really trying to do at the granularity of every millisecond? That again, is a task design problem. It's a UX problem, it's a machine learning problem. It's a software problem, sort of touches all those different domains. The second thing you can think about to improve BPS further is either completely changing the thing you're decoding, or just extending the number of things that you're decoding. This is serving the direction of functionality, basically, you can imagine giving more clicks. For example, left click, a right click, a middle click, different actions like click and drag, for example. And that can improve the effective bitrate of your communication prosthesis. If you're trying to allow the user to express themselves through any given communication channel, you can measure that with bits per second. But what actually matters at the end of the day is how effective are they at navigating their computer. And so from the perspective of the downstream tasks that you care about functionality, and extending functionality is something we're very interested in, because not only can it improve the sort of number of vps, but it can also improve the downstream sort of independence that the user has and the skill and efficiency with which they can operate their computer.
Speaker A: Would the number of threads increasing also potentially help?
Speaker E: Yes. Short answer is yes. It's a bit nuanced how that curve, or how that manifests in the numbers. What you'll see is that if you plot a curve of number of channels that you're using for decode, versus either the offline metric of how good you are at decoding, or the online metric of in practice, how good is the user using this device, you see roughly a log curve. So as you move further out in number of channels, you get a corresponding logarithmic improvement in control quality and offline validation metrics. The important nuance here is that each channel corresponds with a specific represented intention in the brain. So for example, if you have a channel 254, it might correspond with moving to the right. Channel 256 might mean move to the left. If you want to expand the number of functions you want to control, you really want to have a broader set of channels that covers a broader set of imagined movements. You can think of it kind of like Mister potato man, actually. If you had a bunch of different imagined movements you could do, how would you map those imagined movements to input to a computer? You could imagine handwriting to output characters on the screen. You could imagine just typing with your fingers and have that output text on the screen. You could imagine different finger modulations for different clicks. You can imagine wiggling your big nose for opening some menu, or wiggling your big toe to have command tab occur, or something like this. It's really the amount of different actions you can take in the world depends on how many channels you have and the information content that they carry, right?
Speaker A: So that's more about the number of actions. So actually, as you increase the number of threads, that's more about increasing the number of actions you're able to perform.
Speaker E: One other nuance there that is worth mentioning. So again, our goal is really to enable a user with paralysis to control the computer as fast as I can. So that's bps with all the same functionality I have, what we just talked about, but then also as reliably as I can. And that last point is very related to channel count discussion. So as you scale out number of channels, the relative importance of any particular feature of your model input to the output control of the user diminishes, which means that if the sort of neural non stationary effect is per channel, or if the noise is independent, such that more channels means, on average, less output effect, then your reliability of your system will improve. So one sort of core thesis that at least I have is that scaling channel count should improve the reliability system without any work on the decoder itself.
Speaker A: Can you linger on reliability here? So, first of all, when you see non stationarity of the signal, which aspect are you referring to?
Speaker E: Yeah, so maybe let's talk briefly what the actual underlying signal looks like. So again, I spoke very briefly at the beginning about how when you imagine moving to the right or imagine moving to the left, neurons might fire more or less, and their frequency content of that signal, at least in the motor cortex, is very correlated with the output intention, the behavioral task that the user is doing. You can imagine, actually, this is not obvious, that rate coding, which is the name of that phenomenon, is the only way the brain could represent information. You can imagine many different ways in which the brain could encode intention. There's actually evidence in bats, for example, that there's temporal codes, timing codes, of exactly when particular neurons fire is the mechanism of information representation. But at least in the motor cortex, there's substantial evidence that it's rate coding, or at least one first order effect is that it's rate coding. So then, if the brain is representing information by changing the frequency of a neuron firing, what really matters is the delta between the baseline state of the neuron and what it looks like when it's modulated. What we've observed and what has also been observed in academic work is that that baseline rate sort of the, if you're to tar the scale, if you imagine, uh, that analogy for, like, measuring, you know, flour or something when you're baking, that baseline state of how much the pot weighs is actually different day to day. And so if what you're trying to measure is how much rice is in the pot, you're going to get a different measurement, different days because you're measuring with different pots. So that baseline rate shifting is really the thing that, uh, at least from a first order description of the problem is what's causing this downstream bias. There can be other effects, nonlinear effects on top of that, but at least a very first order description of the problem, that's what we observe day to day, is that the baseline firing rate of any particular neuron or observed on a particular channel is changing.
Speaker A: So can you just adjust to the baseline to make it relative to the baseline nonstop?
Speaker E: Yeah, this is a great question. So, um, with monkeys, we have found various ways to do this. Um, one example would do this is you ask them to do some behavioral task, like play the game with a joystick. You measure what's going on in the brain. You compute some mean of what's going on across all the input features, and you subtract that in the input when you're doing your BCI session. Works super well for whatever reason, that doesn't work super well with Nolan. I actually don't know the full reason why, but I can imagine several explanations. One such explanation could be that the context effect difference between some open loop task and some closed loop task is much more significant with Nolan than it is with monkey. Maybe in this open loop task, he's watching the Lex Freeman podcast while he's doing the task, or hes whistling and listening to music and talking with his friend and ask his mom whats for dinner while hes doing this task. And so the exact difference in context between those two states may be much larger and thus lead to a bigger generalization gap between the features that youre normalizing at open loop time and what youre trying to use at closed loop time.
Speaker A: Thats interesting. Just on that point, its kind of incredible to watch Nolan be able to multitask, to do multiple tasks at the same time, to be able to move the mouse cursor effectively while talking and while being nervous because he's talking in front of me.
Speaker E: Kicking my ass in chest, too.
Speaker A: Yeah, kicking your ass. And now we're and talk trash while doing it. So all at the same time. And yes, if you're trying to normalize to the baseline, that might throw everything off. Boy, is that interesting.
Speaker E: Maybe one comment on that, too. For folks that aren't familiar with assistive technology, I think there's a common belief that, you know, well, why can't you just use an eye tracker or something like this for helping somebody move a mouse on the screen? And it's really a fair question, and one that I actually did was not confident before Noland that this was going to be a profoundly transformative technology for people like him. And I'm very confident now that it will be. But the reasons are subtle. It really has to do with ergonomically how it fits into their life, even if you can just offer the same level of control as what they would have with an eye tracker or with a mouse stick. But you don't need to have that thing in your face. You don't need to be positioned a certain way. You don't need your caretaker to be around to set it up for you. You can activate it when you want, how you want, wherever you want. That level of independence is so game changing for people. It means that they can text a friend at night, privately without their mom needing to be in the loop. It means that they can, like, open up, you know, and browse the Internet at 02:00 a.m. when nobody's around to set their iPad up for them. This is like a profoundly game changing thing for folks in that situation. And this is even before we start talking about folks at may not be able to communicate at all or ask for help when they want to. This can be potentially the only link that they have to the outside world. And yeah, that one doesn't, I think, need explanation of why that's so impactful.
Speaker A: You mentioned neural decoder. How much machine learning is in the decoder, how much magic, how much science, how much art, how difficult is it to come up with a decoder that figures out what these sequence of spikes mean?
Speaker E: Yeah, good question. There's a couple of different ways to answer this, so maybe I'll zoom out briefly first, and then I'll go down one of the rabbit holes. So the zoomed out view is that building the decoder is really the process of building the dataset plus compiling it into the weights and each of those steps is important. The direction, I think, of further improvement is primarily going to be in the data set side of how do you construct the optimal labels for the model. But there's an entirely separate challenge of then how do you compile it in the best model. And so I'll go briefly down the second one, down the second rabbit hole. One of the main challenges with designing the optimal model for BCI is that offline metrics don't necessarily correspond to online metrics. It's fundamentally a control problem. The user is trying to control something on the screen, and the exact user experience of how you output the intention impacts their ability to control. For example, if you just look at validation loss as predicted by your model, there can be multiple ways to achieve the same validation loss. Not all of them are equally controllable by the end user. It might be as simple as saying, oh, you could just add auxiliary loss terms that help you capture the thing that actually matters. But this is a very complex, nuanced process. How you turn the labels into the model is more of a nuanced process than just a standard supervised learning problem. One very fascinating anecdote here. We've tried many different neural network architectures that translate brain data to velocity outputs. For example, one example that stuck in my brain from a couple of years ago now is at one point we were using just fully connected networks to decode the brain activity. We tried a b test where we were measuring the relative performance in online control sessions of 1D Convolution over the input signal. If you imagine per channel, you have a sliding window that's producing some convolved feature for each of those input sequences for every single channel simultaneously, you can actually get better validation metrics, meaning you're fitting the data better and it's generalizing better on offline data. If you use this convolutional architecture, you're reducing parameters. It's a standard procedure when you're dealing with time series data. Now, it turns out that when using that model online, the controllability was worse. It was far worse, even though the offline metrics were better. There can be many ways to interpret that. But what that taught me at least, was that, hey, it's at least the case right now that if you were to just throw a bunch of compute at this problem and you were trying to hyperparameter optimize or let some GPT model hard code or come up with or invent many different solutions, if you were just optimizing for loss, it would not be sufficient, which means that there's still some inherent modeling gap here. There's still some artistry left to be uncovered here of how to get your model to scale with more compute. And that may be fundamentally labeling problem, but there may be other components to this as well.
Speaker A: Is it data constraint at this time, which is what it sounds like. How do you get a lot of good labels?
Speaker E: Yeah, I think it's data quality constrained, not necessarily data quantity constrained, but even.
Speaker A: Just the quantity because it has to be trained on the interactions. I guess theres not that many interactions.
Speaker E: Yeah. So it depends what version of this youre talking about. So if youre talking about, lets say the simplest example of just 2d velocity, then I think, yeah, data quality is the main thing. If youre talking about how to build a multifunction output that lets you do all the inputs to the computer that you and I can do, then its actually a much more sophisticated nuanced modeling challenge, because now you need to think about not just when the users left clicking, but when youre building the left click model. You also need to be thinking about how to make sure it doesnt fire when theyre trying to right click or when they're trying to move the mouse. So one example of an interesting bug from week one of BCI with Nolan was when he moved the mouse, the click signal dropped off a cliff, and when he stopped, the click signal went up. So again, there's a contamination between the two inputs. Another good example was at one point he was trying to do a left click and drag, and the minute he started moving, the left click signal dropped off a cliff again. Because there's some contamination between the two signals. You need to come up with some way to, either in the dataset or in the model, build robustness against this kind of, uh, you think of it like overfitting, but really it's just that the model has not seen this kind of variability before. So you need to find some way to help the model with that.
Speaker A: This is super cool, because I, it feels like all of this is very solvable, but it's hard.
Speaker E: Yes, it is fundamentally an engineering challenge. This is important to emphasize, and it's also important to emphasize that it may not need fundamentally new techniques, which means that, you know, people who work on, let's say, unsupervised speech classification, using CTC loss, for example, with internal siri, they could potentially have very applicable skills to this.
Speaker A: So what things are you excited about in the future development of the software stack on Neuralink? So everything we've been talking about, the decoding, the UX, I think there's some.
Speaker E: I'm excited about, like something I'm excited about from the technology side, and some I'm excited about understanding how this technology is going to be best situated for entering the world. So I'll work backwards on the technology entering the world side of things. I'm really excited to understand how this device works for folks that cannot speak at all, that have no ability to bootstrap themselves into useful control by voice command, for example, and are extremely limited in their current capabilities, I think that will be an incredibly useful signal for us to understand, really what is an existential threat for all startups, which is product market fit. Does this device have the capacity and potential to transform people's lives in the current state? And if not, what are the gaps? And if there are gaps, how do we solve them most efficiently? So that's what I'm very excited about for the next year or so of clinical trial operations. The technology side, I'm quite excited about basically everything we're doing. I think it's going to be awesome. The most prominent one, I would say, is scaling channel count. Right now we have a thousand channel device. The next version will have between three and 6000 channels. And I expect that curve to continue in the future. And it's unclear what set of problems will just disappear completely at that scale and what set of problems will remain and require further focus. And so I'm excited about the clarity of gradient that that gives us in terms of the user experience that we choose to focus our time and resources on. And also in terms of the even things as simple as non stationary. Like does that problem just completely go away at that scale? Or do we need to come up with new creative uxs still even at that point? And also when you get to that time point, when we start expanding out dramatically the set of functions that you can output from one brain, how to deal with all the nuances of both the user experience of not being able to feel the different keys under your fingertips, but still needing to be able to modulate all of them in synchrony to achieve the thing you want. Again, you don't have that properly set to feedback loop, so how can you make that intuitive? For a user to control a high dimensional control surface without feeling the thing physically, I think that's going to be a super interesting problem. I'm also quite excited to understand, uh, you know, do these scaling laws continue like as you scale channel count, how much further out do you go before that saturation point is, is truly hit? And it's not obvious today? I think we only know what's in the sort of interpolation space. We only know what's between zero and 1024, but we don't know what's beyond that. Um, and then there's a whole sort of like range of interesting sort of neuroscience and brain questions, which is when you stick more stuff in the brain, in more places, you get to learn much more quickly about what those brain regions represented. And so I'm excited about that fundamental neuroscience learning, which is also important for figuring out how to most efficiently insert electrodes in the future. So yeah, I think all those dimensions I'm really, really excited about that doesn't get close to touching the sort of software stack that we work on every single day and what we're working on right now.
Speaker A: Yeah, it seems virtually impossible to me that 1000 electrodes is where it saturates. It feels like this would be one of those silly notions in the future where obviously you should have millions of electrodes. And this is where the true breakthroughs happen. You tweeted, some thoughts are most precisely described in poetry. Why do you think that is?
Speaker E: I think it's because the information bottleneck of language is pretty steep, and yet youre able to reconstruct on the other persons brain more effectively without being literal. If you can express the sentiment such that in their brain, they can reconstruct the actual true underlying meaning and beauty of the thing that youre trying to get across. The generator function in their brain is more powerful than what language can express. And so the mechanism of poetry is really just to feed or seed that generator function.
Speaker A: So being literal sometimes is a suboptimal compression for the thing you're trying to convey.
Speaker E: And it's actually in the process of the user going through that generation that they understand what you mean. That's the beautiful part. It's also, when you look at a beautiful painting, it's not the pixels of the painting that are beautiful. It's the thought process that occurs when you see the experience of that that actually is the thing that matters.
Speaker A: Yeah. It's resonating with some deep thing within you that the artist also experienced and was able to convey that through the pixels. And that's actually going to be relevant for full on telepathy. You know, it's like if you just read the poetry literally, that doesn't say much of anything interesting. It requires a human to interpret it. So it's the combination of the human mind and all the experiences that human being has within the context of the collective intelligence of the human species that makes that poem make sense, and they load that in. And so in that same way, the signal that carries from human to human meaning might not, may seem trivial, but may actually carry a lot of power because of the complexity of the human mind and the receiving end. Yeah, that's interesting. Poetry still doesn't. Who was it? I think Yosha Bacho first was said something about all the people that think we've achieved AGI. Explain why humans like music.
Speaker E: Oh, yeah.
Speaker A: And until the GI likes music, you haven't achieved AGI or something like that.
Speaker E: Do you not think that's like some next token entropy, surprise kind of thing going on there?
Speaker A: I don't know.
Speaker E: I don't know either. I listen to a lot of classical music and also read a lot of poetry. And, yeah, I do wonder if there is some element of the next token surprise factor going on there. Yeah, maybe because, I mean, like, a lot of the tricks in both poetry and music are basically, you have some repeated structure, and then you do like a twisted. Like, it's like, okay, clause one, two, three is one thing, and then clause four is like, okay, now we're onto the next theme. And they kind of play with exactly when the surprise happens and the expectations of the user. And that's even true, like, through history, as musicians evolve music, they take some known structure that people are familiar with, and they just tweak it a little bit. Like, they tweak and add a surprising element. This is especially true in, like, in classical music heritage. But that's what I'm wondering. Is it all just entropy? Like the.
Speaker A: So breaking structure or breaking symmetry is something that humans seem to like, maybe as simple as that.
Speaker E: Yeah. And, I mean, great artists copy, and they also, you know, knowing which rules to break is the important part. And fundamentally, it must be about the listener of the piece. Like, which rule is the right one to break? It's about the user or the audience member perceiving that as interesting.
Speaker A: What do you think is the meaning of human existence?
Speaker E: There's a tv show I really like called the West Wing. And in the West Wing, there's a character, he's the president of the United States, who's having a discussion about the Bible with one of their colleagues. And the colleague says something about the Bible says X, y, and Z. And the president says, yeah, but it also says ABC. And the person says, well, do you believe the Bible to be literally true? And the president says, yes, but I also think that neither of us are smart enough to understand it. I think the analogy here for the meaning of life is that largely, we don't know the right question to ask. And so I think I'm very aligned with the hitchhiker's guide, the galaxy version of this question, which is basically, if we can ask the right questions, it's much more likely we find the meaning of human existence. And so in the short term, as a heuristic in the sort of search policy space, we should try to increase the diversity of people asking such questions, or generally of consciousness and conscious beings asking such questions. So, again, I think I'll take the I don't know card here, but say I do think there are meaningful things we can do that improve the likelihood of answering that question.
Speaker A: It's interesting how much value you assigned to the task of asking the right questions. That's the main thing, is not the answers, is the questions.
Speaker E: This point, by the way, is driven home in a very painful way when you try to communicate with someone who cannot speak. Because a lot of the time, the last thing to go is they have the ability to somehow wiggle a lip or move something that allows them to say yes or no. And in that situation, it's very obvious that what matters is, are you asking them the right question to be able to say yes or no to.
Speaker A: Wow, that's powerful. Well, bliss, thank you for everything you do, and thank you for being you. And thank you for talking today.
Speaker E: Thank you.
Speaker A: Thanks for listening to this conversation with Bliss Chapman. And now, dear friends, here's Nolan Arbaugh, the first human being to have a neuralink device implanted in his brain. You had a diving accident in 2016 that left you paralyzed with no feeling from the shoulders down. How did that accident change your life?
Speaker C: There's sort of a freak thing that happened. Imagine you're running into the ocean, although this is a lake, but you're running into the ocean, and you get to about waist high, and then you kind of, like, dive in, take the rest of the plunge under the wave or something. That's what I did. And then I just never came back up. But not sure what happened. I did it running into the water with a couple of guys. And so my idea of what happened is really just that I took, like, a stray fist, elbow, knee, foot, something to the side of my head. The left side of my head was sore for about a month afterwards, so must have taken a pretty big knock. And then they both came up, and I didn't. And so I was face down in the water for a while. I was conscious, and then eventually just, you know, realized I couldn't hold my breath any longer. And I keep saying, took a big drink. People, I don't know if they like that I say that it seems like I'm making light of it all, but this is kind of how I am. And I don't know, like, I'm a very relaxed, sort of stress free person. I rolled with the punches for a lot of this. I kind of took it in stride. It's like, all right, well, what can I do next? How can I improve my life even a little bit on a day to day basis? At first, just trying to find some way to heal as much of my body as possible, to try to get healed, to try to get off a ventilator, learn as much as I could so I could somehow survive once I left the hospital. And then, thank God, I had my family around me. If I didn't have my parents, my siblings, then I would have never made it this far. They've done so much for me. Um, more than, like, I can ever thank them for, honestly. And a lot of people don't have that. A lot of people in my situation, their families either aren't capable of providing for them or honestly just don't want to. And so they get placed somewhere and, you know, in some sort of home. Uh, so thankfully, I had my family. I have a great group of friends, a great group of buddies from college who have all rallied around me, and we're all still incredibly close. People always say, you know, if you're lucky, you'll end up with one or two friends from high school that you keep throughout your life. I have about 1010 or twelve from high school that have all stuck around. And we still get together, all of us, twice a year. We call it the spring series and the fall series. This last one we all did, we dressed up like X Men. So I did Professor Xavier, and it was freaking awesome. It was so good. So, yeah, I have such a great support system around me. And so, you know, being a quadriplegic isn't that bad. I get weighted on all the time. People bring me food and drinks and I get to sit around and watch as much tv and movies and anime as I want. I get to read as much as I want. I mean, it's, it's great.
Speaker A: It's beautiful to see that. You see, the silver lining in all of this was just going back. Do you remember the moment when you first realized you're paralyzed from the neck down?
Speaker C: Yep. I was face down in the water right when I. Whatever. Something hit my head. I tried to get up and I realized I couldn't move. And it just sort of clicked. I'm like, all right, I'm paralyzed, can't move. What do I do? If I can't get up? I can't flip over, can't do anything. Then I'm going to drown eventually. And I knew I couldn't hold my breath forever, so I just held my breath and thought about it for maybe 1015 seconds. I've heard from other people that like onlookers, I guess the two girls that pulled me out of the water were two of my best friends. They are lifeguards. And one of them said that it looked like my body was sort of shaking in the water, like I was trying to flip over and stuff. But I knew. I knew immediately. And I just kind of, I realized that thats what my situation was from here on out. Maybe if I got to the hospital, theyd be able to do something. When I was in the hospital right before surgery, I was trying to calm one of my friends down. I had brought her with me from college to camp and she was just bawling over me and I was like, hey, its going to be fine, dont worry. I was cracking some jokes to try to lighten the mood. The nurse had called my mom and I was like, dont tell my mom. She's just going to be stressed out. Call her after I'm out of surgery because at least she'll have some answers then, like whether I live or not. Really. And I didn't want her to be stressed through the whole thing, but I knew. And then when I first woke up after surgery, I was super drugged up. They had me on fentanyl, like three ways, which was awesome. I don't recommend it, but I saw, I saw some crazy stuff on that fentanyl and it was still the best I've ever felt. On drugs. Medication. Sorry, on medication. And I remember the first time I saw my mom in the hospital. I was just bawling. I had like ventilator in like, I couldn't talk or anything and I just started crying because it was more like seeing her. Not that, I mean, the whole situation obviously was pretty rough, but it was just like seeing her face for the first time was pretty hard. But, yeah, I never had like a moment of, you know, man, I'm paralyzed. This sucks. I don't want to be around anymore. It was always just, I hate that I have to do this, but, like, sitting here and wallowing isn't going to help.
Speaker A: So immediate acceptance?
Speaker C: Yeah, yeah.
Speaker A: Has there been low points along the way?
Speaker C: Yeah, yeah, sure. I mean, there are days when I don't really feel like doing anything. Not so much anymore. Like, not for the last couple years. I don't really feel that way. I've more so just wanted to try to do anything possible to make my life better at this point. But at the beginning, there were some ups and downs. There were some really hard things to adjust to. First off, just like the first couple months, the amount of pain I was in was really, really hard. I mean, I remember screaming at the top of my lungs in the hospital because I thought my legs were on fire and obviously I cant feel anything, but its all nerve pain. And so that was a really hard night. I asked them to give me as much pain meds as possible. Theyre like, youve had as much as you can have, so just kind of deal with it, go to a happy place sort of thing. So that was a pretty low point. And then every now and again, its hard, like realizing things that I wanted to do in my life that I wont be able to do anymore. I always wanted to be a husband and father and I just don't think that I could do it now. As a quadriplegic, maybe it's possible, but I'm not sure I would ever put someone I love through that, like having to take care of me and stuff, not being able to go out and play sports. I was a huge athlete growing up, so that was pretty hard. Little things too. When I realize I can't do them anymore, like, there's something really special about being able to hold a book and smell a book. Like the feel, the texture, the smell. Like, as you turn the pages, like, I just love it, I can't do it anymore. It's little things like that. The two year mark was pretty rough. Two years is when they say you will get back basically as much as you're ever going to get back as far as movement and sensation goes. And so for the first two years, that was the only thing on my mind was like, try as much as I can to move my fingers, my hands, my feet, everything possible to try to get sensation and movement back. And then when the two year mark hit, so June 30, 2018, I was really sad. That's kind of where I was, and then just randomly here and there. But I was never depressed for long periods of time. It never seemed worthwhile to me.
Speaker A: What gave you strength?
Speaker C: My faith. My faith in God was a big one. My understanding that it was all for purpose. And even if that purpose wasn't anything involving neuralink, even if that purpose was, you know, there's, there's a story in the Bible about job, and I think it's a really, really popular story about how job, you know, has all of these terrible things happen to him, and he praises God throughout the whole situation. I thought, and I think a lot of people think for most of their lives that they are job, that they're the ones going through something terrible and they just need to, you know, praise God through the whole thing and everything will work out. At some point after my accident, I realized that I might not be job, that I might be, you know, one of his children that gets killed or kidnapped or taken from him. And so it's about terrible things that happen to those around you who you love. So maybe, you know, in this case, my mom would be job, and she has to get through something extraordinarily hard. And I just need to try and make it as best as possible for her because shes the one thats really going through this massive trial. And that gave me a lot of strength. And obviously my family, my family and my friends, they give me all the strength that I need on a day to day basis. Makes things a lot easier. Having that great support system around me.
Speaker A: From everything I've seen of you online, your streams, and the way you are today, I really admire, let's say, your unwavering, positive outlook on life. Has that always been this way?
Speaker C: Yeah. Yeah, I've, I mean, I've just always thought I could do anything I ever wanted to do. There was never anything too big. Whatever I set my mind to, I felt like I could do it. I didn't want to do a lot. I wanted to travel around and be sort of like a gypsy and go work odd jobs. I had this dream of traveling around Europe and being, I don't know, a shepherd in Wales or Ireland and then going and being a fisherman in Italy, doing all these things for like a year. Like, it's such cliche things, but I just thought it would be so much fun to go and travel and do different things. And so I've always just seen the best in people around me, too. And I've always tried to be good to people. And growing up with my mom, too, she's like the most positive, energetic person in the world. And we're all just people people. Like, I just get along great with people. I really enjoy meeting new people, and so I just wanted to do everything. This is kind of just how I've been.
Speaker A: It's just great to see that cynicism didn't take over, given everything you've been through.
Speaker D: Yeah, that's.
Speaker A: Was that, like, a deliberate choice you made, that you're not going to let this keep you down?
Speaker C: Yeah, a bit. Also, like, I just, just kind of how I am. I just, like I said, I roll with the punches with everything. I always used to tell people, like, I don't stress about things much. And whenever I'd see people getting stressed, I just say, you know, like, it's not hard, just don't stress about it. And, like, that's all you need to do. And they're like, that's not how that works. Like, it works for me. Like, just don't stress and everything will be fine. Like, everything will work out. Obviously, not everything always goes well, and it's not like it all works out for the best all the time. But I just don't think stress has had any place in my life since I was a kid.
Speaker A: What was the experience like of you being selected to be the first human being to have a neural link device implanted in your brain? Are you scared? Excited? No.
Speaker C: No. It was cool. Like, I was. I was never afraid of it. I had to think through a lot. Should I. Should I do this? Like, be the first person? I could wait until number two or three and get a better version of the neuralink. Like, the first one might not work. Maybe. It's actually going to kind of suck. Um. It's going to be the worst version ever in a person. So why would I do the first one? Like, I've already kind of been selected. I could just tell them, you know, like, okay, find someone else, and then I'll do number two or three. Like, I'm sure they would let me. They're looking for a few people anyways. But ultimately I was like, I don't know. There's something about being the first one to do something. It's pretty cool. I always thought that if I had the chance, that I would like to do something for the first time. This seemed like a pretty good opportunity, and I was never scared. I think my faith had a huge part in that. I always felt like God was preparing me for something. I almost wish it wasn't this, because I had many conversations with God about not wanting to do any of this as a quadriplegic. I told him, you know, I'll go out and talk to people. I'll go out and travel the world and talk to stadiums. Thousands of people. Give my testimony, I'll do all of it, but heal me first. Don't make me do all this in a chair. That sucks. And I guess he won that argument. I didn't really have much of a choice. I always felt like there was something going on, and to see how, I guess, easily, I made it through the interview process and how quickly everything happened, how the star sort of aligned with all of this. It just told me, as the surgery was getting closer, it just told me that it was all meant to happen, it was all meant to be, and so I shouldn't be afraid of anything that's to come. And so I wasn't. I kept telling myself, like, you know, you say that now, but as soon as the surgery comes, you're probably going to be freaking out like you're about to have brain surgery. And brain surgery is a big deal for a lot of people, but it's an even bigger deal for me. Like, it's all I have left the amount of times I've been like, thank you, God, that you didn't take my brain and my personality and my ability to think, my, like, love of learning, like, my character, everything. Like, thank you so much. Like, as long as you left me, that then I think I can get by. And I was about to let people go, like, root around, and they were like, hey, we're going to go put some stuff in your brain. Like, hopefully it works out. And so it was something that gave me pause, but like I said, how smoothly everything went. I never expected for a second that anything would go wrong. Plus, the more people I met on the borrows side and on the knurling side, they're just the most impressive people in the world. Like, I can't speak enough to how much I trust these people with my life and how impressed I am with all of them. And to see the excitement on their faces, to, like, walk into a room and roll into a room and see all of these people looking at me, like, we're just. We're so excited. Like, we've been working so hard on this, and it's finally happening. It's super infectious, and it just makes me want to do it even more and to help them achieve their dreams, like, I don't know. It's so rewarding, and I'm so happy for all of them, honestly.
Speaker A: What was the day of surgery like? When did you wake up? What'd you feel? Minute by minute?
Speaker C: Yeah.
Speaker A: Were you freaking out?
Speaker C: No, no. I thought I was going to, but as surgery approached the night before, the morning of, I was just excited. I was like, let's make this happen. I think I said that something like that to elon on the phone beforehand. We were, like, facetiming. And I was like, let's rock and roll. And he's like, let's do it. I don't know. I wasn't scared. So we woke up. I think we had to be at the hospital at 05:30 a.m. i think surgery was at 07:00 a.m. so we woke up pretty early. I'm not sure much of us slept that night. Got to the hospital, 530, went through, like, all the pre op stuff. Everyone was super nice. Elon was supposed to be there in the morning, but something went wrong with his plane, so we ended up facetiming. That was cool. Had one of the greatest one liners of my life after that phone call, hung up with him. There were, like, 20 people around me, and I was like, I just hope I. He wasn't too starstruck. Talking to me.
Speaker A: Nice.
Speaker C: Yeah, it was good.
Speaker A: Well done.
Speaker C: Yeah. Yeah.
Speaker A: Did you write that ahead of time?
Speaker C: No, it just came to me. I was like, this is. This seems right, you know, went into surgery. I asked if I could pray right beforehand, so I, like, prayed over the room. I asked God, if you, like, be with my mom in case anything happened to me and just, like, calm her nerves out there. Woke up, played a bit of a prank on my mom. I don't know if you've heard about it.
Speaker A: Yeah, I read about it.
Speaker C: Yeah, she was, she was not happy.
Speaker A: Can you take me through the prank?
Speaker C: Yeah, this is.
Speaker A: Do you regret doing that now?
Speaker C: No, no, not one bit. It was something. It was something I had talked about ahead of time with my buddy bane. I was like, I would really like to play a prank on my mom. Very specifically, my mom, she's very gullible. I think she had knee surgery once even. And after she came out of knee surgery, she was super groggy. She's like, I can't feel my legs. And my dad looked at her, he was like, you don't have any legs. They had to amputate both your legs. We just do very mean things to her all the time.
Speaker A: Yes.
Speaker C: I'm so surprised that she still loves us. But right after surgery, I was really worried that I was going to be too groggy, not all there. I had had anesthesia once before, and it messed me up. I could not function for a while afterwards, and I said a lot of things that I was really worried that I was going to start, I don't know, like, dropping some bombs, and I wouldn't even know. I wouldn't remember. So I was like, please, God, don't let that happen, and please let me be there enough to do this to my mom. And so she walked in after surgery. It was like the first time they had been able to see me after surgery. And she just looked at me. She said, hi, how are you? How are you doing? How do you feel? And I looked at her, and this very, I think the anesthesia helped. Very, like, groggy, sort of confused look on my face. It's like, who are you? And she just started looking around the room, like, at the surgeons, at the doctors. Like, what did you do to my son? Like, you need to fix this right now. Tears started streaming. I saw how much she was freaking out. I was like, I can't let this go on. And so I was like, mom, mom, I'm fine. Like, it's all right. And still she was not happy about it. She still says she's going to get me back someday, but, I mean, I don't know. I don't know what that's going to look like.
Speaker A: It's a lifelong battle.
Speaker C: Yeah.
Speaker A: It was good in some sense. It was a demonstration that you still got.
Speaker C: That's, that's all I wanted. That's all I wanted it to be. And I knew that doing something super mean to her like that would show her.
Speaker A: Yeah, to show that you're still there, that you love her.
Speaker C: Yeah, exactly. Exactly.
Speaker A: It's a dark way to do it, but I love it.
Speaker C: Yeah.
Speaker A: What was the first time you were able to feel that you can use the neuralink device to affect the world around you?
Speaker C: Yeah. The first little taste I got of it was actually not too long after surgery. Some of the neuralink team had brought in, like, a little iPad, a little tablet screen, and they had put up eight different channels that were recording some of my neuron spikes. And they put it in front of me like, this is real time, your brain firing. Like, that's super cool. My first thought was, I mean, if theyre firing now, lets see if I can affect them in some way. So I started trying to wiggle my fingers, and I just started scanning through the channels, and one of the things I was doing was moving my index finger up and down, and I just saw this yellow spike on top row, like, third box over or something. I saw this yellow spike every time I did it. And I was like, oh, thats cool. And everyone around me was just like, what are you seeing? I was like, look, look at this one. Look at, like, this top row, third box over, this yellow spike. Like, that's me right there. There, there. And everyone was freaking out. They started, like, clapping. I was like, that's super unnecessary. Like, this is what's supposed to happen, right?
Speaker A: Like, so you're imagining yourself moving each individual finger one at a time.
Speaker C: Yeah.
Speaker A: And then seeing, like, you can notice something. And then when you did the index finger, you're like, oh, yeah, I was.
Speaker C: I was wiggling kind of all of my fingers to see if anything would happen. There was a lot of other things going on, but that big yellow spike was the one that stood out to me. Like, I'm sure that if I would have stared at it long enough, I could have mapped out maybe a hundred different things. But the big yellow spike was the one that I noticed.
Speaker A: Maybe you could speak to what it's like to sort of wiggle your fingers, to, like, to imagine that the mental the cognitive effort required to sort of wiggle your index finger, for example. How easy is that to do?
Speaker C: Pretty easy. For me, it's something that, at the very beginning, after my accident, they told me to try and move my body as much as possible. Even if you can't just keep trying, because that's going to create new neural pathways, or pathways, in my spinal cord to reconnect these things, um, to hopefully regain some movement someday.
Speaker A: That's fascinating.
Speaker C: Yeah, I know, it's. It's bizarre, but that's part of the.
Speaker A: Recovery process, is to keep trying to move your body.
Speaker E: Yep.
Speaker A: And as much as you can, and the nervous system does its thing, it starts reconnecting.
Speaker C: It'll start reconnecting. Um, for some people. Some people, it never works. Some people, they'll do it. Like, for me, I got some bicep control back, and that's about it. I can. If I try enough, I can wiggle some of my fingers. Not like, on command, it's more like, if I try to move, say, my right pinky, and I just keep trying to move it, after a few seconds, it'll wiggle. So I know there's stuff there. I know that happens with a few different. Of my fingers and stuff, but, yeah, that's what they tell you to do. One of the people at the time, when I was in the hospital, came in and told me, for one guy who had recovered most of his control, what he thought about every day was actually walking. Like the act of walking just over and over again. So I tried that for years. I tried just imagining walking, which is. It's hard. It's hard to imagine, like, all of the steps that go into, well, taking a step, like, all of the things that have to move, like, all the activations that have to happen along your leg in order for one step to occur.
Speaker A: But you're not just imagining, you're doing it, right?
Speaker C: I'm trying, yeah. So it's like, it's imagining over again what I had to do to take a step, because it's not something any of us think about. We just. You want to walk and you take a step. You don't think about all of the different things that are going on in your body. So I had to recreate that in my head as much as I could, and then I practice it over and over and over.
Speaker A: So it's not like a third person perspective as a first person perspective, you're like. It's not like you're imagining yourself walking. You're, like, literally doing this. Everything all the same stuff as if you're walking, which.
Speaker C: Which was hard. It was hard at the beginning, like.
Speaker A: Frustrating hard, or like, actually cognitively hard, like, which way?
Speaker C: It was both. There's a. There's a scene in one of the kill Bill movies, actually, oddly enough, where she is, like, paralyzed, I don't know, from, like, a drug that was in her system. And then she, like, finds some way to get into the back of a truck or something, and she stares at her toe and she says, move, like, move your big toe. And after, you know, a few seconds on screen, she does it. And she did that with every one of her, like, body parts until she can move again. I did that for years. Just stared at my body and said, move your index finger, move your big toe. Sometimes vocalizing it, like, out loud, sometimes just thinking it. I tried every different way to do this, to try to get some movement back. And it's hard because it actually is like taxing, like physically taxing on my body, which is something I would have never expected because it's not like I'm moving, but it feels like there's a buildup of. I don't know. The only way I can describe it is there are, like, signals that aren't getting through from my brain, um, down, because my, there's that gap in my spinal cord, so brain down, and then from my hand back up to the brain. And so it feels like those signals, um, get stuck in whatever body part that I'm trying to move. And they just build up and build up and build up until they burst. Um, and then once they burst, I get, like, this really weird sensation of everything sort of, like, dissipating back out to level, and then I do it again. Um, it's also just like a fatigue thing, like a muscle fatigue, but without actually moving your muscles. It's very, very bizarre. And then, you know, uh, if you try to stare at a body part or think about a body part and move for two, three, four, sometimes 8 hours, it's very taxing on your mind. It takes a lot of focus. It was a lot easier at the beginning because I wasn't able to, like, control a tv in my room or anything. I wasn't able to control any of my environment. So for the first few years, a lot of what I was doing was staring at walls. And so obviously I did a lot of thinking and I tried to move a lot just over and over and over again.
Speaker A: Do you never give up sort of hope there training hard, essentially?
Speaker C: Yep. And I still do it I do it, like, subconsciously, and I think that that helped a lot with things, with neuralink, honestly, it's something that I talked about the other day at the all hands that I did at Neuralink's Austin facility.
Speaker A: Welcome to Austin, by the way.
Speaker D: Yeah.
Speaker C: Hey, thanks, man. I would just.
Speaker A: Hatch.
Speaker C: Hey, thanks. Thanks, man. The gigafactory was super cool. I went to school at Texas A and M, so I've been around for.
Speaker A: So you should be saying welcome to me. Yeah, welcome to Texas Lights.
Speaker C: I get you, but, yeah, I was talking about how a lot of what they've had me do, especially at the beginning, well, I still do it now, is body mapping. So, like, there will be a visualization of a hand or an arm on the screen, and I have to do that motion. And that's how they sort of train the algorithm to, like, understand what I'm trying to do. And so it made things very seamless for me.
Speaker A: I think that's really, really cool. So it's amazing to know because I've learned a lot about the body mapping procedure with the interface and everything like that. It's cool to know that you've been a century, like, training to be like, world class at that task.
Speaker C: Yeah, yeah. I don't know if other quadriplegics, like other paralyzed people, give up. I hope they don't. I hope they keep trying because I've heard other paralyzed people say, like, don't ever stop. They tell you two years, but you just never know. You the human bodies capable of amazing things. So I've heard other people say, don't give up. Like, I think one girl had spoken to me through some family members and said that she had been paralyzed for 18 years and she'd been trying to wiggle her index finger for all that time, and she finally got it back 18 years later. So I know that it's possible and I'll never give up doing it. I just. I do it when I'm lying down, like, watching tv, I'll find myself doing it kind of just almost like on its own. It's just something I've gotten so used to doing that I don't know, I. I don't think I'll ever stop.
Speaker A: That's really awesome to hear because I think it's one of those things that can really pay off in the long term, because, like, there is training, you're not visibly seeing the results of that training at the moment, but, like, there's that, like, Olympic level nervous system getting.
Speaker C: Yeah, getting ready for something, honestly, was like, something that I think neuralink gave me that I can't thank them enough for. Like, I can't show my appreciation for it enough was being able to visually see that what I'm doing is actually having some effect. It's a huge part of the reason why, like, I know now that I'm going to keep doing it forever, because before neuralink, I was doing it every day, and I was just assuming that things were happening. Like, it's not like I knew I wasn't getting back any mobility or sensation or anything, so I could have been running up against a brick wall for all I knew. And with Nurlink, I get to see, like, I. All the signals happening real time, and I get to see that what I'm doing can actually be mapped. When we started doing click calibrations and stuff, when I go to click my index finger for a left click, that it actually recognizes that it changed how I think about what's possible with retraining my body to move. So, yeah, I'll never give up.
Speaker A: And also just the signal that there's still a powerhouse of a brain there that's, like.
Speaker C: Exactly.
Speaker A: And as the technology develops, that brain is, I mean, that's the most important thing about the human body is the brain, and it can do a lot of the control. So what did it feel like when you first could wiggle the index finger and saw the environment respond? Like, that little. Yeah, wherever. We're just being way too dramatic, according to you.
Speaker C: Yeah, it was very cool. I mean, I. It was cool, but it, I keep telling this to people. It made sense to me. Like, it made sense that, you know, like, there are signals still happening in my brain and that as long as you had something near it that could measure those, that could record those, then you should be able to, like, visualize it in some way, like, see it happen. And so that was not very surprising to me. I was like, oh, cool. Like, we found one. We found something that works. It was cool to see that their technology worked and that everything that they had worked so hard for was going to pay off. But I hadn't moved a cursor or anything at that point. I had interacted with the computer or anything at that point. It just made sense. It was cool. I didn't really know much about BCI at that point either, so I didn't know what sort of step this was actually making. I didn't know if this was a huge deal or if this was just like, okay, it's cool that we got this far, but we're actually hoping for something much better down the road. It's like, okay. I just thought that they knew that it turned on, so I was like, cool. This is cool.
Speaker A: Well, did you read up on the specs of the hardware you get installed, the number of threads?
Speaker C: Yeah, I knew all of that, but it's all like. It's all greek to me. I was like, okay, threads, 64 threads, 16 electrodes, 1024 channels. Okay, like, that. That math checks out.
Speaker A: Sounds right.
Speaker C: Yeah.
Speaker A: When was the first time you were able to move a mouse cursor?
Speaker C: I know it must have been within the first maybe week or two weeks that I was able to, like, first move the cursor. And again, like, it kind of made sense to me. Like, it. It didn't seem like that big of a deal. Like, it. It was like, okay, well, how do I explain this? When everyone around you starts clapping for something that you've done, it's. It's easy to say, okay, like, I did something cool. Like, that was. That was impressive in some way. What exactly that meant, what it was hadn't really, like, set in for me. So, again, I knew that me trying to move a body part and then that being mapped in some sort of, like, machine learning algorithm to be able to, um, identify, like, my brain signals and then take that and give me cursor control, that all kind of made sense to me. I don't know, like, all the ins and outs of it, but I was like, there are still signals in my brain firing. They just can't get through because there's, like, a gap in my spinal cord, and so they just. They can't get all the way down and back up, but they're still there. So when I moved the cursor for the first time, I was like, that's cool, but I expected that that should happen. Like, it made sense to me when I moved the cursor for the first time with just my mind without physically trying to move. So I guess I can get into that just a little bit. Like, the difference between attempted movement and imagined movement.
Speaker A: Yeah. That's a fascinating difference from one to the other.
Speaker C: Yeah, yeah, yeah. So, like, attempted movement is me physically trying to attempt to move, say, my hand. I try to attempt to move my hand to the right, to the left, forward and back, and that's all. Attempted attempt to lift my finger up and down, attempt to kick or something. I'm physically trying to do all of those things. Even if you can't see it, this would be me attempting to shrug my shoulders or something. That's all attempted movement. That all. That's what I was doing for the first couple of weeks when they were going to give me cursor control. When I was doing body mapping, it was attempt to do this, attempt to do that. When Nier was telling me to imagine doing it, it kind of made sense to me. But it's not something that people practice. Like, if you started school as a child and they said, okay, write your name with this pencil. And so you do that. Like, okay, now imagine writing your name with that pencil. Kids would think, uh, like, I guess, like, that kind of makes sense, and they would do it. Um, but that's not something we're taught. It's all, like, how to do things physically. We think about, like, thought experiments and things. But that's not like, that's not like a physical action of doing things. It's more like what you would do in certain situations. So imagine movement. It never really connected with me. Like, I guess you could maybe describe it as, like, a professional athlete, like, swinging a baseball bat or swinging, like, a golf club. Like, imagine what you're supposed to do, but then you go right to that and physically do it. Like, you. Then you get a bat in your hand, and then you do what you've been imagining. And so I don't have that, like, connection. So telling me to imagine something versus attempting it, there wasn't a lot that I could do there mentally. I just kind of had to accept what was going on and try. But the attempted moving thing, it all made sense to me. If I try to move, then there's a signal being sent in my brain, and as long as they can pick that up, then they should be able to map it to what I'm trying to do. And so when I first moved the cursor like that, it was like, yes, this should happen. Like, I'm not surprised by that.
Speaker A: But can you clarify, is there supposed to be a difference between imagined movement and attempted movement?
Speaker C: Yeah, just that in imagined movement, you're not attempting to move at all. So it's.
Speaker A: You're, like, visualizing yourself doing, and then theoretically, is that supposed to be a different part of the brain that lights up in those two different situations?
Speaker E: Yeah, not necessarily. I think all these signals can still be represented in motor cortex, but the difference, I think, has to do with the naturalness of imagining something versus attempting it and sort of the fatigue of that over time.
Speaker A: And by the way, on the mic is bliss. So, like, this is just different ways to prompt you to kind of get to the thing that you arrived at. Yeah, attempted movement does sound like the right thing.
Speaker C: Yeah, try. Yeah, I mean, it makes sense to.
Speaker A: Me because imagine for me, I'll be. I would start visualizing, like, in my mind. Visualizing attempted. I would actually start trying to, like. Yeah, there's a. I mean, I, you know, I did, like, combat sports my whole life. Like, wrestling. When I'm imagining a move, see, I'm, like, moving my muscle exactly. Like, there's a. There is a bit of an activation almost, versus, like, visualizing yourself, like a picture doing it.
Speaker C: Yeah, it's something that I feel like naturally anyone would do. If you try to tell someone to imagine doing something, they might close their eyes and then start physically doing it. But it's just. Yeah, it's hard.
Speaker A: It was very hard at the beginning, but attempted worked.
Speaker C: Attempted worked. It worked just like it should work. Like, work like a charm.
Speaker E: I remember there was, like, one Tuesday, we were messing around, I think I forget what swear word you used, but there's a swear word that came out of your mouth when you figured out you could just do the direct cursor control.
Speaker C: Yeah, it blew my mind. Like, no pun intended. Blew my mind when I first moved the cursor, just with my thoughts and not attempting to move, it's something that I found, like, over the couple of weeks, like, building up to that, that as I get better cursor controls, like, the model gets better, then it gets easier for me to, like, like, I don't have to attempt as much to move it. And part of that is something that I'd even talked with them about. Um, when I was watching the signals of my brain one day, I was watching when I, like, attempted to move to the right, and I watched the screen as, like, I saw the spikes. Like, I was seeing the spike the signal was being sent before I was actually attempting to move. Um, I imagine just because, you know, when you go to, say, move your hand or any body part, that signal gets sent before you're actually moving. Has to make it all the way down and back up before you actually do any sort of movement. So there's a delay there. And I noticed that there was something going on in my brain before I was actually attempting to move, that my brain was, like, anticipating what I wanted to do. And that all started sort of, I don't know, like, percolating in my brain. Like, it just. It was just sort of there, like, always in the back. Like, that's so weird that it could do that. It kind of makes sense, but I wonder what that means as far as, like, using the neuralink and, you know, and then as I was playing around with the attempted movement and playing around with the cursor, and I saw that, like, as the cursor control got better, that it was anticipating my movements and what I wanted it to do, like, cursor movements, what I wanted to do a bit better and a bit better. And then one day, I just randomly, as I was playing web grid, I, like, looked at a target before I had started attempting to move. I was just trying to get over, train my eyes to start looking ahead. This is the target I'm on. But if I look over here to this target, I know I can maybe be a bit quicker. Getting there just shot over. It was wild. Like, I had to take a step back. I was like, this should not be happening all day. I was just smiling. I was so giddy. I was like, guys, do you know that this works? I can just think it, and it happens, which they had all been saying this entire time. I can't believe you're doing all this with your mind. I'm like, yeah, but is it really with my mind, I'm attempting to move, and it's just picking that up so it doesn't feel like it's with my mind. But when I moved it for the first time like that, it was, oh, man. It, like, it made me think that this technology, that what I'm doing is actually way, way more impressive than I ever thought. It was way cooler than I ever thought, and it just opened up a whole new world of possibilities of, like, what could possibly happen with this technology and what I might be able to be capable of with it.
Speaker A: Because you had felt for the first time, like, this was digital telepathy. Like, you're controlling a digital device with your mind.
Speaker C: Yep.
Speaker A: I mean, this is. That's a real moment of discovery. That's really cool. Like, you've discovered something. I've seen, like, scientists talk about, like, a big aha moment, you know, like, Nobel Prize winning. They'll have this, like, holy crap.
Speaker C: Yeah. Like, that's what it felt like. Like, I didn't feel like. Like, I felt like I had discovered something, but for me, maybe not necessarily for, like, the world at large or, like, this field at large. It just felt like an aha moment for me. Like, oh, this works. Like, obviously it works. And so that's what I do, like, all the time now. I kind of intermix the attempted movement and imagine movement. I do it all together. Because ive found that there is some interplay with it that maximizes efficiency with the cursor. So its not all one or the other. Its not all just I only use attempted or I only use imagined movements. Its more I use them in parallel and I can do one or the other. I can just completely think about whatever I'm doing. But I don't know, I like to play around with it. I also like to just experiment with these things. Every now and again I'll get this idea in my head, like, hmm, I wonder if this works. And I'll just start doing it and then afterwards I'll tell them, by the way, I wasn't doing that like you guys wanted me to. I thought of something and I wanted to try it, and so I did. It seems like it works. So maybe we should like explore that a little bit.
Speaker A: So I think that discovery is not just for you, at least from my perspective, that's a discovery for everyone else who ever uses in your link that this is possible. Like, I don't think that's an obvious thing that this is even possible. It's like I was saying to bliss earlier, it's like the four minute mile. People thought it was impossible to run a mile in four minutes, and once the first person did it, then everyone just started doing it. So like, just to show that it's possible, that paves the way to, like, anyone can now do it. That's the thing. That's actually possible. You don't need to do the attempted movement. You can just go direct. That's crazy.
Speaker C: That is crazy.
Speaker A: It is crazy for people who don't know. Can you explain how the link app works? You have an amazing stream on the topic. Your first stream, I think, on x describing the app. Can you just describe how it works?
Speaker C: Yeah, so it's just an app that Neuralink created to help me interact with the computer. So on the link app, there are a few different settings and different modes and things I can do on it. So there's like the body mapping, which we kind of touched on. There's a calibration. Calibration is how I actually get cursor control. So calibrating what's going on in my brain to translate that into cursor control so it will pop out models, what they use, I think, is time. So it would be five minutes in calibration will give me so good of a model. And then if I'm in it for ten minutes and 15 minutes, the models will progressively get better. And so, you know, the longer I'm in it, generally the better the models will get.
Speaker A: That's really cool, because you often refer to the models. The model is the thing that's constructed once you go through the calibration step.
Speaker C: Yeah.
Speaker A: And then you also talked about sometimes you'll play like, a really difficult game like snake, just to see how good the model is.
Speaker C: Yeah, yeah. So snake is kind of like my litmus test for models. If I can control snake decently, well, then I know I have it. Pretty good model. So, yeah, the link app has all of those. It has webgrid in it now. It's also how I connect to the computer. Just in general, they've given me a lot of voice controls with it at this point. So I can say connect or implant disconnect. And as long as I have that charger handy, then I can connect to it. So the charger is also how I connect to the link app. To connect to the computer, I have to have the implant charger over my head when I want to connect, to have it wake up. Because the implants in hibernation mode, always when I'm not using it, I think there's a setting to wake it up every so long. So we could set it to half an hour or 5 hours or something. If I just want it to wake up periodically, I'll connect to the link app and then go through all sorts of things. Calibration for the day, maybe body mapping. I made them give me a little homework tab because I am very forgetful and I forget to do things a lot. So I have a lot of data collection things that they want me to do.
Speaker A: Is the body mapping part of the data collection, or is that also part of the.
Speaker C: Yeah, it is. It's something that they want me to do daily, which I've been slacking on because I've been doing so much media and traveling so much.
Speaker A: So I've been super famous.
Speaker C: Yeah, I've been a terrible first candidate for how much I've been slacking on my homework. But, yeah, it's just something that they want me to do every day to track how well the neuralink is performing over time and have something to give, I imagine, to give to the FDA to create all sorts of fancy charts and stuff and show like, hey, this is what the neuralink, this is how it's performing day one versus day 90 versus day 180, and things like that.
Speaker A: What's the calibration step like? Is it like, move left, move right?
Speaker C: It's a bubble game. So there will be yellow bubbles that pop up on the screen. At first, it is open loop. This is something that I still don't fully understand, the open loop and closed loop thing.
Speaker A: Me and Blizz talked for a long time about the difference between the two from the. On the technical side.
Speaker C: Okay.
Speaker A: So it'd be great to hear your.
Speaker C: Okay, so your side of the story. Open loop is basically, I have no control over the cursor. The cursor will be moving on its own across the screen, and I am following by intention the cursor to different bubbles. And then my. The algorithm is training off of what? Like, the signals it's getting are, as I'm doing this, there are a couple different ways that they've done it. They call it center out target. So there will be a bubble in the middle and then eight bubbles around that, and the cursor will go from the middle to one side. So say middle to left, back to middle to up to middle, like upright. And they'll do that all the way around the circle. And I will follow that cursor the whole time, and then it will train off of my intentions, what it is expecting my intentions to be throughout the whole process.
Speaker A: Can you actually speak to. When you say follow, you don't mean with your eyes, you mean with your intentions.
Speaker C: Yeah. So, generally, for calibration, I'm doing attempted movements because I think it works better. I think the better models, as I progress through calibration, make it easier to use imagined movements.
Speaker A: Wait, wait, wait. So calibrated, unattempted movement will create a model that makes it really effective for you to then use the force?
Speaker C: Yes. I've tried doing calibration with imagined movement, and it just doesn't work as well for some reason. So that was the center out targets. There's also one where a random target will pop up on the screen and it's the same. I just move, I follow along wherever the cursor is to that target all across the screen. I've tried those with imagine movement, and for some reason, the models just don't, they don't give as high levels quality when we get into closed loop. I haven't played around with it a ton, so maybe, like, the different ways that we're doing calibration now might make it a bit better. But what I've found is there will be a point in calibration where I can use imagine movement before that point. It doesn't really work. So if I do calibration for 45 minutes, the first 15 minutes, I can't use imagine movement. It just, like, doesn't work for some reason. And after a certain point, I can just sort of feel it. I can tell it moves different. That's the best way I can describe it. It's almost as if it is anticipating what I am going to do again before I go to do it. And so using attempted movement for 15 minutes, at some point, I can kind of tell when I, like, move my eyes to the next target that the cursor is starting to pick up. Its starting to understand, its learning what im going to do.
Speaker A: So, first of all, its really cool that, I mean, youre our true pioneer in all of this. Youre exploring how to do every aspect of this most effectively. And there's just, I imagine so many lessons learned from this. So thank you for being a pioneer in all these kinds of different, like, super technical ways. And it's also cool to hear that there's a different, like, feeling to the experience when it's calibrated in different ways. Like, just because I imagine your brain is doing something different and that's why there's a different feeling to it. And then trying to find the words and the measurements to those feelings would be also interesting. But at the end of the day, you can also measure that your actual performance on whether it's snake or webgrid, you can see, like, what actually works well. And you're saying for the open loop calibration, the attempted movement works best for now.
Speaker E: Yep.
Speaker C: Yep.
Speaker A: So the open loop, you don't get the feedback that something, that you did something.
Speaker C: Yeah, I'm frustrating. No, no, it makes sense to me. Like, we've done it with a cursor and without a cursor in open loop. So sometimes it's just say for, like, the center out, the. You'll start calibration with a bubble lighting up, and I push towards that bubble. And then when that bubble, you know, when it's a push towards that bubble for, say, 3 seconds, a bubble will pop, and then I come back to the middle. So I'm doing it all just by my intentions. That's what it's learning anyway. So it makes sense that as long as I follow what they want me to do, follow the yellow brick road, that it'll all work out.
Speaker A: You're full of great references. Is the bubble game fun?
Speaker C: Yeah. They always feel so bad making me do calibration. Oh, we're about to do, you know, a 40 minutes calibration. I'm like, all right, do you guys want to do two of them? Like, I'm always asking to, like, whatever they need, I'm more than happy to do. And it's not, it's not bad. Like, I get to lie there and. Or sit in my chair and, like, do these things with some great people. I get to have great conversations. I can give them feedback. I can talk about all sorts of things. I could throw something on, on my tv in the background and kind of, like, split my attention between them. Like, it's not bad at all. I don't score that.
Speaker A: You get, like, can you do better on the bubble game?
Speaker C: No, I would love that. I would love. Yeah.
Speaker A: Writing down suggestions from Nolan that's make it more fun gamified.
Speaker C: Yeah. That's one thing that I really, really enjoy about webgrid is because I'm so competitive. Like, the higher the bps, the higher the score I know, the better I'm doing. And so if I think I've asked at 1.1 of the guys, like, if he could give me some sort of numerical feedback for calibration, like, I would like to know what they're looking at. Like, oh, you know, it is. We see, like, this number while you're doing calibration. And that means I, at least on our end, that we think calibration is going well. And I would love that because I would like to know if what I'm doing is going well or not. But then they've also told me, like, yeah, not necessarily, like, one to one. It doesn't actually mean that calibration is going well in some ways. So it's not like 100%. And they don't want to, like, skew what I'm experiencing or want me to change things based on that. If that number isn't always accurate to, like, how the model will turn out or, like, the end result, that's at least what I got from it. One thing I do, I have asked them, and something that I really enjoy striving for is towards the end of calibration, there is, like, a time between targets. And so I like to keep, like, at the end, that number as low as possible. So at the beginning it can be, you know, four, five, 6 seconds between me popping bubbles, but towards the end, I like to keep it below, like, 1.5. Or if I could get it to, like, 1 second between, like, bubbles, because in my mind, that translates really nicely to something like web grid, where I know if I can hit a target one every second that I'm doing real, real well.
Speaker A: There you go. That's the way to get a score on the calibration is like the speed. How quickly can you get from bubble to bubble?
Speaker C: Yeah.
Speaker A: So there's the open loop, and then it goes to the closed loop. The closed loop can already start giving you a sense, because you're getting feedback of how good the model is.
Speaker C: Yeah. So, closed loop is when I first get cursor control and how they've described it to me, someone who does not understand this stuff, I am the dumbest person in the room. Every time I'm with any of these humility is that I am closing the loop. So I am actually now, um, the one that is, like, finishing the loop of whatever this loop is. I don't even know what the loop is. They've never told me. They just say there is a loop, and at one point it's open and I can't control, and then I get control, and it's closed. So I'm finishing the loop.
Speaker A: So how long the calibration usually takes? You said like 1015 minutes.
Speaker C: Well, yeah, they're. They're trying to get that number down pretty low. That's what we've been working on a lot recently, is getting that down is low as possible. So that way, you know, if this is something that people need to do on a daily basis, or if some people need to do on a, like, every other day basis or once a week, they don't want people to be sitting in calibration for long periods of time. I think they wanted to get it down seven minutes or below, at least where we're at right now, it'd be nice if you never had to do calibration. So we'll have get there at some point. I'm sure. The more we learn about the brain and, like, I think that's, you know, the dream, I think right now for me to get, like, really, really good models, I'm in calibration 40 or 45 minutes. And I don't mind. Like I said, they always feel really bad. But if it's going to get me a model that can, like, break these records on web grid, I'll stay in it for flipping 2 hours.
Speaker A: Let's talk business. So, web grid, I saw a presentation that where Bliss said by March, you selected 89,000 targets in web grid. Can you explain this game? What is Webgrid, and what does it take to be a world class performer in Webgrid as you continue to break world records?
Speaker C: Yeah.
Speaker A: It'S like a gold medalist talk.
Speaker C: Well, I'd like to thank. I'd like to thank everyone who's helped me get here, my coaches, my parents, for driving me to practice every day at five in the morning. I like to thank God and just overall my dedication to my.
Speaker A: The interviews with athletes are always like that exact. It's like that template.
Speaker C: Yeah.
Speaker A: So web grid.
Speaker C: Web grid is a grid. It's literally just a grid. They can make it as big or small as you can make a grid. A single box on that grid will light up and you go and click it. And it is a way for them to benchmark how good a BCI is. So it's, you know, pretty straightforward. You just click targets.
Speaker A: Only one blue cell appears and you're supposed to move the mouse to there and click on it.
Speaker C: So I like playing on, like, bigger grids because it. The bigger the griddenne, the more bps. It's bits per second that you get every time you click one. So I'll say I'll play on like a 35 by 35 grid, and then one of those little squares, cell, call it target, whatever, will light up. And you move the cursor there and you click it. And then you do that forever.
Speaker A: And you've been able to achieve, at first, eight bits per second. And you recently broke that.
Speaker C: Yeah, I'm at 8.5 right now. Beaten that literally the day before I came to Austin. But I had like a. I don't know, like a five second lag right at the end. And I just had to wait until the latency calmed down. And then I kept clicking, but I was at, like, 8.01 and then 5 seconds of lag, and then the next, like, three targets I clicked all stayed at 8.01. So if I would have been able to click, um, during that time of lag, I probably would have hit. I don't know, I might have hit nine. So I'm there. I'm like, I'm really close. And then this whole Austin trip has really gotten in the way of my web grid playing ability. Yeah.
Speaker A: So that's all you're thinking about right now?
Speaker C: Yeah, I know, it's just. I just want. I want to do better. I want to do better. I want to hit nine, I think. Well, I know nine is very, very achievable. I'm right there. I think ten I could hit maybe in the next month. Like, I could do it probably in the next few weeks if I really push.
Speaker A: I think you and Elon are basically the same person. Cause last time I did a podcast with him, he came in extremely frustrated that he can't beat Uber Lilith as a droid. That was like a year ago. I think I forget, like, solo. And I could just tell there's some percentage of his brain, the entire time was thinking, like, I wish I was right now attempting to.
Speaker C: I think he did it that night.
Speaker A: He did it that night. He stayed up and did it that night. It's just crazy to me. I mean, it's, in a fundamental way, it's really inspiring. And what you're doing is inspiring in that way because, I mean, it's not just about the game. Everything you're doing there has impact. By striving to do well on web grid, you're helping everybody figure out how to create the system all along. Like the decoding, the software, the hardware, the calibration, all of it. How to make all of that work so you can do everything else really well.
Speaker C: Yeah, it's just really fun.
Speaker A: Well, that's also. That's part of the thing, is making it fun.
Speaker C: Yeah, it's addicting. I'm. I've joked about, um, like, what they actually did when they went in and put this thing in my brain. They must have flipped a switch to make me more susceptible to these kinds of games, to make me addicted to, like, web grid or something.
Speaker A: Yeah.
Speaker C: Do you know Bliss's high score?
Speaker A: Yeah, he said like, 14 or something.
Speaker C: 17.
Speaker D: Oh, boy.
Speaker C: 17.1 or something. 17 on the dot.
Speaker E: 17.01.
Speaker A: Yeah, he told me he, like, does it on the floor with peanut butter and he's like, fasts. It's weird. That sounds like cheating. Sounds like performance enhancing.
Speaker E: No one's like, the first time Nolan played this game, he asked, how good are we at this game? And I think you told me right then you're not. You're gonna try to beat me.
Speaker C: I'm gonna get there someday. Yeah, I think. I think I can. Yeah. So I've been playing, first off, with the dwell cursor, which really hampers my web grid playing ability. Basically, I have to wait 3 seconds for every click.
Speaker A: Oh, so you can't do the click.
Speaker D: So.
Speaker A: Yeah, so you click by dwelling.
Speaker C: You said 0.30.3 seconds, which sucks. It really slows down how much I'm able to, how high I'm able to get. I still hit. I think I hit 50 something trials. Net trials per minute in that, which was pretty good because I'm able to. One of the settings is also how slow you need to be moving in order to initiate a click. To start a click. So I can tell when I'm on that threshold to start initiating a click just a bit early. So I'm not fully stopped over the target when I go to click. I'm doing it, like, on my way to the targets a little to try to time it just right.
Speaker A: So you're slowing down?
Speaker C: Yeah, just. Just a hair right before the targets.
Speaker A: This is like a lead performance. Okay. But that's still. It's. It sucks that there's a ceiling of the.
Speaker C: .3 well, there I can get down to .2 and. .1.1 yeah. And I've played with that a little bit, too. I have to adjust a ton of different parameters in order to play with, .1 and I don't have control over all that on my end yet. It also changes how the models are trained. Like, if I train a model, like in web grid, I bootstrap on a model, which basically is them training models as I'm playing webgrid based off of the web grid data that I'm. So if I play web grid for ten minutes, they can train off that data specifically in order to get me a better model. If I do that with three versus 0.1, the models come out different. The way that they interact, it's just much, much different. So I have to be really careful. I found that doing it with 0.3 is actually better in some ways. Unless I can do it with 0.1 and change all of the different parameters, then that's more ideal, because obviously 0.3 is faster than one. I could get there. I can get there.
Speaker A: Can you click using your brain for.
Speaker C: Right now, it's the hover. Clicking with the dwell cursor. Before all the thread retraction stuff happened, we were calibrating clicks. Left click, right click. That was my previous ceiling before I broke the record. Again with the dwell cursor was, I think, on a 35 by 35 grid with left and right click. And you get more bps, more bits per second using multiple clicks, because it's more difficult.
Speaker A: Oh, because what is it? You're supposed to do? Either a left click or, like, right click. The different colors.
Speaker C: Yeah. Blue targets for left click, orange targets for right click is what they had done. So my previous record of 7.5 was with blue and the orange targets. Yeah. Which I think if I went back to that now, doing the click calibration, I would be able to. And being able to, like, initiate clicks on my own, I think I would break that ten ceiling, like, in a couple days max.
Speaker A: Like, yeah, you start making bliss nervous about his 17.
Speaker E: Why do you think we haven't given him the.
Speaker A: Exactly. So what did it feel like with the retractions that there is some of the threads retracted?
Speaker C: That sucked. It was really, really hard. The day they told me, was the day of my big neuralink tour at their Fremont facility. They told me right before we went over there, it was really hard to hear. My initial reaction was, all right, go in, fix it. Go and take it out and fix it. The first surgery was so easy. I went to sleep. A couple hours later, I woke up, and here we are. I didn't feel any pain, didn't take any pain pills or anything. So I just knew that if they wanted to, they could go in and put in a new one, like, next day, if that's what it took, because I just wanted. I wanted it to be better, and I wanted not to lose the capability. I had so much fun playing with it for a few weeks, for a month, I had, like, it had opened up so many doors for me. It had opened up so many more possibilities that I didn't want to lose it after a month. I thought it would have been a cruel twist of fate if I had gotten to see the view from, like, the top of this mountain and then have it all come crashing down after a month. And I knew, like, say, the top of the mountain, but, like, I. How I saw it was I was just now starting to climb the mountain, and I was like, there was so much more that I knew was possible. And so to have all of that be taken away was really, really hard. But then on the drive over to the facility, I don't know, like, five minute drive, whatever it is. I talked with my parents about it. I prayed about it. I was just like, you know, I'm not going to let this ruin my day. I'm not going to let this ruin this amazing tour that they have set up for me. I want to go show everyone how much I appreciate all the work they're doing. I want to go meet all of the people who have made this possible, and I want to go have one of the best days of my life. And I did, and it was amazing. And it absolutely was one of the best days I've ever been privileged to experience. And then for a few days, I was pretty down in the dumps. But for, like, the first few days afterwards, I was just like, I didn't know if it was ever going to work again. And then I just. I made the decision that it. Even if I lost the ability to use the neuralink, even if I lost. Even if I, like, lost out on everything to come. If I could keep giving them data in any way, then I would do that. If I needed to just do some of the data collection every day or body mapping every day for a year, then I would do it, because I know that everything I'm doing helps everyone to come after me, and that's all I wanted. I guess the whole reason that I did this was to help people. I knew that anything I could do to help, I would continue to do, even if I never got to use the cursor again, then, you know, I was just happy to be a part of it. And everything that I'd done was just a perk. It was something that I got to experience, and I know how amazing it's going to be for everyone to come after me, so might as well just keep trucking along, you know?
Speaker A: That said, you were able to get to work your way up to get the performance back. So this is like going from Rocky one to Rocky two. So when did you first realize that this is possible and what gave you sort of the strength, the motivation, determination to do it, to increase back up and be your previous record?
Speaker C: Yeah, it was within a couple weeks.
Speaker A: Again, this feels like I'm interviewing an athlete. This is great. I like the thing. My parents.
Speaker C: The road back was long and hard from many difficulties. There were dark days. It was a couple weeks, I think. And then there was just a turning point. I think they had switched how they were measuring the neuron spikes in my brain. Like, the bliss. Help me out.
Speaker E: Yeah. The way in which we were measuring the behavior of individual neurons. So we're switching from sort of individual spike detection to something called spike band power, which, if you watch the previous segments with either me or DJ, you probably have some content.
Speaker C: Yeah. Okay. So when they did that, it was kind of like a light over the head, like, light bulb moment. Like, oh, this works. And this seems like we can run with this. And I saw the uptick in performance immediately. Like, I could feel it when they switched over, I was like, this is better. Like, this is good. Like, everything up till this point, for the last few weeks, last, like, whatever, three or four weeks, because it was before they even told me everything, before this sucked. Let's keep doing what we're doing now. And at that point, it was not like, oh, I know, I'm still only at, like, say, in web grid terms, like, four or five bps compared to my 7.5 before. But I know that if we keep doing this, then I can. I can get back there and then they gave me the dwell cursor, and the dwell cursor sucked.
Speaker F: At first.
Speaker C: It's not, obviously not what I want, but it gave me a path forward to be able to continue using it and hopefully to continue to help out. And so I just ran with it, never looked back. Like I said, I just kind of person. I roll with the punches.
Speaker A: Anyway, what was the process? What was the feedback loop on the figuring out how to do the spike detection in a way that would actually work well for another.
Speaker E: Yeah, it's a great question. So maybe just describe first how the actual update worked. It was basically an update to your implant. So we just did an over the air software update to his implant. Somebody could update your Tesla or your iPhone. And that firmware change enabled us to record averages of populations of neurons nearby individual electrodes. So we have less resolution about which individual neuron is doing what, but we have a broader picture of what's going on nearby an electrode overall. And that feedback loop, basically, as Nolan described it was immediate when we flipped that switch. I think the first day we did that, you hit three or four vps right out of the box, and that was a light bulb moment for, okay, this is the right path to go down from there. There's a lot of feedback around how to make this useful for independent use. What we care about ultimately is that you can use it independently to do whatever you want. To get to that point, it required us to re engineer the ux, as you talked about, the dwell cursor, to make it something that you can use independently without us needing to be involved all the time. This is obviously the start of this journey. Still, hopefully we get back to the places where you're doing multiple clicks and using that to control much more fluidly everything and much more naturally the applications that you're trying to interface with.
Speaker A: And most importantly, get that web grid number up on the hover. Click. Do you accidentally click stuff? Sometimes. How hard is it to avoid accidentally clicking?
Speaker C: I have to continuously keep it moving. Basically, like I said, there's a threshold where it will initiate a click. So if I ever drop below that, it'll start and I have 0.3 seconds to move it before it clicks anything. If I don't want it to ever get there, I just keep it moving at a certain speed and just constantly doing circles on screen, moving it back and forth to keep it from clicking stuff. I actually noticed a couple weeks back that when I was not using the implant, I was just moving my hand back and forth or in circles like, I was trying to keep the cursor from clicking, and I was just doing it, like, while I was trying to go to sleep, and I was like, okay, this is a problem to avoid.
Speaker A: The clicking, I guess. Does that create problems? Like, when you're gaming, accidentally click a thing?
Speaker C: Yeah. Yeah. It happens in chess. I've lost. I've lost a number of games because I'll accidentally click something.
Speaker E: I think the first time I ever beat you was because of an accident.
Speaker A: It's a nice excuse, right?
Speaker E: Yeah.
Speaker A: Anytime you lose, you could just say that was accidental.
Speaker C: Yeah.
Speaker A: You said the app improved a lot from version one. When you first started using it, it was very different. So can you just talk about the trial and error that you went through with the team? Like, 200 plus pages of notes? Like, what's that process, like, of going back and forth and working together to improve the thing?
Speaker C: It's a lot of me just using it, like, day in and day out and saying, like, hey, can you guys do this for me? Like, give me this. I want to be able to do that. I need this. I think a lot of it just doesn't occur to them. Maybe until someone is actually using the app, using the implant, it's just something that they just never would have thought of, or it's very specific to even, like, me. Maybe what I want, it's something I'm a little worried about with the next people that come is, you know, maybe they will want things much different than how I set it up or what the advice I've given the team, and they're going to look at some of the things they've added for me, like, that's a dumb idea. Why would he ask for that? And so I'm really looking forward to get the next people on because I guarantee that they're going to think of things that I've never thought of. They're going to think of improvements. I'm like, wow, that's a really good idea. I wish I would have thought of that. And then they're also going to give me some pushback about, like, yeah, what you are asking them to do here, that's a bad idea. Let's do it this way. And I'm more than happy to have that happen, but it's just a lot of, like, you know, different interactions with different games or applications, the Internet, just with the computer in general, there's tons of bugs that end up popping up left, right, center. So it's just me trying to use it as much as possible and showing them what works and what doesn't work and what I would like to be better. And then they take that feedback and they usually create amazing things for me. They solve these problems in ways I would have never imagined. They're so good at everything they do. And so I'm just really thankful that I'm able to give them feedback and they can make something of it because a lot of my feedback is like really dumb. It's just like, I want this, please do something about it. And they'll come back and super well thought out and it's way better than anything I could have ever thought of or implemented myself. So they're just great. They're really, really cool.
Speaker A: As the BCI community grows, would you like to hang out with the other folks with neuralinks? Like what, what relationship, if any, would you want to have with them? Because you said, like, they might have a different set of like, ideas of how to use the thing.
Speaker C: Yeah.
Speaker A: Would you be intimidated by their web grid performance?
Speaker C: No, no.
Speaker A: I hope compete.
Speaker C: I hope day one they like wipe the floor with me. I hope they beat it and they crush it, you know, double it if they can, just because on one hand it's only going to push me to be better because Im super competitive. I want other people to push me. I think that is important for anyone trying to achieve greatness is they need other people around them who are going to push them to be better. I even made a joke about it on x. Once the next people get chosen, cue buddy cop music. Im just excited to have other people to do this with and to share experiences with. I'm more than happy to interact with them as much as they want, more than happy to give them advice. I don't know what kind of advice I could give them, but if they have questions, I'm more than happy.
Speaker A: What advice would you have for the next participant in the clinical trial?
Speaker C: That they should have fun with this because it is a lot of fun and that I hope they work really, really hard because it's not just for us, it's for everyone that comes after us and, you know, come to me if they need anything and to go to Neuralink if they need anything. Man, Neuralink moves mountains like they do absolutely anything for me that they can. And it's an amazing support system to have. It puts my mind at ease for like so many things that I have had, like questions about, so many things I want to do and theyre always there and thats really, really nice. I would tell them not to be afraid to go to Neuralink with any questions that they have, any concerns, anything that theyre looking to do with this, and any help that neuralink is capable of providing. I know they will. I dont know. I dont know. Just work your ass off, because its really important we try to give our all to this.
Speaker A: So have fun and work hard.
Speaker C: Yeah. Yeah. There we go. Maybe that's what I'll just start saying to people, have fun. Work hard.
Speaker A: Now you're a real pro athlete. Just keep it short. Maybe it's good to talk about what you've been able to do now that you have a neuralink implant. Like the freedom you gain from this way of interacting with the outside world. Like, you play video games all night and you do that by yourself, and that's a kind of freedom. Can you speak to that freedom that you gain?
Speaker C: Yeah, it's what all, I don't know. People in my position want. They just want more independence. The more load that I can take away from people around me, the better. If I'm able to interact with the world without using my family, without going through any of my friends, like, needing them to help me with things, the better. If I'm able to sit up on my computer all night and not need someone to sit me up, say, on my iPad, in a position where I can use it, and then have to have them wait up for me all night until I'm ready to be done using it, it takes a load off of all of us, and it's really, like, all I can ask for. It's something that, you know, I could never thank neuralink enough for. And I know my family feels the same way. You know, just being able to have the freedom to do things on my own at any hour of the day or night, it means the world to me. And I don't know.
Speaker A: When you're up at 02:00 a.m. playing web grid by yourself. Yeah, I just imagine, like, it's darkness and there's a light glowing, and you're just focused what's going through your mind? Or you're like, in a state of flow where it's like the mind is empty. Like those, like Zen masters.
Speaker C: Yeah. Generally it is me playing music of some sort. I have a massive playlist, and so I'm just, like, rocking out to music, and then it's also just like a race against time, because I'm constantly, constantly looking at how much battery percentage I have left on my implant. Like, all right, I have 30%, which equates to x amount of time, which means I have to break this record in the next hour and a half or else it's not happening tonight. And so it's a little stressful when that happens. When it's like, when it's above 50%, I'm like, okay. Like, I got time. It starts getting down to 30 and then 20. It's like, all right, 10%. A little pop up is going to pop up right here. And it's going to really screw my web grid flow. It's going to tell me that, you know, like, there's a, like, the low battery, low battery pop up comes up and I'm like, it's really going to screw me over. So if I have to, if I'm going to break this record, I have to do it in the next, like 30 seconds or else that pop up is going to get in the way, like cover my web gridden. And then after that I go click on it, go back into webgrid, and I'm like, all right, that means I have ten minutes left before this thing's dead. That's what's going on in my head generally. That and whatever song is playing. And I just want, I want to break those records so bad. Like, it's all I want. When I'm playing web grid, it has become less of like, oh, this is just a leisurely activity. Like, I just enjoy doing this because it just feels so nice and it puts me at ease. It is. No, once I'm in web grid, you better break this record or you're going to waste like 5 hours of your life right now. And I don't know, it's just fun. It's fun, man.
Speaker A: Have you ever tried web grid with like two targets and three targets? Can you get higher BPS with that?
Speaker C: Can you do that?
Speaker E: You mean like different color targets or.
Speaker A: You mean, oh, multiple targets change the thing.
Speaker E: Yeah. So BPS is log of number of targets times correct minus incorrect divided by time. And so you can think of like different clicks as basically doubling the number of active targets.
Speaker D: Got it.
Speaker E: So, you know, basically higher BPS, the more options there are, the more difficult to task. And there's also like, zen mode you've played in before, which is infinite canvas.
Speaker C: It covers the whole screen with a grid. And I don't know what.
Speaker A: Yeah, and so you can go like, that's insane. To.
Speaker E: Yeah, he doesn't like it. Cause it didn't show BPS. So like, you know.
Speaker C: Yeah, I had them put in a giant BpS in the background. So now it's like, the opposite of Zen mode. It's like. It's like, super hard mode, like, just metal mode. If it's just, like, a giant number in the back counter, we should rename that.
Speaker E: Metal mode is a much better name.
Speaker A: So you also play civilization vi.
Speaker C: I love civ six.
Speaker A: Yeah. Usually go with Korea.
Speaker C: I do. So the great part about Korea is they focus on science tech victories, which was not planned. Like, I've been playing Korea for years, and then all of the knurling stuff happened, so it kind of aligns. But what I've noticed with tech victories is if you can just rush tech, rush science, then you can do anything. Like, at one point in the game, you will be so far ahead of everyone technologically that you will have, like, musket men, infantrymen, planes sometimes, and people will still be fighting with, like, bows and arrows. And so if you want to win a domination victory, you just get to a certain point with the science and then go and wipe out the rest of the world. Or you can just take science all the way and win that way, and you're going to be so far ahead of everyone because you're producing so much science that it's not even close. I've accidentally won in different ways just by focusing on science. I was. Yeah, I, like, I was playing only science, obviously. Like, just science all the way. Just tech. And I was trying to get, like, every tech in the tech tree and stuff, and then I accidentally won through a diplomatic victory. And I was so mad. I was so mad because it just ends the game. One turn. It was like, oh, you won. You're so diplomatic. I'm like, I don't want to do this. I should have declared war on more people or something. It was terrible. But you don't need giant civilizations with tech, especially with Korea, you can keep it pretty small. So I generally just get to a certain military unit and put them all around my border to keep everyone out of, and then I will just build up. So very isolationist.
Speaker A: Nice. Yeah, just work on the science.
Speaker C: That's it.
Speaker A: You're making it sound so fun.
Speaker C: It's so much fun.
Speaker A: And I also saw a civilization seven trailer.
Speaker C: Oh, man, I'm so pumped.
Speaker A: And that's probably coming out.
Speaker C: Come on, Civ seven. Hit me up. All alpha beta tests. Whatever.
Speaker A: When is it coming out?
Speaker C: 2025.
Speaker B: Yeah.
Speaker A: Next year. Yeah. What other stuff would you like to see improved about the Neuralink app and just the entire experience?
Speaker C: I would like to, like I said, get back to the click on demand, like the regular clicks. That would be great. I would like to be able to connect to more devices. Right now it's just the computer. I'd like to be able to use it on my phone or use it on different consoles, different platforms. I'd like to be able to control as much stuff as possible, honestly. Like an Optimus robot would be pretty cool. That would be sick if I could control an Optimus robot. The link app itself, it seems like we are getting pretty dialed in to what it might look like down the road. Seems like we've gotten through a lot of what I want from it, at least. The only other thing I would say is, like, more control over all the parameters that I can tweak with my cursor and stuff. There's a lot of things that go into how the cursor moves in certain ways, and I have, I don't know, like three or four of those parameters.
Speaker A: And there might be gain in friction.
Speaker C: Gain friction? Yeah. And there's maybe double the amount of those with just like velocity and then with the actual dwell cursor. So I would like all of it. I want as much control over my environment as possible, especially advanced mode, you.
Speaker A: Know, like in like, there's men usually this basic mode, and you're like one of those folks like the power user, advanced. Yeah, that's got it.
Speaker C: That's what I want. I want as much control over this as possible. So, yeah, that's really all I can ask for. Just give me everything.
Speaker A: Has speech been useful? Just being able to talk also in addition to everything else?
Speaker C: Yeah. You mean like while I'm using it.
Speaker A: While you're using it, like speech to text.
Speaker C: Oh, yeah.
Speaker A: Or do you type or like. Because there's also a keyboard.
Speaker C: Yeah, so there's a virtual keyboard. That's another thing I would like to work more on, is finding some way to type or text in a different way. Right now it is like a dictation, basically, and a virtual keyboard that I can use with the cursor, but we've played around with fingerspelling, like sign language fingerspelling, and that seems really promising. So I have this thought in my head that it's going to be a very similar learning curve that I had with the cursor, where I went from attempted movement to imagine movement at one point. I have a feeling this is just my intuition that at some point I'm going to be doing fingerspelling and I won't need to actually attempt to fingerspell anymore, that I'll just be able to think the, like, letter that I want, and it'll pop up.
Speaker A: That will be epic. Yeah, that's challenging. That's hard. That's a lot of work for you to kind of take that leap, but that would be awesome.
Speaker C: And then, like, going from letters to words is another step. Like, you would go from, you know, right now it's fingerspelling of, like, just the sign language alphabeth. But if it's able to pick that up, then it should be able to pick up the whole sign language language. And so then if I could do something along those lines or just the sign language spelled word, if I can spell it at a reasonable speed and it can pick that up, then I would just be able to think that through and it would do the same thing. I don't see why not. After what I saw with the. With the cursor control, I don't see why it wouldn't work, but we'd have to play around with it more.
Speaker A: What was the process in terms of training yourself to go from attempted movement to imagine movement? How long does that take? So how long would this kind of process take?
Speaker C: Well, it was a couple weeks before it just happened upon me, but now that I know that that was possible, I think I could make it happen with other things. I think it would be much, much simpler.
Speaker A: Would you get an upgraded implant device?
Speaker C: Sure, absolutely. Whenever. Whenever they'll let me.
Speaker A: So you don't have any concerns? For you, the surgery experience, all of it was like, no regrets?
Speaker C: No.
Speaker A: So everything's been good so far?
Speaker C: Yep.
Speaker A: You just keep getting upgrades?
Speaker C: Yeah. I mean, why not? I've seen how much it's impacted my life already, and I know that everything from here on out going to get better and better. So I would love to. I would love to get the upgrade.
Speaker A: What future capabilities are you excited about?
Speaker D: Sort of.
Speaker A: Beyond this kind of telepathy is vision. Interesting. So for folks, for example, who are blind, so you're enabling people to see or for speech?
Speaker C: Yeah. There's a lot that's very, very cool about this. I mean, we're talking about the brain, so there's like, this is just motor cortex stuff. There's so much more that can be done. The vision one is fascinating to me. I think that is going to be very, very cool. To give someone the ability to see for the first time in their life would just be. I mean, it might be more amazing than even helping someone like me. That just sounds incredible. The speech thing is really interesting. Being able to have some sort of real time translation and cut away. That language barrier would be really cool. Any sort of, like, actual impairments that it could solve, like, with speech, would be very, very cool. And then also, there are a lot of different disabilities that all originate in the brain, and you would be able to hopefully be able to solve a lot of those. I know there's already stuff to help people with seizures that can be implanted in the brain. This would do, I imagine, the same thing. And so you could do something like that. I know that even someone like Joe Rogan has talked about the possibilities with being able to stimulate the brain in different ways. I'm not sure. I'm not sure what, you know, like, how ethical a lot of that would be. That's beyond me, honestly. But I know that there's a lot that can be done when we're talking about the brain of and being able to go in and physically make changes to help people or to improve their lives. So I'm really looking forward to everything that comes from this, and I don't think it's all that far off. I think a lot of this can be implemented within my lifetime, assuming that I live a long life.
Speaker A: What you're referring to is things like, people suffering from depression or things of that nature potentially getting help.
Speaker C: Yeah. Flip a switch like that. Make someone happy. I know. I think Joe has talked about it more in terms of, like, you want to experience, like, what a drug trip feels like. Like, you want to experience what it'd be like to be on, of course. Oh, yeah. Mushrooms or something like that. DMT. Like, you can just flip that switch in the brain. My buddy Bane has talked about being able to, like, wipe parts of your memory and re experience things that, like, for the first time, like your favorite movie or your favorite book. Like, just wipe that out real quick and then refall in love with Harry Potter or something. I told him, I was like, I don't know how I feel about, like, people being able to just wipe parts of your memory. That seems a little sketchy to me. He's like, they're already doing it, so.
Speaker A: Sounds legit. I would love memory replay, just, like, actually, like, high resolution replay of all memories.
Speaker C: Yeah, I saw an episode of Black Mirror about that once. I don't think I want it.
Speaker A: Yeah, so black mirror always kind of considers the worst case, which is important. I think people don't consider the best case or the average case enough. I don't know what it is about us humans. We want to think about the worst possible thing. We love drama. Yeah, it's like, how is this new technology going to kill everybody? We just love that.
Speaker C: Okay.
Speaker A: Like, yes. Let's watch.
Speaker C: Hopefully people don't think about that too much with me. It'll ruin a lot of my plans.
Speaker A: Yeah, I assume you're gonna have to take over the world. I mean, I love your twitter. You. You tweeted, I'd like to make jokes about hearing voices in my head since getting the neural link, but I feel like people would take it the wrong way. Plus, the voices in my head told me not to.
Speaker C: Yeah, yeah, yeah.
Speaker A: Please never stop. So you're talking about Optimus. Is that something you would love to be able to do to control the robotic arm or the entirety of optimus?
Speaker C: Oh, yeah, for sure. For sure. Absolutely.
Speaker A: You think there's something, like, fundamentally different about just being able to physically interact with the world?
Speaker C: Yeah. Oh, 100%. This. I know. Another thing with, like, being able to, like, give people the ability to, like, feel sensation and stuff too, by going in with the brain and having a neuro, like, maybe do that, that could be something that. That could be transferred through the Optimus as well. There's all sorts of really cool interplay between that. And then also, like you said, just physically interacting. I mean, 99% of the things that I can't do myself, obviously, I need a caretaker for someone to physically do things for me. If an optimist robot could do that, like, I could live an incredibly independent life and not be such a burden on those around me, and that would change the way people like me live, at least until whatever this is gets cured. But being able to interact with the world physically, like, that would just be amazing. And they're not just, like, for having it be a caretaker or something, but something like I talked about just being able to read a book. Imagine an optimist robot just being able to hold a book open in front of me, like, get that smell again. I might not be able to feel it at that point, or maybe I could again with the sensation and stuff, but there's something different about reading a physical book than staring at a screen or listening to an audiobook. I actually don't like audiobooks. I've listened to a ton of them at this point, but I don't really like them. I would much rather, like, read a physical copy.
Speaker A: So one of the things you would love to be able to experience is opening the book, bringing it up to you and to feel the touch of the paper.
Speaker C: Yeah. Oh, man. The touch, the smell. I mean, it's just something about the words on the page, and, you know, they've replicated, you know, that page color on, like, the kindle and stuff. Yeah, it's just not the same. Yeah. So just something as simple as that.
Speaker A: So one of the things you miss is touch?
Speaker C: I do, yeah. A lot of. A lot of things that I interact with in the world, like clothes or literally any physical thing that I interact with in the world. A lot of times, what people around me will do is they'll just come, like, rub it on my face. They'll, like, lay something on me so I can feel the weight. They will rub, you know, a shirt on me so I can feel fabric. Like, there's something very profound about touch, and it is. It's something that I miss a lot and something I would love to do again. We'll see.
Speaker A: What would be the first thing you do with a hand that can touch your mom? A hug after that. Right?
Speaker B: Yeah.
Speaker C: Yeah, I know. That's. It's one thing that I've. That ive asked God for basically every day since my accident was just being able to one day move, even if it was only my hand. So that way I could squeeze my moms hand or something just to show her how much I care and how much I love her and everything. Something along those lines. Being able to just interact with the people around me. Handshake, give someone a hug. I don't know, anything like that. Being able to help me eat, like, I'd probably get really fat, which would be a terrible, terrible thing.
Speaker A: Also beat bliss in chess. On a physical chessboard.
Speaker C: Yeah. Yeah. I mean, there are just so many upsides, you know? Any. Any way to find some way to feel like I'm bringing bliss down to my level.
Speaker A: Yeah.
Speaker C: He's just such an amazing guy, and everything about him is just so above and beyond that. Anything I can do to take him down a notch.
Speaker A: Yeah. Humble him a bit. He needs it. Okay. As he's sitting next to me, did you ever make sense of why God puts good people through such hardship?
Speaker C: Oh, man. I think it's all about understanding how much we need God. And I don't think that there's any light without the dark. I think that if all of us were happy all the time, there would be no reason to turn to God, ever. I feel like there would be no concept of good or bad. And I think that as much of the darkness and the evil that's in the world, it makes us all appreciate the good and the things we have so much more. And I think when I had my accident. One of the first things I said to one of my best friends, Washington, and this was within the first month or two after my accident. I said, everything about this accident has just made me understand and believe that God is real, and that there really is a God, basically, and that my interactions with him have all been real and worthwhile. And he said, if anything, seeing me go through this accident, he believes that there isn't a God. And it's a very different reaction. But I believe that it is a way for God to test us, to build our character, to send us through trials and tribulations, to make sure that we understand how precious he is and the things that he has given us and the time that he has given us, and then hopefully grow from all of that. I think that's a huge part of being here, is to not just, you know, have an easy life and do everything that's easy, but to step out of our comfort zones and really challenge ourselves, because I think that's how we grow.
Speaker A: What gives you hope about this whole thing we have going on? Human civilization?
Speaker C: Oh, man. I think people are my biggest inspiration. And even just being at Neuralink for a few months, looking people in the eyes and hearing their motivations for why they're doing this, it's so inspiring. And I know that they could be other places, at cushier jobs, working somewhere else, doing x, y, or z. That doesn't really mean that much, but instead, they're here, and they want to better humanity, and they want to better just the people around them, the people that they've interacted with in their life. They want to make better lives for their own family members who might have disabilities, or they look at someone like me and they say, you know, I can do something about that. So I'm going to. And it's always been what I've connected with most in the world are people. I've always been a people person, and I love learning about people, and I love learning how people developed and where they came from, and to see how much people are willing to do for someone like me when they don't have to, and they're going out of their way to make my life better. It gives me a lot of hope for just humanity in general, how much we care and how much we're capable of when we all get together and try to make a difference. And I know there's a lot of bad out there in the world, but there always has been and there always will be. And I think that that is, it shows human resiliency and it shows what we're able, what we're able to endure and how much. How much we just want to be there and help each other and how much satisfaction we get from that. Because I think that's one of the reasons that we're here, is just to help each other. And I don't know, that always gives me hope, is just realizing that there are people out there who still care and who want to help.
Speaker A: And thank you for being one such human being and continuing to be a great human being through everything you've been through, and being an inspiration to many people, to myself, for many reasons, including your epic, unbelievably great performance on Webgridgest. I will be training all night tonight.
Speaker C: To try, to try to catch up.
Speaker A: And I believe in you, that you can, once you come back. So sorry to interrupt with the Austin trip. Once you come back, eventually beat bliss.
Speaker C: Yeah, yeah, for sure. Absolutely.
Speaker A: I'm rooting for you though. The whole world is rooting for you. Thank you for everything you've done, man.
Speaker C: Thanks. Thanks man.
Speaker A: Thanks for listening to this conversation with Nolan Arbaugh and before that with Elon Musk, DJ saw, Matthew McDougall and bliss Chapmandhe. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Aldous Huxley. In the doors of perception, we live together. We act on and react to one another, but always and in all circumstances, we are by ourselves. The martyrs go hand in hand into the arena. They are crucified, alone embrace. The lovers desperately try to fuse their insulated ecstasies into a single self transcendence in vain but its very nature. Every embodied spirit is doomed to suffer and enjoy its solitude. Sensations, feelings, insights, fancies. All these are private and except through symbols and at second hand, incommunicable. We can pool information about experiences, but never the experiences themselves. From family to nation, every human group is a society of island universes. Thank you for listening and hope to see you next time.
