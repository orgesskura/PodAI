Transcription for Jim Keller： Moore's Law, Microprocessors, and First Principles ｜ Lex Fridman Podcast #70.mp3:
Full transcript: The following is a conversation with Jim Keller, legendary microprocessor engineer who has worked at AMD, Apple, Tesla, and now intel. He's known for his work on AMD K seven, K eight, K twelve, and Zen microarchitectures, Apple A four and a five processors, and co author of the specification for the X 86 64 instruction set and Hypertransport Interconnected. Hes a brilliant first principles engineer, an out of the box thinker, and just an interesting and fun human being to talk to. This is the artificial intelligence podcast. If you enjoy it, subscribe on YouTube, give it five stars on Apple Podcast, follow on Spotify, support it on Patreon, or simply connect with me on Twitter. Alexfridman spelled F R I D M A N. I recently started doing ads at the end of the introduction. I'll do one or two minutes after introducing the episode and never any ads in the middle that can break the. Flow of the conversation. I hope that works for you and doesn't hurt the listening experience. This show is presented by Cash app, the number one finance app in the app store. I personally use Cash app to send money to friends, but you can also use it to buy, sell and deposit bitcoin in just seconds. Cash app also has a new investing feature. You can buy fractions of a stock, say one dollar's worth no matter what the stock price is. Brokerage services are provided by Cash app Investing, a subsidiary of Square and member SIPC. I'm excited to be working with Cash app to support one of my favorite organizations called first. Best known for their first robotics and Lego competitions. They educate and inspire hundreds of thousands of students in over 110 countries and have a perfect rating at charity Navigator, which means that donated money is used to maximum effectiveness. When you get cash app from the App Store or Google Play and use code Lex podcast, you'll get $10. And cash app will also donate $10 to first, which again, is an organization that I've personally seen inspire girls and boys to dream of engineering a better world. And now, here's my conversation with Jim Keller. What are the differences and similarities between the human brain and a computer with the microprocessor at its core? Let's start with the philosophical question, perhaps. Well, since people don't actually understand how human brains work, I think that's true. I think that's true. So it's hard to compare them. Computers are, you know, there's really two things. There's memory and there's computation. And to date, almost all computer architectures are global memory, which is a thing and then computation where you pull data and you do relatively simple operations on it and write data back. So it's decoupled in modern computers and you think in the human brain everything's a mesh, a mess that's combined together. What people observe is there's some number of layers of neurons which have local and global connections and information is stored in some distributed fashion. And people build things called neural networks in computers where the information is distributed in some kind of fashion. You know, there's a mathematics behind it. I don't know that the understanding of that is super deep. The computations we run on those are straightforward computations. I don't believe anybody has said a neuron does this computation. So to date it's hard to compare them, I would say. So let's get into the basics before we zoom back out. How do you build a computer from scratch? What is a microprocessor? What is the microarchitecture? What's an instruction set architecture, maybe even as far back as what is a transistor. So the special charm of computer engineering is there's a relatively good understanding of abstraction layers. So down at the bottom you have atoms and atoms get put together in materials like silicon or dope, silicon or metal, and we build transistors on top of that. We build logic gates, right. And then functional units like an adder of subtractor, an instruction parsing unit, and then we assemble those into, you know, processing elements. Modern computers are built out of probably ten to 20 locally organic processing elements or coherent processing elements. And then that runs computer programs. So there's abstraction layers and then software, there's an instruction set you run and then there's assembly language C, C, Java, JavaScript. There's abstraction layers essentially from the atom to the data center. So when you build a computer, first there's a target, like what's it for? Like how fast does it have to be? Which today there's a whole bunch of metrics about what that is. And then in an organization of 1000 people who build a computer, there's lots of different disciplines that you have to operate on. Does that make sense? So there's a bunch of levels, abstraction of in organization, I can tell. And in your own vision there's a lot of brilliance that comes in at every one of those layers. Some of it is science, some of it is engineering, some of it is art. What's the most, if you could pick favorites, what's the most important, your favorite layer on these layers of obstructions? Where does the magic entertain this hierarchy. I don't really care. That's the fun. You know, I'm somewhat agnostic to that. So I would say for relatively long periods of time, instruction sets are stable. So the x 86 instruction set, the arm instruction set. What's an instruction set? So it says how do you encode the basic operations, load, store, multiply, add, subtract, conditional branch. You know, there aren't that many interesting instructions. Like if you look at a program and it runs, you know, 90% of the execution is on 25 opcodes, you know, 25 instructions and those are stable, right? What does it mean, stable? Intel architecture has been around for 25 years. It works. It works. And that's because the basics, you know are defined a long time ago, right? Now, the way an old computer ran is you fetched instructions and you executed them in order. Do the load, do the add, do the compare. The way a modern computer works is you fetch large numbers of instructions, say 500, and then you find the dependency graph between the instructions. And then you execute in independent units those little micrographs. So a modern computer, like people like to say computers should be simple and clean. But it turns out the market for simple, complete, clean, slow computers is zero. Right? We don't sell any simple, clean computers. Now you can, there's how you build it can be clean, but the computer people want to buy, that's, say, in a phone or a data center, fetches a large number of instructions, computes the dependency graph and then executes it in a way that gets the right answers. And optimizes that graph somehow. They run deeply out of order. And then there's semantics around how memory ordering works and other things work. So the computer has a bunch of bookkeeping tables that says what order studies operations finish in or appear to finish in. But to go fast, you have to fetch a lot of instructions and find all the parallelism. Now there's a second kind of computer which we call GPU's today, and I call it the difference. There's found parallelism. Like you have a program with a lot of dependent instructions. You fetch a bunch and then you go figure out the dependency graph and you issues instructions out of order. That's because you have one serial narrative to execute which in fact can be done out of order. Did you call it a narrative? Yeah. Wow. So. Yeah, so humans think in serial narrative. So read a book, right? There's a sentence after sentence after sentence, and there's paragraphs. Now you could diagram that. Imagine you diagrammed it properly and you said, which sentences could be read in any order without changing the meaning. Right. That's a fascinating question to ask of a book. Yeah. Yeah. You could do that, right? So, some paragraphs could be reordered. Some sentences can be reordered. You could say he is tall and smart and x, right? And it doesn't matter the order of tall and smart. But if you say the tall man is wearing a red shirt, what colors? You know, like, you can create dependencies, right? And so GPU's, on the other hand, run simple programs on pixels, but you're given a million of them, and the first order, the screen you're looking at doesn't care which order you do it in. So I call that, given parallelism, simple narratives around the large numbers of things where you can just say it's parallel because you told me it was. So found parallelism, where the narrative is sequential, but you discover, like, little pockets of parallelism versus, turns out, large pockets of parallelism. Large. So how hard is it to discuss? Well, how hard is it? That's just transistor count, right? So, once you crack the problem, you say, here's how you fetch ten instructions at a time. Here's how you calculated dependencies between them. Here's how you describe the dependencies. Here's, you know, these are pieces, right? So, once you describe the dependencies, then it's just a graph, sort of. It's an algorithm defines what is that. I'm sure there's a graph theory, theoretical answer here that's solved. But in general, programs, modern programs that human beings write, how much found parallelism is there in ten x? What does ten x mean? So, when you execute it in order versus. You would get what's called cycles per instruction, and it would be about three instructions, three cycles per instruction because of the latency of the operations and stuff. And a modern computer executes it like 0.20.25 cycles per instruction. So it's about, we today find ten x. And there's two things. One is the found parallelism in the narrative, right? And the other is to predictability of the narrative, right? So certain operations, say, do a bunch of calculations, and if greater than one, do this, else do that. That decision is predicted in modern computers to high 90% accuracy. So branches happen a lot. So, imagine you have. You have a decision to make every six instructions, which is about the average, right? But you want to fetch 500 instructions, figure out the graph, and execute them all in parallel. That means you have, let's say, if you fix 600 instructions and it's every six, you have to fetch, you have to predict 99 out of 100 branches correctly for that window to be effective. Okay, so parallelism. You can't parallelize branches, or you can pretty you. What does predictive branch mean? What does predictive branch? So imagine you do a computation over and over. You're in a loop. So while n is greater than one do, and you go through that loop a million times. So every time you look at the branch, you say, it's probably still greater than one. And you're saying you could do that accurately? Very accurately. Modern computer. My mind is blown. How the heck do you do that? Wait a minute. Well, you want to know? This is really sad. 20 years ago, yes. You simply recorded which way the branch went last time and predicted the same thing. Right. Okay, what's the accuracy of that? 85%. So then somebody said, hey, let's keep a couple of bits and have a little counter. So when it predicts one way, we count up and then pins. So say you have a three bit counter. So you count up and then you count down. And if it's, you know, you can use the top bit as the sign bit. So you have a signed two bit number. So if it's greater than one, you predict taken and less than one, you predict not taken. Right. Or less than zero, whatever the thing is. And that got us to 92%. Oh, okay. No, it gets better. This branch depends on how you got there. So if you came down the code one way, you're talking about Bob and Jane, right? And then said, does Bob like Jane? It went one way, but if you're talking about Bob and Jill, does Bob like, change? You go a different way. So that's called history. So you take the history and encounter. That's cool. But that's not how anything works today. They use something that looks a little like a neural network. So modern, you take all the execution flows and then you do basically deep pattern recognition of how the program is executing. And you do that multiple different ways, and you have something that chooses what the best result is. There's a little supercomputer inside the computer that's trying to predict, calculates which way branches go. So the effective window that it's worth finding grass and gets bigger. Why was that going to make me sad? Because that's amazing. It's amazingly complicated. Oh, well, well, here's the funny thing. So to get to 85% took 1000 bits, to get to 99% takes tens of megabits. So this is one of those to get the result, you know, to get from a window of, say, 50 instructions to 500. It took three orders of magnitude, or four orders of magnitude, four bits. Now, if you get the prediction of a branch wrong, what happens then? Flush the pipe. You flush the pipe. So it's just the performance cost, but. It gets even better. Yeah. So we're starting to look at stuff that says so executed down this path. And then you had two ways to go, but far away, there's something that doesn't matter which path you went. So you took the wrong path, you executed a bunch of stuff. Then you had the mis predicting, you backed it up, but you remembered all the results you already calculated. Some of those are just fine. Like, if you read a book and you misunderstand a paragraph, your understanding of the next paragraph sometimes is invariant to that understanding. Sometimes it depends on it. And you can kind of anticipate that invariance. Yeah, well, you can keep track of whether the data changed. And so when you come back through a piece of code, should you calculate it again or do the same thing? Okay, how much of this is art and how much of it is science? Because it sounds pretty complicated. So, well, how do you describe a situation? So imagine you come to a point in the road where you have to make a decision, and you have a bunch of knowledge about which way to go. Maybe you have a map, so you want to go the shortest way, or do you want to go the fastest way, or you want to take the nicest road. There's some set of data. So imagine you're doing something complicated, like building a computer. And there's hundreds of decision points, all with hundreds of possible ways to go. And the ways you pick interact in a complicated way. Right. And then you have to pick the right spot. Right. So those are your science. I don't know. You avoided the question you just described. The Robert Frost poem wrote less taken. I described the Robin Frost problem. What we do as computer designers, it's all poetry. Okay, great. Yeah. I don't know how to describe that, because some people are very good at making those intuitive leaps. It seems like this, combinations of things. Some people are less good at it, but they're really good at evaluating the alternatives. Right. And everybody has a different way to do it, and some people can't make those leaps, but they're really good at analyzing it. So when you see computers are designed by teams of people who have very different skill sets, and a good team has lots of different kinds of people, I suspect you would describe some of them as artistic, but not very many, unfortunately or fortunately. Fortunately, well, you know, computer design is hard. It's 99% perspiration, and the 1% inspiration is really important. But you still need the 99. Yeah, you got to do a lot of work. And then there are interesting things to do at every level of that stack. At the end of the day, if you run the same program multiple times, does it always produce the same result? Is there some room for fuzziness there? That's a math problem. So if you run a correct c program, the definition is every time you run it, you get the same answer. Yeah, well, that's a math statement. Well, that's a language definitional statement. So for years, when people did, when we first did 3d acceleration of graphics, you could run the same scene multiple times and get different answers. And then some people thought that was okay, and some people thought it was a bad idea. Then when the HPC world used GPU's for calculations, they thought it was a really bad idea. Now, in modern AI stuff, people are looking at networks where the precision of the data is low enough that the data is somewhat noisy. The observation is the input data is unbelievably noisy, so why should the calculation be not noisy? And people have experimented with algorithms that, say, can get faster answers by being noisy. Like, as the network starts to converge, if you look at the computation graph, it starts out really wide and it gets narrower, and you can say, is that last little bit that important, or should I start the graph on the next rev before we whittle it all the way down to the answer? So you can create algorithms that are noisy. Now, if you're developing something and every time you run it, you get a different answer, it's really annoying. And so most people think, even today, every time you run the program, you get the same answer. No, I know, but the question is, that's the formal definition of a programming language. There is a definition of languages that don't get the same answer, but people who use those, you always want something because you get a bad answer, and then you're wondering, is it because of something in the algorithm or because of this? And so everybody wants a little switch that says, no matter what, do it deterministically. And it's really weird because almost everything going into modern calculations is noisy. So why are the answers have to be so clear? It's, where do you stand? I design computers for people who run programs. So if somebody says, I want a deterministic answer, like most people want that. Can you deliver a deterministic answer? I guess, is the question, like, when you? Yeah, hopefully. Sure. What people don't realize is you get a deterministic answer, even though the execution flow is very undeterministic. So you run this program 100 times. It never runs the same way twice, ever. And the answer, it arrives at the. Same answer, but it gets the same answer every time. It's just amazing. Okay. You've achieved, in the eyes of many people, legend, status as a chip architect. What design creation are you most proud of? Perhaps because it was challenging, because of its impact, or because of the set of brilliant ideas that. That were involved in bringing it to life. I find that description odd. And I have two small children, and I promise you, they think it's hilarious, this question. Yeah. So I do it for them. I'm really interested in building computers, and I've worked with really, really smart people. I'm not unbelievably smart. I'm fascinated by how they go together, both as a. As a thing to do and as an endeavor that people do. How people and computers go together. Yeah. Like how people think and build a computer. And I find sometimes that the best computer architects aren't that interested in people or the best people. Managers aren't that good at designing computers. So the whole stack of human beings is fascinating. So the managers, individual engineers. Yeah. I said, I realized after a lot of years of building computers, where you sort of build them out of transistors, logic gates, functional units, computational elements, that you could think of people the same way. So people are functional units, and then you could think of organizational design as a computer architectural problem. And then it was like, oh, that's super cool, because the people are all different, just like the computational elements are all different, and they like to do different things. And so I had a lot of fun, like reframing how I think about organizations. Just like with computers, we were saying execution paths. You can have a lot of different paths that end up at a Ydez at the same good destination. So, what have you learned about the human abstractions, from individual functional human units to the broader organization? What does it take to create something special? Well, most people don't think simple enough. All right, so, do you know the difference between a recipe and the understanding? There's probably a philosophical description of this. So, imagine you're gonna make a loaf of bread. Yeah. The recipe says, get some flour, add some water, add some yeast, mix it up, let it rise, put it in a pan, put it in the oven. It's a recipe. Right. Understanding bread, you can understand biology. Supply chains, you know, grain grinders. Yeast physics, you know, thermodynamics. Like, there's so many levels of understanding there. And then when people build and design things, they frequently are executing some stack of recipes, right. And the problem with that is the recipes all have limited scope. Like, if you have a really good recipe book for making bread, it won't tell you anything about how to make an omelet, right? But if you have a deep understanding of cooking, right, then bread, omelets, sandwich. There's a different way of viewing everything. And most people, when you get to be an expert at something, you're hoping to achieve deeper understanding, not just a large set of recipes to go execute. And it's interesting to walk groups of people, because executing recipes is unbelievably efficient. If it's what you want to do, if it's not what you want to do, you're really stuck. And that difference is crucial. And everybody has a balance of, let's say, deeper understanding the recipes. And some people are really good at recognizing when the problem is to understand something deeply, deeply. Does that make sense? It totally makes sense. Does every stage of development deep understanding on the team needed? Well, this goes back to the art versus science question. Sure. If you constantly unpacked everything for deeper understanding, you never get anything done. Right. And if you don't unpack understanding when you need to, you'll do the wrong thing. And then at every juncture, like human beings are these really weird things, because everything you tell them has a million possible outputs. Right. And then they all interact in a hilarious way. Yeah, it's very nice. And then having some intuition about what do you tell them? What do you do? When do you intervene? When do you not? It's. It's complicated. Right. So it's. It's, you know, essentially computationally unsolvable. Yeah, it's the intractable problem. Sure, humans are a mess, but with deep understanding, do you mean also sort of fundamental questions of things like, what is a computer? Or why? The why question is, why are we even building this of purpose? Or do you mean more going towards the fundamental limits of physics, really getting into the core of the science? Well, in terms of building a computer, think a little simpler. So, common practice is you build a computer, and then when somebody says, I want to make it 10% faster, you'll go in and say, all right, I need to make this buffer bigger, and maybe I'll add an ad unit, or I have this thing that's three instructions wide. I'm going to make it four instructions wide. And what you see, is each piece gets incrementally more complicated, and then at some point you hit this limit. Adding another feature or buffer doesn't seem to make it any faster. And then people will say, well, that's because it's a fundamental limit. And then somebody else will look at it and say, well, actually the way you divided the problem up and the way the different features are interacting is limiting you, and it has to be rethought. Rewritten. Right. So then you refactor it and rewrite it. And what people commonly find is the rewrite is not only faster, but half as complicated from scratch. Yes. So how often in your career purchase have you seen as needed, maybe more generally, to just throw the whole out, the whole thing out? This is where I'm on one end of it every three to five years. Which end are you on? Wait. Rewrite more often. Rewrite and three to five years is. If you want to really make a lot of progress on computer architecture, every five years you should do one from scratch. So where does the x 86 64 standard come in? How often do you. I was the co author of that back in 98. That's 20 years ago. Yeah. So that's still around. The instruction set itself has been extended quite a few times. Yes. And instruction sets are less interesting than the implementation underneath. There's been, on x 86 architecture, Intel's designed a few, aim designed a few very different architectures. And I don't want to go into too much of the detail about how often, but there's a tendency to rewrite it every, you know, ten years, and it really should be every five. So you're saying you're an outlier in that sense? In the. Rewrite more often. Rewrite more often. Well, and here's scary. Yeah, of course. Well, scary to who? To everybody involved, because like you said, repeating the recipe is efficient. Companies want to make money, no individual engineers want to succeed. So you want to incrementally improve, increase the buffer from three to four will increase. So this is where you get into diminishing return curves. I think Steve Jobs said this. Right? So every, you have a project and you start here and it goes up and you have dimitri and return. And to get to the next level, you have to do a new one. And the initial starting point will be lower than the old optimization point, but it'll get higher. So now you have two kinds of fear, short term disaster and long term disaster. And you're grown up. Right? Like people with a quarter by quarter business objective are terrified about changing everything and people who are trying to run a business or build a computer for a long term objective know that the short term limitations block them from the long term success. So if you look at leaders of companies that had really good long term success, every time they saw that they had to redo something, they did. And so somebody has to speak up. Or you do multiple projects in parallel, like, you optimize the old one while you build a new one. But the marketing guys, they're always like, promise me that the new computer is faster on every single thing. And the computer architect says, well, the new computer will be faster on the average, but there's a distribution of results and performance, and you'll have some outliers that are slower, and that's very hard because they have one customer who cares about that one. So, speaking of the long term, for over 50 years now, Moore's Law has served for me and millions of others as an inspiring beacon. What kind of amazing future brilliant engineers can build. I'm just making your kids laugh all of today. That was great. So, first, in your eyes, what is Moore's law, if you could define for people who don't know? Well, the simple statement was, from Gordon Moore, was double the number of transistors every two years, something like that. And then my operational model is we increase the performance of computers by two x every two or three years. And it's wiggled around substantially over time. And also in how we deliver performance has changed. The foundational idea was two x to transistors every two years. The current cadence is something like, they call it a shrink factor, like 0.6 every two years, which is not 0.5. But that's referring strictly again to the. Original definition of transistor count and shrink factors. Just getting them smaller, smaller and smaller. Well, it's for a constant chip area. If you make the transistor smaller by 0.6, then you get one over 0.6 more transistors. So can you linger on it a little longer? What's a broader, what do you think should be the broader definition of Moore's law? We mentioned how you think of performance just broadly. What's a good way to think about Moore's law? Well, first of all, so I've been aware of Moore's law for 30 years, in which sense? Well, I've been designing computers for 40. You're just watching it before your eyes kind of thing. Well, and somewhere where I became aware of it, I was also informed that Moore's law was going to die in ten to 15 years. And I thought that was true at first, but then after ten years, it was going to die in ten to 15 years. And then at one point, it was going to die in five years, and then it went back up to ten years. And at some point, I decided not to worry about that particular prognostication for the rest of my life, which is fun. And then I joined intel, and everybody said, moore's law is dead. And I thought, that's sad, because it's the Moore's law company and it's not dead, and it's always been going to die. And humans like these apocryphal kind of statements, like, we'll run out of food or run out of air or run out of room or run out of, you know, something. Right? But it's still incredible that it's lived for as long as it has. And yes, there's many people who believe now that Moore's law is dead, you. Know, they can join the last 50 years of people who had the same. Yeah, there's a long tradition, but why do you think, if you can try to understand it, why do you think it's not dead currently? Just think. People think. Moore's law is one thing. Transistors get smaller, but actually, under the sheet, there's literally thousands of innovations. And almost all those innovations have their own diminishing return curves. So if you graph it, it looks like a cascade of diminishing return curves. I don't know what to call that, but the result is an exponential curve, at least it has been. And we keep inventing new things. So if you're an expert in one of the things on a diminishing return curve, right, and you can see its plateau, you will probably tell people, well, this is done. Meanwhile, some other pile of people are doing something different. So that's just normal. So then there's the observation of how small could a switching device be? So a modern transistor is something like 1000 by 1000 by 1000 atoms, and you get quantum effects down around two to ten atoms. So you can imagine a transistor as small as ten by ten by ten. So that's a million times smaller. And then the quantum computational people are working away at how to use quantum effects. So. A thousand by thousand by thousand atoms, that's a really clean way of putting it. Well, a fan, like a modern transistor, if you look at the fan, it's like 120 atoms wide, but we can make that thinner. And then there's a gate wrapped around it, and then there's spacing there's a whole bunch of geometry, and a competent transistor designer could count both atoms in every single direction. Like, there's techniques now to already put down atoms in a single atomic layer, and you can place atoms if you want to. It's just, you know, from a manufacturing process, if placing an atom takes ten minutes and you need to put ten to the 23rd atoms together to make a computer, it would take a long time. So the methods are both shrinking things and then coming up with effective ways to control what's happening. Manufacture stably and cheaply. Yeah. So the innovation stack's pretty broad. There's equipment, there's optics, there's chemistry, there's physics, there's materials science, there's metallurgy. There's lots of ideas about when you put different materials together, how do they interact? Are they stable? Is stable over temperature? Are there repeatable? You know, there's, like, literally thousands of technologies involved. But just for the shrinking, you don't think we're quite yet close to the fundamental limits of physics. I did a talk on Moore's law, and I asked for a roadmap to a path of 100. And after two weeks, they said, we only got to 5100. What? Sorry, 100 x shrink. 100 x shrink. We only got to 50 50. And I said, why don't you give it another two weeks? Well, here's the thing about Moore's law, right? So I believe that the next ten or 20 years of shrinking is going to happen right now, as a computer designer, you have two stances. You think it's going to shrink, in which case you're designing and thinking about architecture in a way that you'll use more transistors, or conversely, not be swamped by the complexity of all the transistors you get. You have to have a strategy, so. You'Re open to the possibility and waiting for the possibility of a whole new army of transistors ready to work. I'm expecting more transistors every two or three years by a number large enough that how you think about design, how you think about architecture has to change. Like, imagine you build brick buildings out of bricks, and every year, the bricks are half the size, or every two years. Well, if you kept building bricks the same way, you know, so many bricks per person per day, the amount of time to build a building would. Would go up exponentially. Right, right. But if you said, I know that's coming, so now I'm going to design equipment that moves bricks faster, uses them better, because maybe you're getting something out of the smaller bricks, more strength, thinner walls, you know, less material efficiency out of that. So once you have a roadmap with what's going to happen, transistors, they're going to get, we're going to get more of them. Then you design all this collateral around it to take advantage of it and also to cope with it. Like, that's the thing people don't understand. It's like if I didn't believe in Moore's law and then Moore's law, transistors showed up, my design teams were all drowned. So what's the, what's the hardest part of this inflood of new transistors? I mean, even if you just look historically throughout your career, what's the thing? What fundamentally changes when you add more transistors in the task of designing an architecture? Well, there's two constants, right? One is people don't get smarter. By the way, there's some science showing that we do get smarter because of nutrition, whatever. Sorry to bring that. Plant effect. Yes. Yeah. Familiar with it. Nobody understands it. Nobody knows if it's still going on. So that's. Or whether it's real or not. But yeah, I sort of, anyway, but not explicitly. I would believe for the most part, people aren't getting much smarter. The evidence doesn't support it. That's right. And then teams can't grow that much. Right, right. So human beings, you know, we're really good in teams of ten, you know, up to teams of 100, they can know each other. Beyond that, you have to have organizational boundaries. So those are pretty hard constraints. So then you have to divide and conquer. As the designs get bigger, you have to divide it into pieces. The power of abstraction layers is really high. We used to build computers out of transistors. Now we have a team that turns transistors into logic cells and another team that turns them into functional units, another one that turns into computers. So we have abstraction layers in there. And you have to think about when do you shift gears on that. We also use faster computers to build faster computers. So some algorithms run twice as fast on new computers, but a lot of algorithms are n squared. So, you know, a computer with twice as many transistors and it might take four times as long to run. So you have to refactor the software. Like simply using faster computers to build bigger computers doesn't work. So you have to think about all these things. So in terms of computing performance and the exciting possibility that more powerful computers bring is shrinking. The thing we just been talking about one of the, for you, one of the biggest exciting possibilities of advancement in performance, or is there other directions that you're interested in, like in the direction of sort of enforcing given parallelism, or like doing massive parallelism in terms of many, many cpu's, stacking, cpu's on top of each other, that kind of parallelism, or any kind of, well, think about. It a different way. So old computers, slow computers, you said a equal b plus c times d, pretty simple. And then we made faster computers with vector units, and you can do proper equations and matrices, and then modern AI computations, or convolutional neural networks, where you convolve one large dataset against another. And so there's sort of this hierarchy of mathematics, from simple equation, to linear equations, to matrix equations, to deeper kind of computation. And the data sets are getting so big that people are thinking of data as a topology problem. You know, data is organized in some immense shape, and then the computation, which sort of wants to be, get data from immense shape and do some computation on it. So the, what computers have allowed people to do is have algorithms go much, much further. So that paper you, you referenced, the Sutton paper, they talked about, you know, like when AI started, it was apply rule sets to something. That's a very simple computational situation. And then when they did first chess thing, they solved deep searches. So have a huge database of moves and results, deep search, but it's still just a search right now. We take large numbers of images and we use it to train these weight sets that we convolve across. It's a completely different kind of phenomena. We call that AI, and now they're doing the next generation. And if you look at it, they're going up this mathematical graph, right? And then computations, both computation and data set support going up that graph. Yeah. The kind of computation, though, might. I mean, I would argue that all of it is still a search, right? Just like you said, a topology problem of datasets. You're searching the data sets for valuable data, and also the actual optimization of neural networks is a kind of search for the. I don't know, if you had looked at the inner layers of finding a cat, it's not a search, it's a set of endless projections. So a projection, here's a shadow of this phone, and then you can have a shadow of that onto something, a shadow on that of something. If you look in the layers, you'll see this layer actually describes pointy ears and round eye ness and fuzziness. But the computation to tease out the attributes is not search. Right? I mean, like the inference part might be search, but the training is not search. And then in deep networks, they look at layers and they don't even know it's represented. And yet if you take the layers out, it doesn't work. Okay, so I don't think it's search. All right, well, but you have to talk to a mathematician about what that actually is. We could disagree, but it's just semantics. I think it's not, but it's certainly not. I would say it's absolutely not semantics, but. Okay. All right, well, if you want to go there. So, optimization to me is search, and we're trying to optimize the ability of a neural network to detect cad ears. And the difference between chess and the space, the incredibly multi dimensional, hundred thousand dimensional space that neural networks are trying to optimize over is nothing like the chess board database. So it's a totally different kind of thing. Okay. In that sense, you can say, yeah, it loses. I can see how you might say if you. The funny thing is it's the difference between given search space and found search space. Right? Exactly. Yeah. Maybe that's a different way to. That's a beautiful way to put it. Okay, but you're saying, what's your sense in terms of the basic mathematical operations and the architectures compute hardware that enables those operations? Do you see the cpu's of today still being a really core part of executing those mathematical operations? Yes. Well, the operations continue to be add, subtract, load, store, compare and branch. It's remarkable. So it's interesting that the building blocks of computers are transistors under that, atoms. So you got atoms, transistors, logic gates, computers, functional units, and computers. The building blocks of mathematics at some level, are things like adds and subtracts and multiplies. The space mathematics can describe is, I think, essentially infinite, but the computers that run the algorithms are still doing the same things. Now, a given algorithm might say, I need sparse data, or I need 32 bit data, or I need a convolution operation that naturally takes eight bit data, multiplies it, and sums it up a certain way. So the data types in tensorflow imply an optimization set. But when you go right down and look at the computers, it's Ann and and or gates doing adds and multiplies. Like, like that hasn't changed much. Now the quantum researchers think they're going to change that radically. And then there's people who think about analog computing, because you look in the brain and it seems to be more analog ish. You know that maybe there's a way to do that more efficiently. But we have a million x on computation, and I don't know the relationship between computational, let's say, intensity and ability to hit mathematical abstractions. I don't know anybody's described that. But just like you saw in AI, you went from rule sets to simple search to complex search, to, say, found search, like those are orders of magnitude more computation to do. And as we get the next two orders of magnitude, like a friend Raja Godori said, every order of magnitude changes the computation. It fundamentally changes what the computation is doing. Yeah. Oh, you know the expression, the difference in quantity is a difference in kind? The difference between ant and anthill, or neuron and brain. There's indefinable place where the quantity changed the quality. We've seen that happen in mathematics multiple times, and my guess is it's going to keep happening. So your sense is that if you focus head down and shrinking the transistor. Well, it's not just head down. We're aware of the software stacks that are running and the computational loads of. We're kind of pondering, what do you do with a petabyte of memory that wants to be accessed in a sparse way and have the kind of calculations AI programmers want? So there's a dialogue interaction, but when you go in the computer chip, you find adders and subtractors and multipliers. So if you zoom out, then with, as you mentioned, Ray Sutton, the idea that most of the development in the last many decades in AI research came from just leveraging computation and just simple algorithms waiting for the computation to improve. Well, software guys have a thing that they call it the problem of early optimization. So you write a big software stack, and if you start optimizing, the first thing you write, the odds of that being the performance limiter is low. But when you get the whole thing working, can you make it two x faster by optimizing the right things? Sure. While you're optimizing that, could you've written a new software stack, which would have been a better choice? Maybe now you have creative tension, but. The whole time, as you're doing the writing, that's the software we're talking about. The hardware underneath gets faster. This goes back to the Moore's law. If Moore's law is going to continue, then your AI research should expect that to show up, and then you make a slightly different set of choices. Then we've hit the wall, nothing's going to happen. And from here, it's just us rewriting algorithms like that seems like a failed strategy for the last 30 years of Moore's law's death. Can you just linger on it? I think you've answered it, but I'll just ask the same dumb question over and over. Why do you think Moore's law is not going to die? Which is the most promising, exciting possibility of why it won't die in the next 510 years. So is it the continued shrinking of the transistor, or is it another s curve that steps in and it totally. Sort of, well, shrinking the transistor is literally thousands of innovations. Right. So there's. So there's a whole bunch of s curves just kind of running their course and being reinvented. And I. New things. You know, the semiconductor fabricators and technologists have all announced what's called nanowire. So they took a fin, which had a gate around it, and turned that into little wires, so you have better control of that, and they're smaller. And then from there, there's some obvious steps about how to shrink that. The metallurgy around wire stacks and stuff has very obvious abilities to shrink. And, you know, there's a whole combination of things there to do. Your sense is that we're gonna get a lot if this innovation formed just that. Shrinking. Yeah. Like a factor of one hundred's a lot. Yeah. I would say that's incredible. And it's totally. It's only ten or 15 years now. You're smarter, you might know, but to me, it's totally unpredictable of what that hundred x would bring in terms of the nature of the computation that people would be. Yeah. Are you familiar with Bell's law? So, for a long time, it was mainframes, minis, workstation, PC, mobile. Moore's law drove faster, smaller computers. And then when we were thinking about Moore's law, Raja Guduri said, every ten x generates a new computation. So scalar vector matrix. Topological computation. Right. And if you go look at the industry trends, there was mainframes and then mini computers and NPC's, and then the Internet took off, and then we got mobile devices, and now we're building 5g wireless with one millisecond latency. And people are starting to think about the smart world where everything knows you, recognizes you, the transformations are going to be, like, unpredictable. How does it make you feel that you're one of the key architects of this kind of future? So you're not. We're not talking about the architects of the high level people who build the angry Bird apps and snaps angry bird apps. Who knows? I'm gonna take a stand of the universe. I'm gonna take a stand at that. And the attention distracting nature of mobile phones. I'll take a stand. But anyway, in terms of, I think that matters much. The side effects of smartphones or the attention distraction. Which part? Well, who knows where this is all leading? It's changing so fast. My parents used to yell at my sisters for hiding in the closet with a wired phone with a dial on it. Stop talking to your friends all day. Now my wife yells at my kids for talking to her friends all day on text. It looks the same to me. It's always echoes of the same thing. You are one of the key people architecting the hardware of this future. How does that make you feel? Do you feel responsible? Do you feel excited? So we're in a social context. So there's billions of people on this planet. There are literally millions of people working on technology. I feel lucky to be doing what I do and getting paid for it. There's an interest in it, but there's so many things going on in parallel. It's like the actions are so unpredictable. If I wasn't here, somebody else would do it. The vectors of all these different things are happening all the time. There's, I'm sure, some philosopher or metaphhilosophers wondering about how we transform our world. You can't deny the fact that these tools, whether these tools are changing our world. That's right. So do you think it's changing for the better? I read this thing recently. It said the two disciplines with the highest GRe scores in college are physics and philosophy. And they're both sort of trying to answer the question, why is there anything? And the philosophers are on the theological side, and the physicists are obviously on the material side. And there's 100 billion galaxies with 100 billion stars. It seems, well, repetitive at best. So there's on our way to 10 billion people. I mean, it's hard to say what it's all for, if that's what you're asking. Yeah, I guess I. Things do tend to significantly increases in complexity, and I'm curious about how computation, like our world, our physical world, inherently generates mathematics. It's kind of obvious, right? So we have XYZ coordinates. You take a sphere, you make it bigger, you get a surface that falls, you know, grows by r squared like it generally generates mathematics. And the mathematicians and the physicists have been having a lot of fun talking to each other for years, and computation has been let's say, relatively pedestrian, like computation in terms of mathematics has been doing binary algebra while those guys have been gallivanting through the nether realms of possibility. Now, recently the computation lets you do mathematical computations that are sophisticated enough that nobody understands how the answers came out. Machine learning. Machine learning. It used to be you get dataset, you guess at a function. The function is considered physics if it's predictive of new functions, new data sets. Modern, you can take a large data set with no intuition about what it is and use machine learning to find a pattern that has no function. And it can arrive at results that I don't know if they're completely mathematically describable. So computation has kind of done something interesting compared to a equal b plus. C. There's something reminiscent of that step from the basic operations of addition to taking a step towards neural networks that's reminiscent of what life on earth at its origins was doing. Do you think we're creating sort of the next step in our evolution in creating artificial intelligence systems that will? I don't know. I mean, there's so much in the universe already, it's hard to say where. We stand in this whole thing. Are human beings working on additional abstraction layers and possibilities? Yeah, it appears so. Does that mean that human beings don't need dogs? You know, no. There's so many things that are all simultaneously interesting and useful. Well, you've seen throughout your career, you've seen greater and greater level abstractions built in artificial machines, right? Do you think when you look at humans, do you think look of all life on earth as a single organism building this thing, this machine with greater and greater levels of abstraction? Do you think humans are the peak, the top of the food chain in this long arc of history on earth? Or do you think we're just somewhere in the middle? Are we, are we the basic functional operations of a cpu? Are we the C program, the Python program, or with the neural network, like. Somebody'S, you know, people have calculated, like how many operations does the brain do something? You know, I've seen the number ten to the 18th a bunch of times, arrived different ways. So could you make a computer that did ten to the 20th operations? Yes, sure. Do you think we're gonna do that now? Is there something magical about how brains compute things? I don't know. You know, my personal experiences is interesting because, you know, you think, you know how you think and then you have all these ideas and you can't figure out how they happened. And if you meditate, you know, the like what? What you can be aware of is interesting. So I don't know if brains are magical or nothing. You know, the physical evidence says no. Lots of people's personal experience says yes. So what would be funny is if brains are magical and yet we can make brains with more computation. You know, I don't know what to. Say about that, but do you think magic is an emergent phenomena? I have no explanation for it. Let me ask Jared Keller of what, in your view, is consciousness? What's consciousness? Yeah, like what? You know, consciousness love. Things that are these deeply human things that seems to emerge from our brain. Is that something that will be able to make encode in chips that get faster and faster and faster and faster? That's like a ten hour conversation. Nobody really knows. Can you summarize it in a couple of. Couple of words? Many people have observed that organisms run at lots of different levels. If you had two neurons, somebody said you'd have one sensory neuron and one motor neuron. So we move towards things and away from things, and we have physical integrity and safety or not. Then if you look at the animal kingdom, you can see brains that are a little more complicated. And at some point, there's a planning system, and then there's an emotional system that's, you know, happy about being safe or unhappy about being threatened. Right? And then our brains have massive numbers of structures, you know, like planning and movement and thinking and feeling and drives and emotions. And we seem to have multiple layers of thinking systems. And we have a brain, a dream system, that nobody understands whatsoever, which I find completely hilarious. And you can think in a way that those systems are more independent, and you can observe, you know, the different parts of yourself can observe them. I don't know which one's magical. I don't know which one's not computational. So is it possible that it's all computation? Probably. Is there a limit to computation? I don't think so. Do you think the universe is a computer? Like, it's a weird kind of computer, because if it was a computer, right, like, when they do calculations on what it. How much calculation it takes to describe quantum effects is unbelievably high. So, if it was a computer, wouldn't you have built it out of something that was easier to compute? Right. That's. That's a funny. It's a funny system. But then the simulation guys have pointed out that the rules are kind of interesting. Like, when you look really close, it's uncertainty. And the speed of light says, you can only look so far. And things can't be simultaneous, except for the odd entanglement problem where they seem to be like, the rules are all kind of weird. And somebody said physics is like having 50 equations with 50 variables to define 50 variables. Physics itself has been a shit show for thousands of years. It seems odd when you get to the corners of everything. It's either uncomputable or uncomfortable, definable or uncertain. It's almost like the designers of the simulation are trying to prevent us from understanding it perfectly. But also, the things that require calculations require so much calculation that our idea of the universe, of a computer is absurd, because every single little bit of it takes all the computation in the universe to figure out. Well, that's a weird kind of computer. You know, you say simulation is running in the computer, which has, by definition, infinite computation. Not infinite. Oh, you mean if the universe is infinite? Yeah. Well, every little piece of our universe seems to take infinite computation and figure. Out how to do just a lot. Well, a lot's a pretty big number. Compute. This little teeny spot takes all the mass in the local one light year by one light year space. It's close enough to infinite. Oh, it's a heck of a computer if it is one. I know. It's. It's a weird description because the simulation description seems to break when you look closely at it. But the rules of the universe seem to imply something's up. That seems a little arbitrary. The universe, the whole thing, the laws of physics, it just seems like, how did it come out to be the way it is? But lots of people talk about that. Like I said, the two smartest groups of humans are working on the same problem from different, different aspects, and they're both complete failures. So that's kind of cool. They might succeed eventually. Well, after 2000 years, the trend isn't good. 2000 years is nothing in the span of the history of the universe. So we have some time. But the next thousand years doesn't look good either. So that's what everybody says at every stage. But with Moore's law, as you've just described, not being dead, the exponential growth of technology, the future seems pretty incredible. Well, it'll be interesting, that's for sure. That's right. So what are your thoughts on Ray Kurzweil sense that exponential improvement in technology will continue indefinitely? Is that how you see Moore's law? Do you see Moore's law more broadly, in sense that technology of all kinds has a way of stacking s curves on top of each other, where it'll be exponential. And then we'll see all kinds of. What does an exponential of a million mean? That's. That's a pretty amazing number. And that's just for a local little piece of silicon. Now, let's imagine you, say, decided to get 1000 tons of silicon to collaborate in one computer at a million times the density. Like, now. Now you're talking, I don't know, ten to the 20th more computation power than our current already unbelievably fast computers. Like, nobody knows what that's going to mean. You know, the Sci-Fi guys call it computronium, like when, like, a local civilization turns the nearby star into a computer. Right? Like, I don't. That's true. But so just even when you shrink. The transistor, the, that's only one dimension. The ripple effects of that. Like, people tend to think about computers as a cost problem, right? So computers are made out of silicon and minor amounts of metals and, you know, this and that. None of those things cost any money. Like, there's plenty of sand. Like. Like you could just turn the beach and a little bit of ocean water into computers. So all the cost is in the equipment to do it. And the trend on equipment is once you figure out how to build equipment, the trend of cost is zero. Elon said, first you figure out what configuration you want the atoms in and then how to put them there. Right? Yeah. Well, what, here's the, you know, his great insight is people are how constrained. I have this thing, I know how it works, and then little tweaks to that will generate something as opposed to what do I actually want, and then figure out how to build it. It's a very different mindset, and almost nobody has it, obviously. Well, let me ask on that topic. You were one of the key early people in the development of autopilot, at least in the hardware side. Elon Musk believes that autopilot and vehicle autonomy, if we just look at that problem, can follow this kind of exponential improvement. In terms of the how question that we're talking about, there's no reason why it can't. What are your thoughts on this particular space of vehicle autonomy? And you're part of it, and Elon Musk's and Tesla's vision for, well, the. Computer you need to build was straightforward, and you could argue, well, does it need to be two times faster or five times or ten times? But that's just a matter of time or price in the short run, so that's not a big deal. You don't have to be especially smart to drive a cardinal. So it's not like a super hard problem. I mean, the big problem with safety is attention, which computers are really good at, not skills. Well, let me push back on one. You see, everything you said is correct, but we, as humans, tend to, tend to take for granted how incredible our vision system is. So you can drive a car with 2050 vision and you can train a neural network to extract the distance of any object and the shape of any surface from a video and data. It's really simple. No, it's not simple. That's a simple data problem. It's not simple because it's not just detecting object, it's understanding the scene, and it's being able to do it in a way that doesn't make errors. So the beautiful thing about the human vision system and our entire brain around the whole thing is we're able to fill in the gaps. It's not just about perfectly detecting cars, it's inferring the occluded cars. It's trying to. It's understanding the. I think that's mostly a data problem. So you think what data would compute with improvement of computation, with improvement in collection? Well, there is a. You know when you're driving a car and somebody cuts you off, your brain has theories about why they did it. You know, they're a bad person, they're distracted, they're dumb. You know, you can listen to yourself, right? So, you know, if you think that narrative is important to be able to successfully drive a car, then current autopilot systems can't do it. But if cars are ballistic things with tracks and probabilistic changes of speed and direction, and roads are fixed and given by the way, they don't change dynamically. You can map the world really thoroughly. You can place every object really thoroughly, you can calculate trajectories of things really thoroughly. But everything you said about really thoroughly has a different degree of difficulty. You could say at some point, computer autonomous systems will be way better at things that humans are lousy at. Like, they'll be better at attention. They'll always remember there was a pothole in the road that humans keep forgetting about. They'll remember that this set of roads has these weird old lines on it that the computers figured out once, and especially if they get updates. So somebody changes a given. Like the key to robots and stuff, somebody said, is to maximize the givens, right? Right. So having a robot pick up this bottle cap is way easier if you put a red dot on the top, because then you have to figure out, you know, and if you want to do a certain thing with it, you know, maximize the givens is the thing. And autonomous systems are happily maximizing the givens like humans. When you drive someplace new, you remember it because you're processing it the whole time. And after the 50th time you drove to work, you get to work, you don't know how you got there. Right. You're on autopilot. Right. Autonomous cars are always on autopilot, but the cars have no theories about why they got cut off or why they're in traffic. So that never stopped paying attention. Right. So I tend to believe you do have to have theories. Met the models of other people, especially with pedestrian cyclists, but also with other cars. So everything you said is like. Is actually essential to driving. Driving is a lot more complicated than people realize. I think. So sort of to push back slightly. But cut into traffic. Right. Yep. You can't just wait for a gap. You have to be somewhat aggressive. You'll be surprised how simple a calculation for that is. I may be on that particular point, but there's. It. I. Maybe I asked you to push back. I would be surprised. You know what? Yeah. I'll just say where I stand. I would be very surprised. But I think it's. You might be surprised how complicated it is that I say. I tell people progress disappoints in the short run, surprises in the long run. It's very possible. Yeah. I suspect in ten years it'll be just like, taken for granted. Yeah, probably. But you're probably right now look like. It'S going to be a $50 solution that nobody cares about. It's like, GPS is like, wow. GPS is. We have satellites in space that tell you where your location is. It was a really big deal. Now everything has a gps in it. Yeah, that's true. But I do think that systems that involve human behavior are more complicated than we give them credit for. So we can do incredible things with technology that don't involve humans. But when you think humans are less complicated than people, you know, frequently obscribe, maybe I. We tend to operate out of large numbers of patterns and just keep doing it over and over. But I can't trust you because you're human. That's something. Something a human would say. But my hope is on the point you've made is even if no matter who's right, I'm hoping that there's a lot of things that humans aren't good at that machines are definitely good at. Like you said, attention and things like that. Well, they'll be so much better that the overall picture of safety and autonomy will be. Obviously, cars will be safer, even if they're not as good at. I'm a big believer in safety. I mean, there are already the current safety systems, like cruise control, that doesn't let you run into people, and lane keeping. There are so many features that you just look at the Pareto of accidents and knocking off, like, 80% of them is, you know, super doable just to linger on. The autopilot team and the efforts there, it seems to be that there's a very intense scrutiny by the media and the public in terms of safety. The pressure the bar put before autonomous vehicles. What are your sort of, as a person there, working on the hardware and trying to build a system that builds a safe vehicle and so on, what was your sense about that pressure? Is it unfair? Is it expected of new technology? Yeah, it seems reasonable. I was interested, I talked to both american and european regulators, and I was worried that the regulations would write into the rules. Technology solutions like modern brake systems imply hydraulic brakes. So if you read the regulations, to meet the letter of the law for brakes, it sort of has to be hydraulic. Right? And the regulator said they're interested in the use cases like a head on crash, an offset crash, don't hit pedestrians, don't run into people, don't leave the road, don't run a red light or a stoplight. They were very much into the scenarios, and they had all the data about which scenarios injured or killed the most people. And for the most part, those conversations were like, what's the right thing to do to take the next step. Now, Elon's very interested also in the benefits of autonomous driving or freeing people's time and attention, as well as safety. And I think that's also an interesting thing. You know, building autonomous systems so they're safe and safer than people seemed. Since the goal is to be ten x safer than people, having the bar to be safer than people and scrutinizing accidents seems philosophically, you know, correct. So I think that's a good thing. What are, it's different than the things you worked at the Intel AMD Apple with autopilot chip design and hardware design. What are interesting or challenging aspects of building this specialized kind of computing system in the automotive space? I mean, there's two tricks to building like an automotive computer. One is the software team. The machine learning team is developing algorithms that are changing fast. So as you're building the accelerator, you have this worry or intuition that the algorithms will change enough that the accelerator will be the wrong one. There's a generic thing, which is if you build a really good general purpose computer, say its performance is one, and then GPU guys will deliver about five x to performance for the same amount of silicon, because instead of discovering parallelism, you're given parallelism. Special accelerators get another two to five x on top of a GPU. Because you say, I know the math is always eight bit integers into 32 bit accumulators, and the operations are the subset of mathematical possibilities. So, auto, you know, AI accelerators have a claimed performance benefit over GPU's, because in the narrow math space, you're nailing the algorithm. Now, you still try to make it programmable, but the AI field is changing really fast. So there's a, you know, there's a little creative tension there of, I want the acceleration afforded by specialization without being over specialized, so that the new algorithm is so much more effective that you would have been better off on a gpu. So there is a tension there. To build a good computer for an application like automotive, there's all kinds of sensor inputs and safety processors and a bunch of stuff. So one of Elon's goal is to make it super affordable. So every car gets an autopilot computer. So some of the recent startups you look at, and they have a server and a trunk, because they're saying, I'm going to build this autopilot computer replaces the driver. So their cost budget is ten or $20,000. And Elon's constraint was, I'm going to put one every, in every car, whether people buy autonomous driving or nothing. So the cost constraint he had in mind was great. And to hit that, you had to think about the system design. That's complicated. It's fun, it's craftsman's work. Like a violin maker, you can say Stradivarius is this incredible thing, and the musicians are incredible, but the guy making the violin picked wood and sanded it, and then he cut it and he glued it, and he waited for the right day so that when he put the finish on it, YoU KnoW, do something dumb. ThAt's craftsman's work, right? You may be a genius craftsman because you have the best techniques and you discover a new one, but most engineering craftsman's work, and humans really like to do that, you know, express humans? No, everybody. All humans. I don't know. I used to. I dug ditches when I was in college. I got really good at it. Satisfying. Yeah. So digging ditches, also craftsman. Yeah. Of course. So there's an expression called complex mastery behavior. So when you're learning something, that's fine because you're learning something. When you do something, it's row it and simple. It's not that satisfying. But if the steps that you have to do are complicated and you're good at them, it's satisfying to do them. And then if you're intrigued by it all as you're doing them, you sometimes learn new things that you can raise your game. But Cressman's work is good in engineers. Like, engineering is complicated enough that you have to learn a lot of skills. And then a lot of what you do is then craftsman's work, which is fun. Autonomous driving, building a very resource constrained computer. So a computer has to be cheap enough to put in every single car. That essentially boils down to craftsman's work. It's engineering. You know, there's thoughtful decisions and problems to solve and trade offs to make. You need ten camera imports or eight, you know, you're building for the current car or the next one. You know, how do you do the safety stuff? You know, there's a whole bunch of details, but it's fun. But it's not like I'm building a new type of neural network which has a new mathematics and a new computer to work. You know, that that's like, there's, there's more invention than that, but the rejection to practice. Once you pick the architecture, you look inside and what do you see? Adders and multipliers, memories and, you know, the basics. So computers, there's always this, this weird set of abstraction, layers of ideas and thinking that reduction to practice is transistors and wires and, you know, pretty basic stuff. And that's an interesting phenomena, by the way. Like factory work. Like, lots of people think factory work is road assembly stuff. I've been on the assembly line. Like, the people who work there really like it. It's a really great job. It's really complicated. Putting cars together is hard, right? And the car is moving and the parts are moving and sometimes the parts are damaged and you have to coordinate putting all the stuff together. And people are good at it. They're good at it. And I remember one day I went to work and the line was shut down for some reason. And some of the guys sitting around were really bummed because they had reorganized a bunch of stuff and they were going to hit a new record for the number of cars built that day. And they were all gung ho to do it. And these were big, tough buggers you know, but what they did was complicated and you couldn't do it. Yeah. And I mean, well, after a while you could, but you'd have to work your way up because, you know, like, putting the bright, what's called the brights, the trim on a car on a moving assembly line where it has to be attached 25 places in a minute and a half is unbelievably complicated. And human beings can do it. It's really good. I think that's harder than driving a. Car, by the way, putting together, working. On a factory, too. Smart people can disagree. Yeah. I think driving a car, we'll get. You in the factory someday and then we'll see how it goes. Not for us humans. Driving a car is easy. I'm saying building a machine that drives a car is not easy. No. Okay. Driving a car is easy for humans because we've been evolving for billions of years. Drive cars. Yeah, the pale lithic cars are super cool. Now you join the rest of the Internet in mocking me. Okay. Intrigued by your, you know, your anthropology. Yeah, I'll have to go dig into that. There's some inaccuracies there. Yes. Okay. But in general, what have you learned in terms of thinking about passion, craftsmanship, tension, chaos. Jesus, the whole mess of it, what have you learned have taken away from your time working with Elon Musk, working at Tesla, which is known to be a place of chaos, innovation, craftsmanship, and all that. I really liked the way he thought. Like, you think you have an understanding about what first principles of something is, and then you talk to Elon about it, and you. You didn't scratch the surface. You know, he has a deep belief that no matter what you do is a local maximum. Right. And I had a friend, he invented a better electric motor, and it was, like, a lot better than what we were using. And one day he came by, he said, you know, I'm a little disappointed because, you know, this is really great. And you didn't seem that impressed. And I said, you know, when the super intelligent aliens come, are they going to be looking for you? Like, where is he? The guy who built the motor? Yeah, probably not, you know, like, but doing interesting work that's both innovative and let's say craftsman's work on the current thing is really satisfying. It's good, and that's cool. And then Elon was good at taking everything apart. Like, what's the deep first principle? Oh, no, what's really. No, what's really. That ability to look at it without assumptions and how constraints is super wild. We build rocket ship and use an electric car, everything. And that's super fun. And he's into it, too. Like when they first landed two SpaceX rockets to Tesla, we had a video projector in the big room, and like 500 people came down. And when they landed, everybody cheered and some people cried. It was so cool. But how did you do that? Well, it was super hard. And then people say, well, it's chaotic, really, to get out of all your assumptions, you think that's not going to be unbelievably painful. And is Elon tough? Yeah, probably. Do people look back on it and say, boy, I'm really happy I had that experience to go take apart that many layers of assumptions. Sometimes super fun, sometimes painful. So it could be emotionally and intellectually painful, that whole process just stripping away assumptions. Yeah. Imagine 99% of your thought process is protecting your self conception, and 98% of that's wrong. Now you got the math right. How do you think you're feeling when you get down to that one bit that's useful? And now you're open and you have the ability to do something different. I don't know if I got the math right. It might be 99.9, but it ain't 50. Imagining it, the 50% is hard enough. Yeah. Now, for a long time, I've suspected you could get better. Like, you can think better, you can think more clearly, you can take things apart, and there's lots of examples of that, people who do that, so. And Elon is an example of that. You are an example. So I don't know if I am. I'm fun to talk to. Certainly. I've learned a lot of stuff. Right. Well, here's the other thing is, like, I joke, like, I read books and people think, oh, you read books? Well, no, I've read a couple books a week for 55 years. Wow. Well, maybe 50, because I didn't learn to read until I was eight or something. And it turns out when people write books, they often take 20 years of their life where they passionately did something, reduce it to 200 pages. That's kind of fun. And then you go online and you can find out who wrote the best books and who, like, you know, that's kind of wild. So there's this wild selection process, and then you can read it and for the most part, understand it, and then you can go apply it. Like, I went to one company, I thought, I haven't managed much before, so I read 20 management books, and I started talking to them and basically compared to all the vps running around. I'd ran, read 19 more management books than anybody else. It wasn't even it that hard. And half the stuff worked. Like, first time, it wasn't even rocket science. But at the core of that is questioning the assumptions or sort of entering the thinking, first principles thinking, sort of looking at the reality of the situation and using. Using that knowledge, applying that knowledge. Yes. So I would say my brain has this idea that you can question first assumptions and. But I can go days at a time and forget that. And you have to kind of like, circle back to that observation because it is emotionally challenging. Well, it's hard to just keep it front and center because, you know, you're. You operate on so many levels all the time, and, you know, getting this done takes priority, or, you know, being happy takes priority, or, you know, screwing around takes priority. Like. Like how you go through life is complicated, and then you remember, oh, yeah, I could really think first principles. Oh, shit, that's. That's tiring, you know, but you do for a while, and that's kind of cool. So just as a last question, in. Your sense, from the big picture, from the first principles, do you think. You kind of answered already, but do you think autonomous driving is something we can solve on a timeline of years? So one, two, 3510 years, as opposed to a century? Yeah, definitely. Just to linger on it a little longer. Where's the confidence coming from? Is it the fundamentals of the problem, the fundamentals of building the hardware and. The software as a computational problem, understanding ballistics roles, topography, it seems pretty solvable. I mean, and you can see this, you know, like. Like speech recognition. For a long time, people are doing, you know, frequency and domain analysis and. And all kinds of stuff, and that didn't work for. At all, right? And then they did deep learning about it, and it worked great. And it took multiple iterations and, you know, autonomous driving is way past the frequency analysis point. You know, use radar, don't run into things, and the data gathering is going up and the computation is going up and the algorithm understanding is going up, and there's a whole bunch of problems getting solved like that. The data side is really powerful. But I disagree with both you and Elon. I'll tell Elon once again, as I did before, that when you add human beings into the picture, it's no longer a ballistics problem. It's something more complicated. But I could be very well proven wrong. Cars are hardly damped in terms of rate of change, like the steering and the steering system is really slow compared to a computer. The acceleration of the acceleration is really slow. Yeah. On a certain time scale, on a ballistics timescale. But human behavior, I don't know. Yeah, I shouldn't say beings are really slow, too. Weirdly, we operate, you know, half a second behind reality. Nobody really understands that one either. It's pretty funny. Yeah. Yeah. So we very well could be surprised. And I think with the rate of improvement on all aspects on both the compute and the software and the hardware, there's going to be pleasant surprises all over the place. Speaking of unpleasant surprises, many people have worries about a singularity in the development of AI. Forgive me for such questions. When AI improves exponentially and reaches a point of superhuman level, general intelligence, you know, beyond the point, there's no looking back. Do you share this worry of existential threats from artificial intelligence, from computers becoming superhuman level intelligence? No, not really. You know, like, we already have a very stratified society, and then if you look at the whole animal kingdom of capabilities and abilities and interests and, you know, smart people have their niche and, you know, normal people have their niche and craftsmen's have their niche, and, you know, animals have their niche, I suspect that the domains of interest for things that, you know, astronomically different, like the whole something got ten times smarter than us and wanted to track us all down because what, we like to have coffee at Starbucks? Like, it doesn't seem plausible. No. Is there an existential problem that, how do you live in a world where there's something way smarter than you, and you based your kind of self esteem on being the smartest local person? Well, there's, what, 0.1% of the population who thinks that? Because the rest of the population has been dealing with it since they were born. So the breadth of possible experience that can be interesting is really big. You know, superintelligence seems likely, although we still don't know if we're magical, but I suspect we're not. And it seems likely that it'll create possibilities that are interesting for us, and its interests will be interesting for that. For whatever it is, it's not obvious why its interests would somehow want to fight over some square foot of dirt or, you know, whatever the usual fears are about. So you don't think you'll inherit some of the darker aspects of human nature? Depends on how you think reality is constructed. So for whatever reason human beings are in, let's say, creative tension and opposition with both our good and bad forces, like, there's lots of philosophical understanding of that, right? I don't know why that would be different. So you think the evil is necessary for the good? I mean, the tension. I don't know about evil, but, like, we live in a competitive world where your good is somebody else's, you know, evil, you know, there's the malignant part of it, but that seems to be self limiting, although occasionally it's. It's super horrible. But yes, there's a debate over ideas, and some people have different beliefs, and that debate itself is a process. So that arriving at something. Yeah. And why wouldn't that continue? Yeah, but you don't think that whole process will leave humans behind in a way that's painful, emotionally painful, yes, for the 0.1% they'll be. Why isn't it already painful for a large percentage of population? And it is. I mean, society does have a lot of stress in it about the 1% and about this and about that, but, you know, everybody has a lot of stress in their life about what they find satisfying. And, you know, know yourself seems to be the proper dictum and pursue something that makes your life meaningful, seems proper. And there's so many avenues on that. Like there's so much unexplored space at every single level. You know, I'm somewhat of. My nephew called me a jaded optimist and, you know, so it's. There's a beautiful tension that. In that label. But if you were to look back at your life and could relive a moment, a set of moments, because there were the happiest times of your life outside of family, what would that be? I don't want to relive any moments. I like that. I like that situation where you have some amount of optimism and then the anxiety of the unknown. So you love the unknown, do you? The mystery of it? I don't know about the mystery. Sure. Get your blood pumping. What do you think is the meaning of this whole thing of life on this pale blue dot? It seems to be what it does. Like, the universe, for whatever reason, makes atoms, which makes us, which we do stuff and we figure out things and we explore things, and that's just what it is. It's not just. Yeah, it is, Jim. I don't think there's a better place to end it. It's a huge honor and, well, that was super fun. Thank you so much for talking today. All right, great. Thanks for listening to this conversation and thank you to our presenting sponsor. Cash app. Download it, use code LexPodcast. You'll get $10, and $10 will go to first, a STEM education nonprofit that inspires hundreds of thousands of young minds to become future leaders and innovators. If you enjoy this podcast, subscribe on YouTube, give it five stars on Apple Podcast, follow on Spotify, support it on Patreon, or simply connect with me on Twitter. And now, let me leave you with some words of wisdom from Gordon Moore. If everything you try works, you aren't trying hard enough. Thank you for listening and hope to. See you next time.

Utterances:
Speaker A: The following is a conversation with Jim Keller, legendary microprocessor engineer who has worked at AMD, Apple, Tesla, and now intel. He's known for his work on AMD K seven, K eight, K twelve, and Zen microarchitectures, Apple A four and a five processors, and co author of the specification for the X 86 64 instruction set and Hypertransport Interconnected. Hes a brilliant first principles engineer, an out of the box thinker, and just an interesting and fun human being to talk to. This is the artificial intelligence podcast. If you enjoy it, subscribe on YouTube, give it five stars on Apple Podcast, follow on Spotify, support it on Patreon, or simply connect with me on Twitter. Alexfridman spelled F R I D M A N. I recently started doing ads at the end of the introduction. I'll do one or two minutes after introducing the episode and never any ads in the middle that can break the.
Speaker B: Flow of the conversation.
Speaker A: I hope that works for you and doesn't hurt the listening experience. This show is presented by Cash app, the number one finance app in the app store. I personally use Cash app to send money to friends, but you can also use it to buy, sell and deposit bitcoin in just seconds. Cash app also has a new investing feature. You can buy fractions of a stock, say one dollar's worth no matter what the stock price is. Brokerage services are provided by Cash app Investing, a subsidiary of Square and member SIPC. I'm excited to be working with Cash app to support one of my favorite organizations called first. Best known for their first robotics and Lego competitions. They educate and inspire hundreds of thousands of students in over 110 countries and have a perfect rating at charity Navigator, which means that donated money is used to maximum effectiveness. When you get cash app from the App Store or Google Play and use code Lex podcast, you'll get $10. And cash app will also donate $10 to first, which again, is an organization that I've personally seen inspire girls and boys to dream of engineering a better world. And now, here's my conversation with Jim Keller.
Speaker B: What are the differences and similarities between the human brain and a computer with the microprocessor at its core? Let's start with the philosophical question, perhaps.
Speaker C: Well, since people don't actually understand how human brains work, I think that's true.
Speaker B: I think that's true.
Speaker C: So it's hard to compare them. Computers are, you know, there's really two things. There's memory and there's computation. And to date, almost all computer architectures are global memory, which is a thing and then computation where you pull data and you do relatively simple operations on it and write data back.
Speaker B: So it's decoupled in modern computers and you think in the human brain everything's a mesh, a mess that's combined together.
Speaker C: What people observe is there's some number of layers of neurons which have local and global connections and information is stored in some distributed fashion. And people build things called neural networks in computers where the information is distributed in some kind of fashion. You know, there's a mathematics behind it. I don't know that the understanding of that is super deep. The computations we run on those are straightforward computations. I don't believe anybody has said a neuron does this computation. So to date it's hard to compare them, I would say.
Speaker B: So let's get into the basics before we zoom back out. How do you build a computer from scratch? What is a microprocessor? What is the microarchitecture? What's an instruction set architecture, maybe even as far back as what is a transistor.
Speaker C: So the special charm of computer engineering is there's a relatively good understanding of abstraction layers. So down at the bottom you have atoms and atoms get put together in materials like silicon or dope, silicon or metal, and we build transistors on top of that. We build logic gates, right. And then functional units like an adder of subtractor, an instruction parsing unit, and then we assemble those into, you know, processing elements. Modern computers are built out of probably ten to 20 locally organic processing elements or coherent processing elements. And then that runs computer programs. So there's abstraction layers and then software, there's an instruction set you run and then there's assembly language C, C, Java, JavaScript. There's abstraction layers essentially from the atom to the data center. So when you build a computer, first there's a target, like what's it for? Like how fast does it have to be? Which today there's a whole bunch of metrics about what that is. And then in an organization of 1000 people who build a computer, there's lots of different disciplines that you have to operate on. Does that make sense?
Speaker B: So there's a bunch of levels, abstraction of in organization, I can tell. And in your own vision there's a lot of brilliance that comes in at every one of those layers. Some of it is science, some of it is engineering, some of it is art. What's the most, if you could pick favorites, what's the most important, your favorite layer on these layers of obstructions? Where does the magic entertain this hierarchy.
Speaker C: I don't really care. That's the fun. You know, I'm somewhat agnostic to that. So I would say for relatively long periods of time, instruction sets are stable. So the x 86 instruction set, the arm instruction set.
Speaker B: What's an instruction set?
Speaker C: So it says how do you encode the basic operations, load, store, multiply, add, subtract, conditional branch. You know, there aren't that many interesting instructions. Like if you look at a program and it runs, you know, 90% of the execution is on 25 opcodes, you know, 25 instructions and those are stable, right?
Speaker B: What does it mean, stable?
Speaker C: Intel architecture has been around for 25 years.
Speaker B: It works.
Speaker C: It works. And that's because the basics, you know are defined a long time ago, right? Now, the way an old computer ran is you fetched instructions and you executed them in order. Do the load, do the add, do the compare. The way a modern computer works is you fetch large numbers of instructions, say 500, and then you find the dependency graph between the instructions. And then you execute in independent units those little micrographs. So a modern computer, like people like to say computers should be simple and clean. But it turns out the market for simple, complete, clean, slow computers is zero. Right? We don't sell any simple, clean computers. Now you can, there's how you build it can be clean, but the computer people want to buy, that's, say, in a phone or a data center, fetches a large number of instructions, computes the dependency graph and then executes it in a way that gets the right answers.
Speaker B: And optimizes that graph somehow.
Speaker C: They run deeply out of order. And then there's semantics around how memory ordering works and other things work. So the computer has a bunch of bookkeeping tables that says what order studies operations finish in or appear to finish in. But to go fast, you have to fetch a lot of instructions and find all the parallelism. Now there's a second kind of computer which we call GPU's today, and I call it the difference. There's found parallelism. Like you have a program with a lot of dependent instructions. You fetch a bunch and then you go figure out the dependency graph and you issues instructions out of order. That's because you have one serial narrative to execute which in fact can be done out of order.
Speaker B: Did you call it a narrative?
Speaker C: Yeah. Wow. So. Yeah, so humans think in serial narrative. So read a book, right? There's a sentence after sentence after sentence, and there's paragraphs. Now you could diagram that. Imagine you diagrammed it properly and you said, which sentences could be read in any order without changing the meaning. Right.
Speaker B: That's a fascinating question to ask of a book. Yeah.
Speaker C: Yeah. You could do that, right? So, some paragraphs could be reordered. Some sentences can be reordered. You could say he is tall and smart and x, right? And it doesn't matter the order of tall and smart. But if you say the tall man is wearing a red shirt, what colors? You know, like, you can create dependencies, right? And so GPU's, on the other hand, run simple programs on pixels, but you're given a million of them, and the first order, the screen you're looking at doesn't care which order you do it in. So I call that, given parallelism, simple narratives around the large numbers of things where you can just say it's parallel because you told me it was.
Speaker B: So found parallelism, where the narrative is sequential, but you discover, like, little pockets of parallelism versus, turns out, large pockets of parallelism. Large. So how hard is it to discuss?
Speaker C: Well, how hard is it? That's just transistor count, right? So, once you crack the problem, you say, here's how you fetch ten instructions at a time. Here's how you calculated dependencies between them. Here's how you describe the dependencies. Here's, you know, these are pieces, right?
Speaker B: So, once you describe the dependencies, then it's just a graph, sort of. It's an algorithm defines what is that. I'm sure there's a graph theory, theoretical answer here that's solved. But in general, programs, modern programs that human beings write, how much found parallelism is there in ten x? What does ten x mean?
Speaker C: So, when you execute it in order versus. You would get what's called cycles per instruction, and it would be about three instructions, three cycles per instruction because of the latency of the operations and stuff. And a modern computer executes it like 0.20.25 cycles per instruction. So it's about, we today find ten x. And there's two things. One is the found parallelism in the narrative, right? And the other is to predictability of the narrative, right? So certain operations, say, do a bunch of calculations, and if greater than one, do this, else do that. That decision is predicted in modern computers to high 90% accuracy. So branches happen a lot. So, imagine you have. You have a decision to make every six instructions, which is about the average, right? But you want to fetch 500 instructions, figure out the graph, and execute them all in parallel. That means you have, let's say, if you fix 600 instructions and it's every six, you have to fetch, you have to predict 99 out of 100 branches correctly for that window to be effective.
Speaker B: Okay, so parallelism. You can't parallelize branches, or you can pretty you. What does predictive branch mean? What does predictive branch?
Speaker C: So imagine you do a computation over and over. You're in a loop. So while n is greater than one do, and you go through that loop a million times. So every time you look at the branch, you say, it's probably still greater than one.
Speaker B: And you're saying you could do that accurately?
Speaker C: Very accurately. Modern computer.
Speaker B: My mind is blown. How the heck do you do that? Wait a minute.
Speaker C: Well, you want to know? This is really sad. 20 years ago, yes. You simply recorded which way the branch went last time and predicted the same thing.
Speaker B: Right. Okay, what's the accuracy of that?
Speaker C: 85%. So then somebody said, hey, let's keep a couple of bits and have a little counter. So when it predicts one way, we count up and then pins. So say you have a three bit counter. So you count up and then you count down. And if it's, you know, you can use the top bit as the sign bit. So you have a signed two bit number. So if it's greater than one, you predict taken and less than one, you predict not taken. Right. Or less than zero, whatever the thing is. And that got us to 92%. Oh, okay. No, it gets better. This branch depends on how you got there. So if you came down the code one way, you're talking about Bob and Jane, right? And then said, does Bob like Jane? It went one way, but if you're talking about Bob and Jill, does Bob like, change? You go a different way. So that's called history. So you take the history and encounter. That's cool. But that's not how anything works today. They use something that looks a little like a neural network. So modern, you take all the execution flows and then you do basically deep pattern recognition of how the program is executing. And you do that multiple different ways, and you have something that chooses what the best result is. There's a little supercomputer inside the computer that's trying to predict, calculates which way branches go. So the effective window that it's worth finding grass and gets bigger.
Speaker B: Why was that going to make me sad? Because that's amazing.
Speaker C: It's amazingly complicated. Oh, well, well, here's the funny thing. So to get to 85% took 1000 bits, to get to 99% takes tens of megabits. So this is one of those to get the result, you know, to get from a window of, say, 50 instructions to 500. It took three orders of magnitude, or four orders of magnitude, four bits.
Speaker B: Now, if you get the prediction of a branch wrong, what happens then?
Speaker C: Flush the pipe.
Speaker B: You flush the pipe. So it's just the performance cost, but.
Speaker C: It gets even better.
Speaker B: Yeah.
Speaker C: So we're starting to look at stuff that says so executed down this path. And then you had two ways to go, but far away, there's something that doesn't matter which path you went. So you took the wrong path, you executed a bunch of stuff. Then you had the mis predicting, you backed it up, but you remembered all the results you already calculated. Some of those are just fine. Like, if you read a book and you misunderstand a paragraph, your understanding of the next paragraph sometimes is invariant to that understanding. Sometimes it depends on it.
Speaker B: And you can kind of anticipate that invariance.
Speaker C: Yeah, well, you can keep track of whether the data changed. And so when you come back through a piece of code, should you calculate it again or do the same thing?
Speaker B: Okay, how much of this is art and how much of it is science? Because it sounds pretty complicated.
Speaker C: So, well, how do you describe a situation? So imagine you come to a point in the road where you have to make a decision, and you have a bunch of knowledge about which way to go. Maybe you have a map, so you want to go the shortest way, or do you want to go the fastest way, or you want to take the nicest road. There's some set of data. So imagine you're doing something complicated, like building a computer. And there's hundreds of decision points, all with hundreds of possible ways to go. And the ways you pick interact in a complicated way.
Speaker B: Right.
Speaker C: And then you have to pick the right spot.
Speaker B: Right.
Speaker C: So those are your science. I don't know.
Speaker B: You avoided the question you just described. The Robert Frost poem wrote less taken.
Speaker C: I described the Robin Frost problem. What we do as computer designers, it's all poetry.
Speaker B: Okay, great.
Speaker C: Yeah. I don't know how to describe that, because some people are very good at making those intuitive leaps. It seems like this, combinations of things. Some people are less good at it, but they're really good at evaluating the alternatives. Right. And everybody has a different way to do it, and some people can't make those leaps, but they're really good at analyzing it. So when you see computers are designed by teams of people who have very different skill sets, and a good team has lots of different kinds of people, I suspect you would describe some of them as artistic, but not very many, unfortunately or fortunately. Fortunately, well, you know, computer design is hard. It's 99% perspiration, and the 1% inspiration is really important.
Speaker B: But you still need the 99.
Speaker C: Yeah, you got to do a lot of work. And then there are interesting things to do at every level of that stack.
Speaker B: At the end of the day, if you run the same program multiple times, does it always produce the same result? Is there some room for fuzziness there?
Speaker C: That's a math problem. So if you run a correct c program, the definition is every time you run it, you get the same answer.
Speaker B: Yeah, well, that's a math statement.
Speaker C: Well, that's a language definitional statement. So for years, when people did, when we first did 3d acceleration of graphics, you could run the same scene multiple times and get different answers. And then some people thought that was okay, and some people thought it was a bad idea. Then when the HPC world used GPU's for calculations, they thought it was a really bad idea. Now, in modern AI stuff, people are looking at networks where the precision of the data is low enough that the data is somewhat noisy. The observation is the input data is unbelievably noisy, so why should the calculation be not noisy? And people have experimented with algorithms that, say, can get faster answers by being noisy. Like, as the network starts to converge, if you look at the computation graph, it starts out really wide and it gets narrower, and you can say, is that last little bit that important, or should I start the graph on the next rev before we whittle it all the way down to the answer? So you can create algorithms that are noisy. Now, if you're developing something and every time you run it, you get a different answer, it's really annoying. And so most people think, even today, every time you run the program, you get the same answer.
Speaker B: No, I know, but the question is, that's the formal definition of a programming language.
Speaker C: There is a definition of languages that don't get the same answer, but people who use those, you always want something because you get a bad answer, and then you're wondering, is it because of something in the algorithm or because of this? And so everybody wants a little switch that says, no matter what, do it deterministically. And it's really weird because almost everything going into modern calculations is noisy. So why are the answers have to be so clear?
Speaker B: It's, where do you stand?
Speaker C: I design computers for people who run programs. So if somebody says, I want a deterministic answer, like most people want that.
Speaker B: Can you deliver a deterministic answer? I guess, is the question, like, when you?
Speaker C: Yeah, hopefully. Sure. What people don't realize is you get a deterministic answer, even though the execution flow is very undeterministic. So you run this program 100 times. It never runs the same way twice, ever.
Speaker B: And the answer, it arrives at the.
Speaker C: Same answer, but it gets the same answer every time.
Speaker B: It's just amazing. Okay. You've achieved, in the eyes of many people, legend, status as a chip architect. What design creation are you most proud of? Perhaps because it was challenging, because of its impact, or because of the set of brilliant ideas that. That were involved in bringing it to life.
Speaker C: I find that description odd. And I have two small children, and I promise you, they think it's hilarious, this question. Yeah. So I do it for them. I'm really interested in building computers, and I've worked with really, really smart people. I'm not unbelievably smart. I'm fascinated by how they go together, both as a. As a thing to do and as an endeavor that people do.
Speaker B: How people and computers go together.
Speaker C: Yeah. Like how people think and build a computer. And I find sometimes that the best computer architects aren't that interested in people or the best people. Managers aren't that good at designing computers.
Speaker B: So the whole stack of human beings is fascinating. So the managers, individual engineers.
Speaker C: Yeah. I said, I realized after a lot of years of building computers, where you sort of build them out of transistors, logic gates, functional units, computational elements, that you could think of people the same way. So people are functional units, and then you could think of organizational design as a computer architectural problem. And then it was like, oh, that's super cool, because the people are all different, just like the computational elements are all different, and they like to do different things. And so I had a lot of fun, like reframing how I think about organizations.
Speaker B: Just like with computers, we were saying execution paths. You can have a lot of different paths that end up at a Ydez at the same good destination. So, what have you learned about the human abstractions, from individual functional human units to the broader organization? What does it take to create something special?
Speaker C: Well, most people don't think simple enough. All right, so, do you know the difference between a recipe and the understanding? There's probably a philosophical description of this. So, imagine you're gonna make a loaf of bread.
Speaker B: Yeah.
Speaker C: The recipe says, get some flour, add some water, add some yeast, mix it up, let it rise, put it in a pan, put it in the oven. It's a recipe. Right. Understanding bread, you can understand biology. Supply chains, you know, grain grinders. Yeast physics, you know, thermodynamics. Like, there's so many levels of understanding there. And then when people build and design things, they frequently are executing some stack of recipes, right. And the problem with that is the recipes all have limited scope. Like, if you have a really good recipe book for making bread, it won't tell you anything about how to make an omelet, right? But if you have a deep understanding of cooking, right, then bread, omelets, sandwich. There's a different way of viewing everything. And most people, when you get to be an expert at something, you're hoping to achieve deeper understanding, not just a large set of recipes to go execute. And it's interesting to walk groups of people, because executing recipes is unbelievably efficient. If it's what you want to do, if it's not what you want to do, you're really stuck. And that difference is crucial. And everybody has a balance of, let's say, deeper understanding the recipes. And some people are really good at recognizing when the problem is to understand something deeply, deeply. Does that make sense?
Speaker B: It totally makes sense. Does every stage of development deep understanding on the team needed?
Speaker C: Well, this goes back to the art versus science question.
Speaker B: Sure.
Speaker C: If you constantly unpacked everything for deeper understanding, you never get anything done.
Speaker B: Right.
Speaker C: And if you don't unpack understanding when you need to, you'll do the wrong thing. And then at every juncture, like human beings are these really weird things, because everything you tell them has a million possible outputs. Right. And then they all interact in a hilarious way.
Speaker B: Yeah, it's very nice.
Speaker C: And then having some intuition about what do you tell them? What do you do? When do you intervene? When do you not? It's. It's complicated.
Speaker B: Right.
Speaker C: So it's. It's, you know, essentially computationally unsolvable.
Speaker B: Yeah, it's the intractable problem. Sure, humans are a mess, but with deep understanding, do you mean also sort of fundamental questions of things like, what is a computer? Or why? The why question is, why are we even building this of purpose? Or do you mean more going towards the fundamental limits of physics, really getting into the core of the science?
Speaker C: Well, in terms of building a computer, think a little simpler. So, common practice is you build a computer, and then when somebody says, I want to make it 10% faster, you'll go in and say, all right, I need to make this buffer bigger, and maybe I'll add an ad unit, or I have this thing that's three instructions wide. I'm going to make it four instructions wide. And what you see, is each piece gets incrementally more complicated, and then at some point you hit this limit. Adding another feature or buffer doesn't seem to make it any faster. And then people will say, well, that's because it's a fundamental limit. And then somebody else will look at it and say, well, actually the way you divided the problem up and the way the different features are interacting is limiting you, and it has to be rethought. Rewritten. Right. So then you refactor it and rewrite it. And what people commonly find is the rewrite is not only faster, but half as complicated from scratch. Yes.
Speaker B: So how often in your career purchase have you seen as needed, maybe more generally, to just throw the whole out, the whole thing out?
Speaker C: This is where I'm on one end of it every three to five years.
Speaker B: Which end are you on? Wait.
Speaker C: Rewrite more often.
Speaker B: Rewrite and three to five years is.
Speaker C: If you want to really make a lot of progress on computer architecture, every five years you should do one from scratch.
Speaker B: So where does the x 86 64 standard come in? How often do you.
Speaker C: I was the co author of that back in 98. That's 20 years ago.
Speaker B: Yeah. So that's still around.
Speaker C: The instruction set itself has been extended quite a few times.
Speaker B: Yes.
Speaker C: And instruction sets are less interesting than the implementation underneath. There's been, on x 86 architecture, Intel's designed a few, aim designed a few very different architectures. And I don't want to go into too much of the detail about how often, but there's a tendency to rewrite it every, you know, ten years, and it really should be every five.
Speaker B: So you're saying you're an outlier in that sense? In the.
Speaker C: Rewrite more often.
Speaker B: Rewrite more often. Well, and here's scary.
Speaker C: Yeah, of course. Well, scary to who?
Speaker B: To everybody involved, because like you said, repeating the recipe is efficient. Companies want to make money, no individual engineers want to succeed. So you want to incrementally improve, increase the buffer from three to four will increase.
Speaker C: So this is where you get into diminishing return curves. I think Steve Jobs said this. Right? So every, you have a project and you start here and it goes up and you have dimitri and return. And to get to the next level, you have to do a new one. And the initial starting point will be lower than the old optimization point, but it'll get higher. So now you have two kinds of fear, short term disaster and long term disaster.
Speaker B: And you're grown up.
Speaker C: Right? Like people with a quarter by quarter business objective are terrified about changing everything and people who are trying to run a business or build a computer for a long term objective know that the short term limitations block them from the long term success. So if you look at leaders of companies that had really good long term success, every time they saw that they had to redo something, they did.
Speaker B: And so somebody has to speak up.
Speaker C: Or you do multiple projects in parallel, like, you optimize the old one while you build a new one. But the marketing guys, they're always like, promise me that the new computer is faster on every single thing. And the computer architect says, well, the new computer will be faster on the average, but there's a distribution of results and performance, and you'll have some outliers that are slower, and that's very hard because they have one customer who cares about that one.
Speaker B: So, speaking of the long term, for over 50 years now, Moore's Law has served for me and millions of others as an inspiring beacon. What kind of amazing future brilliant engineers can build. I'm just making your kids laugh all of today.
Speaker C: That was great.
Speaker B: So, first, in your eyes, what is Moore's law, if you could define for people who don't know?
Speaker C: Well, the simple statement was, from Gordon Moore, was double the number of transistors every two years, something like that. And then my operational model is we increase the performance of computers by two x every two or three years. And it's wiggled around substantially over time. And also in how we deliver performance has changed. The foundational idea was two x to transistors every two years. The current cadence is something like, they call it a shrink factor, like 0.6 every two years, which is not 0.5.
Speaker B: But that's referring strictly again to the.
Speaker C: Original definition of transistor count and shrink factors.
Speaker B: Just getting them smaller, smaller and smaller.
Speaker C: Well, it's for a constant chip area. If you make the transistor smaller by 0.6, then you get one over 0.6 more transistors.
Speaker B: So can you linger on it a little longer? What's a broader, what do you think should be the broader definition of Moore's law? We mentioned how you think of performance just broadly. What's a good way to think about Moore's law?
Speaker C: Well, first of all, so I've been aware of Moore's law for 30 years, in which sense? Well, I've been designing computers for 40.
Speaker B: You're just watching it before your eyes kind of thing.
Speaker C: Well, and somewhere where I became aware of it, I was also informed that Moore's law was going to die in ten to 15 years. And I thought that was true at first, but then after ten years, it was going to die in ten to 15 years. And then at one point, it was going to die in five years, and then it went back up to ten years. And at some point, I decided not to worry about that particular prognostication for the rest of my life, which is fun. And then I joined intel, and everybody said, moore's law is dead. And I thought, that's sad, because it's the Moore's law company and it's not dead, and it's always been going to die. And humans like these apocryphal kind of statements, like, we'll run out of food or run out of air or run out of room or run out of, you know, something.
Speaker B: Right? But it's still incredible that it's lived for as long as it has. And yes, there's many people who believe now that Moore's law is dead, you.
Speaker C: Know, they can join the last 50 years of people who had the same.
Speaker B: Yeah, there's a long tradition, but why do you think, if you can try to understand it, why do you think it's not dead currently?
Speaker C: Just think. People think. Moore's law is one thing. Transistors get smaller, but actually, under the sheet, there's literally thousands of innovations. And almost all those innovations have their own diminishing return curves. So if you graph it, it looks like a cascade of diminishing return curves. I don't know what to call that, but the result is an exponential curve, at least it has been. And we keep inventing new things. So if you're an expert in one of the things on a diminishing return curve, right, and you can see its plateau, you will probably tell people, well, this is done. Meanwhile, some other pile of people are doing something different. So that's just normal. So then there's the observation of how small could a switching device be? So a modern transistor is something like 1000 by 1000 by 1000 atoms, and you get quantum effects down around two to ten atoms. So you can imagine a transistor as small as ten by ten by ten. So that's a million times smaller. And then the quantum computational people are working away at how to use quantum effects. So.
Speaker B: A thousand by thousand by thousand atoms, that's a really clean way of putting it.
Speaker C: Well, a fan, like a modern transistor, if you look at the fan, it's like 120 atoms wide, but we can make that thinner. And then there's a gate wrapped around it, and then there's spacing there's a whole bunch of geometry, and a competent transistor designer could count both atoms in every single direction. Like, there's techniques now to already put down atoms in a single atomic layer, and you can place atoms if you want to. It's just, you know, from a manufacturing process, if placing an atom takes ten minutes and you need to put ten to the 23rd atoms together to make a computer, it would take a long time. So the methods are both shrinking things and then coming up with effective ways to control what's happening.
Speaker B: Manufacture stably and cheaply.
Speaker C: Yeah. So the innovation stack's pretty broad. There's equipment, there's optics, there's chemistry, there's physics, there's materials science, there's metallurgy. There's lots of ideas about when you put different materials together, how do they interact? Are they stable? Is stable over temperature? Are there repeatable? You know, there's, like, literally thousands of technologies involved.
Speaker B: But just for the shrinking, you don't think we're quite yet close to the fundamental limits of physics.
Speaker C: I did a talk on Moore's law, and I asked for a roadmap to a path of 100. And after two weeks, they said, we only got to 5100.
Speaker B: What?
Speaker C: Sorry, 100 x shrink.
Speaker B: 100 x shrink. We only got to 50 50.
Speaker C: And I said, why don't you give it another two weeks? Well, here's the thing about Moore's law, right? So I believe that the next ten or 20 years of shrinking is going to happen right now, as a computer designer, you have two stances. You think it's going to shrink, in which case you're designing and thinking about architecture in a way that you'll use more transistors, or conversely, not be swamped by the complexity of all the transistors you get. You have to have a strategy, so.
Speaker B: You'Re open to the possibility and waiting for the possibility of a whole new army of transistors ready to work.
Speaker C: I'm expecting more transistors every two or three years by a number large enough that how you think about design, how you think about architecture has to change. Like, imagine you build brick buildings out of bricks, and every year, the bricks are half the size, or every two years. Well, if you kept building bricks the same way, you know, so many bricks per person per day, the amount of time to build a building would. Would go up exponentially. Right, right. But if you said, I know that's coming, so now I'm going to design equipment that moves bricks faster, uses them better, because maybe you're getting something out of the smaller bricks, more strength, thinner walls, you know, less material efficiency out of that. So once you have a roadmap with what's going to happen, transistors, they're going to get, we're going to get more of them. Then you design all this collateral around it to take advantage of it and also to cope with it. Like, that's the thing people don't understand. It's like if I didn't believe in Moore's law and then Moore's law, transistors showed up, my design teams were all drowned.
Speaker B: So what's the, what's the hardest part of this inflood of new transistors? I mean, even if you just look historically throughout your career, what's the thing? What fundamentally changes when you add more transistors in the task of designing an architecture?
Speaker C: Well, there's two constants, right? One is people don't get smarter.
Speaker B: By the way, there's some science showing that we do get smarter because of nutrition, whatever. Sorry to bring that.
Speaker C: Plant effect.
Speaker B: Yes.
Speaker C: Yeah. Familiar with it. Nobody understands it. Nobody knows if it's still going on. So that's.
Speaker B: Or whether it's real or not. But yeah, I sort of, anyway, but not explicitly.
Speaker C: I would believe for the most part, people aren't getting much smarter.
Speaker B: The evidence doesn't support it. That's right.
Speaker C: And then teams can't grow that much. Right, right. So human beings, you know, we're really good in teams of ten, you know, up to teams of 100, they can know each other. Beyond that, you have to have organizational boundaries. So those are pretty hard constraints. So then you have to divide and conquer. As the designs get bigger, you have to divide it into pieces. The power of abstraction layers is really high. We used to build computers out of transistors. Now we have a team that turns transistors into logic cells and another team that turns them into functional units, another one that turns into computers. So we have abstraction layers in there. And you have to think about when do you shift gears on that. We also use faster computers to build faster computers. So some algorithms run twice as fast on new computers, but a lot of algorithms are n squared. So, you know, a computer with twice as many transistors and it might take four times as long to run. So you have to refactor the software. Like simply using faster computers to build bigger computers doesn't work. So you have to think about all these things.
Speaker B: So in terms of computing performance and the exciting possibility that more powerful computers bring is shrinking. The thing we just been talking about one of the, for you, one of the biggest exciting possibilities of advancement in performance, or is there other directions that you're interested in, like in the direction of sort of enforcing given parallelism, or like doing massive parallelism in terms of many, many cpu's, stacking, cpu's on top of each other, that kind of parallelism, or any kind of, well, think about.
Speaker C: It a different way. So old computers, slow computers, you said a equal b plus c times d, pretty simple. And then we made faster computers with vector units, and you can do proper equations and matrices, and then modern AI computations, or convolutional neural networks, where you convolve one large dataset against another. And so there's sort of this hierarchy of mathematics, from simple equation, to linear equations, to matrix equations, to deeper kind of computation. And the data sets are getting so big that people are thinking of data as a topology problem. You know, data is organized in some immense shape, and then the computation, which sort of wants to be, get data from immense shape and do some computation on it. So the, what computers have allowed people to do is have algorithms go much, much further. So that paper you, you referenced, the Sutton paper, they talked about, you know, like when AI started, it was apply rule sets to something. That's a very simple computational situation. And then when they did first chess thing, they solved deep searches. So have a huge database of moves and results, deep search, but it's still just a search right now. We take large numbers of images and we use it to train these weight sets that we convolve across. It's a completely different kind of phenomena. We call that AI, and now they're doing the next generation. And if you look at it, they're going up this mathematical graph, right? And then computations, both computation and data set support going up that graph. Yeah.
Speaker B: The kind of computation, though, might. I mean, I would argue that all of it is still a search, right? Just like you said, a topology problem of datasets. You're searching the data sets for valuable data, and also the actual optimization of neural networks is a kind of search for the.
Speaker C: I don't know, if you had looked at the inner layers of finding a cat, it's not a search, it's a set of endless projections. So a projection, here's a shadow of this phone, and then you can have a shadow of that onto something, a shadow on that of something. If you look in the layers, you'll see this layer actually describes pointy ears and round eye ness and fuzziness. But the computation to tease out the attributes is not search.
Speaker B: Right?
Speaker C: I mean, like the inference part might be search, but the training is not search. And then in deep networks, they look at layers and they don't even know it's represented. And yet if you take the layers out, it doesn't work. Okay, so I don't think it's search. All right, well, but you have to talk to a mathematician about what that actually is.
Speaker B: We could disagree, but it's just semantics. I think it's not, but it's certainly not.
Speaker C: I would say it's absolutely not semantics, but.
Speaker B: Okay. All right, well, if you want to go there. So, optimization to me is search, and we're trying to optimize the ability of a neural network to detect cad ears. And the difference between chess and the space, the incredibly multi dimensional, hundred thousand dimensional space that neural networks are trying to optimize over is nothing like the chess board database. So it's a totally different kind of thing. Okay. In that sense, you can say, yeah, it loses.
Speaker C: I can see how you might say if you. The funny thing is it's the difference between given search space and found search space.
Speaker B: Right? Exactly.
Speaker C: Yeah. Maybe that's a different way to.
Speaker B: That's a beautiful way to put it. Okay, but you're saying, what's your sense in terms of the basic mathematical operations and the architectures compute hardware that enables those operations? Do you see the cpu's of today still being a really core part of executing those mathematical operations?
Speaker C: Yes. Well, the operations continue to be add, subtract, load, store, compare and branch. It's remarkable. So it's interesting that the building blocks of computers are transistors under that, atoms. So you got atoms, transistors, logic gates, computers, functional units, and computers. The building blocks of mathematics at some level, are things like adds and subtracts and multiplies. The space mathematics can describe is, I think, essentially infinite, but the computers that run the algorithms are still doing the same things. Now, a given algorithm might say, I need sparse data, or I need 32 bit data, or I need a convolution operation that naturally takes eight bit data, multiplies it, and sums it up a certain way. So the data types in tensorflow imply an optimization set. But when you go right down and look at the computers, it's Ann and and or gates doing adds and multiplies. Like, like that hasn't changed much. Now the quantum researchers think they're going to change that radically. And then there's people who think about analog computing, because you look in the brain and it seems to be more analog ish. You know that maybe there's a way to do that more efficiently. But we have a million x on computation, and I don't know the relationship between computational, let's say, intensity and ability to hit mathematical abstractions. I don't know anybody's described that. But just like you saw in AI, you went from rule sets to simple search to complex search, to, say, found search, like those are orders of magnitude more computation to do. And as we get the next two orders of magnitude, like a friend Raja Godori said, every order of magnitude changes the computation.
Speaker B: It fundamentally changes what the computation is doing.
Speaker C: Yeah. Oh, you know the expression, the difference in quantity is a difference in kind? The difference between ant and anthill, or neuron and brain. There's indefinable place where the quantity changed the quality. We've seen that happen in mathematics multiple times, and my guess is it's going to keep happening.
Speaker B: So your sense is that if you focus head down and shrinking the transistor.
Speaker C: Well, it's not just head down. We're aware of the software stacks that are running and the computational loads of. We're kind of pondering, what do you do with a petabyte of memory that wants to be accessed in a sparse way and have the kind of calculations AI programmers want? So there's a dialogue interaction, but when you go in the computer chip, you find adders and subtractors and multipliers.
Speaker B: So if you zoom out, then with, as you mentioned, Ray Sutton, the idea that most of the development in the last many decades in AI research came from just leveraging computation and just simple algorithms waiting for the computation to improve.
Speaker C: Well, software guys have a thing that they call it the problem of early optimization. So you write a big software stack, and if you start optimizing, the first thing you write, the odds of that being the performance limiter is low. But when you get the whole thing working, can you make it two x faster by optimizing the right things? Sure. While you're optimizing that, could you've written a new software stack, which would have been a better choice? Maybe now you have creative tension, but.
Speaker B: The whole time, as you're doing the writing, that's the software we're talking about. The hardware underneath gets faster.
Speaker C: This goes back to the Moore's law. If Moore's law is going to continue, then your AI research should expect that to show up, and then you make a slightly different set of choices. Then we've hit the wall, nothing's going to happen. And from here, it's just us rewriting algorithms like that seems like a failed strategy for the last 30 years of Moore's law's death.
Speaker B: Can you just linger on it? I think you've answered it, but I'll just ask the same dumb question over and over. Why do you think Moore's law is not going to die? Which is the most promising, exciting possibility of why it won't die in the next 510 years. So is it the continued shrinking of the transistor, or is it another s curve that steps in and it totally.
Speaker C: Sort of, well, shrinking the transistor is literally thousands of innovations.
Speaker B: Right. So there's.
Speaker C: So there's a whole bunch of s curves just kind of running their course and being reinvented. And I. New things. You know, the semiconductor fabricators and technologists have all announced what's called nanowire. So they took a fin, which had a gate around it, and turned that into little wires, so you have better control of that, and they're smaller. And then from there, there's some obvious steps about how to shrink that. The metallurgy around wire stacks and stuff has very obvious abilities to shrink. And, you know, there's a whole combination of things there to do.
Speaker B: Your sense is that we're gonna get a lot if this innovation formed just that. Shrinking.
Speaker C: Yeah. Like a factor of one hundred's a lot.
Speaker B: Yeah. I would say that's incredible. And it's totally.
Speaker C: It's only ten or 15 years now.
Speaker B: You're smarter, you might know, but to me, it's totally unpredictable of what that hundred x would bring in terms of the nature of the computation that people would be.
Speaker C: Yeah. Are you familiar with Bell's law? So, for a long time, it was mainframes, minis, workstation, PC, mobile. Moore's law drove faster, smaller computers. And then when we were thinking about Moore's law, Raja Guduri said, every ten x generates a new computation. So scalar vector matrix. Topological computation. Right. And if you go look at the industry trends, there was mainframes and then mini computers and NPC's, and then the Internet took off, and then we got mobile devices, and now we're building 5g wireless with one millisecond latency. And people are starting to think about the smart world where everything knows you, recognizes you, the transformations are going to be, like, unpredictable.
Speaker B: How does it make you feel that you're one of the key architects of this kind of future? So you're not. We're not talking about the architects of the high level people who build the angry Bird apps and snaps angry bird apps.
Speaker C: Who knows? I'm gonna take a stand of the universe.
Speaker B: I'm gonna take a stand at that. And the attention distracting nature of mobile phones. I'll take a stand. But anyway, in terms of, I think that matters much. The side effects of smartphones or the attention distraction. Which part?
Speaker C: Well, who knows where this is all leading? It's changing so fast. My parents used to yell at my sisters for hiding in the closet with a wired phone with a dial on it. Stop talking to your friends all day. Now my wife yells at my kids for talking to her friends all day on text. It looks the same to me.
Speaker B: It's always echoes of the same thing. You are one of the key people architecting the hardware of this future. How does that make you feel? Do you feel responsible? Do you feel excited?
Speaker C: So we're in a social context. So there's billions of people on this planet. There are literally millions of people working on technology. I feel lucky to be doing what I do and getting paid for it. There's an interest in it, but there's so many things going on in parallel. It's like the actions are so unpredictable. If I wasn't here, somebody else would do it. The vectors of all these different things are happening all the time. There's, I'm sure, some philosopher or metaphhilosophers wondering about how we transform our world.
Speaker B: You can't deny the fact that these tools, whether these tools are changing our world.
Speaker C: That's right.
Speaker B: So do you think it's changing for the better?
Speaker C: I read this thing recently. It said the two disciplines with the highest GRe scores in college are physics and philosophy. And they're both sort of trying to answer the question, why is there anything? And the philosophers are on the theological side, and the physicists are obviously on the material side. And there's 100 billion galaxies with 100 billion stars. It seems, well, repetitive at best. So there's on our way to 10 billion people. I mean, it's hard to say what it's all for, if that's what you're asking.
Speaker B: Yeah, I guess I.
Speaker C: Things do tend to significantly increases in complexity, and I'm curious about how computation, like our world, our physical world, inherently generates mathematics. It's kind of obvious, right? So we have XYZ coordinates. You take a sphere, you make it bigger, you get a surface that falls, you know, grows by r squared like it generally generates mathematics. And the mathematicians and the physicists have been having a lot of fun talking to each other for years, and computation has been let's say, relatively pedestrian, like computation in terms of mathematics has been doing binary algebra while those guys have been gallivanting through the nether realms of possibility. Now, recently the computation lets you do mathematical computations that are sophisticated enough that nobody understands how the answers came out.
Speaker B: Machine learning.
Speaker C: Machine learning. It used to be you get dataset, you guess at a function. The function is considered physics if it's predictive of new functions, new data sets. Modern, you can take a large data set with no intuition about what it is and use machine learning to find a pattern that has no function. And it can arrive at results that I don't know if they're completely mathematically describable. So computation has kind of done something interesting compared to a equal b plus.
Speaker B: C. There's something reminiscent of that step from the basic operations of addition to taking a step towards neural networks that's reminiscent of what life on earth at its origins was doing. Do you think we're creating sort of the next step in our evolution in creating artificial intelligence systems that will?
Speaker C: I don't know. I mean, there's so much in the universe already, it's hard to say where.
Speaker B: We stand in this whole thing.
Speaker C: Are human beings working on additional abstraction layers and possibilities? Yeah, it appears so. Does that mean that human beings don't need dogs? You know, no. There's so many things that are all simultaneously interesting and useful.
Speaker B: Well, you've seen throughout your career, you've seen greater and greater level abstractions built in artificial machines, right? Do you think when you look at humans, do you think look of all life on earth as a single organism building this thing, this machine with greater and greater levels of abstraction? Do you think humans are the peak, the top of the food chain in this long arc of history on earth? Or do you think we're just somewhere in the middle? Are we, are we the basic functional operations of a cpu? Are we the C program, the Python program, or with the neural network, like.
Speaker C: Somebody'S, you know, people have calculated, like how many operations does the brain do something? You know, I've seen the number ten to the 18th a bunch of times, arrived different ways. So could you make a computer that did ten to the 20th operations?
Speaker B: Yes, sure.
Speaker C: Do you think we're gonna do that now? Is there something magical about how brains compute things? I don't know. You know, my personal experiences is interesting because, you know, you think, you know how you think and then you have all these ideas and you can't figure out how they happened. And if you meditate, you know, the like what? What you can be aware of is interesting. So I don't know if brains are magical or nothing. You know, the physical evidence says no. Lots of people's personal experience says yes. So what would be funny is if brains are magical and yet we can make brains with more computation. You know, I don't know what to.
Speaker B: Say about that, but do you think magic is an emergent phenomena?
Speaker C: I have no explanation for it.
Speaker B: Let me ask Jared Keller of what, in your view, is consciousness?
Speaker C: What's consciousness?
Speaker B: Yeah, like what? You know, consciousness love. Things that are these deeply human things that seems to emerge from our brain. Is that something that will be able to make encode in chips that get faster and faster and faster and faster?
Speaker C: That's like a ten hour conversation. Nobody really knows.
Speaker B: Can you summarize it in a couple of.
Speaker C: Couple of words? Many people have observed that organisms run at lots of different levels. If you had two neurons, somebody said you'd have one sensory neuron and one motor neuron. So we move towards things and away from things, and we have physical integrity and safety or not. Then if you look at the animal kingdom, you can see brains that are a little more complicated. And at some point, there's a planning system, and then there's an emotional system that's, you know, happy about being safe or unhappy about being threatened. Right? And then our brains have massive numbers of structures, you know, like planning and movement and thinking and feeling and drives and emotions. And we seem to have multiple layers of thinking systems. And we have a brain, a dream system, that nobody understands whatsoever, which I find completely hilarious. And you can think in a way that those systems are more independent, and you can observe, you know, the different parts of yourself can observe them. I don't know which one's magical. I don't know which one's not computational.
Speaker B: So is it possible that it's all computation?
Speaker C: Probably. Is there a limit to computation? I don't think so.
Speaker B: Do you think the universe is a computer?
Speaker C: Like, it's a weird kind of computer, because if it was a computer, right, like, when they do calculations on what it. How much calculation it takes to describe quantum effects is unbelievably high. So, if it was a computer, wouldn't you have built it out of something that was easier to compute? Right. That's. That's a funny. It's a funny system. But then the simulation guys have pointed out that the rules are kind of interesting. Like, when you look really close, it's uncertainty. And the speed of light says, you can only look so far. And things can't be simultaneous, except for the odd entanglement problem where they seem to be like, the rules are all kind of weird. And somebody said physics is like having 50 equations with 50 variables to define 50 variables. Physics itself has been a shit show for thousands of years. It seems odd when you get to the corners of everything. It's either uncomputable or uncomfortable, definable or uncertain.
Speaker B: It's almost like the designers of the simulation are trying to prevent us from understanding it perfectly.
Speaker C: But also, the things that require calculations require so much calculation that our idea of the universe, of a computer is absurd, because every single little bit of it takes all the computation in the universe to figure out. Well, that's a weird kind of computer. You know, you say simulation is running in the computer, which has, by definition, infinite computation.
Speaker B: Not infinite. Oh, you mean if the universe is infinite?
Speaker C: Yeah. Well, every little piece of our universe seems to take infinite computation and figure.
Speaker B: Out how to do just a lot.
Speaker C: Well, a lot's a pretty big number. Compute. This little teeny spot takes all the mass in the local one light year by one light year space. It's close enough to infinite.
Speaker B: Oh, it's a heck of a computer if it is one.
Speaker C: I know. It's. It's a weird description because the simulation description seems to break when you look closely at it. But the rules of the universe seem to imply something's up. That seems a little arbitrary.
Speaker B: The universe, the whole thing, the laws of physics, it just seems like, how did it come out to be the way it is?
Speaker C: But lots of people talk about that. Like I said, the two smartest groups of humans are working on the same problem from different, different aspects, and they're both complete failures. So that's kind of cool.
Speaker B: They might succeed eventually.
Speaker C: Well, after 2000 years, the trend isn't good.
Speaker B: 2000 years is nothing in the span of the history of the universe. So we have some time.
Speaker C: But the next thousand years doesn't look good either.
Speaker B: So that's what everybody says at every stage. But with Moore's law, as you've just described, not being dead, the exponential growth of technology, the future seems pretty incredible.
Speaker C: Well, it'll be interesting, that's for sure.
Speaker B: That's right. So what are your thoughts on Ray Kurzweil sense that exponential improvement in technology will continue indefinitely? Is that how you see Moore's law? Do you see Moore's law more broadly, in sense that technology of all kinds has a way of stacking s curves on top of each other, where it'll be exponential. And then we'll see all kinds of.
Speaker C: What does an exponential of a million mean? That's. That's a pretty amazing number. And that's just for a local little piece of silicon. Now, let's imagine you, say, decided to get 1000 tons of silicon to collaborate in one computer at a million times the density. Like, now. Now you're talking, I don't know, ten to the 20th more computation power than our current already unbelievably fast computers. Like, nobody knows what that's going to mean. You know, the Sci-Fi guys call it computronium, like when, like, a local civilization turns the nearby star into a computer. Right? Like, I don't. That's true.
Speaker B: But so just even when you shrink.
Speaker C: The transistor, the, that's only one dimension.
Speaker B: The ripple effects of that.
Speaker C: Like, people tend to think about computers as a cost problem, right? So computers are made out of silicon and minor amounts of metals and, you know, this and that. None of those things cost any money. Like, there's plenty of sand. Like. Like you could just turn the beach and a little bit of ocean water into computers. So all the cost is in the equipment to do it. And the trend on equipment is once you figure out how to build equipment, the trend of cost is zero. Elon said, first you figure out what configuration you want the atoms in and then how to put them there. Right?
Speaker B: Yeah.
Speaker C: Well, what, here's the, you know, his great insight is people are how constrained. I have this thing, I know how it works, and then little tweaks to that will generate something as opposed to what do I actually want, and then figure out how to build it. It's a very different mindset, and almost nobody has it, obviously.
Speaker B: Well, let me ask on that topic. You were one of the key early people in the development of autopilot, at least in the hardware side. Elon Musk believes that autopilot and vehicle autonomy, if we just look at that problem, can follow this kind of exponential improvement. In terms of the how question that we're talking about, there's no reason why it can't. What are your thoughts on this particular space of vehicle autonomy? And you're part of it, and Elon Musk's and Tesla's vision for, well, the.
Speaker C: Computer you need to build was straightforward, and you could argue, well, does it need to be two times faster or five times or ten times? But that's just a matter of time or price in the short run, so that's not a big deal. You don't have to be especially smart to drive a cardinal. So it's not like a super hard problem. I mean, the big problem with safety is attention, which computers are really good at, not skills.
Speaker B: Well, let me push back on one. You see, everything you said is correct, but we, as humans, tend to, tend to take for granted how incredible our vision system is.
Speaker C: So you can drive a car with 2050 vision and you can train a neural network to extract the distance of any object and the shape of any surface from a video and data. It's really simple.
Speaker B: No, it's not simple.
Speaker C: That's a simple data problem.
Speaker B: It's not simple because it's not just detecting object, it's understanding the scene, and it's being able to do it in a way that doesn't make errors. So the beautiful thing about the human vision system and our entire brain around the whole thing is we're able to fill in the gaps. It's not just about perfectly detecting cars, it's inferring the occluded cars. It's trying to. It's understanding the.
Speaker C: I think that's mostly a data problem.
Speaker B: So you think what data would compute with improvement of computation, with improvement in collection?
Speaker C: Well, there is a. You know when you're driving a car and somebody cuts you off, your brain has theories about why they did it. You know, they're a bad person, they're distracted, they're dumb. You know, you can listen to yourself, right? So, you know, if you think that narrative is important to be able to successfully drive a car, then current autopilot systems can't do it. But if cars are ballistic things with tracks and probabilistic changes of speed and direction, and roads are fixed and given by the way, they don't change dynamically. You can map the world really thoroughly. You can place every object really thoroughly, you can calculate trajectories of things really thoroughly.
Speaker B: But everything you said about really thoroughly has a different degree of difficulty.
Speaker C: You could say at some point, computer autonomous systems will be way better at things that humans are lousy at. Like, they'll be better at attention. They'll always remember there was a pothole in the road that humans keep forgetting about. They'll remember that this set of roads has these weird old lines on it that the computers figured out once, and especially if they get updates. So somebody changes a given. Like the key to robots and stuff, somebody said, is to maximize the givens, right?
Speaker B: Right.
Speaker C: So having a robot pick up this bottle cap is way easier if you put a red dot on the top, because then you have to figure out, you know, and if you want to do a certain thing with it, you know, maximize the givens is the thing. And autonomous systems are happily maximizing the givens like humans. When you drive someplace new, you remember it because you're processing it the whole time. And after the 50th time you drove to work, you get to work, you don't know how you got there. Right. You're on autopilot. Right. Autonomous cars are always on autopilot, but the cars have no theories about why they got cut off or why they're in traffic. So that never stopped paying attention.
Speaker B: Right. So I tend to believe you do have to have theories. Met the models of other people, especially with pedestrian cyclists, but also with other cars. So everything you said is like. Is actually essential to driving. Driving is a lot more complicated than people realize. I think. So sort of to push back slightly.
Speaker C: But cut into traffic. Right.
Speaker B: Yep.
Speaker C: You can't just wait for a gap. You have to be somewhat aggressive. You'll be surprised how simple a calculation for that is.
Speaker B: I may be on that particular point, but there's. It. I. Maybe I asked you to push back. I would be surprised. You know what?
Speaker C: Yeah.
Speaker B: I'll just say where I stand. I would be very surprised. But I think it's. You might be surprised how complicated it is that I say.
Speaker C: I tell people progress disappoints in the short run, surprises in the long run.
Speaker B: It's very possible. Yeah.
Speaker C: I suspect in ten years it'll be just like, taken for granted.
Speaker B: Yeah, probably. But you're probably right now look like.
Speaker C: It'S going to be a $50 solution that nobody cares about. It's like, GPS is like, wow. GPS is. We have satellites in space that tell you where your location is. It was a really big deal. Now everything has a gps in it.
Speaker B: Yeah, that's true. But I do think that systems that involve human behavior are more complicated than we give them credit for. So we can do incredible things with technology that don't involve humans.
Speaker C: But when you think humans are less complicated than people, you know, frequently obscribe, maybe I. We tend to operate out of large numbers of patterns and just keep doing it over and over.
Speaker B: But I can't trust you because you're human. That's something. Something a human would say. But my hope is on the point you've made is even if no matter who's right, I'm hoping that there's a lot of things that humans aren't good at that machines are definitely good at. Like you said, attention and things like that. Well, they'll be so much better that the overall picture of safety and autonomy will be. Obviously, cars will be safer, even if they're not as good at.
Speaker C: I'm a big believer in safety. I mean, there are already the current safety systems, like cruise control, that doesn't let you run into people, and lane keeping. There are so many features that you just look at the Pareto of accidents and knocking off, like, 80% of them is, you know, super doable just to linger on.
Speaker B: The autopilot team and the efforts there, it seems to be that there's a very intense scrutiny by the media and the public in terms of safety. The pressure the bar put before autonomous vehicles. What are your sort of, as a person there, working on the hardware and trying to build a system that builds a safe vehicle and so on, what was your sense about that pressure? Is it unfair? Is it expected of new technology?
Speaker C: Yeah, it seems reasonable. I was interested, I talked to both american and european regulators, and I was worried that the regulations would write into the rules. Technology solutions like modern brake systems imply hydraulic brakes. So if you read the regulations, to meet the letter of the law for brakes, it sort of has to be hydraulic. Right? And the regulator said they're interested in the use cases like a head on crash, an offset crash, don't hit pedestrians, don't run into people, don't leave the road, don't run a red light or a stoplight. They were very much into the scenarios, and they had all the data about which scenarios injured or killed the most people. And for the most part, those conversations were like, what's the right thing to do to take the next step. Now, Elon's very interested also in the benefits of autonomous driving or freeing people's time and attention, as well as safety. And I think that's also an interesting thing. You know, building autonomous systems so they're safe and safer than people seemed. Since the goal is to be ten x safer than people, having the bar to be safer than people and scrutinizing accidents seems philosophically, you know, correct. So I think that's a good thing.
Speaker B: What are, it's different than the things you worked at the Intel AMD Apple with autopilot chip design and hardware design. What are interesting or challenging aspects of building this specialized kind of computing system in the automotive space?
Speaker C: I mean, there's two tricks to building like an automotive computer. One is the software team. The machine learning team is developing algorithms that are changing fast. So as you're building the accelerator, you have this worry or intuition that the algorithms will change enough that the accelerator will be the wrong one. There's a generic thing, which is if you build a really good general purpose computer, say its performance is one, and then GPU guys will deliver about five x to performance for the same amount of silicon, because instead of discovering parallelism, you're given parallelism. Special accelerators get another two to five x on top of a GPU. Because you say, I know the math is always eight bit integers into 32 bit accumulators, and the operations are the subset of mathematical possibilities. So, auto, you know, AI accelerators have a claimed performance benefit over GPU's, because in the narrow math space, you're nailing the algorithm. Now, you still try to make it programmable, but the AI field is changing really fast. So there's a, you know, there's a little creative tension there of, I want the acceleration afforded by specialization without being over specialized, so that the new algorithm is so much more effective that you would have been better off on a gpu. So there is a tension there. To build a good computer for an application like automotive, there's all kinds of sensor inputs and safety processors and a bunch of stuff. So one of Elon's goal is to make it super affordable. So every car gets an autopilot computer. So some of the recent startups you look at, and they have a server and a trunk, because they're saying, I'm going to build this autopilot computer replaces the driver. So their cost budget is ten or $20,000. And Elon's constraint was, I'm going to put one every, in every car, whether people buy autonomous driving or nothing. So the cost constraint he had in mind was great. And to hit that, you had to think about the system design. That's complicated. It's fun, it's craftsman's work. Like a violin maker, you can say Stradivarius is this incredible thing, and the musicians are incredible, but the guy making the violin picked wood and sanded it, and then he cut it and he glued it, and he waited for the right day so that when he put the finish on it, YoU KnoW, do something dumb. ThAt's craftsman's work, right? You may be a genius craftsman because you have the best techniques and you discover a new one, but most engineering craftsman's work, and humans really like to do that, you know, express humans? No, everybody.
Speaker B: All humans.
Speaker C: I don't know. I used to. I dug ditches when I was in college. I got really good at it. Satisfying. Yeah.
Speaker B: So digging ditches, also craftsman.
Speaker C: Yeah. Of course. So there's an expression called complex mastery behavior. So when you're learning something, that's fine because you're learning something. When you do something, it's row it and simple. It's not that satisfying. But if the steps that you have to do are complicated and you're good at them, it's satisfying to do them. And then if you're intrigued by it all as you're doing them, you sometimes learn new things that you can raise your game. But Cressman's work is good in engineers. Like, engineering is complicated enough that you have to learn a lot of skills. And then a lot of what you do is then craftsman's work, which is fun.
Speaker B: Autonomous driving, building a very resource constrained computer. So a computer has to be cheap enough to put in every single car. That essentially boils down to craftsman's work. It's engineering.
Speaker C: You know, there's thoughtful decisions and problems to solve and trade offs to make. You need ten camera imports or eight, you know, you're building for the current car or the next one. You know, how do you do the safety stuff? You know, there's a whole bunch of details, but it's fun. But it's not like I'm building a new type of neural network which has a new mathematics and a new computer to work. You know, that that's like, there's, there's more invention than that, but the rejection to practice. Once you pick the architecture, you look inside and what do you see? Adders and multipliers, memories and, you know, the basics. So computers, there's always this, this weird set of abstraction, layers of ideas and thinking that reduction to practice is transistors and wires and, you know, pretty basic stuff. And that's an interesting phenomena, by the way. Like factory work. Like, lots of people think factory work is road assembly stuff. I've been on the assembly line. Like, the people who work there really like it. It's a really great job. It's really complicated. Putting cars together is hard, right? And the car is moving and the parts are moving and sometimes the parts are damaged and you have to coordinate putting all the stuff together. And people are good at it. They're good at it. And I remember one day I went to work and the line was shut down for some reason. And some of the guys sitting around were really bummed because they had reorganized a bunch of stuff and they were going to hit a new record for the number of cars built that day. And they were all gung ho to do it. And these were big, tough buggers you know, but what they did was complicated and you couldn't do it.
Speaker B: Yeah.
Speaker C: And I mean, well, after a while you could, but you'd have to work your way up because, you know, like, putting the bright, what's called the brights, the trim on a car on a moving assembly line where it has to be attached 25 places in a minute and a half is unbelievably complicated. And human beings can do it. It's really good. I think that's harder than driving a.
Speaker B: Car, by the way, putting together, working.
Speaker C: On a factory, too.
Speaker B: Smart people can disagree.
Speaker C: Yeah.
Speaker B: I think driving a car, we'll get.
Speaker C: You in the factory someday and then we'll see how it goes.
Speaker B: Not for us humans. Driving a car is easy. I'm saying building a machine that drives a car is not easy. No. Okay. Driving a car is easy for humans because we've been evolving for billions of years.
Speaker C: Drive cars. Yeah, the pale lithic cars are super cool.
Speaker B: Now you join the rest of the Internet in mocking me.
Speaker C: Okay. Intrigued by your, you know, your anthropology. Yeah, I'll have to go dig into that.
Speaker B: There's some inaccuracies there. Yes. Okay. But in general, what have you learned in terms of thinking about passion, craftsmanship, tension, chaos. Jesus, the whole mess of it, what have you learned have taken away from your time working with Elon Musk, working at Tesla, which is known to be a place of chaos, innovation, craftsmanship, and all that.
Speaker C: I really liked the way he thought. Like, you think you have an understanding about what first principles of something is, and then you talk to Elon about it, and you. You didn't scratch the surface. You know, he has a deep belief that no matter what you do is a local maximum. Right. And I had a friend, he invented a better electric motor, and it was, like, a lot better than what we were using. And one day he came by, he said, you know, I'm a little disappointed because, you know, this is really great. And you didn't seem that impressed. And I said, you know, when the super intelligent aliens come, are they going to be looking for you? Like, where is he? The guy who built the motor? Yeah, probably not, you know, like, but doing interesting work that's both innovative and let's say craftsman's work on the current thing is really satisfying. It's good, and that's cool. And then Elon was good at taking everything apart. Like, what's the deep first principle? Oh, no, what's really. No, what's really. That ability to look at it without assumptions and how constraints is super wild. We build rocket ship and use an electric car, everything. And that's super fun. And he's into it, too. Like when they first landed two SpaceX rockets to Tesla, we had a video projector in the big room, and like 500 people came down. And when they landed, everybody cheered and some people cried. It was so cool. But how did you do that? Well, it was super hard. And then people say, well, it's chaotic, really, to get out of all your assumptions, you think that's not going to be unbelievably painful. And is Elon tough? Yeah, probably. Do people look back on it and say, boy, I'm really happy I had that experience to go take apart that many layers of assumptions. Sometimes super fun, sometimes painful.
Speaker B: So it could be emotionally and intellectually painful, that whole process just stripping away assumptions.
Speaker C: Yeah. Imagine 99% of your thought process is protecting your self conception, and 98% of that's wrong. Now you got the math right. How do you think you're feeling when you get down to that one bit that's useful? And now you're open and you have the ability to do something different. I don't know if I got the math right. It might be 99.9, but it ain't 50.
Speaker B: Imagining it, the 50% is hard enough.
Speaker C: Yeah. Now, for a long time, I've suspected you could get better. Like, you can think better, you can think more clearly, you can take things apart, and there's lots of examples of that, people who do that, so.
Speaker B: And Elon is an example of that. You are an example.
Speaker C: So I don't know if I am. I'm fun to talk to.
Speaker B: Certainly.
Speaker C: I've learned a lot of stuff.
Speaker B: Right.
Speaker C: Well, here's the other thing is, like, I joke, like, I read books and people think, oh, you read books? Well, no, I've read a couple books a week for 55 years. Wow. Well, maybe 50, because I didn't learn to read until I was eight or something. And it turns out when people write books, they often take 20 years of their life where they passionately did something, reduce it to 200 pages. That's kind of fun. And then you go online and you can find out who wrote the best books and who, like, you know, that's kind of wild. So there's this wild selection process, and then you can read it and for the most part, understand it, and then you can go apply it. Like, I went to one company, I thought, I haven't managed much before, so I read 20 management books, and I started talking to them and basically compared to all the vps running around. I'd ran, read 19 more management books than anybody else. It wasn't even it that hard. And half the stuff worked. Like, first time, it wasn't even rocket science.
Speaker B: But at the core of that is questioning the assumptions or sort of entering the thinking, first principles thinking, sort of looking at the reality of the situation and using. Using that knowledge, applying that knowledge.
Speaker C: Yes. So I would say my brain has this idea that you can question first assumptions and. But I can go days at a time and forget that. And you have to kind of like, circle back to that observation because it is emotionally challenging. Well, it's hard to just keep it front and center because, you know, you're. You operate on so many levels all the time, and, you know, getting this done takes priority, or, you know, being happy takes priority, or, you know, screwing around takes priority. Like. Like how you go through life is complicated, and then you remember, oh, yeah, I could really think first principles. Oh, shit, that's. That's tiring, you know, but you do for a while, and that's kind of cool.
Speaker A: So just as a last question, in.
Speaker B: Your sense, from the big picture, from the first principles, do you think. You kind of answered already, but do you think autonomous driving is something we can solve on a timeline of years? So one, two, 3510 years, as opposed to a century?
Speaker C: Yeah, definitely.
Speaker B: Just to linger on it a little longer. Where's the confidence coming from? Is it the fundamentals of the problem, the fundamentals of building the hardware and.
Speaker C: The software as a computational problem, understanding ballistics roles, topography, it seems pretty solvable. I mean, and you can see this, you know, like. Like speech recognition. For a long time, people are doing, you know, frequency and domain analysis and. And all kinds of stuff, and that didn't work for. At all, right? And then they did deep learning about it, and it worked great. And it took multiple iterations and, you know, autonomous driving is way past the frequency analysis point. You know, use radar, don't run into things, and the data gathering is going up and the computation is going up and the algorithm understanding is going up, and there's a whole bunch of problems getting solved like that.
Speaker B: The data side is really powerful. But I disagree with both you and Elon. I'll tell Elon once again, as I did before, that when you add human beings into the picture, it's no longer a ballistics problem. It's something more complicated. But I could be very well proven wrong.
Speaker C: Cars are hardly damped in terms of rate of change, like the steering and the steering system is really slow compared to a computer. The acceleration of the acceleration is really slow.
Speaker B: Yeah. On a certain time scale, on a ballistics timescale. But human behavior, I don't know.
Speaker C: Yeah, I shouldn't say beings are really slow, too. Weirdly, we operate, you know, half a second behind reality. Nobody really understands that one either. It's pretty funny.
Speaker B: Yeah. Yeah. So we very well could be surprised. And I think with the rate of improvement on all aspects on both the compute and the software and the hardware, there's going to be pleasant surprises all over the place. Speaking of unpleasant surprises, many people have worries about a singularity in the development of AI. Forgive me for such questions. When AI improves exponentially and reaches a point of superhuman level, general intelligence, you know, beyond the point, there's no looking back. Do you share this worry of existential threats from artificial intelligence, from computers becoming superhuman level intelligence?
Speaker C: No, not really. You know, like, we already have a very stratified society, and then if you look at the whole animal kingdom of capabilities and abilities and interests and, you know, smart people have their niche and, you know, normal people have their niche and craftsmen's have their niche, and, you know, animals have their niche, I suspect that the domains of interest for things that, you know, astronomically different, like the whole something got ten times smarter than us and wanted to track us all down because what, we like to have coffee at Starbucks? Like, it doesn't seem plausible. No. Is there an existential problem that, how do you live in a world where there's something way smarter than you, and you based your kind of self esteem on being the smartest local person? Well, there's, what, 0.1% of the population who thinks that? Because the rest of the population has been dealing with it since they were born. So the breadth of possible experience that can be interesting is really big. You know, superintelligence seems likely, although we still don't know if we're magical, but I suspect we're not. And it seems likely that it'll create possibilities that are interesting for us, and its interests will be interesting for that. For whatever it is, it's not obvious why its interests would somehow want to fight over some square foot of dirt or, you know, whatever the usual fears are about.
Speaker B: So you don't think you'll inherit some of the darker aspects of human nature?
Speaker C: Depends on how you think reality is constructed. So for whatever reason human beings are in, let's say, creative tension and opposition with both our good and bad forces, like, there's lots of philosophical understanding of that, right? I don't know why that would be different.
Speaker B: So you think the evil is necessary for the good? I mean, the tension.
Speaker C: I don't know about evil, but, like, we live in a competitive world where your good is somebody else's, you know, evil, you know, there's the malignant part of it, but that seems to be self limiting, although occasionally it's. It's super horrible.
Speaker B: But yes, there's a debate over ideas, and some people have different beliefs, and that debate itself is a process. So that arriving at something.
Speaker C: Yeah. And why wouldn't that continue?
Speaker B: Yeah, but you don't think that whole process will leave humans behind in a way that's painful, emotionally painful, yes, for the 0.1% they'll be.
Speaker C: Why isn't it already painful for a large percentage of population? And it is. I mean, society does have a lot of stress in it about the 1% and about this and about that, but, you know, everybody has a lot of stress in their life about what they find satisfying. And, you know, know yourself seems to be the proper dictum and pursue something that makes your life meaningful, seems proper. And there's so many avenues on that. Like there's so much unexplored space at every single level. You know, I'm somewhat of. My nephew called me a jaded optimist and, you know, so it's.
Speaker B: There's a beautiful tension that. In that label. But if you were to look back at your life and could relive a moment, a set of moments, because there were the happiest times of your life outside of family, what would that be?
Speaker C: I don't want to relive any moments. I like that. I like that situation where you have some amount of optimism and then the anxiety of the unknown.
Speaker B: So you love the unknown, do you? The mystery of it?
Speaker C: I don't know about the mystery. Sure. Get your blood pumping.
Speaker B: What do you think is the meaning of this whole thing of life on this pale blue dot?
Speaker C: It seems to be what it does. Like, the universe, for whatever reason, makes atoms, which makes us, which we do stuff and we figure out things and we explore things, and that's just what it is. It's not just.
Speaker B: Yeah, it is, Jim. I don't think there's a better place to end it. It's a huge honor and, well, that was super fun. Thank you so much for talking today.
Speaker C: All right, great.
Speaker A: Thanks for listening to this conversation and thank you to our presenting sponsor. Cash app. Download it, use code LexPodcast. You'll get $10, and $10 will go to first, a STEM education nonprofit that inspires hundreds of thousands of young minds to become future leaders and innovators. If you enjoy this podcast, subscribe on YouTube, give it five stars on Apple Podcast, follow on Spotify, support it on Patreon, or simply connect with me on Twitter. And now, let me leave you with some words of wisdom from Gordon Moore. If everything you try works, you aren't trying hard enough. Thank you for listening and hope to.
Speaker B: See you next time.
