Transcription for Dmitry Korkin： Evolution of Proteins, Viruses, Life, and AI ｜ Lex Fridman Podcast #153.mp3:
Full transcript: The following is a conversation with Dmitry Corkin, his second time in the podcast. He's a professor of bioinformatics and computational biology at WPI, where he specializes in bioinformatics of complex disease, computational genomics, systems biology, and biomedical data analytics. He loves biology. He loves computing. Plus, he is russian and recites a poem in Russian at the end of the podcast. What else could you possibly ask for in this world? Quick mention of our sponsors brave browser, netsuite, business management software, magic spoon, low carb cereal and eight sleep self cooling mattress. So the choice is browsing, privacy, business success, healthy diet or comfortable sleep. Choose wisely, my friends, and if you wish, click the sponsor links below to get a discount and to support this podcast. As a side note, let me say that to me, the scientists that did the best, apolitical, impactful, brilliant work of 2020 are the biologists who study viruses without an agenda, without much sleep, to be honest. Just a pure passion for scientific discovery and exploration of the mysteries within viruses. Viruses are both terrifying and beautiful. Terrifying because they can threaten the fabric of human civilization, both biological and psychological. Beautiful because they give us insights into the nature of life on Earth, and perhaps even extraterrestrial life of the not so intelligent variety that might meet us one day as we explore the habitable planets and moons in our universe. If you enjoy this thing, subscribe on YouTube, review it on Apple Podcasts, follow on Spotify support on Patreon, or connect with me on Twitter exfriedmande. And now here's my conversation with Dmitry Corkin. It's often said that proteins and the amino acid residues that make them up are the building blocks of life. Do you think of proteins in this way as the basic building blocks of life? Yes and no. So the proteins, indeed, is the basic unit, biological unit that carries out important functioning of the cell. However, through studying the proteins and comparing the proteins across different species, across different kingdoms, you realize that proteins are actually much more complicated. So they have so called modular complexity. And so what I mean by that is, an average protein consists of several structural units. So we call them protein domains. And so you can imagine a protein as a string of beads where each bead is a protein domain. And in the past 20 years, scientists have been studying the nature of the protein domains because we realize that it's the unit, because if you look at the functions, many proteins have more than one function, and those protein functions are often carried out by those protein domains. We also see that in the evolution, those proteins domains get shuffled so they act actually as a unit. Also from the structural perspective, some people think of a protein as a sort of a globular molecule, but as a matter of fact, the globular part of this protein is a protein domain. So we often have this. Again, the collection of these protein domains align on a string as beads, and. The protein domains are made up of amino acid residues. So this is the basic. So you're saying the protein domain is the basic building block of the function that we think about proteins doing. So, of course, you can always talk about different building blocks. It turtles all the way down. But there's a point where there is at the point of the hierarchy where it's the most, the cleanest element block based on which you can put them together in different kinds of ways to form complex function. And you're saying protein domains. Why is that not talked about as often in popular culture? Well, you know, there are several perspectives on this and one, of course, is the historical perspective. Right. Historically, scientists have been able to structurally resolve to obtain the 3d coordinates of a protein for smaller proteins, and smaller proteins tend to be a single domain protein. So we have a protein equal to a protein domain. And so because of that, the initial suspicion was that the proteins are, have globular shapes. And the more of smaller proteins you obtain structurally, the more you became convinced that that's the case, and only later when we started having alternative approaches. So the traditional ones are X ray crystallography and NMR spectroscopy. So this is sort of the, the two main techniques that give us the 3d coordinates. But nowadays there is huge breakthrough in cryo electron microscopy. So the more advanced methods that allow us to get into the 3d shapes of much larger molecules, molecular complexes. To give you one of the common examples for this year, the first experimental structure of a SARS CoV two protein was the cryo EM structure of the S protein. So the spike protein. And so it was solved very quickly. And the reason for that is the advancement of this technology is pretty spectacular. How many domains does the. Is it more than one domain? Oh, yes. I mean, it's a very complex structure and we, on top of the complexity of a single protein, so this structure actually is a complex, is a trimer, so it needs to form a trimer in order to function properly. What's a complex? A complex is agglomeration of multiple proteins. And so we can have the same protein copied in multiple, you know, made up in multiple copies and forming something that we called a homo oligomer. Homo means the same. Right. So in this case, the spike protein is the. Is an example of a homo tetrame. Homo trimer. Sorry. So three copies of a. Three copies in order to. Exactly. We have these three chains, the three molecular chains coupled together and performing the function. That's when you look at this protein from the top, you see a perfect triangle. So other complexes are made up of different proteins. Some of them are completely different, some of them are similar. The hemoglobin molecule protein complex, it's made of four basic subunits. Two of them are identical to each other and two other identical to each other, but they are also similar to each other, which sort of gives us some ideas about the evolution of this molecule. And perhaps one of the hypotheses is that in the past it was just a homo tetramer. Right. So four identical copies. And then it became, you know, sort of modified, it became mutated over the time and became more specialized. Can we linger on the spike protein for a little bit? Is there something interesting or, like, beautiful you find about it? I mean, first of all, it's an incredibly challenging protein. And so we, as a part of our sort of research to understand the structural basis of this virus, to sort of decode, structurally decode every single protein in its proteome, which, you know, we've been working on this spike protein. And one of the main challenges was that the cryo EM data allows us to reconstruct or to obtain the 3d coordinates of roughly two thirds of the protein. The rest of the one third of this protein, it's a part that is buried into the membrane of the virus and of the viral envelope, and it also has a lot of unstable structures around it. So it's chemically interacting somehow with whatever the hex is connecting. Yeah. People are still trying to understand the nature of and the role of this one third, because the top part, the primary function, is to get attached to the ace two receptor, human receptor. There is also beautiful mechanics of how this thing happens. So because there are three different copies of these chains or there are three different domains. So we're talking about domains. So this is the receptor binding domains, rbds, that gets untangled and get ready to get attached to the receptor. And now they are not necessarily going in a sync mode, as a matter. Of fact, say, synchronous. So, yes, and this is where another level of complexity comes into play, because right now what we see is we typically see just one of the arms going out and getting ready to be attached to the ace two receptors. However, there was a recent mutation that people studied in that spike protein. And very recently, a group from UMass medical school, we happen to collaborate with groups. So this is a group of Jeremy Lubin and a number of other faculty. They actually solve the mutated structure of the spike, and they showed that actually, because of these mutations, you have more than one arms opening up. And so now the frequency of two arms going up increase quite drastically. Interesting. Does that change the dynamics somehow? It potentially can change the dynamics of, because now you have two possible opportunities to get attached to the ace two receptor. It's a very complex molecular process, mechanistic process. But the first step of this process is the attachment of this spike protein, of the spike trimer, to the human ace two receptor. So this is a molecule that sits on the surface of the human cell, and that's essentially what initiates the. What triggers the whole process of, you know, encapsulation. If this was dating, this would be the first date. So this is the. In the way. Yes. So is it. Is it possible to have the spike protein just, like, floating about on its own, or does it need that interactive ability with. With the membrane? Yeah. So it needs to be attached, at least as far as I know. But when you get this thing attached on the surface, there is also a lot of dynamics on how it sits on the surface. For example, there was a recent work, again, where people use the cryoctoral microscopy to get the first glimpse of the overall structure. It's a very low res, but you still get some interesting details about the surface, about what is happening inside, because we have literally no clue until recent work about how the capsid is organized. So capsid is essentially the inner core of the viral particle, where there is the rna of the virus, and it's protected by another protein, n protein that essentially acts as a shield. Now we are learning more and more. So it's actually, it's not just this shield. It's potentially used for the stability of the outer shell of the virus. So it's pretty complicated. And, I mean, understanding all of this is really useful for trying to figure out, like, developing a vaccine or some kind of drug to attack any aspects of this. Right. So, I mean, there are many different implications to that. First of all, you know, it's important to understand the virus itself, right? So in order to understand how it acts, what is the overall mechanistic process of this virus? Replication of this virus, proliferation to the cell. So that's one aspect. The other aspect is designing new treatments. So one of the possible treatments is designing nanoparticles. And so some nanoparticles that will resemble the viral shape that would have the spike integrated and essentially would act as a competitor to the real virus by blocking the ace two receptors and thus preventing the real virus entering the cell. Now, there are also, you know, there is a very interesting direction in looking at the membrane, at the envelope portion of the protein and attacking its m protein. So there are, to give you a brief overview, there are four structural proteins. These are the proteins that made up a structure of the virus spike s protein that acts as a trimer. So it needs three copies. E envelope protein that acts as a pentamer, so it needs five copies to act properly. M is a. Is a membrane protein. It forms dimers, and actually, it forms beautiful lattice. And this is something that we've been studying, and we are seeing it in simulations. It actually forms a very nice grid, or, you know, threads of different dimers attached next to each other. There's a bunch of copies of each other, and they naturally, when you have a bunch of copies of each other, they form an interesting lattice. Exactly. And if you think about this complex, the viral shape needs to be organized somehow, self organized somehow. If it was a completely random process, you probably wouldn't have the envelope shell of the ellipsoid shape. You would have something pretty random shape. So there is some regularity in how these m dimers get to attach to each other in a very specific directed way. Is that understood at all? It's not understood. We are now, we've been working in the past six months since we met, actually, this is where we started working on trying to understand the overall structure of the envelope and the key components that made up this structure. Wait, does the envelope also have the lattice structure, or. No. So the envelope is essentially. Is the outer shell of the viral particle. The n, the nucleocapsid protein is something that is inside. But get that, the n is likely to interact with m. Does it go m and e? Like, where's the e? And so, e, those different proteins, they occur in different copies on the viral particle. So, e, this pentamer complex, we only have two or three, maybe per each particle. Okay. We have thousand or so of m dimers that essentially made up, that makes up the entire, you know, outer shell. So most of the outer shell is. The M dimer and lithium protein. When you say particle, that's the viron, the virus, the individual virus, yes. Single element of the virus. Single virus, single virus, right. And we have about, you know, roughly 50 to 90 spike trimers. Right. So, so, so when you you know, when you show a per virus particle. Per virus particle. Sorry, what did you say? 50 to 90. 50 to 90, right. So this is how this thing is organized. And so now, typically. Right, so you see these, the antibodies that target spike protein, certain parts of the spike protein. But there could be also some treatments. Right. So these are small molecules that bind strategic parts of these proteins, disrupting its functioning. So one of the promising directions, it's one of the newest directions, is actually targeting the mdimer of the protein, targeting the proteins that make up this outer shell, because if you're able to destroy the outer shell, you are essentially destroying the viral particle itself, so preventing it from functioning at all. So that's, you think is from a sort of cyber security perspective, virus security perspective, that's the best attack vector is, or like, that's a promising attack vector. I would say, yes. I mean, there's still tons of research needs to be, you know, to be done, but, yes, I think, you know, so there's more attack surface, I guess, more attack surface. But, you know, from. From our analysis, from other evolutionary analysis, this protein is evolutionary more stable compared to the, say, to the spike protein. Stable means a more static target. Well, yeah. So it doesn't change. It doesn't evolve from the evolutionary perspective so drastically as, for example, the spike protein. There's a bunch of stuff in the news about mutations of the virus in the United Kingdom. I also saw in South Africa something maybe that was yesterday you just kind of mentioned about stability and so on, which aspects of this are mutatable and which aspects, if mutated, become more dangerous and maybe even zooming out. What are your thoughts and knowledge and ideas about the way it's mutated? All the news that we've been hearing, are you worried about it from a biological perspective? Are you worried about it from a human perspective? So, I mean, you know, mutations are sort of a general way for these viruses to evolve. Right. So it's, you know, it's essentially, this is the way they evolved. This is the way they were able to jump from, you know, one species to another. We also see, you know, some recent jumps. There were some incidents of this virus jumping from human to dogs. So, you know, there is some danger in those jumps, because every time it jumps, it also mutates. Right. So when it jumps to the species and jumps back, so it acquires some mutations that are sort of driven by the environment of a new host. Right. And it's different from the human environment. And so we don't know whether the mutations that are acquired in the new species are neutral with respect to the human host or maybe damaging. Yeah, change is always scary. So are you worried about. I mean, it seems like because the spread is during winter now, seems to be exceptionally high, and especially with a vaccine just around the corner already being actually deployed, there's some worry that this puts evolutionary pressure, selective pressure on the virus for you to mutate. Is that a source of worry? Well, I mean, there is always this thought in the scientist's mind what will happen. So I know there have been discussions about sort of the arms race between the ability of the humanity to get vaccinated faster than the virus essentially becomes resistant to the vaccine. I mean, I don't worry that much simply because, you know, there is not that much evidence to that. To aggressive mutation around the vaccine. Exactly. You know, obviously there are mutations around the vaccine. So the reason we get vaccinated every year against the season of mutations. Right. But I think it's important to study it. No doubts. I think one of the. To me, and again, I might be biased, because we've been trying to do that as well, but one of the critical directions in understanding the virus is to understand its evolution in order to understand the mechanisms, the key mechanisms that lead the virus to jump the nordic viruses to jump from species, from species to another, that the mechanisms that lead the virus to become resistant to vaccines, also to treatments. And hopefully that knowledge will enable us to forecast the evolutionary traces, the future evolutionary traces of this virus. From a biological perspective, this might be a dumb question, but is there parts of the virus that, if souped up, like through mutation, could make it more effective at doing its job? We're talking about this specific coronavirus because we were talking about the different. The membrane, the m protein, the e protein, the n and s. The spike. Is there some, and there are 20 or so more in addition to that. But is that a dumb way to look at it? Which of these, if mutated, could have the greatest impact, potentially damaging impact, on the effectiveness of the virus? So it's actually, it's a very good question because. And the short answer is, we don't know yet, but of course, there is capacity of this virus to become more efficient. The reason for that is. So if you look at the virus, I mean, it's a machine, right? So it's a machine that does a lot of different functions, and many of these functions are sort of nearly perfect, but they're not perfect, and those mutations can make those functions more perfect. For example, the attachment to ace two receptor of the spike. So is it, has this virus reached the efficiency in which the attachment is carried out, or there are some mutations that still to be discovered that will make this attachment stronger or something in a way more efficient from the point of view of this virus functioning? That's the obvious example. But if you look at each of these proteins, it's there for a reason. It performs certain function, and it could be that certain mutations will enhance this function. It could be that some mutations will make this function much less efficient. So that's also the case since we're. Talking about the evolutionary history of a virus. Zoom back out and, uh, look at the evolution of proteins. I I glanced at this, uh, 2010 nature paper on the, quote, ongoing expansion of the protein universe. And then, you know, it kind of implies and, uh, talks about that, uh, protein started with a common ancestor, which is, you know, kind of interesting. It's interesting to think about, like, even just like, the first organic thing that started life on earth. And from that, there's now, you know, what is it? 3.5 billion years later, there's now millions of proteins, and they're still evolving. And that's, in part one of the things that you're researching. Is there something interesting to you about the evolution of proteins from this initial ancestor to today? Is there something beautiful insightful about this long story? So I think if I were to pick a single keyword about protein evolution, I would pick modularity, something that we talked about in the beginning. And that's the fact that the proteins are no longer considered as a sequence of letters. There are hierarchical complexities in the way these proteins are organized, and these complexities are actually going beyond the protein sequence. It's actually going all the way back to the gene, to the nucleotide sequence. Again, these protein domains, they are not only functional building blocks, they are also evolutionary building blocks. And so what we see in the sort of later stages of evolution, I mean, once this stable structurally and functionally building blocks were discovered, they essentially, they stay, those domains stay as such. So that's why if you start comparing different proteins, you will see that many of them will have similar fragments, and those fragments will correspond to something that we call protein domain families. And so they are still different because you still have mutations, and different mutations are attributed to diversification of the function of this protein domains. However, you very rarely see the evolutionary events that would split this domain into fragments. Once you have the domain split, you can completely cancel out its function, or at the very least, you can reduce it. And that's not efficient from the point of view of the cell functioning. So the protein domain level is a very important one. Now, on top of that, if you look at the proteins, you have this structural units and they carry out the function. But then much less is known about things that connect this protein domains, something that we called linkers. And those linkers are completely flexible parts of the protein that nevertheless carry out a lot of function. It's like little tails, little heads. So we do have tails. So they call termini, c and n termini. So these are things right on one and another ends of the protein sequence. So they are also very important. So they attribute it to very specific interactions between the proteins. So, but you're referring to the links. Between domains that connect the domains. And, you know, apart from the. Just the simple perspective, if you have a very short domain, you have, sorry, a very short linker, you have two domains next to each other. They are forced to be next to each other. If you have a very long one, you have the domains that are extremely flexible and they carry out a lot of sort of spatial reorganization. Right? That's right. But on top of that. Right, just this linker itself, because it's so flexible, it actually can adapt to a lot of different shapes. And therefore it's a very good interactor when it comes to interaction between this protein and other protein. So these things also evolve and they, in a way, have different sort of laws of the driving laws that underlie the evolution, because they no longer need to preserve certain structure, unlike protein domains. On top of that, you have something that is even less studied. And this is something that attributed to the concept of alternative splicing. So alternative splicing. So it's a very cool concept. It's something that we've been fascinated about for over a decade in my lab and trying to do research with that. But so typically, a simplistic perspective is that one gene is equal, one protein product. So you have a gene, you transcribe it and translate it, and it becomes a protein. In reality, when we talk about eukaryotes, especially sort of more recent eukaryotes that are very complex, the gene is no longer equal to one protein. It actually can produce multiple functionally active protein products, and each of them is called an alternatively spliced product. The reason it happens is that if you look at the gene, it has also blocks and the blocks, some of which. And essentially it goes like this. So we have a block that will later be translated, we call it exon, then we'll have a block that is not translated cut out. We call it intron. So we have exon, intron, exon, intron, et cetera, et cetera, et cetera. Right. So sometimes you can have dozens of these exons and introns. So what happens is, during the process, when the gene is converted to rna, we have things that are cut out, the introns that cut out and exons that now get assembled together. And sometimes we will throw out some of the exons and the remaining protein product will become still be the same. Different. Oh, different, right. So now you have fragments of the protein that no longer there. They were cut out with the introns. Sometimes you will essentially take one exon and replace it with another one. Right. So there's some flexibility in the process. So. So that creates a whole new level. Of complexity, because random, though, is it random? It's not random. We. And this is where I think now the appearance of this modern single cell. And before that, tissue level sequencing, next generation sequencing techniques such as rna seq, allows us to see that these are the events that often happen in response. It's a dynamic event that happens in response to disease or in response to certain developmental stage of a cell. And this is an incredibly complex layer that also undergoes, I mean, because it's at the gene level, right. So it undergoes certain evolution. Right. And now we have this interplay between what happening, what is happening in the proteins world and what is happening in the gene and rna world. And for example, you know, it's, it's often that we see that the boundaries of these exons coincide with the boundaries of the protein domains. Right. So there is close interplay to that. It's not always, I mean, otherwise it would be too simple. Right. But we do see the connection between those sort of machineries. And obviously, the evolution will pick up this complexity and select for whatever is successful. Yeah. We see that complexity in play and makes this question more complex, but more exciting. A small detour. I don't know if you think about this, into the world of computer science. There's Douglas Hostatter, I think, came up with a name of Quine, which are, I don't know if you're familiar with these things, but it's computer programs that have, I guess, Exxon and intron, and they copy. The whole purpose of the program is to copy itself. So it prints copies of itself, but can also carry information inside of it. So it's a very kind of crude, fun exercise of can we sort of replicate these ideas from cells of can we have a computer program that when you run it, just print itself, the entirety of itself, and does it in different programming languages and so on. I've been playing around and writing them. It's a kind of fun little exercise. You know, when I was a kid. So, you know, it was essentially one of the. Of the sort of main stages in informatics olympiads that you have to reach in order to be any so good is you should be able to write a program that replicates itself. And so the tags then becomes even sort of more complicated. So what is the shortest. What is the shortest program? And of course, it's a function of a programming language. But, yeah, I remember a long, long, long time ago when we tried to make it shorter and shorter and find the shortcut. There's actually a stack exchange. There's an entire site called Codegolf, I think, where the entirety is just the competition. People just come up with whatever task, I don't know, like write code that reports the weather today. And the competition is about in whatever programming language, what is the shortest program. And it makes you actually, people should check it out because it makes you realize there's some. Some weird programming languages out there. But just to dig on that a little deeper, do you think in computer science, we don't often think about programs. There's like the machine learning world now that's still kind of basic programs, and then there's humans that replicate themselves and there's these mutations and so on. Do you think we'll ever have a world where there's programs that kind of have an evolutionary process? So I'm not talking about evolutionary algorithms, but I'm talking about programs that kind of mate with each other and evolve and on their own replicate themselves. So this is kind of the idea here is that's how you can have a runaway thing. So we think about machine learning as a system that gets smarter and smarter and smarter and smarter. At least the machine learning systems of today are like, it's a program that you can, like, turn off, as opposed to throwing a bunch of little programs out there and letting them, like, multiply and mate and evolve and replicate. Do you ever think about that kind of world, you know, when we jump from the biological systems that you're looking at to artificial ones? I mean, it's almost like you take the area of intelligent agents, which are essentially the independent sort of codes that run and interact and exchange the information. So I don't see why not. It could be sort of a natural evolution in this area of computer science. I think it's kind of interesting possibility. It's terrifying, too, but I think it's a really powerful tool. Like, to have, like, agents that inter. You know, we have social networks with millions of people, and they interact. I think it's interesting to inject into that was already injected into that bots. Right. But those bots are pretty dumb. You know, they're. They're probably pretty dumb algorithms. You know, it's interesting to think that there might be bots that evolve together with humans, and there's the sea of humans and robots that are operating first in the digital space, and then you can also think, I love the idea. Some people worked, I think, at Harvard, at Penn. There's robotics labs that take as a fundamental task to build a robot that, given extra resources, can build. Build another copy of itself, like, in the physical space, which is super difficult to do, but super interesting. I remember there's, like, research on robots that can build a bridge. So they make a copy of themselves and they connect themselves. And so there's, like, self building bridge based on building blocks. You can imagine, like, a building that self assembles. So it's basically self assembling structures from. From robotic parts. But it's interesting to, within that robot, add the ability to mutate and do all the interesting little things that you're referring to in evolution, to go from a single origin protein building block to this weird complex. If you think about this, the bits and pieces are there. So you mentioned revolutionary algorithm. So this is sort of. And maybe sort of the goal is, in a way, different. Right. So the goal is to essentially to optimize your search. Right. So, but sort of the ideas are there. So people recognize that the recombination events lead to global changes in its search trajectories, the mutations event, the more refined step in the search. Then you have other nature inspired algorithms. One of the reason that I think it's one of the funnest one is the slime based algorithm. I think the first was introduced by the japanese group, where it was able to solve some pretty complex problems. And then I think there are still a lot of things we've yet to borrow from the nature. So there are a lot of ideas that nature gets to offer us that it's up to us to grab it, to, you know, get the best use. Of it, including neural networks. You know, that we have a very crude inspiration from nature on neural networks. Maybe there's other inspirations to be discovered in the brain or other aspects of the various systems, even like the immune system, the way it interplays I recently started to understand that, like, the immune system has something to do with the way the brain operates. Like, there's multiple things going on in there, which all of which are not modeled in artificial neural networks. And maybe if you throw a little bit of that biological spice in there, you'll come up with something. Something cool. I'm not sure if you're familiar with the Drake equation, that estimate. I just did a video on it yesterday because I wanted to give my own estimate of it. It's an equation that combines a bunch of factors to estimate how many alien civilizations in the galaxy. I've heard about it, yes. So one of the interesting parameters, you know, it's like, how many stars are born every year? How many planets are, on average per star for this? How many habitable planets are there? And then the one that starts being really interesting is the probability that life emerges on a habitable planet. So, like, I don't know if you think about. You certainly think a lot about evolution, but do you think about the thing which evolution doesn't describe, which is, like, the beginning of evolution, the origin of life? I think I put the probability of life developing in a habitable planet at 1%. This is very scientifically rigorous. Okay, well, first, at a high level, for the Drake equation, what would you put that percent at on Earth? And in general, do you have something. Do you have thoughts about how life might have started? You know, like, the proteins being the first kind of one of the early jumping points? Yeah. So I think back in 2018, there was a very exciting paper published in nature where they found one of the simplest amino acids, glycine, in a comet dust. So this is. I apologize if I don't pronounce. It's a russian named comets, I think chugrium of Gerasimenko. This is the comment where. And there was this mission to get close to this comment and get the stardust from its tail. And when scientists analyzed it, they actually found traces of glycine, which makes up. It's one of the 20 basic amino acids that makes up proteins. So that was kind of exciting. Very exciting. Right. But the question is very interesting. So if there is some alien life, is it going to be made of proteins or maybe rna's? So we see that the rna viruses are certainly very well established sort of group of molecular machines. So, yeah, it's a very interesting question. What probability would you put? Like, how hard is this jump? Like, how unlikely, just on earth do you think this whole thing is that we got going? Like, is that we really lucky or is it inevitable? Like, what's your sense when you sit back and think about life on Earth? Is it higher or lower than 1%? Well, cause 1% is pretty low, but it still is like, damn, that's pretty good chance. Yes, it's a pretty good chance. I mean I would personally. But again, I'm probably not the best person to do such estimations, but I would intuitively I would probably put it lower. But still, I mean, we're really lucky here on Earth. I mean, or the conditions are really good. I mean that's. I think that there was everything Washington. Right in a way. Right. So still it's not. The conditions were not like ideal. If you try to look at what was several billions years ago when the life merged. So there is something called the rare earth hypothesis that encounter to the Drake equation says that the conditions of Earth, if you actually were to describe Earth, it's quite a special place. So special might be unique in our galaxy and potentially, you know, close to unique in the entire universe. Like it's very difficult to reconstruct those same conditions. And what the rare earth hypothesis argues is all those different conditions are essential for life. And so that's sort of the counter, you know, like all the things we thinking that Earth is pretty average. I mean, I can't really, I'm trying to remember to go through all of them, but just the fact that it is shielded from a lot of asteroids, the obviously the distance to the sun, but also the fact that it's like a perfect balance between the amount of water and land and all those kinds of things. I don't know, there's a bunch of different factors that I remember. There's a long list, but it's fascinating to think about if in order for something like proteins and then DNA and rna to emerge, you need and basic living organisms, you need to be a very close and Earth like planet, which would be sad or exciting, I don't. Know which if you ask me, I, in a way I put a parallel between, you know, between our own research and I mean, from the intuitive perspective, you know, you have those two extremes and the reality is never, very rarely falls into the extremes. It's always the optimum is always reached somewhere in between. So I would. And that's what I tend to think. I think that, you know, we're probably somewhere in between. So they were not unique unique. But again, the chances are, you know, reasonably small. The problem is we don't know. The other extreme is like, I tend to think that we don't actually understand the basic mechanisms of, like, what this is all originated from. Like, it seems like we think of life as this distinct thing. Maybe intelligence is a distinct thing. Maybe the physics that, from which planets and suns are born is a distinct thing, but that could be a very. It's like the Stephen Wolfram thing. It's like the. From simple rules emerges greater and greater complexity. So, you know, I tend to believe that just life finds a way. It, like, we don't know the extreme of how common life is because it could be. Life is, like, everywhere. Like, like so everywhere that it's almost, like, laughable like that. We're such idiots to think it's ridiculous to even think it's like ants thinking that their little colony is the unique thing and everything else doesn't exist. I mean, it's also very possible that that's the extreme, and we're just not able to maybe comprehend the nature of that life just to stick on alien life for just a brief moment more. There is some signs of. Signs of life on Venus in gaseous form. There's hope for life on Mars, probably extinct. We're not talking about intelligent life, although that has been in the news recently. We're talking about basic, like, you know, bacteria, protobacteria. Yeah. And then also, I guess there's a couple moonshi. Europa. Yeah, Europa, which is Jupiter's moon. I think there's another one. Is that exciting, or is it terrifying to you that we might find life? Do you hope we find life? I certainly do hope that we find life. I mean, it was very exciting to hear about this news about the possible life on Venus. It'd be nice to have hard evidence of something, which is what the hope is for Mars and Europa. But do you think those organisms would be similar biologically, or would they even be sort of carbon based? If we do find them? I would say they would be carbon based. How similar? It's a big question. Right. So it's the moment we discovered things outside Earth. Right. Even if it's a tiny little single cell. I mean, there is so much. Just imagine that that would be so. I think that that would be another turning point for the science, you know. And especially if it's different in some very new way, that's exciting, because that says that's a definitive state. Not a definitive, but a pretty strong statement that life is everywhere in the. In the. The universe. To me, at least, that's really exciting. You brought up Joshua Lederberg in an offline conversation. I think I'd love to talk to you about alphafold. And this might be an interesting way to enter that conversation, because. So he won the 1958 Nobel prize in physiology and medicine for discovering that bacteria can mate and exchange genes. But he also did a ton of other stuff, like we mentioned, helping NASA find life on Mars. And the dendrill. Dendril, the chemical expert system. Expert systems. Remember those? What do you find interesting about this guy and his ideas about artificial intelligence in general? So I have a kind of personal story to share. So I started my PhD in Canada back in 2000. And so essentially my PhD was. So we were developing a new language for symbolic machine learning. So it's different from the feature based machine learning. And one of the sort of cleanest applications of this approach of this formalism was to chemo informatics and computer aided drug design. Essentially, as a part of my research, I developed a system that essentially looked at chemical compounds of, say, the same therapeutic category, male hormones, and try to figure out the structural fragments that are the structural building blocks that are important, that define this class versus structural building blocks that are there just because to complete the structure. But they are not essentially the ones that make up the key chemical properties of this therapeutic category. For me, it was something new. I was trained as an applied mathematicians, as with some machine learning background, but computer drug design was a completely new territory. So because of that, I often find myself asking lots of questions on one of these sort of central forums. Back then, there were no. No facebooks or stuff like that. There were. It's a forum, you know, it's a forum. It's essentially, it's like a bulletin board on the Internet. Yeah. So you essentially, you have a bunch of people and you post a question and you get an answer from different people. And back then, one of the most popular forums was CCL, I think, computational chemistry, not library, but something like that. But CCL. That was the forum. And there I asked a lot of dumb questions. Yes, I asked questions, also shared some. Some information about our formulas and how we do and whether whatever we do makes sense. And so, you know, and I remember that one of these posts, I mean, I still remember, you know, I would call it desperately looking for a chemist advice, something like that. Right? And so I post my question, I explained, you know, how my. Our formalism is what it does and what kind of applications I'm planning to do. And it was in the middle of the night, and I went back to bed, and next morning have a phone call from my advisor, who also looked at this forum. You won't believe who replied to you. And it's like, who? And he said, well, you know, there is a message to, from Joshua Lederberg. And my reaction was like, who is Joshua Lederberg? Your advisor hung up. So. And essentially, Joshua wrote me that we had conceptually similar ideas in the Dendrill project. You may want to look it up. And, you know, we should also. Sorry. And it's a side comment, say that even though he, he won the Nobel Prize at a really young age in 58, but so he was, I think. He was, what, 33? Yeah, it's just crazy. Yeah. So anyway, so that's. So hence in the nineties, responding to young whippersnappers on the CCL forum. Okay. And so back then, he was already very senior. I mean, he unfortunately passed away back in 2008. But I know back in 2001, he was. I mean, he was a professor emeritus at Rockefeller University. And, you know, that was actually, believe it or not, one of the, one of the reasons I decided to join, you know, as a postdoc, the group of Andre Saleh, who was at Rockefeller University, with the hope that I could actually have a chance to meet Joshua in person. And I met him very briefly just because he was walking. There's a little bridge that connects the research campus with the sort of skyscraper that the Rockefeller owns, where postdocs and faculty and graduate students live. And so I met him and had a very short conversation. So I started reading about dendrite, and I was amazed. We're talking about 1960. The ideas were so profound. What's the fundamental ideas of it? The reason to make this is even crazier. So Ledderberg wanted to make a system that would help him study the extraterrestrial molecules. So the idea was that the way you study the extraterrestrial molecules is you do the mass spec analysis. The mass spec gives you bits, numbers, essentially gives you the ideas about the possible fragments or atoms and maybe little fragments, pieces of this molecule that make up the molecule. So now you need to decompose this information and to figure out what was the whole before it became fragments, bits and pieces. So in order to make this, to have this tool, the idea of Lederberg was to connect chemistry, computer science, and to design this so called expert system that looks, that takes into account, that takes as an input the mass spec data, the possible database of possible molecules, and essentially try to sort of induce the molecule that would correspond to this spectra or, you know, essentially the what this project ended up being was that, you know, it would provide a list of candidates that then a chemist would look at and make final decisions. So. But the original idea, I suppose, is to solve the entirety of this problem automatically. Yes. So he, you know, so he. Back then, he approached. Yes, believe that. It's amazing. I mean, it still blows my mind, you know, that it's that. And this was essentially the origin of the modern bioinformatics, chemo informatics back in sixties. So that's, that's, you know, so every time you deal with projects like this, with the, you know, research like this, you just, you know, so the power of the, of the, you know, intelligence of these people is just, you know, overwhelming. Do you think about expert systems, is there, and why they kind of didn't become successful, especially in the space of bioinformatics, where it does seem like there's a lot of expertise in humans and, you know, it's. It's possible to see that a system like this could be made very useful. Right. And be built up. It's, it's actually, it's a, it's a great question, and this is something. So, you know, so at my university, I teach artificial intelligence, and we start, my first two lectures are on the history of AI. And there we try to go through the main stages of AI, the question of why expert systems failed or became a obsolete. It's actually a very interesting one. And there are, you know, if you try to read the, you know, the historical perspectives, there are actually two lines of thoughts. One is that the, they were essentially not up to the expectations, and so therefore they were replaced, you know, by other things. Right. The other one was that completely opposite one, that they were too good. And as a result, they essentially became sort of a household name. And then essentially they got transformed. I mean, in both cases, sort of the outcome was the same. They evolved into something. Yeah, right. And that's what I, you know, if, if I look at this, right, so the modern machine learning. Right, so there's echoes in the modern machine learning. I think so. I think so. Because, you know, if you think about this, you know, and how we design, you know, the most successful algorithms, including alphafold. Right? You built in the knowledge about the domain that you study, right? So you built in your expertise. So speaking of alphafold, so deepminds, alphafold two recently was announced to have, quote unquote, solved protein folding. How exciting is this to you? It seems to be one of the exciting things that have happened in 2020. It's incredible accomplishment from the looks of it. What part of it is amazing to you, what part would you say is overhyped or maybe misunderstood? It's definitely a very exciting achievement to give you a little bit of perspective. Right. So in bioinformatics, we have several competitions. And so the way you often hear how those competitions have been explained to sort of to non bioinformaticians is that they call it bioinformatics Olympic games. And there are several disciplines, right? So the, historically, one of the first one was the discipline in predicting the protein structure, predicting the 3d coordinates of the proteins. There are some others. So the predicting protein functions, predicting effects of mutations on protein functions, then predicting protein, protein interactions. So the original one was Casp, or a critical assessment of protein structure. And the typically what happens during these competitions is experimental scientists solve these structures but don't put them into the protein databank, which is the centralized database that contains all these 3d coordinates. Instead they hold it and release protein sequences. And now the challenge of the community is to predict the 3d structures of these proteins and then use the experimentary solve structures to assess which one is the closest one. And this competition, by the way, just a bunch of different tangents. And maybe you can also say, what is protein folding? Then this competition, Casp competition, has become the gold standard. And that's what was used to say that protein folding was solved. So just add a little. Yeah, just a bunch. So if you can, whenever you say stuff, maybe throw in some of the basics for the folks that might be outside of the field. Anyway, sorry for. Yeah, so, you know, so the reason it's, it's, you know, it's relevant to our understanding of protein folding is because, you know, we've yet to learn how the folding mechanistically works, right? So there are different hypotheses. What happens to this fold? For example, there is a hypothesis that the folding happens by, you know, also in the modular fashion, right? So that we have protein domains that get folded independently because their structure is stable and then the whole protein structure gets formed. But, you know, within those domains we also have so called secondary structure, the small alpha helices, beta sheets. So these are elements that are structurally stable. And the question is, when do they get formed? Because some of the secondary structure elements, you have to have a fragment in the beginning and say the fragment in the middle. So you cannot potentially start having the full fold from the get go, right? So it's still, you know, it's still a big enigma. What, what happens? We know that it's an extremely efficient and stable process, right? So there's, there's this long sequence and the fold happens really quickly. Exactly. Well, that's really weird. Right. And it happens like the same way almost every time. Exactly, exactly. Right. It's really weird. That's freaking weird. It's, it's, yeah, that's why it's such a thing. But most importantly, so when you see the translation process, so when you don't have the whole protein translated, it's still being translated. Getting out from the ribosome, you already see some structural fragmentation. So folding starts happening before the whole protein gets produced. Right. And so this is obviously, you know, one of the biggest questions in, you know, in modern molecular biologies, not, not. Like maybe what happens? Like, that's not, that's bigger than the question of folding. That's the question of like, like deeper, fundamental idea of folding. Yes. Behind folding. Exactly, exactly. So, you know, so obviously, if we are able to predict the end product of protein folding, we are one step closer to understanding the mechanistics of the protein folding because we can then potentially look and start probing what are the critical parts of this process and what are not so critical parts of this process. We can start decomposing this in a way, this protein structure prediction algorithm can be used as a tool. Right. So you change the, you know, you modify the protein, you get back to this tool, it predicts, okay, it's completely, it's completely unstable. Yeah. Which, which aspects of the input will have a big impact on the output? Exactly, exactly. So what happens is we typically have some sort of incremental advancement. Each stage of this Casp competition, you have groups with incremental advancement. And historically, the top performing groups, they were not using machine learning, they were using very advanced biophysics combined with bioinformatics, combined with the data mining, and that would enable them to obtain protein structures of those proteins that don't have any structural source relatives, because if we have another protein, say the same protein, but coming from a different species, we could potentially derive some ideas, and that's so called homology or comparative modeling, where we'll derive some ideas from the previously known structures and that would help us tremendously in reconstructing the 3d structure overall. But what happens when we don't have these relatives? This is when it becomes really, really hard. So that's so called de novo protein structure prediction. And in this case, those methods were traditionally very good. But what happened in the last year, the original alphafold came into and over sudden, it's much better than everyone else. This is 2018. Yeah. And the competition is only every two years. I think. And then, so it was sort of over shockwave to the bioinformatics community that we have a state of the art machine learning system that does structure prediction. And essentially what it does, if you look at this, it actually predicts the context. So the process of reconstructing the 3d structure starts by predicting the, the context between the different parts of the protein, and the context is essentially the part of the proteins that are in a close proximity to each other. Right. So it actually, the machine learning part seems to be estimating. You can correct me if I'm wrong here, but it seems to be estimating the distance matrix, which is like the distance between the different parts. Yeah. So we call it the. A contact map. Contact map. So once you have the contact map, the reconstruction is becoming more straightforward. The contact map is the key. And so that what happened. And now we started seeing, in this current stage, in the most recent one, we started seeing the emergence of these ideas in others. People works, right? But yet here is alphafold two that again outperforms everyone else. And also by introducing yet another wave of the machine learning ideas. Yeah, there does seem to be also an incorporation. First of all, the paper is not out yet, but there's a bunch of ideas already out. There does seem to be an incorporation of this other thing. I don't know if it's something that you could speak to, which is like the incorporation of other structures, like evolutionary, similar structures that are used to give you hints. Yes. Evolutionary similarity is something that we can detect at different levels. We know, for example, that the structure of proteins is more conserved than the sequence. The sequence could be very different, but the structural shape is actually still very conserved. So that's sort of the intrinsic property that, in a way related to protein folds, to the evolution of proteins and protein domains, etcetera, but we know that there have been multiple studies, and ideally, if you have structures, you should use that information. However, sometimes we don't have this information. Instead, we have a bunch of sequences. Sequences. We have a lot. We have hundreds, thousands of different organisms sequenced by taking the same protein, but in different organisms and aligning it, so making the corresponding positions aligned. We can actually say a lot about what is conserved in this protein, and therefore structurally more stable, what is diverse in these proteins. On top of that, we could provide the information about the secondary structure of this protein, et cetera, et cetera. So this information is extremely useful, and it's already there. So, so while it's tempting to, you know, to do a complete ab initio, so you just have a protein sequence and nothing else. The reality is such that we, we are overwhelmed with this data, so why not use it? And so. Yeah, so I'm looking forward to reading the, this paper. It does seem to like they've, in the previous version of Alphafold, they didn't, for this evolutionary similarity thing. They didn't use machine learning for that, or they rather, they used it as the input to the entirety of the neural net. Like the features derived from the similarity. It seems like there's some kind of iterative thing where it seems to be part of the, part of the learning process is the incorporation of this evolutionary similarity. Yeah, I don't think there is a biorxiv paper. Right. There's no, nothing. There's a blog post that's written by a marketing team, essentially. Yeah. Which, you know, it has some scientific similarity probably to the actual methodology used, but it could be, it's like interpreting scripture. It could be just poetic interpretations of the actual work as opposed to direct connection to the work. So now speaking about protein folding. Right. So, you know, in order to answer the question whether or not we, we have solved this. Right? Yeah. So we need to go back to the beginning of our conversation, you know, with the realization that, you know, an average protein, is that typically what the, the cusp has been focusing on is the, you know, the, this competition has been focusing on the single, maybe two domain proteins that are still very compact and even those ones are extremely challenging to solve. Right. But now we talk about an average protein that has two, three protein domains. If you look at the proteins that are in charge of the process with the neural system, perhaps one of the most recently evolved sort of systems in the organism. Right? All of them. Well, the majority of them are highly multi domain proteins. So they are, some of them have five, six, seven and more domains. And we are very far away from understanding how these proteins are folded. So the complexity of the protein matters here. The complexity, the complexity of the protein modules or the protein domains. So you're saying solved. So the definition of solved here is particularly the cast competition, achieving human level, not human level, achieving experimental level performance on these particular sets of proteins that have been used in these competitions. Well, I mean, I do think that especially with regards to the alpha fold, it is able to solve, at the near experimental level, pretty big majority of the more compact proteins or protein domains. Because again, in order to understand how the overall protein, multi domain protein fault, we do need to understand the structure of its individual domains. Unlike if you look at alpha zero, or even mu zero. If you look at that work, it's nice. Reinforcement learning. Self playing mechanisms are nice because it's all in simulation, so you can learn from just huge amounts. Like, you don't need data. It would like the problem with proteins, like the size. I forget how many 3d structures have been mapped, but the training data is very small no matter what. It's like millions, maybe one or 2 million, something like that, but some very small number. But like, it doesn't seem like that's scalable. There has to be, I don't know, it feels like you want to somehow ten x the data or 100 x the data somehow. Yes, but we also can take advantage of homology models. So the models that are of very good quality because they are essentially obtained based on the evolutionary information. So there is a potential to enhance this information and use it again to empower the training set. And it's. I think I am actually very optimistic. I think it's been one of these sort of churning events where you have a system that is a machine learning system that is truly better than the more conventional biophysics based methods. That's a huge leap. This is one of those fun questions, but where would you put it in the ranking of the greatest breakthroughs in artificial intelligence history? Okay, so let's see who's in the running. Maybe you can correct me. So you got like Alphazero and Alphago beating, you know, beating the world champion at the game of go thought to be impossible like 20 years ago, or at least the a community was highly skeptical then you got like also deep blue, original Kasparov. You have deep learning itself. Like the. Maybe, what would you say, the Alex. Net imagenet moment. So the first neural network achieving human level performance, super. Not, that's not true. Achieving like a big leap in performance on the computer vision problem. There is OpenAI, the whole like GPT-3 that whole space of transformers and language models, just achieving this incredible performance of application of neural networks to language models. Boston dynamics, pretty cool. Like robotics, even though people are like, there's no AI. No, no, there's no machine learning currently, but AI is much bigger than machine learning. So that just the engineering aspect, I would say it's one of the greatest accomplishments in engineering side engineering, meaning like mechanical engineering of robotics ever. Then of course, autonomous vehicles. You can argue for Waymo, which is like the Google self driving car, or you can argue for Tesla, which is like actually being used by hundreds of thousands of people on the road today, machine learning system. And I don't know if you can, what else is there? But I think that's it. And then alpha fold, many people are saying as up there potentially number one, would you put them at number one? Well, in terms of the impact on. On the science and on the society beyond is definitely, you know, to me would be one of the, you know, top three, which maybe, I mean, I. Probably not the best person to answer that, you know, but, you know, I, you know, I do have. I remember my, you know, back in, I think, 1997 when deep blue, bad Kasparov, it was, I mean, it was a shock. I mean, it was. And I think for the, for the, you know, for the, you know, pre substantial part of the world that especially people who have some, you know, some experience with chess. Right. And realizing how incredibly human this game, how, you know, how much of a brainpower you need, you know, to. To reach those, you know, those levels of grandmasters. Right. Level, yeah, it's probably one of the first time and how good Kasparov was. And again. Yeah. So Kasparov is arguably one of the best ever. Right. And get a machine that beats him. Right? So it's first time a machine probably beat a human at that scale of a thing, of anything. Yes. Yes. So that was, to me, that was like, you know, one of the groundbreaking events in the history of AI. Yeah, that's probably number one. Like, we don't. It's hard to remember. It's like Muhammad Ali versus, I don't know, any other Mike Tyson, something like that. It's like, nah, you gotta put Muhammad Ali at number one. Same with deep blue. Even though it's not machine learning based. Still, it uses advanced search. And search is the integral part of AI, right? So as you said, people don't think. Of it that way at this moment. In vogue currently, search is not seen as a fundamental aspect of intelligence, but it very, well, very likely is. In fact, that's what neural networks are. They're just performing search on the space of parameters. It's all search, all of intelligence is some form of search, and you just have to become cleverer and clever at that search problem. I also have another one that you didn't mention. That's one of my favorite ones. You probably heard of this. I think it's called Deep Rembrandt. It's the project where they trained. I think there was a collaboration between the sort of the experts in Rembrandt painting in Netherlands and a group, an artificial intelligence group, where they train an algorithm to replicate the style of the Rembrandt and they actually printed a portrait that never existed before in the style of Rembrandt. They. I think they printed it on a. On a sort of. On the canvas that, you know, using pretty much same types of paints and stuff. To me, it was mind blowing. Yeah. It's in the space of art that's interesting. There hasn't been. Maybe that's. That's it. But I. I think there hasn't been an image in that moment yet. In the space of art. You haven't been able to achieve superhuman level performance in the space of art, even though there was, you know, there's a big famous thing where there was a piece of art was purchased, I guess, for a lot of money. Yes. Yeah. But it's still, you know, people are like, in the space of music, at least that's, you know, it's clear that human created pieces are much more popular. So there hasn't been a moment where it's like, oh, this is. We're now, I would say, in the space of music, what makes a lot of money. We're talking about serious money. It's music and movies or, like, shows and so on, and entertainment. There hasn't been a moment where AI created. AI was able to create a piece of music or a piece of cinema, like Netflix show that is, you know, that's sufficiently popular to make a ton of money. And that moment would be very, very powerful. Cause that's like, that's an AI system being used to make a lot of money. And, like, direct, of course, AI tools, like even premiere audio editing, all the editing, everything I do to edit this podcast, there's a lot of AI involved. I want, actually, there's this program. I want to talk to those folks just because I want to nerd out. It's called izotope. I don't know if you're familiar with it. They have a bunch of tools of audio processing, and they have, I think they're Boston based. Just. It's so exciting to me to use it, like, on the audio here, because it's all machine learning. It's not, because most, most audio production stuff is, like, any kind of processing you do is very basic signal processing, and you're tuning knobs and so on. They have all of that, of course, but they also have all of this machine learning stuff, like where you actually give it training data, you select parts of the audio, you train on. You train on it, and it figures stuff out. It's great. It's able to detect the ability of it to be able to separate voice and music, for example, or voice and anything is incredible. That's clearly exceptionally good at applying these different neural networks models to separate the different kinds of signals from the audio. Okay, so that's really exciting. Photoshop, Adobe, people also use it. But to generate a piece of music that will sell millions. A piece of art. Yeah, no, I agree. And that's, you know, as I mentioned, I offer my AI class, and an integral part of this is a project. Right. So it's my favorite, ultimate favorite part, because it typically we have these project presentations the last two weeks of the class. It's right before the Christmas break. And it's sort of, it adds this cool excitement and every time I'm amazed with some projects that people come up with. And so quite a few of them are actually, they have some link to arts. I mean, I think last year we had a group who designed an AI producing hocus japanese poems. Oh, wow. And some of them, so it got trained on the english base, hikus. Right. And some of them, they get to present the top selection. They were pretty good. Of course, I'm not a specialist, but you read them and you see the. Seems profound. Yes, it seems reasonable. So it's kind of cool. We also had a couple of projects where people tried to teach AI how to play rock music, classical music, I think, and popular music. Interestingly enough, classical music was among the most difficult ones. And of course, if you look at grandmasters of music like Bach, so there is a lot of, there is a lot of almost math. Well, he's very mathematical. Exactly. So I would imagine that at least some style of this music could be picked up. But then you have completely different spectrum of classical composers and so it's almost like you don't have to sort of look at the data, you just listen to it and say, that's not it. Not yet. That's not it. Yeah, that's how I feel too. There's open AI has, I think, open muse or something like that. The system, it's cool, but it's like, yeah, it's not compelling for some, for some reason. It could be a psychological reason too. Maybe we need to have a human being, a tortured soul behind the music. I don't know. Yeah, no, that, absolutely, I completely agree. But yeah. Whether or not we'll have a, one day, we'll have, you know, a song written by an AI engine to be in like in top charts. Yeah, musical charts. I wouldn't be surprised. I wouldn't be surprised. I wonder if we already have one and it just hasn't been announced. We wouldn't know how hard is the multi protein folding problem? Is that kind of something you've already mentioned, which is baked into this idea of greater and greater complexity of proteins, like multi domain proteins, is that basically become multi protein complexes? Yes, you got it right. So it's sort of, it has the components of both of protein folding and protein, protein interactions. Because in order for these domains, I mean, many of these proteins actually, they never form a stable structure. One of my favorite proteins, and pretty much everyone who works in a, I know whom, I know who works with proteins, they always have their favorite proteins. So one of my favorite proteins, probably my favorite protein, the one that I worked when I was a postdoc, is so called postsynaptic density, 95 psd, 95 protein. So it's one of the key actors in the majority of neurological processes at the molecular level, and essentially it's a key player in the postsynaptic density. So this is the crucial part of the synapse where a lot of these chemical eclipse processes are happening. So it has five domains, so five protein domains, pretty large proteins, I think, 600 something. But the way it's organized itself, it's flexible, it acts as a scaffold, so it is used to bring in other proteins, so they start acting in the orchestrated manner. Right. So, and the, the type of, the shape of this protein, it's, in a way, there are some stable parts of this protein, but there are some flexible, and this flexibility is built in, into the protein in order to become sort of this multifunctional machine. So do you think that kind of thing is also learnable through the alphafold two kind of approach? I mean, the time will tell. Is it another level of complexity? Is it like how big of a jump in complexity is that whole thing? To me, it's yet another level of complexity, because when we talk about protein, protein interactions, and there is actually a different challenge for this called Capri, and so this that is focused specifically on macromolecular interactions, protein, protein, DNA, etcetera. So, but it's, you know, there are different mechanisms that govern molecular interactions and that need to be picked up, say, by a machine learning algorithm. Interestingly enough, we actually, we participated for a few years in this competition. We typically don't participate in competitions, I don't know, don't have enough time because it's very intensive. It's a very intensive process. But we participated back in about ten years ago or so. And the way we entered this competition, so we designed a scoring function. So the function that evaluates whether or not your protein protein interaction is supposed to look like experimentally solved. The scoring function is very critical part of the model prediction. So we design it to be a machine learning. And so it was one of the first machine learning based scoring function used in Capri. You know, we essentially, you know, learned what should contribute, what are the critical components contributing into the protein protein interaction. So this. This could be converted into a learning problem, and thereby it could be. It could be learned? I believe so, yes. Do you think alphafold two or something similar to it from deep mind or somebody else will be, will result in Nobel Prize or multiple Nobel prizes? So, like they, you know, obviously or maybe not so obviously, you can't give a Nobel Prize to computer program. You, at least for now, give it to the designers of that program. But do you see one or multiple Nobel prizes where alphafold two is like a large percentage of what that prize is given for? Would it lead to discoveries at the level of Nobel prizes? I mean, I think we are definitely destined to see the Nobel Prize becoming sort of to be evolving with the evolution of science. And the evolution of science is such that it now becomes, like, really multifaceted. Right? So where you don't really have, like, a unique discipline, you have sort of the. A lot of cross disciplinary talks in order to achieve sort of, you know, really big advancements, you know, so I think, you know, the computational methods will be acknowledged in one way or another. And as a matter of fact, you know, they were first acknowledged back in 2013, right, where the first three people were awarded the Nobel Prize for studying the protein folding, the principle. And I think all three of them are computational by physicists. So that, I think, is unavoidable. It will come with the time, the fact that alpha fold and similar approaches, because, again, it's a matter of time that people will embrace this principle and we'll see more and more such tools coming into play. But these methods will be critical in a scientific discovery, no doubts about it. On the engineering side, maybe a dark question, but do you think it's possible to use these machine learning methods to start to engineer proteins? And the next question is something quite a few biologists are against. Some are for study purposes, is to engineer viruses. Do you think machine learning, like something like alphafold, could be used to engineer viruses? So, to answer the first question, it has been a part of the research in the protein science. The protein design is very prominent areas of research. Of course, one of the pioneers is David Baker and Rosetta algorithm that, you know, essentially was doing the de novo design and was used to design new. Proteins, you know, and design a proteins means design a function. So, like when you design a protein, you can control. I mean, the whole point of a protein, with the protein structure comes a function like it's doing something. Correct. So you can design different things that. So you can. Yeah, so you can do. Well, you can look at the proteins from the functional perspective. You can also look at the proteins from the structural perspective. Right. So the structural building blocks. So if you want to have a building block of a certain shape, you can try to achieve it by introducing a new proton sequence and predicting how it will fold. So with that, I mean, it's one of the natural applications of these algorithms. Now talking about engineering a virus with machine learning. With machine learning. Right. So, well, you know, so luckily for us, I mean, we don't have that much data. Right. We actually, right now, one of the projects that we are carrying on in the lab is we're trying to develop a machine learning algorithm that determines whether or not the current strain is pathogenic, the current strain of the coronavirus, of the virus. There are applications to coronaviruses because we have strains of SARS CoV two, also SARS CoV mers that are pathogenic, but we also have strains of other coronaviruses that are not pathogenic. The common cold viruses, you know, and some other ones. Right, so pathogenic, meaning spreading pathogenic means actually inflicting damage. Correct. There are also some, you know, seasonal versus pandemic strains of influenza. Right. And to determining the. What are the molecular determinants. Right. So that are built in. Into the protein sequence, into the gene sequence. Right. So. And whether or not the machine learning can determine those deter those components. Right. Oh, interesting. So, like, using machine learning to do that's really interesting. To. To give and give the input is like, what the sequence, the protein sequence, and then determine if this thing is going to be able to do damage to a biological system. Yeah. So good. Machine learning. You're saying we don't have enough data for that? I mean, for this specific one, we do. We might actually have to back up on this. We're still in the process. There was one work that appeared in Biorxiv by Eugene Kunin, who is one of these pioneers in evolutionary genomics, and they tried to look at this, but the methods were sort of standard supervised learning methods. And now the question is, can you advance it further by using not so standard methods? There's obviously a lot of hope in transfer learning, where you can actually try to transfer the information that the machine learning journey about the proper protein sequences, you know, so there is some promise in going this direction, but if we have this, it would be extremely useful, because then we could essentially forecast the potential mutations that would make a current strain more or less pathogenic, dissipate them. From a vaccine development for the treatment, antiviral drug development. That would be a very crucial task. But you could also use that system to then say, how would we potentially modify this virus to make it more pathogenic? That's true. That's true. I mean, you know, the. Again, the hope is, well, several things, right? So one is that, you know, it's even if you design a. You know, a sequence. Right. So to carry out the actual experimental biology to ensure that all the components working, you know, is. Is a completely different matter. Difficult process. Yes. Then there. You know, we've seen in the past there could be some regulation of the. The moment the scientific community recognizes that it's now becoming no longer a sort of a fun puzzle for machine learning. Could be open. Yeah. So then there might be some regulation. So I think back in, what, 2015, there was an issue on regulating the research on influenza strains. Several groups use the mutation analysis to determine whether or not this strain will jump from one species to another. And I think there was, like, a half a year moratorium on the research on the paper published until scientists analyzed it and decided that it's actually safe. I forgot what that's called. Something of function test and function. Gain a function. Gain a function. Yeah. Gain a function. Loss of function. That's right. Sorry. It's like, let's watch this thing mutate for a while to see. Like, to see what kind of things we can observe. I guess I'm not so much worried about that kind of research. If there's a lot of regulation and if it's done very well and with competence. And seriously, I am more worried about kind of this. You know, the underlying aspect of this question is more like 50 years from now, speaking to the Drake equation. One of the parameters in the Drake equation is how long civilizations last. And that seems to be the most important value, actually, for calculating if there's other alien intelligence civilizations out there. That's where there's most variability. Assuming if that percentage that life can emerge is, like, not zero. Like, if we're super unique, then it's the. How long we last is basically the most important thing from a selfish perspective, but also from a Drake equation perspective, I'm worried about our civilization lasting. And you kind of think about all the ways in which machine learning can be used to design greater weapons of destruction. Right. And, I mean, one way to ask that, if you look sort of 50 years from now, 100 years from now, would you be more worried about natural pandemics or engineered pandemics, like, who's, who is the better designer of viruses, nature or humans? If we look down the line, I. Think, in my view, I would still be worried about the natural pandemics simply because, I mean, the capacity of the nature producing it does pretty good job. Right? Yes. And the motivation for using virus engineering viruses as a weapon is a weird one, because maybe you can correct me on this, but it seems very difficult to target a virus. Right. The whole point of a weapon, the way a rocket works, you have a starting point, you have an end point, and you. You're trying to hit a target. To hit a target with a virus is very difficult. It's basically just. Right. It. It's. The target would be the human species. Oh, man. Yeah. I have. I have a hope in us. I'm forever optimistic that we will not. There's no. There's insufficient evil in the world to do, to lead to that kind of destruction. Well, you know, I also hope that, I mean, that's what we see. I mean, with the way we are getting connected. The world is getting connected. I think it helps for the world to become more transparent. Yeah. So the information spread is, you know, I think it's one of the key things for the, for the society to become more balanced one way or another. This is something that people disagree with me on, but I do think that the kind of secrecy the governments have. So you're kind of speaking more to the other aspects, like research, community being more open, companies are being more open. Government is still, like, we're talking about military secrets, I think. I think military secrets of the kind that could destroy the world will become also a thing of the 20th century. It'll become more and more open. Like, I think nations will lose power in the 21st century. Like, lose sufficient power towards. Secrecy's. Transparency is more beneficial than secrecy. But, of course, it's not obvious. Let's hope so. Let's hope so that the governments will become more transparent. So we last talked, I think, in March or April. What have you learned? How has your philosophical, psychological, biological worldview changed since then, or you've been studying it nonstop from a computational biology perspective. How has your understanding and thoughts about this virus changed over those months? From the beginning, to today. One thing that I was really amazed at, how efficient the scientific community was. I mean, and even just judging on this very narrow domain of protein structure, understanding the structural characterization of this virus from the components point of view, from the whole virus point of view, if you look at SARS, something that happened less than 20, but close enough 20 years ago, and you see when it happened, what was the response by the scientific community? You see that the structural characterizations did occur, but it took several years. Right now, the things that took several years, it's a matter of months, right. So we see that the research pop up. We are at the unprecedented level in terms of the sequencing. Never before we had a single virus sequenced so many times, which allows us to actually to trace very precisely the evolutionary nature of this virus, what happens. And it's not just this virus independently of everything, it's the sequence of this virus linked anchored to the specific geographic place, to specific people, because our genotype influences also the evolution of this. It's always a host pathogen co evolution that occurs. It'd be cool if we also had a lot more data about the spread of this virus. Not maybe. Well, it'd be nice if we had it for contact tracing purposes for this virus, but it would be also nice if we had it for the study, for future viruses to be able to respond and so on. But it's already nice that we have geographical data and basic data from individual humans. Exactly. I think contact tracing is obviously a key component in understanding the spread of this virus. There is a number of challenges, so Xprize is one of them. We just recently took a part of this competition. It's the prediction of the number of infections in different regions. And obviously, the AI is the main topic in those predictions. Yeah, but it's still the data. I mean, that's. That's a competition, but the. The data is weak on the training. Like, it's. It's great. It's much more than probably before, but, like, it'd be nice if it was, like, really rich. Like, I talked to Michael Mina from. From Harvard. I mean, he dreams that the community comes together with, like, a weather map to wherever I. Of viruses, right? Like, really high resolution sensors on, like, how from person to person, the viruses that travel, all the different kinds of viruses, right? Because there's. There's. There's a ton of them. And then you'd be able to tell the story that you've spoken about of the evolution of these viruses, like day to day mutations that are occurring. I mean, that would be fascinating just from a perspective of study and from the perspective of being able to respond to future pandemics, that's ultimately what I'm worried about. People love books. Is there some three or whatever number of books, technical fiction, philosophical, that brought you joy in life, had an impact on your life, and maybe some that you would recommend others. So I'll give you three very different books. And I also have a special runner up and honorable mention. I mean, it's an audiobook, and that's, there's some specific reason behind it. So the first book is something that sort of impacted my earlier stage of life and probably not going to be very original here. It's Bulgakov's master and margarita. So that's probably, you know, well, not. For Russian, maybe it's not super original, but it's, you know, it's a really powerful book for, even in English. So I read it in English. So it is incredibly powerful. And I mean, it's the way it ends, right? So it's, I still have goosebumps when I read the very last sort of, it's called prologue, where it's just so powerful. What impact did you have on you? What ideas, what insights did you get from it? I was just taken by, you know, by the fact that you have those parallel lives apart from many centuries. Right. And somehow they got sort of intertwined into one story. And that, to me, was fascinating. And, you know, of course, the, you know, the romantic part of this book is like, it's not just romance. It's like the romance empowered by sort of magic. Right. And maybe on top of that, you have some irony, which. Unavoidable. Right. So, because it was that, you know, the soviet time, but it's very, it's. Very, it's deeply russian. So that's the wit, the humor, the pain, the love, all of that is one of the books that kind of captures something about russian culture that people outside of Russia should probably read. I agree. What's the second one? So the second one is, again, another one that it happened. I read it later in my life. I think I read it first time when I was a graduate student. And that's the Solzhenitsyn's cancer warden. That is amazingly powerful book. What is it about? It's about, I mean, essentially based on, Solzhenitsyn was diagnosed with cancer when he was reasonably young, and he made a full recovery. So this is about a person who was sentenced for life in one of these campsite, and he had some cancer, so he was transported back to one of these soviet republics, I think, south asian republics. And the book is about his experience being a prisoner, being a patient in the cancer clinic, in a cancer ward surrounded by people, many of which die. Right. But in a way, the way it reads, I mean, first of all, later on, I read the accounts of the doctors who describe the experiences in the book by the patient as incredibly accurate. I read that there was some doctors saying that every single doctor should read this book to understand what the patient feels. But again, as many of the Solzhenitsyn's books, it has multiple levels of complexity. And obviously, if you look above the cancer and the patient, I mean, the tumor that was growing and then disappeared in his body with some consequences. I mean, this is allegorically the Soviet. And he actually, he agreed when he was, he said that this is what make him think about this, how to combine these experiences, being a part of the soviet regime, also being a part of someone sent to Gulag camp, and also someone who experienced cancer in his life, the Gulag archipelago. And this book, these are the works that actually made him receive a Nobel prize. But to me, I've read other books by Solzhenitsyn. This one, to me, is the most powerful one that. And by the way, both this one and the previous one you read in Russian. Yes. Yes. So now there is, the third book is an english book, and it's completely different. So, you know, we're switching the gears completely. So this is the book which. It's not even a book. It's an essay by Jonathan Neumann called the Computer and the Brain. And that was the book he was writing, knowing that he was dying of cancer. So the book was released back. It's a very thin book, right. But the intellectual power in this book, in this essay, is incredible. I mean, you probably know that von Neumann is considered to be one of these biggest thinkers. Right? So his intellectual power was incredible. Right. And you can actually feel this power in this book where the person is writing, knowing that he will die. The book actually got published only after his death back in 1958. He died in 1957. So he tried to put as many ideas that he still hadn't realized. So this book is very difficult to read because every single paragraph is just compact, is filled with these ideas. And the ideas are incredible, even nowadays. So he tried to put the parallels between the brain, computing power, the neural system, and the computers, you know, as they were understood. Year he was working on this, like approximately 57. 57. So. So that was right during his, you know, when he was diagnosed with cancer. And he was essentially. Yeah, he's one of those. There's a few folks people mention, I think Ed Witten is another that. Like everybody, everyone that meets them, they say he's just an intellectual powerhouse. Yes. Okay, so who is the honorable mention runner up? And this is, I mean, the reason I put it sort of in a separate section, because this is a book that I reasonably recently listened to. So it's an audiobook. And this is a book called Lab Girl by Hope Jarron. So Hope Jarran, she is a scientist. She's a geochemist that essentially studies the fossil plants. And so she uses this fossil plant, the chemical analysis, to understand what was the climate back in thousand years, hundreds of thousands of years ago. And so something that incredibly touched me by this book. It was narrated by the author. Nice. And it's an incredibly personal story. Incredibly. So certain parts of the book, you could actually hear the author crying. And that, to me, I mean, I never experienced anything like this, you know, reading the book, but it was like, you know, the connection between you and the authorization. And I think this is really a must read, but even better, a must listen to audiobook for anyone who wants to learn about sort of academia, science research in general, because it's a very personal account about her becoming a scientist. So we're just before New Year's, you know, we talked a lot about some difficult topics of viruses and so on. Do you have some exciting things you're looking forward to in 2021? Some New Year's resolutions maybe silly or fun or something very important and fundamental to the world of science or something completely unimportant? Well, I'm definitely looking forward towards things becoming normal. Yes. I really miss traveling. Every summer, I go to international summer school. It's called the School for Molecular and theoretical biology. It's held in Europe. It's organized by very good friends of mine. And this is the school for gifted kids from all over the world, and they're incredibly bright. It's like every time I go there, it's like, you know, it's a highlight of the year. And we couldn't make it this August, so we did this school remotely, but it's different. So I am definitely looking forward next August, coming there. I also, I mean, you know, one of the. One of my personal resolutions, I realized that being in house and working from home, I realized that actually, I apparently missed a lot, you know, spending time with my family, believe it or not. So you typically, with all the research and teaching and everything related to the academic life. I mean, you get distracted and so you don't feel that the fact that you are away from your family doesn't affect you because you are naturally distracted by other things. And this time I realized that that's so important, spending your time with the family, with your kids. And so that would be my new year resolution in actually trying to spend as much time as possible, even when. The world opens up. Yeah, that's a, that's a beautiful message. That's a beautiful reminder. I asked you if there's a russian poem you could read that I could force you to read, and you said, okay, fine, sure. Do you mind? Do you mind reading? And you're like, you said that no paper needed. Nope. So, yeah, so this poem was written by my namesake, another Dmitri, Dmitri Khemir Feld, and it's a recent poem, and it's called sorceress in Russian, or actually Kaldunya. So that's sort of another sort of connotation of sorceress witch. And I really like it. And it's one of just a handful of poems I actually can recall by heart. I also have a very strong association when I read this poem with Master Margarita, the main female character, Margarita. And also it's about, it's happening about the same time we are talking now. So around new year, around Christmas. Do you mind reading it in Russian? I'll give it a try. Paksachilnika pilsina matreskoy ipadlasnetvaye napagosta. No. Aya bispridra sultigals. That's beautiful. I love how it captures a moment of longing and maybe love even. Yes, to me, it has a lot of meaning about, you know, this, something that is happening, something that is far away but still very close to you. And yes, it's the winter. There's something magical about winter, isn't it? What is the. Well, I don't know. I don't know how to translate it, but kiss in winter is interesting. Lips in winter and all that kind of stuff. It's beautifully russian as a way, as a reason. Russian poetry is just, I'm a fan of poetry in both languages, but English doesn't capture some of the magic that Russian seems to. So thank you for doing that. That was awesome. Dmitry, is great to talk to you again. It's contagious how much you love what you do, how much you love life. So I really appreciate you taking the. Time to talk today, and thank you for having me. Thanks for listening to this conversation with Dimitri Corkin and thank you to our sponsors Brave browser, Netsuite business management software, magic spoon, low carb cereal and a sleep self cooling mattress. So the choice is browsing, privacy, business success, healthy diet, a comfortable sleep. Choose wisely my friends, and if you wish, click the sponsor links below to get a discount and to support this podcast. Now let me leave you with some words from Jeffrey Eugenides. Biology gives you a brain, life turns it into a mind. Thank you for listening and hope to see you next time.

Utterances:
Speaker A: The following is a conversation with Dmitry Corkin, his second time in the podcast. He's a professor of bioinformatics and computational biology at WPI, where he specializes in bioinformatics of complex disease, computational genomics, systems biology, and biomedical data analytics. He loves biology. He loves computing. Plus, he is russian and recites a poem in Russian at the end of the podcast. What else could you possibly ask for in this world? Quick mention of our sponsors brave browser, netsuite, business management software, magic spoon, low carb cereal and eight sleep self cooling mattress. So the choice is browsing, privacy, business success, healthy diet or comfortable sleep. Choose wisely, my friends, and if you wish, click the sponsor links below to get a discount and to support this podcast. As a side note, let me say that to me, the scientists that did the best, apolitical, impactful, brilliant work of 2020 are the biologists who study viruses without an agenda, without much sleep, to be honest. Just a pure passion for scientific discovery and exploration of the mysteries within viruses. Viruses are both terrifying and beautiful. Terrifying because they can threaten the fabric of human civilization, both biological and psychological. Beautiful because they give us insights into the nature of life on Earth, and perhaps even extraterrestrial life of the not so intelligent variety that might meet us one day as we explore the habitable planets and moons in our universe. If you enjoy this thing, subscribe on YouTube, review it on Apple Podcasts, follow on Spotify support on Patreon, or connect with me on Twitter exfriedmande. And now here's my conversation with Dmitry Corkin. It's often said that proteins and the amino acid residues that make them up are the building blocks of life. Do you think of proteins in this way as the basic building blocks of life?
Speaker B: Yes and no. So the proteins, indeed, is the basic unit, biological unit that carries out important functioning of the cell. However, through studying the proteins and comparing the proteins across different species, across different kingdoms, you realize that proteins are actually much more complicated. So they have so called modular complexity. And so what I mean by that is, an average protein consists of several structural units. So we call them protein domains. And so you can imagine a protein as a string of beads where each bead is a protein domain. And in the past 20 years, scientists have been studying the nature of the protein domains because we realize that it's the unit, because if you look at the functions, many proteins have more than one function, and those protein functions are often carried out by those protein domains. We also see that in the evolution, those proteins domains get shuffled so they act actually as a unit. Also from the structural perspective, some people think of a protein as a sort of a globular molecule, but as a matter of fact, the globular part of this protein is a protein domain. So we often have this. Again, the collection of these protein domains align on a string as beads, and.
Speaker A: The protein domains are made up of amino acid residues. So this is the basic. So you're saying the protein domain is the basic building block of the function that we think about proteins doing. So, of course, you can always talk about different building blocks. It turtles all the way down. But there's a point where there is at the point of the hierarchy where it's the most, the cleanest element block based on which you can put them together in different kinds of ways to form complex function. And you're saying protein domains. Why is that not talked about as often in popular culture?
Speaker B: Well, you know, there are several perspectives on this and one, of course, is the historical perspective. Right. Historically, scientists have been able to structurally resolve to obtain the 3d coordinates of a protein for smaller proteins, and smaller proteins tend to be a single domain protein. So we have a protein equal to a protein domain. And so because of that, the initial suspicion was that the proteins are, have globular shapes. And the more of smaller proteins you obtain structurally, the more you became convinced that that's the case, and only later when we started having alternative approaches. So the traditional ones are X ray crystallography and NMR spectroscopy. So this is sort of the, the two main techniques that give us the 3d coordinates. But nowadays there is huge breakthrough in cryo electron microscopy. So the more advanced methods that allow us to get into the 3d shapes of much larger molecules, molecular complexes. To give you one of the common examples for this year, the first experimental structure of a SARS CoV two protein was the cryo EM structure of the S protein. So the spike protein. And so it was solved very quickly. And the reason for that is the advancement of this technology is pretty spectacular.
Speaker A: How many domains does the. Is it more than one domain?
Speaker B: Oh, yes. I mean, it's a very complex structure and we, on top of the complexity of a single protein, so this structure actually is a complex, is a trimer, so it needs to form a trimer in order to function properly.
Speaker A: What's a complex?
Speaker B: A complex is agglomeration of multiple proteins. And so we can have the same protein copied in multiple, you know, made up in multiple copies and forming something that we called a homo oligomer. Homo means the same. Right. So in this case, the spike protein is the. Is an example of a homo tetrame. Homo trimer. Sorry.
Speaker A: So three copies of a.
Speaker B: Three copies in order to. Exactly. We have these three chains, the three molecular chains coupled together and performing the function. That's when you look at this protein from the top, you see a perfect triangle. So other complexes are made up of different proteins. Some of them are completely different, some of them are similar. The hemoglobin molecule protein complex, it's made of four basic subunits. Two of them are identical to each other and two other identical to each other, but they are also similar to each other, which sort of gives us some ideas about the evolution of this molecule. And perhaps one of the hypotheses is that in the past it was just a homo tetramer. Right. So four identical copies. And then it became, you know, sort of modified, it became mutated over the time and became more specialized.
Speaker A: Can we linger on the spike protein for a little bit? Is there something interesting or, like, beautiful you find about it?
Speaker B: I mean, first of all, it's an incredibly challenging protein. And so we, as a part of our sort of research to understand the structural basis of this virus, to sort of decode, structurally decode every single protein in its proteome, which, you know, we've been working on this spike protein. And one of the main challenges was that the cryo EM data allows us to reconstruct or to obtain the 3d coordinates of roughly two thirds of the protein. The rest of the one third of this protein, it's a part that is buried into the membrane of the virus and of the viral envelope, and it also has a lot of unstable structures around it.
Speaker A: So it's chemically interacting somehow with whatever the hex is connecting.
Speaker B: Yeah. People are still trying to understand the nature of and the role of this one third, because the top part, the primary function, is to get attached to the ace two receptor, human receptor. There is also beautiful mechanics of how this thing happens. So because there are three different copies of these chains or there are three different domains. So we're talking about domains. So this is the receptor binding domains, rbds, that gets untangled and get ready to get attached to the receptor. And now they are not necessarily going in a sync mode, as a matter.
Speaker A: Of fact, say, synchronous.
Speaker B: So, yes, and this is where another level of complexity comes into play, because right now what we see is we typically see just one of the arms going out and getting ready to be attached to the ace two receptors. However, there was a recent mutation that people studied in that spike protein. And very recently, a group from UMass medical school, we happen to collaborate with groups. So this is a group of Jeremy Lubin and a number of other faculty. They actually solve the mutated structure of the spike, and they showed that actually, because of these mutations, you have more than one arms opening up. And so now the frequency of two arms going up increase quite drastically.
Speaker A: Interesting. Does that change the dynamics somehow?
Speaker B: It potentially can change the dynamics of, because now you have two possible opportunities to get attached to the ace two receptor. It's a very complex molecular process, mechanistic process. But the first step of this process is the attachment of this spike protein, of the spike trimer, to the human ace two receptor. So this is a molecule that sits on the surface of the human cell, and that's essentially what initiates the. What triggers the whole process of, you know, encapsulation.
Speaker A: If this was dating, this would be the first date. So this is the. In the way.
Speaker B: Yes.
Speaker A: So is it. Is it possible to have the spike protein just, like, floating about on its own, or does it need that interactive ability with. With the membrane?
Speaker B: Yeah. So it needs to be attached, at least as far as I know. But when you get this thing attached on the surface, there is also a lot of dynamics on how it sits on the surface. For example, there was a recent work, again, where people use the cryoctoral microscopy to get the first glimpse of the overall structure. It's a very low res, but you still get some interesting details about the surface, about what is happening inside, because we have literally no clue until recent work about how the capsid is organized. So capsid is essentially the inner core of the viral particle, where there is the rna of the virus, and it's protected by another protein, n protein that essentially acts as a shield. Now we are learning more and more. So it's actually, it's not just this shield. It's potentially used for the stability of the outer shell of the virus. So it's pretty complicated.
Speaker A: And, I mean, understanding all of this is really useful for trying to figure out, like, developing a vaccine or some kind of drug to attack any aspects of this. Right.
Speaker B: So, I mean, there are many different implications to that. First of all, you know, it's important to understand the virus itself, right? So in order to understand how it acts, what is the overall mechanistic process of this virus? Replication of this virus, proliferation to the cell. So that's one aspect. The other aspect is designing new treatments. So one of the possible treatments is designing nanoparticles. And so some nanoparticles that will resemble the viral shape that would have the spike integrated and essentially would act as a competitor to the real virus by blocking the ace two receptors and thus preventing the real virus entering the cell. Now, there are also, you know, there is a very interesting direction in looking at the membrane, at the envelope portion of the protein and attacking its m protein. So there are, to give you a brief overview, there are four structural proteins. These are the proteins that made up a structure of the virus spike s protein that acts as a trimer. So it needs three copies. E envelope protein that acts as a pentamer, so it needs five copies to act properly. M is a. Is a membrane protein. It forms dimers, and actually, it forms beautiful lattice. And this is something that we've been studying, and we are seeing it in simulations. It actually forms a very nice grid, or, you know, threads of different dimers attached next to each other.
Speaker A: There's a bunch of copies of each other, and they naturally, when you have a bunch of copies of each other, they form an interesting lattice.
Speaker B: Exactly. And if you think about this complex, the viral shape needs to be organized somehow, self organized somehow. If it was a completely random process, you probably wouldn't have the envelope shell of the ellipsoid shape. You would have something pretty random shape. So there is some regularity in how these m dimers get to attach to each other in a very specific directed way.
Speaker A: Is that understood at all?
Speaker B: It's not understood. We are now, we've been working in the past six months since we met, actually, this is where we started working on trying to understand the overall structure of the envelope and the key components that made up this structure.
Speaker A: Wait, does the envelope also have the lattice structure, or. No.
Speaker B: So the envelope is essentially. Is the outer shell of the viral particle. The n, the nucleocapsid protein is something that is inside. But get that, the n is likely to interact with m. Does it go m and e?
Speaker A: Like, where's the e?
Speaker B: And so, e, those different proteins, they occur in different copies on the viral particle. So, e, this pentamer complex, we only have two or three, maybe per each particle. Okay. We have thousand or so of m dimers that essentially made up, that makes up the entire, you know, outer shell.
Speaker A: So most of the outer shell is.
Speaker B: The M dimer and lithium protein.
Speaker A: When you say particle, that's the viron, the virus, the individual virus, yes. Single element of the virus.
Speaker B: Single virus, single virus, right. And we have about, you know, roughly 50 to 90 spike trimers. Right. So, so, so when you you know, when you show a per virus particle. Per virus particle.
Speaker A: Sorry, what did you say? 50 to 90.
Speaker B: 50 to 90, right. So this is how this thing is organized. And so now, typically. Right, so you see these, the antibodies that target spike protein, certain parts of the spike protein. But there could be also some treatments. Right. So these are small molecules that bind strategic parts of these proteins, disrupting its functioning. So one of the promising directions, it's one of the newest directions, is actually targeting the mdimer of the protein, targeting the proteins that make up this outer shell, because if you're able to destroy the outer shell, you are essentially destroying the viral particle itself, so preventing it from functioning at all.
Speaker A: So that's, you think is from a sort of cyber security perspective, virus security perspective, that's the best attack vector is, or like, that's a promising attack vector.
Speaker B: I would say, yes. I mean, there's still tons of research needs to be, you know, to be done, but, yes, I think, you know, so there's more attack surface, I guess, more attack surface. But, you know, from. From our analysis, from other evolutionary analysis, this protein is evolutionary more stable compared to the, say, to the spike protein.
Speaker A: Stable means a more static target.
Speaker B: Well, yeah. So it doesn't change. It doesn't evolve from the evolutionary perspective so drastically as, for example, the spike protein.
Speaker A: There's a bunch of stuff in the news about mutations of the virus in the United Kingdom. I also saw in South Africa something maybe that was yesterday you just kind of mentioned about stability and so on, which aspects of this are mutatable and which aspects, if mutated, become more dangerous and maybe even zooming out. What are your thoughts and knowledge and ideas about the way it's mutated? All the news that we've been hearing, are you worried about it from a biological perspective? Are you worried about it from a human perspective?
Speaker B: So, I mean, you know, mutations are sort of a general way for these viruses to evolve. Right. So it's, you know, it's essentially, this is the way they evolved. This is the way they were able to jump from, you know, one species to another. We also see, you know, some recent jumps. There were some incidents of this virus jumping from human to dogs. So, you know, there is some danger in those jumps, because every time it jumps, it also mutates. Right. So when it jumps to the species and jumps back, so it acquires some mutations that are sort of driven by the environment of a new host. Right. And it's different from the human environment. And so we don't know whether the mutations that are acquired in the new species are neutral with respect to the human host or maybe damaging.
Speaker A: Yeah, change is always scary. So are you worried about. I mean, it seems like because the spread is during winter now, seems to be exceptionally high, and especially with a vaccine just around the corner already being actually deployed, there's some worry that this puts evolutionary pressure, selective pressure on the virus for you to mutate. Is that a source of worry?
Speaker B: Well, I mean, there is always this thought in the scientist's mind what will happen. So I know there have been discussions about sort of the arms race between the ability of the humanity to get vaccinated faster than the virus essentially becomes resistant to the vaccine. I mean, I don't worry that much simply because, you know, there is not that much evidence to that.
Speaker A: To aggressive mutation around the vaccine.
Speaker B: Exactly. You know, obviously there are mutations around the vaccine. So the reason we get vaccinated every year against the season of mutations. Right. But I think it's important to study it. No doubts. I think one of the. To me, and again, I might be biased, because we've been trying to do that as well, but one of the critical directions in understanding the virus is to understand its evolution in order to understand the mechanisms, the key mechanisms that lead the virus to jump the nordic viruses to jump from species, from species to another, that the mechanisms that lead the virus to become resistant to vaccines, also to treatments. And hopefully that knowledge will enable us to forecast the evolutionary traces, the future evolutionary traces of this virus.
Speaker A: From a biological perspective, this might be a dumb question, but is there parts of the virus that, if souped up, like through mutation, could make it more effective at doing its job? We're talking about this specific coronavirus because we were talking about the different. The membrane, the m protein, the e protein, the n and s. The spike.
Speaker B: Is there some, and there are 20 or so more in addition to that.
Speaker A: But is that a dumb way to look at it? Which of these, if mutated, could have the greatest impact, potentially damaging impact, on the effectiveness of the virus?
Speaker B: So it's actually, it's a very good question because. And the short answer is, we don't know yet, but of course, there is capacity of this virus to become more efficient. The reason for that is. So if you look at the virus, I mean, it's a machine, right? So it's a machine that does a lot of different functions, and many of these functions are sort of nearly perfect, but they're not perfect, and those mutations can make those functions more perfect. For example, the attachment to ace two receptor of the spike. So is it, has this virus reached the efficiency in which the attachment is carried out, or there are some mutations that still to be discovered that will make this attachment stronger or something in a way more efficient from the point of view of this virus functioning? That's the obvious example. But if you look at each of these proteins, it's there for a reason. It performs certain function, and it could be that certain mutations will enhance this function. It could be that some mutations will make this function much less efficient. So that's also the case since we're.
Speaker A: Talking about the evolutionary history of a virus. Zoom back out and, uh, look at the evolution of proteins. I I glanced at this, uh, 2010 nature paper on the, quote, ongoing expansion of the protein universe. And then, you know, it kind of implies and, uh, talks about that, uh, protein started with a common ancestor, which is, you know, kind of interesting. It's interesting to think about, like, even just like, the first organic thing that started life on earth. And from that, there's now, you know, what is it? 3.5 billion years later, there's now millions of proteins, and they're still evolving. And that's, in part one of the things that you're researching. Is there something interesting to you about the evolution of proteins from this initial ancestor to today? Is there something beautiful insightful about this long story?
Speaker B: So I think if I were to pick a single keyword about protein evolution, I would pick modularity, something that we talked about in the beginning. And that's the fact that the proteins are no longer considered as a sequence of letters. There are hierarchical complexities in the way these proteins are organized, and these complexities are actually going beyond the protein sequence. It's actually going all the way back to the gene, to the nucleotide sequence. Again, these protein domains, they are not only functional building blocks, they are also evolutionary building blocks. And so what we see in the sort of later stages of evolution, I mean, once this stable structurally and functionally building blocks were discovered, they essentially, they stay, those domains stay as such. So that's why if you start comparing different proteins, you will see that many of them will have similar fragments, and those fragments will correspond to something that we call protein domain families. And so they are still different because you still have mutations, and different mutations are attributed to diversification of the function of this protein domains. However, you very rarely see the evolutionary events that would split this domain into fragments. Once you have the domain split, you can completely cancel out its function, or at the very least, you can reduce it. And that's not efficient from the point of view of the cell functioning. So the protein domain level is a very important one. Now, on top of that, if you look at the proteins, you have this structural units and they carry out the function. But then much less is known about things that connect this protein domains, something that we called linkers. And those linkers are completely flexible parts of the protein that nevertheless carry out a lot of function.
Speaker A: It's like little tails, little heads.
Speaker B: So we do have tails. So they call termini, c and n termini. So these are things right on one and another ends of the protein sequence. So they are also very important. So they attribute it to very specific interactions between the proteins.
Speaker A: So, but you're referring to the links.
Speaker B: Between domains that connect the domains. And, you know, apart from the. Just the simple perspective, if you have a very short domain, you have, sorry, a very short linker, you have two domains next to each other. They are forced to be next to each other. If you have a very long one, you have the domains that are extremely flexible and they carry out a lot of sort of spatial reorganization. Right?
Speaker A: That's right.
Speaker B: But on top of that. Right, just this linker itself, because it's so flexible, it actually can adapt to a lot of different shapes. And therefore it's a very good interactor when it comes to interaction between this protein and other protein. So these things also evolve and they, in a way, have different sort of laws of the driving laws that underlie the evolution, because they no longer need to preserve certain structure, unlike protein domains. On top of that, you have something that is even less studied. And this is something that attributed to the concept of alternative splicing. So alternative splicing. So it's a very cool concept. It's something that we've been fascinated about for over a decade in my lab and trying to do research with that. But so typically, a simplistic perspective is that one gene is equal, one protein product. So you have a gene, you transcribe it and translate it, and it becomes a protein. In reality, when we talk about eukaryotes, especially sort of more recent eukaryotes that are very complex, the gene is no longer equal to one protein. It actually can produce multiple functionally active protein products, and each of them is called an alternatively spliced product. The reason it happens is that if you look at the gene, it has also blocks and the blocks, some of which. And essentially it goes like this. So we have a block that will later be translated, we call it exon, then we'll have a block that is not translated cut out. We call it intron. So we have exon, intron, exon, intron, et cetera, et cetera, et cetera. Right. So sometimes you can have dozens of these exons and introns. So what happens is, during the process, when the gene is converted to rna, we have things that are cut out, the introns that cut out and exons that now get assembled together. And sometimes we will throw out some of the exons and the remaining protein product will become still be the same. Different.
Speaker A: Oh, different, right.
Speaker B: So now you have fragments of the protein that no longer there. They were cut out with the introns. Sometimes you will essentially take one exon and replace it with another one. Right.
Speaker A: So there's some flexibility in the process.
Speaker B: So. So that creates a whole new level.
Speaker A: Of complexity, because random, though, is it random?
Speaker B: It's not random. We. And this is where I think now the appearance of this modern single cell. And before that, tissue level sequencing, next generation sequencing techniques such as rna seq, allows us to see that these are the events that often happen in response. It's a dynamic event that happens in response to disease or in response to certain developmental stage of a cell. And this is an incredibly complex layer that also undergoes, I mean, because it's at the gene level, right. So it undergoes certain evolution. Right. And now we have this interplay between what happening, what is happening in the proteins world and what is happening in the gene and rna world. And for example, you know, it's, it's often that we see that the boundaries of these exons coincide with the boundaries of the protein domains. Right. So there is close interplay to that. It's not always, I mean, otherwise it would be too simple. Right. But we do see the connection between those sort of machineries. And obviously, the evolution will pick up this complexity and select for whatever is successful. Yeah. We see that complexity in play and makes this question more complex, but more exciting.
Speaker A: A small detour. I don't know if you think about this, into the world of computer science. There's Douglas Hostatter, I think, came up with a name of Quine, which are, I don't know if you're familiar with these things, but it's computer programs that have, I guess, Exxon and intron, and they copy. The whole purpose of the program is to copy itself. So it prints copies of itself, but can also carry information inside of it. So it's a very kind of crude, fun exercise of can we sort of replicate these ideas from cells of can we have a computer program that when you run it, just print itself, the entirety of itself, and does it in different programming languages and so on. I've been playing around and writing them. It's a kind of fun little exercise.
Speaker B: You know, when I was a kid. So, you know, it was essentially one of the. Of the sort of main stages in informatics olympiads that you have to reach in order to be any so good is you should be able to write a program that replicates itself. And so the tags then becomes even sort of more complicated. So what is the shortest.
Speaker A: What is the shortest program?
Speaker B: And of course, it's a function of a programming language. But, yeah, I remember a long, long, long time ago when we tried to make it shorter and shorter and find the shortcut.
Speaker A: There's actually a stack exchange. There's an entire site called Codegolf, I think, where the entirety is just the competition. People just come up with whatever task, I don't know, like write code that reports the weather today. And the competition is about in whatever programming language, what is the shortest program. And it makes you actually, people should check it out because it makes you realize there's some. Some weird programming languages out there. But just to dig on that a little deeper, do you think in computer science, we don't often think about programs. There's like the machine learning world now that's still kind of basic programs, and then there's humans that replicate themselves and there's these mutations and so on. Do you think we'll ever have a world where there's programs that kind of have an evolutionary process? So I'm not talking about evolutionary algorithms, but I'm talking about programs that kind of mate with each other and evolve and on their own replicate themselves. So this is kind of the idea here is that's how you can have a runaway thing. So we think about machine learning as a system that gets smarter and smarter and smarter and smarter. At least the machine learning systems of today are like, it's a program that you can, like, turn off, as opposed to throwing a bunch of little programs out there and letting them, like, multiply and mate and evolve and replicate. Do you ever think about that kind of world, you know, when we jump from the biological systems that you're looking at to artificial ones?
Speaker B: I mean, it's almost like you take the area of intelligent agents, which are essentially the independent sort of codes that run and interact and exchange the information. So I don't see why not. It could be sort of a natural evolution in this area of computer science.
Speaker A: I think it's kind of interesting possibility. It's terrifying, too, but I think it's a really powerful tool. Like, to have, like, agents that inter. You know, we have social networks with millions of people, and they interact. I think it's interesting to inject into that was already injected into that bots. Right. But those bots are pretty dumb. You know, they're. They're probably pretty dumb algorithms. You know, it's interesting to think that there might be bots that evolve together with humans, and there's the sea of humans and robots that are operating first in the digital space, and then you can also think, I love the idea. Some people worked, I think, at Harvard, at Penn. There's robotics labs that take as a fundamental task to build a robot that, given extra resources, can build. Build another copy of itself, like, in the physical space, which is super difficult to do, but super interesting. I remember there's, like, research on robots that can build a bridge. So they make a copy of themselves and they connect themselves. And so there's, like, self building bridge based on building blocks. You can imagine, like, a building that self assembles. So it's basically self assembling structures from. From robotic parts. But it's interesting to, within that robot, add the ability to mutate and do all the interesting little things that you're referring to in evolution, to go from a single origin protein building block to this weird complex.
Speaker B: If you think about this, the bits and pieces are there. So you mentioned revolutionary algorithm. So this is sort of. And maybe sort of the goal is, in a way, different. Right. So the goal is to essentially to optimize your search. Right. So, but sort of the ideas are there. So people recognize that the recombination events lead to global changes in its search trajectories, the mutations event, the more refined step in the search. Then you have other nature inspired algorithms. One of the reason that I think it's one of the funnest one is the slime based algorithm. I think the first was introduced by the japanese group, where it was able to solve some pretty complex problems. And then I think there are still a lot of things we've yet to borrow from the nature. So there are a lot of ideas that nature gets to offer us that it's up to us to grab it, to, you know, get the best use.
Speaker A: Of it, including neural networks. You know, that we have a very crude inspiration from nature on neural networks. Maybe there's other inspirations to be discovered in the brain or other aspects of the various systems, even like the immune system, the way it interplays I recently started to understand that, like, the immune system has something to do with the way the brain operates. Like, there's multiple things going on in there, which all of which are not modeled in artificial neural networks. And maybe if you throw a little bit of that biological spice in there, you'll come up with something. Something cool. I'm not sure if you're familiar with the Drake equation, that estimate. I just did a video on it yesterday because I wanted to give my own estimate of it. It's an equation that combines a bunch of factors to estimate how many alien civilizations in the galaxy.
Speaker B: I've heard about it, yes.
Speaker A: So one of the interesting parameters, you know, it's like, how many stars are born every year? How many planets are, on average per star for this? How many habitable planets are there? And then the one that starts being really interesting is the probability that life emerges on a habitable planet. So, like, I don't know if you think about. You certainly think a lot about evolution, but do you think about the thing which evolution doesn't describe, which is, like, the beginning of evolution, the origin of life? I think I put the probability of life developing in a habitable planet at 1%. This is very scientifically rigorous. Okay, well, first, at a high level, for the Drake equation, what would you put that percent at on Earth? And in general, do you have something. Do you have thoughts about how life might have started? You know, like, the proteins being the first kind of one of the early jumping points?
Speaker B: Yeah. So I think back in 2018, there was a very exciting paper published in nature where they found one of the simplest amino acids, glycine, in a comet dust. So this is. I apologize if I don't pronounce. It's a russian named comets, I think chugrium of Gerasimenko. This is the comment where. And there was this mission to get close to this comment and get the stardust from its tail. And when scientists analyzed it, they actually found traces of glycine, which makes up. It's one of the 20 basic amino acids that makes up proteins. So that was kind of exciting. Very exciting. Right. But the question is very interesting. So if there is some alien life, is it going to be made of proteins or maybe rna's? So we see that the rna viruses are certainly very well established sort of group of molecular machines. So, yeah, it's a very interesting question.
Speaker A: What probability would you put? Like, how hard is this jump? Like, how unlikely, just on earth do you think this whole thing is that we got going? Like, is that we really lucky or is it inevitable? Like, what's your sense when you sit back and think about life on Earth? Is it higher or lower than 1%? Well, cause 1% is pretty low, but it still is like, damn, that's pretty good chance.
Speaker B: Yes, it's a pretty good chance. I mean I would personally. But again, I'm probably not the best person to do such estimations, but I would intuitively I would probably put it lower. But still, I mean, we're really lucky here on Earth.
Speaker A: I mean, or the conditions are really good. I mean that's.
Speaker B: I think that there was everything Washington. Right in a way. Right. So still it's not. The conditions were not like ideal. If you try to look at what was several billions years ago when the life merged.
Speaker A: So there is something called the rare earth hypothesis that encounter to the Drake equation says that the conditions of Earth, if you actually were to describe Earth, it's quite a special place. So special might be unique in our galaxy and potentially, you know, close to unique in the entire universe. Like it's very difficult to reconstruct those same conditions. And what the rare earth hypothesis argues is all those different conditions are essential for life. And so that's sort of the counter, you know, like all the things we thinking that Earth is pretty average. I mean, I can't really, I'm trying to remember to go through all of them, but just the fact that it is shielded from a lot of asteroids, the obviously the distance to the sun, but also the fact that it's like a perfect balance between the amount of water and land and all those kinds of things. I don't know, there's a bunch of different factors that I remember. There's a long list, but it's fascinating to think about if in order for something like proteins and then DNA and rna to emerge, you need and basic living organisms, you need to be a very close and Earth like planet, which would be sad or exciting, I don't.
Speaker B: Know which if you ask me, I, in a way I put a parallel between, you know, between our own research and I mean, from the intuitive perspective, you know, you have those two extremes and the reality is never, very rarely falls into the extremes. It's always the optimum is always reached somewhere in between. So I would. And that's what I tend to think. I think that, you know, we're probably somewhere in between. So they were not unique unique. But again, the chances are, you know, reasonably small.
Speaker A: The problem is we don't know. The other extreme is like, I tend to think that we don't actually understand the basic mechanisms of, like, what this is all originated from. Like, it seems like we think of life as this distinct thing. Maybe intelligence is a distinct thing. Maybe the physics that, from which planets and suns are born is a distinct thing, but that could be a very. It's like the Stephen Wolfram thing. It's like the. From simple rules emerges greater and greater complexity. So, you know, I tend to believe that just life finds a way. It, like, we don't know the extreme of how common life is because it could be. Life is, like, everywhere. Like, like so everywhere that it's almost, like, laughable like that. We're such idiots to think it's ridiculous to even think it's like ants thinking that their little colony is the unique thing and everything else doesn't exist. I mean, it's also very possible that that's the extreme, and we're just not able to maybe comprehend the nature of that life just to stick on alien life for just a brief moment more. There is some signs of. Signs of life on Venus in gaseous form. There's hope for life on Mars, probably extinct. We're not talking about intelligent life, although that has been in the news recently. We're talking about basic, like, you know, bacteria, protobacteria. Yeah. And then also, I guess there's a couple moonshi.
Speaker B: Europa.
Speaker A: Yeah, Europa, which is Jupiter's moon. I think there's another one. Is that exciting, or is it terrifying to you that we might find life? Do you hope we find life?
Speaker B: I certainly do hope that we find life. I mean, it was very exciting to hear about this news about the possible life on Venus.
Speaker A: It'd be nice to have hard evidence of something, which is what the hope is for Mars and Europa. But do you think those organisms would be similar biologically, or would they even be sort of carbon based? If we do find them?
Speaker B: I would say they would be carbon based. How similar? It's a big question. Right. So it's the moment we discovered things outside Earth. Right. Even if it's a tiny little single cell. I mean, there is so much.
Speaker A: Just imagine that that would be so.
Speaker B: I think that that would be another turning point for the science, you know.
Speaker A: And especially if it's different in some very new way, that's exciting, because that says that's a definitive state. Not a definitive, but a pretty strong statement that life is everywhere in the. In the. The universe. To me, at least, that's really exciting. You brought up Joshua Lederberg in an offline conversation. I think I'd love to talk to you about alphafold. And this might be an interesting way to enter that conversation, because. So he won the 1958 Nobel prize in physiology and medicine for discovering that bacteria can mate and exchange genes. But he also did a ton of other stuff, like we mentioned, helping NASA find life on Mars. And the dendrill. Dendril, the chemical expert system. Expert systems. Remember those? What do you find interesting about this guy and his ideas about artificial intelligence in general?
Speaker B: So I have a kind of personal story to share. So I started my PhD in Canada back in 2000. And so essentially my PhD was. So we were developing a new language for symbolic machine learning. So it's different from the feature based machine learning. And one of the sort of cleanest applications of this approach of this formalism was to chemo informatics and computer aided drug design. Essentially, as a part of my research, I developed a system that essentially looked at chemical compounds of, say, the same therapeutic category, male hormones, and try to figure out the structural fragments that are the structural building blocks that are important, that define this class versus structural building blocks that are there just because to complete the structure. But they are not essentially the ones that make up the key chemical properties of this therapeutic category. For me, it was something new. I was trained as an applied mathematicians, as with some machine learning background, but computer drug design was a completely new territory. So because of that, I often find myself asking lots of questions on one of these sort of central forums. Back then, there were no. No facebooks or stuff like that. There were. It's a forum, you know, it's a forum. It's essentially, it's like a bulletin board on the Internet. Yeah. So you essentially, you have a bunch of people and you post a question and you get an answer from different people. And back then, one of the most popular forums was CCL, I think, computational chemistry, not library, but something like that. But CCL. That was the forum. And there I asked a lot of dumb questions. Yes, I asked questions, also shared some. Some information about our formulas and how we do and whether whatever we do makes sense. And so, you know, and I remember that one of these posts, I mean, I still remember, you know, I would call it desperately looking for a chemist advice, something like that. Right? And so I post my question, I explained, you know, how my. Our formalism is what it does and what kind of applications I'm planning to do. And it was in the middle of the night, and I went back to bed, and next morning have a phone call from my advisor, who also looked at this forum. You won't believe who replied to you. And it's like, who? And he said, well, you know, there is a message to, from Joshua Lederberg. And my reaction was like, who is Joshua Lederberg?
Speaker A: Your advisor hung up.
Speaker B: So. And essentially, Joshua wrote me that we had conceptually similar ideas in the Dendrill project. You may want to look it up. And, you know, we should also.
Speaker A: Sorry. And it's a side comment, say that even though he, he won the Nobel Prize at a really young age in 58, but so he was, I think.
Speaker B: He was, what, 33?
Speaker A: Yeah, it's just crazy.
Speaker B: Yeah.
Speaker A: So anyway, so that's. So hence in the nineties, responding to young whippersnappers on the CCL forum. Okay.
Speaker B: And so back then, he was already very senior. I mean, he unfortunately passed away back in 2008. But I know back in 2001, he was. I mean, he was a professor emeritus at Rockefeller University. And, you know, that was actually, believe it or not, one of the, one of the reasons I decided to join, you know, as a postdoc, the group of Andre Saleh, who was at Rockefeller University, with the hope that I could actually have a chance to meet Joshua in person. And I met him very briefly just because he was walking. There's a little bridge that connects the research campus with the sort of skyscraper that the Rockefeller owns, where postdocs and faculty and graduate students live. And so I met him and had a very short conversation. So I started reading about dendrite, and I was amazed. We're talking about 1960. The ideas were so profound.
Speaker A: What's the fundamental ideas of it?
Speaker B: The reason to make this is even crazier. So Ledderberg wanted to make a system that would help him study the extraterrestrial molecules. So the idea was that the way you study the extraterrestrial molecules is you do the mass spec analysis. The mass spec gives you bits, numbers, essentially gives you the ideas about the possible fragments or atoms and maybe little fragments, pieces of this molecule that make up the molecule. So now you need to decompose this information and to figure out what was the whole before it became fragments, bits and pieces. So in order to make this, to have this tool, the idea of Lederberg was to connect chemistry, computer science, and to design this so called expert system that looks, that takes into account, that takes as an input the mass spec data, the possible database of possible molecules, and essentially try to sort of induce the molecule that would correspond to this spectra or, you know, essentially the what this project ended up being was that, you know, it would provide a list of candidates that then a chemist would look at and make final decisions. So.
Speaker A: But the original idea, I suppose, is to solve the entirety of this problem automatically.
Speaker B: Yes. So he, you know, so he. Back then, he approached. Yes, believe that. It's amazing. I mean, it still blows my mind, you know, that it's that. And this was essentially the origin of the modern bioinformatics, chemo informatics back in sixties. So that's, that's, you know, so every time you deal with projects like this, with the, you know, research like this, you just, you know, so the power of the, of the, you know, intelligence of these people is just, you know, overwhelming.
Speaker A: Do you think about expert systems, is there, and why they kind of didn't become successful, especially in the space of bioinformatics, where it does seem like there's a lot of expertise in humans and, you know, it's. It's possible to see that a system like this could be made very useful.
Speaker B: Right.
Speaker A: And be built up.
Speaker B: It's, it's actually, it's a, it's a great question, and this is something. So, you know, so at my university, I teach artificial intelligence, and we start, my first two lectures are on the history of AI. And there we try to go through the main stages of AI, the question of why expert systems failed or became a obsolete. It's actually a very interesting one. And there are, you know, if you try to read the, you know, the historical perspectives, there are actually two lines of thoughts. One is that the, they were essentially not up to the expectations, and so therefore they were replaced, you know, by other things. Right. The other one was that completely opposite one, that they were too good. And as a result, they essentially became sort of a household name. And then essentially they got transformed. I mean, in both cases, sort of the outcome was the same. They evolved into something. Yeah, right. And that's what I, you know, if, if I look at this, right, so the modern machine learning.
Speaker A: Right, so there's echoes in the modern machine learning.
Speaker B: I think so. I think so. Because, you know, if you think about this, you know, and how we design, you know, the most successful algorithms, including alphafold. Right? You built in the knowledge about the domain that you study, right? So you built in your expertise.
Speaker A: So speaking of alphafold, so deepminds, alphafold two recently was announced to have, quote unquote, solved protein folding. How exciting is this to you? It seems to be one of the exciting things that have happened in 2020. It's incredible accomplishment from the looks of it. What part of it is amazing to you, what part would you say is overhyped or maybe misunderstood?
Speaker B: It's definitely a very exciting achievement to give you a little bit of perspective. Right. So in bioinformatics, we have several competitions. And so the way you often hear how those competitions have been explained to sort of to non bioinformaticians is that they call it bioinformatics Olympic games. And there are several disciplines, right? So the, historically, one of the first one was the discipline in predicting the protein structure, predicting the 3d coordinates of the proteins. There are some others. So the predicting protein functions, predicting effects of mutations on protein functions, then predicting protein, protein interactions. So the original one was Casp, or a critical assessment of protein structure. And the typically what happens during these competitions is experimental scientists solve these structures but don't put them into the protein databank, which is the centralized database that contains all these 3d coordinates. Instead they hold it and release protein sequences. And now the challenge of the community is to predict the 3d structures of these proteins and then use the experimentary solve structures to assess which one is the closest one.
Speaker A: And this competition, by the way, just a bunch of different tangents. And maybe you can also say, what is protein folding? Then this competition, Casp competition, has become the gold standard. And that's what was used to say that protein folding was solved. So just add a little. Yeah, just a bunch. So if you can, whenever you say stuff, maybe throw in some of the basics for the folks that might be outside of the field. Anyway, sorry for.
Speaker B: Yeah, so, you know, so the reason it's, it's, you know, it's relevant to our understanding of protein folding is because, you know, we've yet to learn how the folding mechanistically works, right? So there are different hypotheses. What happens to this fold? For example, there is a hypothesis that the folding happens by, you know, also in the modular fashion, right? So that we have protein domains that get folded independently because their structure is stable and then the whole protein structure gets formed. But, you know, within those domains we also have so called secondary structure, the small alpha helices, beta sheets. So these are elements that are structurally stable. And the question is, when do they get formed? Because some of the secondary structure elements, you have to have a fragment in the beginning and say the fragment in the middle. So you cannot potentially start having the full fold from the get go, right? So it's still, you know, it's still a big enigma. What, what happens? We know that it's an extremely efficient and stable process, right?
Speaker A: So there's, there's this long sequence and the fold happens really quickly.
Speaker B: Exactly.
Speaker A: Well, that's really weird. Right. And it happens like the same way almost every time.
Speaker B: Exactly, exactly.
Speaker A: Right. It's really weird. That's freaking weird.
Speaker B: It's, it's, yeah, that's why it's such a thing. But most importantly, so when you see the translation process, so when you don't have the whole protein translated, it's still being translated. Getting out from the ribosome, you already see some structural fragmentation. So folding starts happening before the whole protein gets produced. Right. And so this is obviously, you know, one of the biggest questions in, you know, in modern molecular biologies, not, not.
Speaker A: Like maybe what happens? Like, that's not, that's bigger than the question of folding. That's the question of like, like deeper, fundamental idea of folding.
Speaker B: Yes.
Speaker A: Behind folding.
Speaker B: Exactly, exactly. So, you know, so obviously, if we are able to predict the end product of protein folding, we are one step closer to understanding the mechanistics of the protein folding because we can then potentially look and start probing what are the critical parts of this process and what are not so critical parts of this process. We can start decomposing this in a way, this protein structure prediction algorithm can be used as a tool. Right. So you change the, you know, you modify the protein, you get back to this tool, it predicts, okay, it's completely, it's completely unstable.
Speaker A: Yeah. Which, which aspects of the input will have a big impact on the output?
Speaker B: Exactly, exactly. So what happens is we typically have some sort of incremental advancement. Each stage of this Casp competition, you have groups with incremental advancement. And historically, the top performing groups, they were not using machine learning, they were using very advanced biophysics combined with bioinformatics, combined with the data mining, and that would enable them to obtain protein structures of those proteins that don't have any structural source relatives, because if we have another protein, say the same protein, but coming from a different species, we could potentially derive some ideas, and that's so called homology or comparative modeling, where we'll derive some ideas from the previously known structures and that would help us tremendously in reconstructing the 3d structure overall. But what happens when we don't have these relatives? This is when it becomes really, really hard. So that's so called de novo protein structure prediction. And in this case, those methods were traditionally very good. But what happened in the last year, the original alphafold came into and over sudden, it's much better than everyone else.
Speaker A: This is 2018.
Speaker B: Yeah.
Speaker A: And the competition is only every two years. I think.
Speaker B: And then, so it was sort of over shockwave to the bioinformatics community that we have a state of the art machine learning system that does structure prediction. And essentially what it does, if you look at this, it actually predicts the context. So the process of reconstructing the 3d structure starts by predicting the, the context between the different parts of the protein, and the context is essentially the part of the proteins that are in a close proximity to each other.
Speaker A: Right. So it actually, the machine learning part seems to be estimating. You can correct me if I'm wrong here, but it seems to be estimating the distance matrix, which is like the distance between the different parts.
Speaker B: Yeah. So we call it the. A contact map.
Speaker A: Contact map.
Speaker B: So once you have the contact map, the reconstruction is becoming more straightforward. The contact map is the key. And so that what happened. And now we started seeing, in this current stage, in the most recent one, we started seeing the emergence of these ideas in others. People works, right? But yet here is alphafold two that again outperforms everyone else. And also by introducing yet another wave of the machine learning ideas.
Speaker A: Yeah, there does seem to be also an incorporation. First of all, the paper is not out yet, but there's a bunch of ideas already out. There does seem to be an incorporation of this other thing. I don't know if it's something that you could speak to, which is like the incorporation of other structures, like evolutionary, similar structures that are used to give you hints.
Speaker B: Yes. Evolutionary similarity is something that we can detect at different levels. We know, for example, that the structure of proteins is more conserved than the sequence. The sequence could be very different, but the structural shape is actually still very conserved. So that's sort of the intrinsic property that, in a way related to protein folds, to the evolution of proteins and protein domains, etcetera, but we know that there have been multiple studies, and ideally, if you have structures, you should use that information. However, sometimes we don't have this information. Instead, we have a bunch of sequences. Sequences. We have a lot. We have hundreds, thousands of different organisms sequenced by taking the same protein, but in different organisms and aligning it, so making the corresponding positions aligned. We can actually say a lot about what is conserved in this protein, and therefore structurally more stable, what is diverse in these proteins. On top of that, we could provide the information about the secondary structure of this protein, et cetera, et cetera. So this information is extremely useful, and it's already there. So, so while it's tempting to, you know, to do a complete ab initio, so you just have a protein sequence and nothing else. The reality is such that we, we are overwhelmed with this data, so why not use it? And so. Yeah, so I'm looking forward to reading the, this paper.
Speaker A: It does seem to like they've, in the previous version of Alphafold, they didn't, for this evolutionary similarity thing. They didn't use machine learning for that, or they rather, they used it as the input to the entirety of the neural net. Like the features derived from the similarity. It seems like there's some kind of iterative thing where it seems to be part of the, part of the learning process is the incorporation of this evolutionary similarity.
Speaker B: Yeah, I don't think there is a biorxiv paper. Right. There's no, nothing.
Speaker A: There's a blog post that's written by a marketing team, essentially.
Speaker B: Yeah.
Speaker A: Which, you know, it has some scientific similarity probably to the actual methodology used, but it could be, it's like interpreting scripture. It could be just poetic interpretations of the actual work as opposed to direct connection to the work.
Speaker B: So now speaking about protein folding. Right. So, you know, in order to answer the question whether or not we, we have solved this. Right?
Speaker A: Yeah.
Speaker B: So we need to go back to the beginning of our conversation, you know, with the realization that, you know, an average protein, is that typically what the, the cusp has been focusing on is the, you know, the, this competition has been focusing on the single, maybe two domain proteins that are still very compact and even those ones are extremely challenging to solve. Right. But now we talk about an average protein that has two, three protein domains. If you look at the proteins that are in charge of the process with the neural system, perhaps one of the most recently evolved sort of systems in the organism. Right? All of them. Well, the majority of them are highly multi domain proteins. So they are, some of them have five, six, seven and more domains. And we are very far away from understanding how these proteins are folded.
Speaker A: So the complexity of the protein matters here. The complexity, the complexity of the protein modules or the protein domains. So you're saying solved. So the definition of solved here is particularly the cast competition, achieving human level, not human level, achieving experimental level performance on these particular sets of proteins that have been used in these competitions.
Speaker B: Well, I mean, I do think that especially with regards to the alpha fold, it is able to solve, at the near experimental level, pretty big majority of the more compact proteins or protein domains. Because again, in order to understand how the overall protein, multi domain protein fault, we do need to understand the structure of its individual domains.
Speaker A: Unlike if you look at alpha zero, or even mu zero. If you look at that work, it's nice. Reinforcement learning. Self playing mechanisms are nice because it's all in simulation, so you can learn from just huge amounts. Like, you don't need data. It would like the problem with proteins, like the size. I forget how many 3d structures have been mapped, but the training data is very small no matter what. It's like millions, maybe one or 2 million, something like that, but some very small number. But like, it doesn't seem like that's scalable. There has to be, I don't know, it feels like you want to somehow ten x the data or 100 x the data somehow.
Speaker B: Yes, but we also can take advantage of homology models. So the models that are of very good quality because they are essentially obtained based on the evolutionary information. So there is a potential to enhance this information and use it again to empower the training set. And it's. I think I am actually very optimistic. I think it's been one of these sort of churning events where you have a system that is a machine learning system that is truly better than the more conventional biophysics based methods.
Speaker A: That's a huge leap. This is one of those fun questions, but where would you put it in the ranking of the greatest breakthroughs in artificial intelligence history? Okay, so let's see who's in the running. Maybe you can correct me. So you got like Alphazero and Alphago beating, you know, beating the world champion at the game of go thought to be impossible like 20 years ago, or at least the a community was highly skeptical then you got like also deep blue, original Kasparov. You have deep learning itself. Like the. Maybe, what would you say, the Alex. Net imagenet moment. So the first neural network achieving human level performance, super. Not, that's not true. Achieving like a big leap in performance on the computer vision problem. There is OpenAI, the whole like GPT-3 that whole space of transformers and language models, just achieving this incredible performance of application of neural networks to language models. Boston dynamics, pretty cool. Like robotics, even though people are like, there's no AI. No, no, there's no machine learning currently, but AI is much bigger than machine learning. So that just the engineering aspect, I would say it's one of the greatest accomplishments in engineering side engineering, meaning like mechanical engineering of robotics ever. Then of course, autonomous vehicles. You can argue for Waymo, which is like the Google self driving car, or you can argue for Tesla, which is like actually being used by hundreds of thousands of people on the road today, machine learning system. And I don't know if you can, what else is there? But I think that's it. And then alpha fold, many people are saying as up there potentially number one, would you put them at number one?
Speaker B: Well, in terms of the impact on. On the science and on the society beyond is definitely, you know, to me would be one of the, you know, top three, which maybe, I mean, I. Probably not the best person to answer that, you know, but, you know, I, you know, I do have. I remember my, you know, back in, I think, 1997 when deep blue, bad Kasparov, it was, I mean, it was a shock. I mean, it was. And I think for the, for the, you know, for the, you know, pre substantial part of the world that especially people who have some, you know, some experience with chess. Right. And realizing how incredibly human this game, how, you know, how much of a brainpower you need, you know, to. To reach those, you know, those levels of grandmasters. Right.
Speaker A: Level, yeah, it's probably one of the first time and how good Kasparov was.
Speaker B: And again. Yeah. So Kasparov is arguably one of the best ever. Right. And get a machine that beats him. Right?
Speaker A: So it's first time a machine probably beat a human at that scale of a thing, of anything.
Speaker B: Yes. Yes. So that was, to me, that was like, you know, one of the groundbreaking events in the history of AI.
Speaker A: Yeah, that's probably number one. Like, we don't. It's hard to remember. It's like Muhammad Ali versus, I don't know, any other Mike Tyson, something like that. It's like, nah, you gotta put Muhammad Ali at number one. Same with deep blue. Even though it's not machine learning based.
Speaker B: Still, it uses advanced search. And search is the integral part of AI, right? So as you said, people don't think.
Speaker A: Of it that way at this moment. In vogue currently, search is not seen as a fundamental aspect of intelligence, but it very, well, very likely is. In fact, that's what neural networks are. They're just performing search on the space of parameters. It's all search, all of intelligence is some form of search, and you just have to become cleverer and clever at that search problem.
Speaker B: I also have another one that you didn't mention. That's one of my favorite ones. You probably heard of this. I think it's called Deep Rembrandt. It's the project where they trained. I think there was a collaboration between the sort of the experts in Rembrandt painting in Netherlands and a group, an artificial intelligence group, where they train an algorithm to replicate the style of the Rembrandt and they actually printed a portrait that never existed before in the style of Rembrandt. They. I think they printed it on a. On a sort of. On the canvas that, you know, using pretty much same types of paints and stuff. To me, it was mind blowing.
Speaker A: Yeah. It's in the space of art that's interesting. There hasn't been. Maybe that's. That's it. But I. I think there hasn't been an image in that moment yet. In the space of art. You haven't been able to achieve superhuman level performance in the space of art, even though there was, you know, there's a big famous thing where there was a piece of art was purchased, I guess, for a lot of money.
Speaker B: Yes.
Speaker A: Yeah. But it's still, you know, people are like, in the space of music, at least that's, you know, it's clear that human created pieces are much more popular. So there hasn't been a moment where it's like, oh, this is. We're now, I would say, in the space of music, what makes a lot of money. We're talking about serious money. It's music and movies or, like, shows and so on, and entertainment. There hasn't been a moment where AI created. AI was able to create a piece of music or a piece of cinema, like Netflix show that is, you know, that's sufficiently popular to make a ton of money. And that moment would be very, very powerful. Cause that's like, that's an AI system being used to make a lot of money. And, like, direct, of course, AI tools, like even premiere audio editing, all the editing, everything I do to edit this podcast, there's a lot of AI involved. I want, actually, there's this program. I want to talk to those folks just because I want to nerd out. It's called izotope. I don't know if you're familiar with it. They have a bunch of tools of audio processing, and they have, I think they're Boston based. Just. It's so exciting to me to use it, like, on the audio here, because it's all machine learning. It's not, because most, most audio production stuff is, like, any kind of processing you do is very basic signal processing, and you're tuning knobs and so on. They have all of that, of course, but they also have all of this machine learning stuff, like where you actually give it training data, you select parts of the audio, you train on. You train on it, and it figures stuff out. It's great. It's able to detect the ability of it to be able to separate voice and music, for example, or voice and anything is incredible. That's clearly exceptionally good at applying these different neural networks models to separate the different kinds of signals from the audio. Okay, so that's really exciting. Photoshop, Adobe, people also use it. But to generate a piece of music that will sell millions. A piece of art.
Speaker B: Yeah, no, I agree. And that's, you know, as I mentioned, I offer my AI class, and an integral part of this is a project. Right. So it's my favorite, ultimate favorite part, because it typically we have these project presentations the last two weeks of the class. It's right before the Christmas break. And it's sort of, it adds this cool excitement and every time I'm amazed with some projects that people come up with. And so quite a few of them are actually, they have some link to arts. I mean, I think last year we had a group who designed an AI producing hocus japanese poems.
Speaker A: Oh, wow.
Speaker B: And some of them, so it got trained on the english base, hikus. Right. And some of them, they get to present the top selection. They were pretty good. Of course, I'm not a specialist, but you read them and you see the.
Speaker A: Seems profound.
Speaker B: Yes, it seems reasonable. So it's kind of cool. We also had a couple of projects where people tried to teach AI how to play rock music, classical music, I think, and popular music. Interestingly enough, classical music was among the most difficult ones. And of course, if you look at grandmasters of music like Bach, so there is a lot of, there is a lot of almost math.
Speaker A: Well, he's very mathematical.
Speaker B: Exactly. So I would imagine that at least some style of this music could be picked up. But then you have completely different spectrum of classical composers and so it's almost like you don't have to sort of look at the data, you just listen to it and say, that's not it. Not yet.
Speaker A: That's not it. Yeah, that's how I feel too. There's open AI has, I think, open muse or something like that. The system, it's cool, but it's like, yeah, it's not compelling for some, for some reason. It could be a psychological reason too. Maybe we need to have a human being, a tortured soul behind the music. I don't know.
Speaker B: Yeah, no, that, absolutely, I completely agree. But yeah. Whether or not we'll have a, one day, we'll have, you know, a song written by an AI engine to be in like in top charts. Yeah, musical charts. I wouldn't be surprised. I wouldn't be surprised.
Speaker A: I wonder if we already have one and it just hasn't been announced. We wouldn't know how hard is the multi protein folding problem? Is that kind of something you've already mentioned, which is baked into this idea of greater and greater complexity of proteins, like multi domain proteins, is that basically become multi protein complexes?
Speaker B: Yes, you got it right. So it's sort of, it has the components of both of protein folding and protein, protein interactions. Because in order for these domains, I mean, many of these proteins actually, they never form a stable structure. One of my favorite proteins, and pretty much everyone who works in a, I know whom, I know who works with proteins, they always have their favorite proteins. So one of my favorite proteins, probably my favorite protein, the one that I worked when I was a postdoc, is so called postsynaptic density, 95 psd, 95 protein. So it's one of the key actors in the majority of neurological processes at the molecular level, and essentially it's a key player in the postsynaptic density. So this is the crucial part of the synapse where a lot of these chemical eclipse processes are happening. So it has five domains, so five protein domains, pretty large proteins, I think, 600 something. But the way it's organized itself, it's flexible, it acts as a scaffold, so it is used to bring in other proteins, so they start acting in the orchestrated manner. Right. So, and the, the type of, the shape of this protein, it's, in a way, there are some stable parts of this protein, but there are some flexible, and this flexibility is built in, into the protein in order to become sort of this multifunctional machine.
Speaker A: So do you think that kind of thing is also learnable through the alphafold two kind of approach?
Speaker B: I mean, the time will tell.
Speaker A: Is it another level of complexity? Is it like how big of a jump in complexity is that whole thing?
Speaker B: To me, it's yet another level of complexity, because when we talk about protein, protein interactions, and there is actually a different challenge for this called Capri, and so this that is focused specifically on macromolecular interactions, protein, protein, DNA, etcetera. So, but it's, you know, there are different mechanisms that govern molecular interactions and that need to be picked up, say, by a machine learning algorithm. Interestingly enough, we actually, we participated for a few years in this competition. We typically don't participate in competitions, I don't know, don't have enough time because it's very intensive. It's a very intensive process. But we participated back in about ten years ago or so. And the way we entered this competition, so we designed a scoring function. So the function that evaluates whether or not your protein protein interaction is supposed to look like experimentally solved. The scoring function is very critical part of the model prediction. So we design it to be a machine learning. And so it was one of the first machine learning based scoring function used in Capri. You know, we essentially, you know, learned what should contribute, what are the critical components contributing into the protein protein interaction.
Speaker A: So this. This could be converted into a learning problem, and thereby it could be. It could be learned?
Speaker B: I believe so, yes.
Speaker A: Do you think alphafold two or something similar to it from deep mind or somebody else will be, will result in Nobel Prize or multiple Nobel prizes? So, like they, you know, obviously or maybe not so obviously, you can't give a Nobel Prize to computer program. You, at least for now, give it to the designers of that program. But do you see one or multiple Nobel prizes where alphafold two is like a large percentage of what that prize is given for? Would it lead to discoveries at the level of Nobel prizes?
Speaker B: I mean, I think we are definitely destined to see the Nobel Prize becoming sort of to be evolving with the evolution of science. And the evolution of science is such that it now becomes, like, really multifaceted. Right? So where you don't really have, like, a unique discipline, you have sort of the. A lot of cross disciplinary talks in order to achieve sort of, you know, really big advancements, you know, so I think, you know, the computational methods will be acknowledged in one way or another. And as a matter of fact, you know, they were first acknowledged back in 2013, right, where the first three people were awarded the Nobel Prize for studying the protein folding, the principle. And I think all three of them are computational by physicists. So that, I think, is unavoidable. It will come with the time, the fact that alpha fold and similar approaches, because, again, it's a matter of time that people will embrace this principle and we'll see more and more such tools coming into play. But these methods will be critical in a scientific discovery, no doubts about it.
Speaker A: On the engineering side, maybe a dark question, but do you think it's possible to use these machine learning methods to start to engineer proteins? And the next question is something quite a few biologists are against. Some are for study purposes, is to engineer viruses. Do you think machine learning, like something like alphafold, could be used to engineer viruses?
Speaker B: So, to answer the first question, it has been a part of the research in the protein science. The protein design is very prominent areas of research. Of course, one of the pioneers is David Baker and Rosetta algorithm that, you know, essentially was doing the de novo design and was used to design new.
Speaker A: Proteins, you know, and design a proteins means design a function. So, like when you design a protein, you can control. I mean, the whole point of a protein, with the protein structure comes a function like it's doing something.
Speaker B: Correct.
Speaker A: So you can design different things that.
Speaker B: So you can. Yeah, so you can do. Well, you can look at the proteins from the functional perspective. You can also look at the proteins from the structural perspective. Right. So the structural building blocks. So if you want to have a building block of a certain shape, you can try to achieve it by introducing a new proton sequence and predicting how it will fold. So with that, I mean, it's one of the natural applications of these algorithms. Now talking about engineering a virus with machine learning. With machine learning. Right. So, well, you know, so luckily for us, I mean, we don't have that much data. Right. We actually, right now, one of the projects that we are carrying on in the lab is we're trying to develop a machine learning algorithm that determines whether or not the current strain is pathogenic, the current strain of the coronavirus, of the virus. There are applications to coronaviruses because we have strains of SARS CoV two, also SARS CoV mers that are pathogenic, but we also have strains of other coronaviruses that are not pathogenic. The common cold viruses, you know, and some other ones. Right, so pathogenic, meaning spreading pathogenic means actually inflicting damage. Correct. There are also some, you know, seasonal versus pandemic strains of influenza. Right. And to determining the. What are the molecular determinants. Right. So that are built in. Into the protein sequence, into the gene sequence. Right. So. And whether or not the machine learning can determine those deter those components. Right.
Speaker A: Oh, interesting. So, like, using machine learning to do that's really interesting. To. To give and give the input is like, what the sequence, the protein sequence, and then determine if this thing is going to be able to do damage to a biological system.
Speaker B: Yeah. So good.
Speaker A: Machine learning. You're saying we don't have enough data for that?
Speaker B: I mean, for this specific one, we do. We might actually have to back up on this. We're still in the process. There was one work that appeared in Biorxiv by Eugene Kunin, who is one of these pioneers in evolutionary genomics, and they tried to look at this, but the methods were sort of standard supervised learning methods. And now the question is, can you advance it further by using not so standard methods? There's obviously a lot of hope in transfer learning, where you can actually try to transfer the information that the machine learning journey about the proper protein sequences, you know, so there is some promise in going this direction, but if we have this, it would be extremely useful, because then we could essentially forecast the potential mutations that would make a current strain more or less pathogenic, dissipate them.
Speaker A: From a vaccine development for the treatment, antiviral drug development.
Speaker B: That would be a very crucial task.
Speaker A: But you could also use that system to then say, how would we potentially modify this virus to make it more pathogenic?
Speaker B: That's true. That's true. I mean, you know, the. Again, the hope is, well, several things, right? So one is that, you know, it's even if you design a. You know, a sequence. Right. So to carry out the actual experimental biology to ensure that all the components working, you know, is. Is a completely different matter.
Speaker A: Difficult process.
Speaker B: Yes. Then there. You know, we've seen in the past there could be some regulation of the. The moment the scientific community recognizes that it's now becoming no longer a sort of a fun puzzle for machine learning.
Speaker A: Could be open.
Speaker B: Yeah. So then there might be some regulation. So I think back in, what, 2015, there was an issue on regulating the research on influenza strains. Several groups use the mutation analysis to determine whether or not this strain will jump from one species to another. And I think there was, like, a half a year moratorium on the research on the paper published until scientists analyzed it and decided that it's actually safe.
Speaker A: I forgot what that's called. Something of function test and function.
Speaker B: Gain a function.
Speaker A: Gain a function. Yeah. Gain a function. Loss of function. That's right. Sorry. It's like, let's watch this thing mutate for a while to see. Like, to see what kind of things we can observe. I guess I'm not so much worried about that kind of research. If there's a lot of regulation and if it's done very well and with competence. And seriously, I am more worried about kind of this. You know, the underlying aspect of this question is more like 50 years from now, speaking to the Drake equation. One of the parameters in the Drake equation is how long civilizations last. And that seems to be the most important value, actually, for calculating if there's other alien intelligence civilizations out there. That's where there's most variability. Assuming if that percentage that life can emerge is, like, not zero. Like, if we're super unique, then it's the. How long we last is basically the most important thing from a selfish perspective, but also from a Drake equation perspective, I'm worried about our civilization lasting. And you kind of think about all the ways in which machine learning can be used to design greater weapons of destruction. Right. And, I mean, one way to ask that, if you look sort of 50 years from now, 100 years from now, would you be more worried about natural pandemics or engineered pandemics, like, who's, who is the better designer of viruses, nature or humans? If we look down the line, I.
Speaker B: Think, in my view, I would still be worried about the natural pandemics simply because, I mean, the capacity of the nature producing it does pretty good job.
Speaker A: Right?
Speaker B: Yes.
Speaker A: And the motivation for using virus engineering viruses as a weapon is a weird one, because maybe you can correct me on this, but it seems very difficult to target a virus. Right. The whole point of a weapon, the way a rocket works, you have a starting point, you have an end point, and you. You're trying to hit a target. To hit a target with a virus is very difficult. It's basically just. Right. It.
Speaker B: It's.
Speaker A: The target would be the human species. Oh, man. Yeah. I have. I have a hope in us. I'm forever optimistic that we will not. There's no. There's insufficient evil in the world to do, to lead to that kind of destruction.
Speaker B: Well, you know, I also hope that, I mean, that's what we see. I mean, with the way we are getting connected. The world is getting connected. I think it helps for the world to become more transparent.
Speaker A: Yeah.
Speaker B: So the information spread is, you know, I think it's one of the key things for the, for the society to become more balanced one way or another.
Speaker A: This is something that people disagree with me on, but I do think that the kind of secrecy the governments have. So you're kind of speaking more to the other aspects, like research, community being more open, companies are being more open. Government is still, like, we're talking about military secrets, I think. I think military secrets of the kind that could destroy the world will become also a thing of the 20th century. It'll become more and more open. Like, I think nations will lose power in the 21st century. Like, lose sufficient power towards. Secrecy's. Transparency is more beneficial than secrecy. But, of course, it's not obvious.
Speaker B: Let's hope so. Let's hope so that the governments will become more transparent.
Speaker A: So we last talked, I think, in March or April. What have you learned? How has your philosophical, psychological, biological worldview changed since then, or you've been studying it nonstop from a computational biology perspective. How has your understanding and thoughts about this virus changed over those months? From the beginning, to today.
Speaker B: One thing that I was really amazed at, how efficient the scientific community was. I mean, and even just judging on this very narrow domain of protein structure, understanding the structural characterization of this virus from the components point of view, from the whole virus point of view, if you look at SARS, something that happened less than 20, but close enough 20 years ago, and you see when it happened, what was the response by the scientific community? You see that the structural characterizations did occur, but it took several years. Right now, the things that took several years, it's a matter of months, right. So we see that the research pop up. We are at the unprecedented level in terms of the sequencing. Never before we had a single virus sequenced so many times, which allows us to actually to trace very precisely the evolutionary nature of this virus, what happens. And it's not just this virus independently of everything, it's the sequence of this virus linked anchored to the specific geographic place, to specific people, because our genotype influences also the evolution of this. It's always a host pathogen co evolution that occurs.
Speaker A: It'd be cool if we also had a lot more data about the spread of this virus. Not maybe. Well, it'd be nice if we had it for contact tracing purposes for this virus, but it would be also nice if we had it for the study, for future viruses to be able to respond and so on. But it's already nice that we have geographical data and basic data from individual humans.
Speaker B: Exactly. I think contact tracing is obviously a key component in understanding the spread of this virus. There is a number of challenges, so Xprize is one of them. We just recently took a part of this competition. It's the prediction of the number of infections in different regions. And obviously, the AI is the main topic in those predictions.
Speaker A: Yeah, but it's still the data. I mean, that's. That's a competition, but the. The data is weak on the training. Like, it's. It's great. It's much more than probably before, but, like, it'd be nice if it was, like, really rich. Like, I talked to Michael Mina from. From Harvard. I mean, he dreams that the community comes together with, like, a weather map to wherever I. Of viruses, right? Like, really high resolution sensors on, like, how from person to person, the viruses that travel, all the different kinds of viruses, right? Because there's. There's. There's a ton of them. And then you'd be able to tell the story that you've spoken about of the evolution of these viruses, like day to day mutations that are occurring. I mean, that would be fascinating just from a perspective of study and from the perspective of being able to respond to future pandemics, that's ultimately what I'm worried about. People love books. Is there some three or whatever number of books, technical fiction, philosophical, that brought you joy in life, had an impact on your life, and maybe some that you would recommend others.
Speaker B: So I'll give you three very different books. And I also have a special runner up and honorable mention. I mean, it's an audiobook, and that's, there's some specific reason behind it. So the first book is something that sort of impacted my earlier stage of life and probably not going to be very original here. It's Bulgakov's master and margarita. So that's probably, you know, well, not.
Speaker A: For Russian, maybe it's not super original, but it's, you know, it's a really powerful book for, even in English. So I read it in English.
Speaker B: So it is incredibly powerful. And I mean, it's the way it ends, right? So it's, I still have goosebumps when I read the very last sort of, it's called prologue, where it's just so powerful.
Speaker A: What impact did you have on you? What ideas, what insights did you get from it?
Speaker B: I was just taken by, you know, by the fact that you have those parallel lives apart from many centuries. Right. And somehow they got sort of intertwined into one story. And that, to me, was fascinating. And, you know, of course, the, you know, the romantic part of this book is like, it's not just romance. It's like the romance empowered by sort of magic. Right. And maybe on top of that, you have some irony, which. Unavoidable. Right. So, because it was that, you know, the soviet time, but it's very, it's.
Speaker A: Very, it's deeply russian. So that's the wit, the humor, the pain, the love, all of that is one of the books that kind of captures something about russian culture that people outside of Russia should probably read.
Speaker B: I agree.
Speaker A: What's the second one?
Speaker B: So the second one is, again, another one that it happened. I read it later in my life. I think I read it first time when I was a graduate student. And that's the Solzhenitsyn's cancer warden. That is amazingly powerful book.
Speaker A: What is it about?
Speaker B: It's about, I mean, essentially based on, Solzhenitsyn was diagnosed with cancer when he was reasonably young, and he made a full recovery. So this is about a person who was sentenced for life in one of these campsite, and he had some cancer, so he was transported back to one of these soviet republics, I think, south asian republics. And the book is about his experience being a prisoner, being a patient in the cancer clinic, in a cancer ward surrounded by people, many of which die. Right. But in a way, the way it reads, I mean, first of all, later on, I read the accounts of the doctors who describe the experiences in the book by the patient as incredibly accurate. I read that there was some doctors saying that every single doctor should read this book to understand what the patient feels. But again, as many of the Solzhenitsyn's books, it has multiple levels of complexity. And obviously, if you look above the cancer and the patient, I mean, the tumor that was growing and then disappeared in his body with some consequences. I mean, this is allegorically the Soviet. And he actually, he agreed when he was, he said that this is what make him think about this, how to combine these experiences, being a part of the soviet regime, also being a part of someone sent to Gulag camp, and also someone who experienced cancer in his life, the Gulag archipelago. And this book, these are the works that actually made him receive a Nobel prize. But to me, I've read other books by Solzhenitsyn. This one, to me, is the most powerful one that.
Speaker A: And by the way, both this one and the previous one you read in Russian.
Speaker B: Yes. Yes. So now there is, the third book is an english book, and it's completely different. So, you know, we're switching the gears completely. So this is the book which. It's not even a book. It's an essay by Jonathan Neumann called the Computer and the Brain. And that was the book he was writing, knowing that he was dying of cancer. So the book was released back. It's a very thin book, right. But the intellectual power in this book, in this essay, is incredible. I mean, you probably know that von Neumann is considered to be one of these biggest thinkers. Right? So his intellectual power was incredible. Right. And you can actually feel this power in this book where the person is writing, knowing that he will die. The book actually got published only after his death back in 1958. He died in 1957. So he tried to put as many ideas that he still hadn't realized. So this book is very difficult to read because every single paragraph is just compact, is filled with these ideas. And the ideas are incredible, even nowadays. So he tried to put the parallels between the brain, computing power, the neural system, and the computers, you know, as they were understood.
Speaker A: Year he was working on this, like approximately 57. 57.
Speaker B: So. So that was right during his, you know, when he was diagnosed with cancer. And he was essentially.
Speaker A: Yeah, he's one of those. There's a few folks people mention, I think Ed Witten is another that. Like everybody, everyone that meets them, they say he's just an intellectual powerhouse.
Speaker B: Yes.
Speaker A: Okay, so who is the honorable mention runner up?
Speaker B: And this is, I mean, the reason I put it sort of in a separate section, because this is a book that I reasonably recently listened to. So it's an audiobook. And this is a book called Lab Girl by Hope Jarron. So Hope Jarran, she is a scientist. She's a geochemist that essentially studies the fossil plants. And so she uses this fossil plant, the chemical analysis, to understand what was the climate back in thousand years, hundreds of thousands of years ago. And so something that incredibly touched me by this book. It was narrated by the author.
Speaker A: Nice.
Speaker B: And it's an incredibly personal story. Incredibly. So certain parts of the book, you could actually hear the author crying. And that, to me, I mean, I never experienced anything like this, you know, reading the book, but it was like, you know, the connection between you and the authorization. And I think this is really a must read, but even better, a must listen to audiobook for anyone who wants to learn about sort of academia, science research in general, because it's a very personal account about her becoming a scientist.
Speaker A: So we're just before New Year's, you know, we talked a lot about some difficult topics of viruses and so on. Do you have some exciting things you're looking forward to in 2021? Some New Year's resolutions maybe silly or fun or something very important and fundamental to the world of science or something completely unimportant?
Speaker B: Well, I'm definitely looking forward towards things becoming normal. Yes. I really miss traveling. Every summer, I go to international summer school. It's called the School for Molecular and theoretical biology. It's held in Europe. It's organized by very good friends of mine. And this is the school for gifted kids from all over the world, and they're incredibly bright. It's like every time I go there, it's like, you know, it's a highlight of the year. And we couldn't make it this August, so we did this school remotely, but it's different. So I am definitely looking forward next August, coming there. I also, I mean, you know, one of the. One of my personal resolutions, I realized that being in house and working from home, I realized that actually, I apparently missed a lot, you know, spending time with my family, believe it or not. So you typically, with all the research and teaching and everything related to the academic life. I mean, you get distracted and so you don't feel that the fact that you are away from your family doesn't affect you because you are naturally distracted by other things. And this time I realized that that's so important, spending your time with the family, with your kids. And so that would be my new year resolution in actually trying to spend as much time as possible, even when.
Speaker A: The world opens up. Yeah, that's a, that's a beautiful message. That's a beautiful reminder. I asked you if there's a russian poem you could read that I could force you to read, and you said, okay, fine, sure. Do you mind? Do you mind reading? And you're like, you said that no paper needed.
Speaker B: Nope. So, yeah, so this poem was written by my namesake, another Dmitri, Dmitri Khemir Feld, and it's a recent poem, and it's called sorceress in Russian, or actually Kaldunya. So that's sort of another sort of connotation of sorceress witch. And I really like it. And it's one of just a handful of poems I actually can recall by heart. I also have a very strong association when I read this poem with Master Margarita, the main female character, Margarita. And also it's about, it's happening about the same time we are talking now. So around new year, around Christmas.
Speaker A: Do you mind reading it in Russian?
Speaker B: I'll give it a try. Paksachilnika pilsina matreskoy ipadlasnetvaye napagosta. No. Aya bispridra sultigals.
Speaker A: That's beautiful. I love how it captures a moment of longing and maybe love even.
Speaker B: Yes, to me, it has a lot of meaning about, you know, this, something that is happening, something that is far away but still very close to you. And yes, it's the winter.
Speaker A: There's something magical about winter, isn't it? What is the. Well, I don't know. I don't know how to translate it, but kiss in winter is interesting. Lips in winter and all that kind of stuff. It's beautifully russian as a way, as a reason. Russian poetry is just, I'm a fan of poetry in both languages, but English doesn't capture some of the magic that Russian seems to. So thank you for doing that. That was awesome. Dmitry, is great to talk to you again. It's contagious how much you love what you do, how much you love life. So I really appreciate you taking the.
Speaker B: Time to talk today, and thank you for having me.
Speaker A: Thanks for listening to this conversation with Dimitri Corkin and thank you to our sponsors Brave browser, Netsuite business management software, magic spoon, low carb cereal and a sleep self cooling mattress. So the choice is browsing, privacy, business success, healthy diet, a comfortable sleep. Choose wisely my friends, and if you wish, click the sponsor links below to get a discount and to support this podcast. Now let me leave you with some words from Jeffrey Eugenides. Biology gives you a brain, life turns it into a mind. Thank you for listening and hope to see you next time.
