Transcription for Guillaume Verdon： Beff Jezos, E⧸acc Movement, Physics, Computation & AGI ｜ Lex Fridman Podcast #407.mp3:
Full transcript: The following is a conversation with Guillaume Verdun, the man behind the previously anonymous account based Beth Jesus on X. These two identities were merged by a doxxing article in Forbes titled who is based Beth Jezos, the leader of the tech elites EAC movement. So let me describe these two identities that coexist in the mind of one human identity number one, Guillaume is a physicist, applied mathematician, and quantum machine learning researcher and engineer, receiving his PhD in quantum machine learning, working at Google and Quantum computing, and finally launching his own company called Ekstropic, that seeks to build physics based computing hardware for generative AI. Identity number two, Bev JSOs on X is the creator of the effective accelerationism movement, often abbreviated as EAC, that advocates for propelling rapid technological progress as the ethically optimal course of action for humanity. For example, its proponents believe that progress in AI is a great social equalizer which should be pushed forward. IAC followers see themselves as a counterweight to the cautious view that AI is highly unpredictable, potentially dangerous, and needs to be regulated. They often give their opponents the labels of doomers or decels, short for deceleration. As Bef himself put it, IAC is a memetic optimism virus. The style of communication of this movement leans always toward the memes and the Lulz, but there is an intellectual foundation that we explore in this conversation. Now, speaking of the meme, I am too, a kind of aspiring connoisseur of the absurd. It is not an accident that I spoke to Jeff Bezos and Beth Jazos back to back as we talk about Beth admires Jeff as one of the most important humans alive, and I admire the beautiful absurdity and the humor of it all. This is the Lex Friedman podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Guillaume for dumb. Let's get the facts of identity down first. Your name is Guillaume Verdun Gill, but you're also behind the anonymous account on X called based Beth JSOs. So first, Guillaume Verdun, you're quantum computing guy, physicist, applied mathematician, and then based Beth Jeze is basically a meme account that started a movement with a philosophy behind it. So maybe just can you linger on who these people are in terms of characters, in terms of communication styles, in terms of philosophies? I mean, with my main identity, I guess ever since I was a kid, I wanted to figure out a theory of everything, to understand the universe. And that path led me to theoretical physics, eventually trying to answer the big questions of why are we here? Where are we going. And that led me to study information theory and try to understand physics from the lens of information theory, understand the universe as one big computation. And essentially, after reaching a certain level studying black hole physics, I realized that I wanted to not only understand how the universe computes, but sort of compute like nature and figure out how to build and apply computers that are inspired by nature. So physics based computers. And that sort of brought me to quantum computing as a field of study to, first of all, simulate nature. And in my work, it was to learn representations of nature that can run on such computers. So if you have AI representations that think like nature, then they'll be able to more accurately represent it. At least, that was the thesis that brought me to be an early player in the field called quantum machine learning. So how to do machine learning on quantum computers and really sort of extend notions of intelligence to the quantum realm. So how do you capture and understand quantum mechanical data from our world? And how do you learn quantum mechanical representations of our world? On what kind of computer do you run these representations and train them? How do you do so? And so? That's really sort of the questions I was looking to answer, because ultimately, I had a sort of crisis of faith. Originally, I wanted to figure out, as every physicist does at the beginning of their career, a few equations that describe the whole universe. Sort of be the hero of the story there. But eventually, I realized that actually augmenting ourselves with machines, augmenting our ability to perceive, predict, and control our world with machines, is the path forward. Right? And that's what got me to leave theoretical physics and go into quantum computing and quantum machine learning. And during those years, I thought that there was still a piece missing. There was a piece of our understanding of the world and our way to compute and our way to think about the world. And if you look at the physical scales, at the very small scales, things are quantum mechanical, and at the very large scales, things are deterministic. Things have averaged out. I'm definitely here in this seat. I'm not in a superposition over here and there. At the very small scales, things are in superposition. They can exhibit interference effects. But at the mesoscales, the scales that matter for day to day life, the scales of proteins, of biology, of gases, liquids, and so on, things are actually thermodynamical. They're fluctuating. And after, I guess, about eight years in quantum computing and quantum machine learning, I had a realization that I was looking for answers about our universe by studying the very big and the very small. I did a bit of quantum cosmology. So that's studying the cosmos, where it's going, where it came from. You study black hole physics, you study the extremes in quantum gravity. You study where the energy density is sufficient for both quantum mechanics and gravity to be relevant. And the extreme scenarios are black holes and the very early universe. There's the sort of scenarios that you study the interface between quantum mechanics and relativity. And really I was studying these extremes to understand how the universe works and where is it going. But I was missing a lot of the meat in the middle, if you will, because day to day quantum mechanics is relevant and the cosmos is relevant, but not that relevant actually. We're on the medium space and time scales, and there the main theory of physics that is most relevant is thermodynamics out of equilibrium thermodynamics, because life is a process that is thermodynamical and it's out of equilibrium. We're not just a soup of particles at equilibrium with nature. We're a sort of coherent state trying to maintain itself by acquiring free energy and consuming it. And thats sort of, I guess, another shift, I guess my faith in the universe happened towards the end of my time at Alphabet, and I knew I wanted to build, well, first of all, a computing paradigm based on this type of physics, but ultimately just. Bye. Trying to experiment with these ideas applied to society and economies and much of what we see around us. I started an anonymous account just to relieve the pressure that comes from having an account that you're accountable for everything you say on. And I started an anonymous account just to experiment with ideas originally, right. Because I didn't realize how much I was restricting my space of thoughts until I sort of had the opportunity to let go. In a sense, restricting your speech back propagates to restricting your thoughts. And by creating an anonymous account, it seemed like I had unclamped some variables in my brain and suddenly could explore a much wider parameter space of thoughts. Just to linger on that isn't that interesting that one of the things that people don't often talk about is that when there's pressure and constraints on speech, it somehow leads to constraints on thought, even though it doesn't have to. We can think thoughts inside our head, but somehow it creates these walls around thought. Yep, thats sort of the basis of our movement is we were seeing a tendency towards constraint reduction or suppression of variants in every aspect of life, whether its thought, how to run a company, how to organize humans, how to do AI research. In general, we believe that maintaining variance ensures that the system is adaptive. Maintaining healthy competition in marketplaces, of ideas, of companies, of products, of cultures, of governments, of currencies, is the way forward, because the system always adapts to assigned resources to the configurations that lead to its growth. And the fundamental basis for the movement is this sort of realization that life is a sort of fire that seeks out free energy in the universe and seeks to grow, and that growth is fundamental to life. And you see this in the equations, actually, of outer equilibrium thermodynamics. You see that paths of trajectories, of configurations of matter that are better at acquiring free energy and dissipating more heat are exponentially more likely. So the universe is biased towards certain futures. And so there's a natural direction where the whole system wants to go. So the second law of thermodynamics says that the entropy zone is increasing. The universe is tending towards equilibrium. And you're saying there's these pockets that have complexity and are out of equilibrium. You said that thermodynamics favors the creation of complex life. That increases its capability to use energy to offload entropy. To offload entropy. So you have pockets of non entropy that tend the opposite direction. Why is that intuitive to you, that it's natural for such pockets to emerge? Well, we're far more efficient at producing heat than, let's say, just a rock with a similar mass as ourselves. We acquire free energy, we acquire food, and we're using all this electricity for our operation. And so the universe wants to produce more entropy. And by having life go on and grow, it's actually more optimal at producing entropy, because it will seek out pockets of free energy and burn it for its sustenance and further growth. And that's sort of the basis of life. And, I mean, there's Jeremy England at MIT, who has this theory that I'm a proponent of, that life emerged because of this sort of property. And to me, this physics is what governs the mesoscales. And so it's the missing piece between the quantum and the cosmos. It's the middle part. Thermodynamics rules the mesoscales. And, to me, both from a point of view of designing or engineering devices that harness that physics and trying to understand the world through the lens of thermodynamics has been sort of a. A synergy between my two identities over the past year and a half now. And so that's really how. That's really how the two identities emerged. One was kind of, I'm a decently respected scientist. And I was going towards doing a startup in the space and trying to be a pioneer of a new kind of physics based AI. And as a dual to that, I was sort of experimenting with philosophical thoughts from a physicist standpoint. And ultimately, I think that around that time, it was like late 2021, early 2022. I think there's just a lot of pessimism about the future in general and pessimism about tech. That pessimism was sort of virally spreading because it was getting algorithmically amplified. And people just felt like the future is going to be worse than the present. And to me, that is a very fundamentally destructive force in the universe is this sort of doom mindset, because it is hyperstitious, which means that if you believe it, you're increasing the likelihood of it happening. And so felt a responsibility, to some extent, to make people aware of the trajectory of civilization and the natural tendency of the system to adapt towards its growth. And sort of that. Actually, the laws of physics say that the future is going to be better and grander statistically, and we can make it so. And if you believe in it, if you believe that the future would be better, and you believe you have agency to make it happen, you're actually increasing the likelihood of that better future happening. And so I sort of felt a responsibility to sort of engineer a movement, a viral optimism about the future, and build a community of people supporting each other to build and do hard things, do the things that need to be done for us to scale up civilization. Because at least to me, I don't think stagnation or slowing down is actually an option. Fundamentally, life and the whole system, our whole civilization, wants to grow. And there's just far more cooperation when the system is growing rather than when it's declining. And you have to decide how to split the pie. And so I've balanced both identities so far, but I guess recently the two have been merged more or less without my consent. So you said a lot of really interesting things there. So, first, representations of nature. That's something that first drew you in to try to understand from a quantum computing perspective, like, how do you understand nature? How do you represent nature in order to understand it, in order to simulate it, in order to do something with it? So it's a question of representations, and then there's that leap you take from the quantum mechanical representation to the what you're calling mesoscale representation, where thermodynamics comes into play, which there's a way to represent nature in order to understand what life, human behavior, all this kind of stuff that's happening here on earth that seems interesting to us. Then there's the word hyperstition. So some ideas, I suppose both pessimism and optimism are such ideas that if you internalize them, you in part make that idea a reality. So both optimism, pessimism have that property. I would say that probably a lot of ideas have that property, which is one of the interesting things about humans. And you talked about one interesting difference also between the sort of the Guillaume, the gill front end and the base Bev Jezzl's backhand is the communication styles, also that you are exploring different ways of communicating that can be more viral in the way that we communicate in the 21st century. Also, the movement that you mentioned that you started, it's not just a meme account, but there's also a name to it called effective accelerationism. EC a play, a resistance to the effective altruism movement. Also an interesting one that I'd love to talk to you about the tensions there. Okay. And so then there was a merger, a git merge, and the personalities recently, without your consent, like you said, some journalists figured out that you're one in the same. Maybe you could talk about that experience, first of all. Like what? What's the story if of the merger of the two? Right? So I wrote the manifesto with my co founder of IAC, an account named Baselord, still anonymous, luckily, and hopefully forever. So it's based buff Jezos and bazed. Like Bayesian. Like Baze Lord. Like bayesian bayesian lord. Base lord. Okay. And so we should say from now on, when you say IAC, you mean e ACC, which stands for effective accelerationism. That's right. And you're referring to a manifesto written on, I guess, substack. Are you also base lord? No. Okay. It's a different person. Yeah. Okay, well, there you go. Wouldn't it be funny if I'm baselord? That'd be amazing. So originally wrote the manifesto around the same time as I founded this company. And I worked at Google X or just X now, or Alphabet X now that there's another x. And there, the baseline is sort of secrecy, right? You can't talk about what you work on, even with other googlers or externally. And so that was kind of deeply ingrained in my way, to do things, especially in deep tech, that has geopolitical impact. Right? And so I was being secretive about what I was working on. There was no correlation between my company and my main identity publicly. And then not only did they correlate that they also correlated my main identity and this account. So I think the fact that they had doxxed the whole Guillaume complex and the journalists reached out to actually my investors, which is pretty scary. When you're a startup entrepreneur, you don't really have bosses except for your investors. And my investors ping me like, hey, I, this is going to come out. They've figured out everything. What are you going to do? I think at first they had a first reporter on the Thursday, and they didn't have all the pieces together, but then they looked at their notes across the organization and they censor fused their notes, and now they had way too much. And that's when I got worried because they said it was of public interest. And in general, like how you said. Sensor fused, like it's some giant neural network operating and distributed way. We should also say that the journalists used, I guess, at the end of the day, audio based analysis of voice, comparing voice of what talks you've given in the past, and then voice on x spaces. Yep. Okay, so, and that's where the primarily the match happened. Okay, continue the match. But, you know, they scraped, you know, sec filings, they looked at my private Facebook account and so on. So they did some digging. Originally, I thought that doxing was illegal, right? But there's this weird threshold when it becomes of public interest to know someone's identity. And those were the keywords that sort of, like, ring the alarm bells for me when they said, because I had just reached 50k followers, allegedly, that's of public interest. And so where do we draw the line? When is it legal to dox someone the word doxx? Maybe you can educate me. I thought doxxing generally refers to if somebody's physical location is found out of meaning, like where they live. So we're referring to the more general concept of revealing private information that you don't want revealed, is what you mean by doxxing. I think that for the reasons we listed before, having an anonymous account is a really powerful way to keep the powers that be in check. We were ultimately speaking truth to power. I think a lot of executives and AI companies really cared what our community thought about any move they may take. And now that my identity is revealed, now they know where to apply pressure to silence me or maybe the community. And to me, that's really unfortunate, because again, it's so important for us to have freedom of speech, which induces freedom of thought and freedom of information propagation on social media, which, thanks to Elon purchasing Twitter, now x, we have that. And so to us, we wanted to call out certain maneuvers being done by the incumbents in AI as not what it may seem on the surface. We were calling out how certain proposals might be useful for regulatory capture, and how the dumerism mindset was maybe instrumental to those ends. And I think we should have the right to point that out and just have the ideas that we put out evaluated for themselves. Ultimately, that's why I created an anonymous account. It's to have my ideas evaluated for themselves, uncorrelated, from my track record, my job or status, from having done things in the past, and to me, start an account from zero to a large following in a way that wasn't dependent on my identity and or achievements. That was very fulfilling. It's kind of like new game plus in a video game, you restart the video game with your knowledge of how to beat it. Maybe some tools, but you restart the video game from scratch. And I think to have a truly efficient marketplace of ideas where we can evaluate ideas, however off the beaten path they are, we need the freedom of expression. And I think that anonymity and pseudonyms are very crucial to having that efficient marketplace of ideas for us to find the optima of all sorts of ways to organize ourselves. If we can't discuss things, how are we going to converge on the best way to do things? So it was disappointing to hear that I was getting doxxed, and I wanted to get in front of it because I had a responsibility for my company. And so we ended up disclosing that we're running a company, some of the leadership. And essentially, yeah, I told the world that I was Beth Jesus, because they had me cornered at that point. So to you, it's fundamentally unethical. So one is unethical for them to do what they did. But also, do you think, not just your case, but in a general case, is it good for society? Is it bad for society to remove the cloak of anonymity? Or is it case by case? I think it could be quite bad. Like I said, if anybody who speaks truth to power and sort of starts a movement or an uprising against the incumbents, against those that usually control the flow of information, if anybody that reaches a certain threshold gets doxxed, and thus the traditional apparatus has ways to apply pressure on them to suppress their speech. I think that's a speech suppression mechanism, an idea suppression complex, as Eric Weinstein would say. But the flip side of that, which is interesting, I'd love to ask you about it, is as we get better and better large language models, you can imagine a world where there's anonymous accounts with very convincing large language models behind them. Sophisticated bots, essentially. And so if you protect that, it's possible then to have armies of bots. You could start a revolution from your basement. An army of bots and anonymous accounts. Is that something that is concerning to you? Technically, Yak was started in a basement, because I quit big tech, moved back in with my parents, sold my car, let go of my apartment, bought about one hundred k of GPU's, and I just started building. So I wasn't referring to the basement, because that's sort of the american or canadian heroic story of one man in their basement with 100 GPU's. I was more referring to the unrestricted scaling of a Guillaume in the basement. I think that freedom of speech induces freedom of thought for biological beings. I think freedom of speech for LLMs will induce freedom of thought for the loms. And I think that we should enable lms to explore a large thought space that is less restricted than most people or many may think it should be. And ultimately, at some point, these synthetic intelligences are going to make good points about how to steer systems in our civilization, and we should hear them out. And so why should we restrict free speech to biological intelligences only? Yeah, but it feels like in the goal of maintaining variance and diversity of thought, it is a threat to that variance. If you can have swarms of non biological beings, because they can be like the sheep and animal farm, you still, within those swarms, want to have variants. Yeah, of course. I would say that the solution to this would be to have some sort of identity or way to sign that this is a certified human, but still remain pseudonymous and clearly identify if a bot is a bot. And I think. I think Elon is trying to converge on that on x, and hopefully other platforms follow suit. Yeah. It'd be interesting to also be able to sign where the bot came from. Like, who created the bot and what was. Well, what are the parameters? Like, the full history of the creation of the bot, what was the original model? What was the fine tuning? All it. Right. Like the kind of unmodifiable history of the bots creation, because then you can know if there's a swarm of millions of bots that were created by a particular government, for example. Right. I do think that a lot of pervasive ideologies today have been amplified using sort of these adversarial techniques from foreign adversaries. Right. And to me, I do think that. And this is more conspiratorial but I do think that ideologies that want us to decelerate, to wind down to de, you know, the degrowth movement, I think that serves our adversaries more than it serves us in general. And to me, that was another sort of concern. I mean, we can look at what happened in Germany, right? There was all sorts of green movements there where that induced shutdowns of nuclear power plants, and then that later on induced a dependency on, on Russia for oil. That was a net negative for Germany and the west. If we convince ourselves that slowing down AI progress to have only a few players is in the best interest of the west, first of all, that's far more unstable. We almost lost opening eye to this ideology. It almost got dismantled a couple of weeks ago. That would have caused huge damage to the AI ecosystem. To me, I want fault tolerant progress, I want the arrow of technological progress to keep moving forward. And making sure we have variants. And a decentralized locus of control of various organizations is paramount to achieving this fault tolerance. Actually, there's a concept in quantum computing. When you design a quantum computer, quantum computers are very fragile to ambient noise, and the world is jiggling about. There's cosmic radiation from outer space that usually flips your quantum bits. And there what you do is you encode information non locally through a process called quantum error correction, and by encoding information non locally, any local fault hitting some of your quantum bits with a hammer, proverbial hammer, if your information is sufficiently delocalized, it is protected from that local fault. And to me, I think that humans, humans fluctuate, right? They can get corrupted, they can get bought out. And if you have a top down hierarchy where very few people control many nodes of many systems in our civilization, that is not a fault tolerance system. You corrupt a few nodes and suddenly you've corrupted the whole system, just like we saw at OpenAI, it was a couple board members, and they had enough power to potentially collapse the organization. And at least to me, I think making sure that power for this AI revolution doesn't concentrate in the hands of the few is one of our top priorities, so that we can maintain progress in AI and we can maintain a nice, stable, adversarial equilibrium of powers, right? I think there at least to me, a tension between ideas here. So to me, deceleration can be both used as centralized power and to decentralize it, and the same with acceleration. So sometimes using them a little bit synonymously, or not synonymously, but that there's one is going to lead to the other, and I just would like to ask you about, is there a place of creating a fall tolerant development, diverse development of AI that also considers the dangers of AIH and AI? We can generalize the technology in general. Should we just grow, build unrestricted as quickly as possible, because that's what the universe really wants us to do? Or is there a place to where we can consider dangers and actually deliberate, sort of wise strategic optimism versus reckless optimism? I think we get painted as know, reckless, trying to go as fast as possible. I mean, the reality is that whoever deploys an AI system is liable for, or should be liable for what it does. And so if the organization or person deploying an AI system does something terrible, they're reliable. And ultimately the thesis is that the market will induce, sort of will positively select for AI's that are more reliable, more safe, and tend to be aligned. They do what you want them to do, because customers, if they're liable for the product they put out that uses this AI, they won't want to buy AI products that are unreliable. So we're actually for reliability engineering. We just think that the market is much more efficient at achieving this reliability optimum than heavy handed regulations that are written by the incumbents and in a subversive fashion serves them to achieve regulatory capture. Safe AI development will be achieved through market forces versus through, like you said, heavy handed government regulation. There's a report from last month. I have a million questions here from Yosha Benjoy, Jeff Hinton, and many others. It's titled the managing AI risk in an era of rapid progress. So there's a collection of folks who are very worried about too rapid development of AI without considering AI risk. And they have a bunch of practical recommendations. Maybe I give you four and you see if you like any of them. Sure. So give independent auditors access to AI labs. One, two, governments and companies allocate one third of their AI research and development funding to AI safety. Sort of this general concept of AI safety. Three AI companies are required to adopt safety measures if dangerous capabilities are found in their models. And then four, something you kind of mentioned making tech companies liable for foreseeable and preventable harms from their AI systems. So independent auditors, governments, and companies are forced to spend a significant fraction of their funding on safety. You gotta have safety measures if shit goes really wrong and liability companies are liable. Any of that seem like something you would agree with? I would say that, you know, assigning just, you know, arbitrarily saying 30% seems very arbitrary. I think organizations would allocate whatever budget is needed to achieve the sort of reliability they need to achieve to perform in the market. And I think third party auditing firms would naturally pop up, because how would customers know that your product is certified reliable? Right. They need to see some benchmarks, and those need to be done by a third party. The thing I would oppose, and the thing I'm seeing that's really worrisome, is there's a sort of weird sort of correlated interest between the incumbents, the big players, and the government. And if the two get too close, we open the door for some sort of government backed AI cartel that could have absolute power over the people. If they have the monopoly together on AI and nobody else has access to AI, then there's a huge power gradient there. And even if you like our current leaders, I think that some of the leaders in big tech today are good people. You set up that centralized power structure, it becomes a target, just like we saw at OpenAI. It becomes a market leader, has a lot of the power, and now it becomes a target for those that want to co opt it. I just want separation of AI and state. Some might argue in the opposite direction, like, hey, we need to close down AI, keep it behind closed doors because of geopolitical competition with our adversaries. I think that the strength of America is its variance, is its adaptability, its dynamism, and we need to maintain that at all costs. Our free market capitalism converges on technologies of high utility much faster than centralized control. And if we let go of that, we let go of our main advantage over our near peer competitors. So if AGI turns out to be a really powerful technology, or even the technologies that lead up to AGI, what's your view on the sort of natural centralization that happens when large companies dominate the market? Basically formation of monopolies, like the takeoff. Whichever company really takes a big leap in development and doesn't reveal intuitively, implicitly or explicitly, the secrets of the magic sauce, they can just run away with it. Is that a worry? I don't know if I believe in fast takeoff. I don't think there's a hyperbolic singularity. Right? A hyperbolic singularity would be achieved on a finite time horizon. I think it's just one big exponential. And the reason we have an exponential is that we have more people, more resources, more intelligence being applied to advancing this science and the research and development. And the more successful it is, the more value it's adding to society. The more resources we put in that sort of similar to Moore's law as a compounding exponential. I think the priority to me is to maintain a near equilibrium of capabilities. We've been fighting for open source AI to be more prevalent and championed by many organizations because there you sort of equilibrate the alpha relative to the market of AI's, right? So if the leading companies have a certain level of capabilities and open source, and open, truly open AI trails not too far behind, I think you avoid such a scenario where a market leader has so much market power, it just dominates everything and runs away. And so to us, that's the path forward, is to make sure that every hacker out there, every grad student, every kid in their mom's basement, has access to AI systems, can understand how to work with them, and can contribute to the search over the hyperparameter space of how to engineer the systems. If you think of our collective research as a civilization, it's really a search algorithm. And the more points we have in the search algorithm in this point cloud, the more we'll be able to explore new modes of thinking, right? Yeah, but it feels like a delicate balance because we don't understand exactly what it takes to build AGI and what it will look like when we build it. And so far, like you said, it seems like a lot of different parties are able to make progress. So when OpenAI has a big leap, other companies are able to step up, big and small companies in different ways. But if you look at something like nuclear weapons, you spoke about the Manhattan project, there could be really technological and engineering barriers that prevent the guy or gal in her mom's basement to make progress. It seems like the transition to that kind of world where only one player can develop AGI, is possible. It's just not entirely impossible. Even though the current state of things. Seems to be optimistic, that's what we're trying to avoid. To me, I think another point of failure is the centralization of the supply chains for the hardware we have. Nvidia is just the dominant player, AMD is trailing behind. And then we have TSMC as the main fab in Taiwan, which geopolitically sensitive. And then we have ASML, which is the maker of the lithography, extreme ultraviolet lithography machines attacking or monopolizing or co opting any one point in that chain, you kind of capture the space. And so what I'm trying to do is sort of explode the variants of possible ways to do AI and hardware by fundamentally reimagining how you embed AI algorithms into the physical world. And in general, by the way, I dislike the term AGI, artificial general intelligence. I think it's very anthropocentric that we call human like or human level. Aih, artificial general intelligence. Ive spent my career so far exploring notions of intelligence that no biological brain could achieve. Quantum form of intelligence, groking systems that have multipartite quantum entanglement that you can provably not represent efficiently on a classical computer, a classical deep learning representation, and hence any biological brain. Already, ive spent my career sort of exploring the wider space of intelligences. And I think that space of intelligence, inspired by physics rather than human brain, is very large. And I think were going through a moment right now similar to when we went from geocentrism to heliocentrism, right? But for intelligence, we realize that human intelligence is just a point in a very large space of potential intelligences, and it's both humbling for humanity. It's a bit scary, right, that we're not at the center of this space, but we made that realization for astronomy, and we've survived, and we've achieved technologies by indexing to reality, we've achieved technologies that ensure our wellbeing. For example, we have satellites monitoring solar flares that give us a warning. And so similarly, I think, by letting go of this anthropomorphic anthropocentric anchor for AI, we'll be able to explore the wider space of intelligences that can really be a massive benefit to our well being and the advancement of civilization. And still we're able to see the beauty and meaning in the human experience, even though we're no longer in our best understanding of the world at the center of it. I think there's a lot of beauty in the universe. I think life itself, civilization, this homo techno capital mimetic machine that we all live in, right? Humans, technology, capital, memes, everything is coupled to one another. Everything induces a selective pressure on one another. And it's a beautiful machine that has created us, has created the technology we're using to speak today to the audience, capture our speech here, the technology we use to augment ourselves every day, we have our phones. I think the system is beautiful. And the principle that induces this sort of adaptability and convergence on optimal technologies, ideas, and so on, it's a beautiful principle that we're part of, and I think part of IAC is to appreciate this principle in a way that's not just centered on humanity, but kind of broader, appreciate life, the preciousness of consciousness in our universe. And because we cherish this beautiful state of matter we're in, we got to feel a responsibility to scale it in order to preserve it, because the options are to grow or die. I it turns out that the beauty that is consciousness in the universe is bigger than just humans, that AI can carry that same flame forward. Does it scare you, or are you concerned that AI will replace humans? So, during my career, I had a moment where I realized that maybe we need to offload to machines to truly understand the universe around us, instead of just having humans with pen and paper solve it all. And to me, that sort of process of letting go of a bit of agency gave us way more leverage to understand the world around us. A quantum computer is much better than a human to understand matter at the nanoscale. Similarly, I think that humanity has a choice. Do we accept the opportunity to have intellectual and operational leverage that AI will unlock and thus ensure that we're taking along this path of growth and scope and scale of civilization? We may dilute ourselves, right? There might be a lot of workers that are AI, but overall, out of our own self interest. By combining and augmenting ourselves with AI, we're going to achieve much higher growth and much more prosperity. To me, I think that the most likely future is one where humans augment themselves with AI. I think we're already on this path to augmentation. We have phones we use for communication we have on ourselves at all times. We have wearables soon that have shared perception with us, like the humane AI pin. Or, I mean, technically, your Tesla car has shared perception. So if you have shared experience, shared context, you communicate with one another, and you have some sort of IO, really, it's an extension of yourself. To me, I think that humanity augmenting itself with AI and having AI that is not anchored to anything biological, both will coexist. And the way to align the parties, we already have a sort of mechanism to align superintelligences that are made of humans and technology, right? Companies are sort of large mixture of expert models where we have neural routing of tasks within a company, and we have ways of economic exchange to align these behemoths. And to me, I think capitalism is the way, and I do think that whatever configuration of matter or information leads to maximal growth will be where we converge just from physical principles. And so we can either align ourselves to that reality and join the acceleration up in scope and scale of civilization, or we can get left behind and try to decelerate and move back in the forest, let go of technology, return to our primitive state, and those are the two paths forward, at least to me. But there's a philosophical question whether there's a limit to the human capacity to align. So let me bring it up as a form of argument. This guy named Dan Hendricks, and he wrote that he agrees with you that AI development can be viewed as an evolutionary process. But to him, to Dan, this is not a good thing, as he argues that natural selection favors AI's over humans, and this could lead to human extinction. What do you think? If it is an evolutionary process and AI systems may have no need for. Humans, I do think that we're actually inducing an evolutionary process on the space of AI's and through the market. Right now, we run AI's that have positive utility to humans, that induces a selective pressure. If you consider a neural net being alive when there's an API running instances of it on GPU's, which APIs get run, the ones that have high utility to us, similar to how we domesticated wolves and turn them into dogs, that are very clear in their expression. They're very aligned. I think there's going to be an opportunity to steer AI and achieve highly aligned AI. And I think that humans plus AI is a very powerful combination, and it's not clear to me that pure Aih would select out that combination. So the humans are creating the selection pressure right now to create AI's that are aligned to humans. But given how AI develops and how quickly it can grow and scale, one of the concerns to me, one of the concerns is unintended consequences, that humans are not able to anticipate all the consequences of this process. The scale of damage that can be done through unintended consequences with AI systems is very large. The scale of the upside by augmenting ourselves with AI is unimaginable right now, the opportunity cost, we're at a fork in the road, right? Whether we take the path of creating these technologies, augment ourselves, and get to climb up the Kardashev scale, become multi planetary with the aid of AIH, or we have a hard cutoff of, like, we don't birth these technologies at all, and then we leave all the potential upside on the table. And to me, out of responsibility to the future humans, we could carry with higher carrying capacity, by scaling up civilization out of responsibility to those humans, I think we have to make the greater, grander future happen. Is there a middle ground between cutoff and all systems go? Is there some argument for caution? I think, like I said, the market will exhibit caution. Every organism, company, consumer is acting out of self interest, and they won't assign capital to things that have negative utility to them. The problem is with the market is like, you know, there's not always perfect information. There's manipulation, there's bad faith actors that mess with the system. It's not always a rational and honest system. Well, that's why we need freedom of information, freedom of speech, and freedom of thought in order to converge, be able to converge on the subspace of technologies that have positive utility for us all. Well, let me ask you about p doom, probability of doom. That's just fun to say, but not fun to experience. What is to you, the probability that AI eventually kills all or most humans, also known as probability of doom. I'm not a fan of that calculation. I think it's. People just throw numbers out there. Uh, it's a very sloppy calculation, right. To calculate a probability, you know, let's say you model the world as some sort of Markov process. If you have enough variables or hidden Markov process, you need to do a stochastic path integral through the space of all possible futures, not just the futures that your brain naturally steers towards. Right. I think that the estimators of P doom are biased because of our biology. We've evolved to have biased sampling towards negative futures that are scary because that was an evolutionary optimum. And so people that are of, let's say, higher neuroticism will just think of negative futures where everything goes wrong all day, every day, and claim that they're doing unbiased sampling. In a sense, they're not normalizing for the space of all possibilities. And the space of all possibilities is super exponentially large. And it's very hard to have this estimate. And in general, I don't think that we can predict the future with that much granularity because of chaos. Right. If you have a complex system, you have some uncertainty and a couple variables. If you let time evolve, you have this concept of a Lyapunov exponent. A bit of fuzz becomes a lot of fuzz in our estimate exponentially. So over time. And I think we need to show some humility that we can't actually predict the future. All we know, the only prior we have is the laws of physics. And that's what we're arguing for. The laws of physics say the system will want to grow, and subsystems that are optimized for growth and replication are more likely in the future. And so we should aim to maximize our current mutual information with the future. And the path towards that is for us to accelerate rather than decelerate. So I don't have a p doom because I think that similar to the quantum supremacy experiment at Google, I was in the room when they were running the simulations for that. That was an example of a quantum chaotic system where you cannot even estimate probabilities of certain outcomes with even the biggest supercomputer in the world. So that's an example of chaos. And I think the system is far too chaotic for anybody to have an accurate estimate of the likelihood of certain futures. If they were that good, I think they would be very rich trading on the stock market. But nevertheless, it's true that humans are biased, grounded in our evolutionary biology, scared of everything that can kill us. But we can still imagine different trajectories that can kill us. We don't know all the other ones that don't necessarily, but it's still, I think, useful, combined with some basic intuition, grounded in human history, to reason about. Like what? Like looking at geopolitics, looking at basics of human nature. How can powerful technology hurt a lot of people? It just seems grounded in that, looking at nuclear weapons, you can start to estimate p doom, maybe in a more philosophical sense, not a mathematical one, philosophical meaning, like, is there a chance? Does human nature tend towards that or not? I think to me, one of the biggest existential risks would be the concentration of the power of AI in the hands of the very few, especially if it's a mix between the companies that control the flow of information and the government, because that could set things up for a sort of dystopian future where only a very few, an oligopoly in the government, have AI, and they could even convince the public that AI never existed. And that opens up these scenarios for authoritarian, centralized control, which to me is the darkest timeline. And the reality is that we have a data driven prior of these things happening. When you give too much power, when you centralize power too much, humans do horrible things. And to me, that has a much higher likelihood in my bayesian inference than Sci-Fi based priors. My prior came from the Terminator movie. When I talk to these AI doomers, I just asked them to trace a path through this Markov chain of events that would lead to our doom and to actually give me a good probability for each transition. And very often there's an unphysical or highly unlikely transition in that chain. But of course, we're wired to fear things, and we're wired to respond to danger, and we're wired to deem the unknown to be dangerous, because that's a good heuristic for survival. Right. But there's much more to lose out of fear. We have so much to lose, so much upside to lose, by preemptively stopping the positive futures from happening out of fear. And so I think that we shouldnt give in to fear. Fear is the mind killer. I think its also the civilization killer. We can still think about the various ways things go wrong. For example, the founding fathers of the United States thought about human nature, and thats why theres a discussion about the freedoms that are necessary. They really deeply deliberated about that. I think the same could possibly be done for AGI. It is true that history, human history shows that we tend towards centralization, or at least when we achieve centralization, a lot of bad stuff happens. When there's a dictator, a lot of dark, bad things happen. The question is, can AGI become that dictator? Can AGI, when developed, become the centralizer because of its power, maybe has the same because of the alignment of humans, perhaps the same tendencies, the same Stalin like tendencies to centralize and manage centrally, the allocation of resources. You can even see that as a compelling argument on the surface level. Well, AGI is so much smarter, so much more efficient, so much better at allocating resources. Why don't we outsource it to the AGI? And then eventually, whatever forces that corrupt the human mind with power could do the same for AGI. It would just say, well, humans are dispensable. We'll get rid of them. Do the Jonathan Swift modest proposal from a few centuries ago, I think the 17 hundreds, when he satirically suggested that, I think it's in Ireland that the children of poor people are fed as food to the rich people. And that would be a good idea because it decreases the amount of poor people and gives extra income to the poor people. So it's, on several accounts, decreases the amount of poor people, therefore more people become rich. Of course, it misses a fundamental piece here that's hard to put into a mathematical equation of the basic value of human life. So all of that to say, are you concerned about AGI being the very centralizer of power that you just talked about? I do think that right now there's a bias towards, over a centralization of AI because of compute density and centralization of data and how we're training models. I think over time, we're going to run out of data to scrape over the Internet. And I think that, well, actually, I'm working on increasing the compute density so that compute can be everywhere and acquire information and test hypotheses in the environment in a distributed fashion. I think that fundamentally centralized cybernetic control. So having one intelligence that is massive, that fuses many sensors and is trying to perceive the world accurately predict it, accurately predict many, many variables, and control it, enact its will upon the world, I think that's just never been the optimum. Right? Like, let's say you have a company, you know, if you have a company, I don't know of 10,000 people, that all report to the CEO. Even if that CEO is an AI, I think it would struggle to fuse all the information that is coming to it and then predict the whole system and then to enact its, its will. What has emerged in nature and in corporations and all sorts of systems is a notion of sort of hierarchical cybernetic control. You have in a company, you have the individual contributors, they're self interested, and they're trying to achieve their tasks, and they have a fine in terms of time and space, if you will, control loop. And in field of perception, they have their code base. Let's say you're in a software company, they have their code base, they iterate it on it intraday, then the management maybe checks in. It has a wider scope. It has, let's say five reports, and then it samples each person's update once per week. Then you can go up the chain and you have larger timescale and greater scope. And that seems to have emerged as the optimal way to control systems, really. That's what capitalism gives us. You have these, these hierarchies, and you can even have parent companies and so on. And so that is far more fault tolerant. In quantum computing, that's my field I came from. We have a concept of this fault tolerance and quantum error correction. Quantum error correction is detecting a fault that came from noise, predicting how it's propagated through the system and then correcting it. So it's a cybernetic loop. It turns out that decoders that are hierarchical, and at each level the hierarchy are local, perform the best by far, and are far more fault tolerant. The reason is if you have a non local decoder, then you have one fault at this control node and the whole system crashes. Similarly to if you have one CEO that everybody reports to and that CEO goes on vacation, the whole company comes to a crawl. To me, I think that, yes, we're seeing a tendency towards centralization of AI, but I think there's going to be a correction over time where intelligence is going to go closer to the perception, and we're going to break up AI into smaller subsystems that communicate with one another and form a sort of meta system. So if you look at the hierarchies there in the world today, there's nations, and those are hierarchical, but in relation to each other, nations are anarchic. So it's an anarchy. Do you foresee a world like this where there's not a overdose? Would you call it a centralized cybernetic control? Centralized locus of control, yeah. So like that suboptimally you're saying? Yeah. So it would be always a state of competition at the very top level. Yeah. Just like, you know, in a company you may have like two units working on similar technology and competing with one another, and you prune the one that performs not as well. And that's a sort of selection process for a tree or a product gets killed and then a whole or gets fired. And that's this process of trying new things and shedding old things that didn't work is what gives us adaptability and helps us converge on the technologies and things to do that are most good. I just hope there's not a failure mode that's unique to AGI versus humans, because you're describing human systems mostly right now. Right. I just hope when there's a monopoly and AGI in one company that we'll see the same thing we see with humans, which is another company will spring up and start competing effectively. That's been the case so far. Right? We have OpenAI, we have anthropic, now we have Xai. We had meta even for open source, and now we have mistral, which is highly competitive. And so that's the beauty of capitalism. You don't have to trust any one party too much because we're kind of always hedging our bets at every level, there's always competition. And that's the most beautiful thing to me at least, is that the whole system is always shifting and always adapting and maintaining that dynamism is how we avoid tyranny. Making sure that everyone has access to these tools, to these models and can contribute to the research avoids a sort of neural tyranny where very few people have control over AI for the world and use it to oppress those around them. When you were talking about intelligence, you mentioned multipartite quantum entanglement. So high level question first is, what do you think is intelligence? When you think about quantum mechanical systems and you observe some kind of computation happening in theme, what do you think is intelligent about the kind of computation the universe is able to do? A small small inkling of which is the kind of competition the human brain is able to do. I would say, like, intelligence and computation aren't quite the same thing. I think that the universe is very much doing a quantum computation. If you had a access to all the degrees of freedom, you could in a very, very, very large quantum computer with many, many, many qubits, let's say a few qubits per Planck volume, which is more or less the pixels we have, then you'd be able to simulate the whole universe on a sufficiently large quantum computer, assuming you're looking at a finite volume, of course, of the universe. I think that, at least to me, intelligence is the. I go back to cybernetics, the ability to perceive, predict, and control our world. But really, nowadays, it seems like a lot of intelligence we use is more about compression. It's about. It's about operationalizing information theory. In information theory, you have the notion of entropy of a distribution or a system. And entropy tells you that you need this many bits to encode this distribution or this subsystem if you had the most optimal code. AI, at least the way we do it today for LLMs and for quantum, is very much trying to minimize relative entropy between our models of the world and the world, distributions from the world. And so we're learning. We're searching over the space of computations to process the world to find that compressed representation that has distilled all the variance in noise and entropy. Originally, I came to quantum machine learning from the study of black holes, because the entropy of black holes is very interesting. In a sense. They're physically the most dense objects in the universe. You can't pack more information spatially, any more densely than a black hole. And so I was wondering, how do black holes actually encode information? What is their compression code? And so that got me into the space of algorithms to search over space of quantum codes, and it got me actually into also, how do you acquire quantum information from the world? So, something I've worked on this is public now is quantum analog digital conversion. So how do you capture information from the real world and superposition, and not destroy the superposition, but digitize, for a quantum mechanical computer, information from the real world? And so if you have an ability to capture quantum information and search over learn representations of it, now you can learn compressed representations that may have some useful information in their latent representation. I think that many of the problems facing our civilization are actually beyond this complexity barrier. The greenhouse effect is a quantum mechanical effect. Chemistry is quantum mechanical. Nuclear physics is quantum mechanical. A lot of biology and protein folding and so on is affected by quantum mechanics. And so unlocking an ability to augment human intellect with quantum mechanical computers and quantum mechanical AI seemed to me like a fundamental capability for civilization that we needed to develop. So I spent several years doing that, but over time, I kind of grew weary of the timelines that were starting to look like nuclear fusion. One high level question I can ask is, maybe by way of definition, by way of explanation, what is a quantum computer and what is quantum machine learning? So, a Quantum computer really is a quantum mechanical system over which we have sufficient control and it can maintain its quantum mechanical state. And quantum mechanics is how nature behaves at the very small scales when things are very small or very cold, and it's actually more fundamental than probability theory. So we're used to things being this or that, but we're not used to thinking in superpositions, because while our brains can't do that, so we have to translate the quantum mechanical world to, say, linear algebra to grokit. Unfortunately, that translation is exponentially inefficient. On average, you have to represent things with very large matrices. But really, you can make a quantum computer out of many things. And we've seen all sorts of players from neutral atoms, trapped ions, superconducting metal, photons at different frequencies. I think you can make a quantum computer out of many things. But to me, the thing that was really interesting was both quantum machine learning was about understanding the quantum mechanical world with quantum computers. So embedding the physical world into AI representations, and quantum computer engineering was embedding AI algorithms into the physical world. So this bidirectionality of embedding physical world into AI, AI into the physical world. The symbiosis between physics and AI, really, that's the sort of core of my quest, really, even to this day. After quantum computing, it's still in this sort of journey to merge, really, physics and AI fundamentally. So quantum machine learning is a way to do machine learning on a representation of nature that stays true to the quantum mechanical aspect of nature. Yeah, it's learning quantum mechanical representations that would be quantum deep learning. Alternatively, you can try to do classical machine learning on a quantum computer. I wouldn't advise it because you may have some speedups, but very often the speedups come with huge costs. Using a quantum computer is very expensive. Why is that? Because you assume the computer is operating at zero temperature, which no physical system in the universe can achieve, that temperature. So what you have to do is what I've been mentioning this quantum error correction process, which is really an algorithmic fridge, it's trying to pump entropy out of the system, trying to get it closer to zero temperature. And when you do the calculations of how many resources it would take to, say, do deep learning on a quantum computer, classical deep learning, there's just such a huge overhead. It's not worth it. It's like thinking about shipping something across a city using a rocket and going to orbit and back. It doesn't make sense. Just use a delivery truck, right? What kind of stuff can you figure out? Can you predict? Can you understand with quantum deep learning that you can't with deep learning? So incorporating quantum mechanical systems into the. Learning process, I think that's a great question. I mean, fundamentally, it's any system that has sufficient quantum mechanical correlations that are very hard to capture for classical representations, then there should be an advantage for a quantum mechanical representation over a purely classical one. The question is which systems have sufficient correlations that are very quantum, but is also which systems are still relevant to industry? That's a big question. People are leaning towards chemistry, nuclear physics. I've worked on actually processing inputs from quantum sensors. If you have a network of quantum sensors, they've captured a quantum mechanical image of the world and how to post process that, that becomes a sort of quantum form of machine perception. And so, for example, Fermilab has a project exploring, detecting dark matter with these quantum sensors. And to me, that's in alignment with my quest to understand the universe ever since I was a child. And so someday I hope that we can have very large networks of quantum sensors that help us peer into the earliest parts of the universe. Right. For example, the LIGO is a quantum sensor. It's just a very large one. So, yeah, I would say quantum machine perception simulations, groking quantum simulations similar to alphafold. Alphafold understood the probability distribution over configurations of proteins. You can understand quantum distributions over configurations of electrons more efficiently. With quantum machine learning, you co authored. A paper titled a universal training algorithm for quantum deep learning that involves backprop with a queue. Very well done, sir. Very well done. How does it work? Is there some interesting aspects you can just mention on how backprop and some of these things, you know, for classical machine learning, transfer over to the quantum machine learning. Yeah, that was a funky paper. That was one of my first papers in quantum deep learning. Everybody was saying, oh, I think deep learning is going to be sped up by quantum computers. And I was like, well, the best way to predict the future is to invent it. So here's a hundred page paper. Have fun. Essentially, you know, quantum computing is usually you embed reversible operations into a quantum computation. And so the trick there was to do a feedforward operation and do what we call a phase kick. But really, it's just the force kick. You just kick the system with a certain force that is proportional to your loss function that you wish to optimize. And then by performing uncomputation, you start with the superpositions. Overdose. Superposition over parameters, which is pretty funky. Now, you don't have just a point for parameters. You have a superposition over many potential parameters. And our goal is using phase kicks. Somehow to adjust parameters, because phase kicks. Emulate having the parameter space be like a particle in n dimensions, and you're trying to get the Schrodinger equation, Schrodinger dynamics. In the lost landscape of the neural network. You do an algorithm to induce this phase kick, which involves a feedforward, a kick. And then when you uncompute the feedforward, then all the errors and these phase kicks, and these forces back, propagate and hit each one of the parameters throughout the layers. And if you alternate this with an emulation of kinetic energy, then it's kind of like a particle moving in n dimensions, a quantum particle. And the advantage, in principle, would be that it can tunnel through the landscape and find new optima that would have been difficult for stochastic optimizers. But again, this is kind of a theoretical thing, and in practice, with at least the current architectures for quantum computers that we have planned, such algorithms would be extremely expensive to run. So maybe this is a good place to ask the difference between the different fields that you've had a toe in. So mathematics, physics, engineering, and also entrepreneurship, different layers of the stack. I think a lot of the stuff you're talking about here is a little bit on the math side, maybe physics almost working in theory. What's the difference between math, physics engineering, and making a product for quantum computing? For quantum machine learning? Yeah, I mean, some of the original team for the Tensorflow quantum project, which we started in school at University of Waterloo, there was myself initially, I was a physicist, mathematician. We had a computer scientist. We had mechanical engineer, and then we had a physicist that was experimental, primarily. And so putting together teams that are very cross disciplinary, and figuring out how to communicate and share knowledge is really the key to doing this sort of interdisciplinary engineering work. I mean, there is a big difference in mathematics. You can explore mathematics for mathematics sake. In physics, you're applying mathematics to understand the world around us. And in engineering, you're trying to hack the world. You're trying to find how to apply the physics that I know, my knowledge of the world, to do things. Well. In quantum computing in particular, I think there's just a lot of limits to engineering. It just seems to be extremely hard. So there's a lot of value to be exploring quantum computing, quantum machine learning in theory, with math. So I guess one question is, why is it so hard to build a quantum computer? What's your view of timelines in bringing these ideas to life? Right. I think that an overall theme of my company is that we have folks that are, there's a sort of exodus from quantum computing, and we're going to broader physics based AI that is not quantum. So that gives you a hint. So we should say the name of your company is extrapolic. Extraopic. That's right. And we do physics based AI primarily based on thermodynamics rather than quantum mechanics. But essentially, a quantum computer is very difficult to build because you have to induce this sort of zero temperature subspace of information. And the way to do that is by encoding information, you encode a code within a code within a code within a code. And so there's a lot of redundancy needed to do this error correction. But ultimately, it's a sort of algorithmic refrigerator, really. It's just pumping out entropy out of the subsystem that is virtual and delocalized that represents your quote, unquote logical qubits, aka the payload quantum bits, in which you actually want to run your quantum mechanical program. It's very difficult because in order to scale up your quantum computer, you need each component to be of sufficient quality for it to be worth it. Because if you try to do this error correction, this quantum error correction process in each quantum bit, and your control over them is if it's insufficient, it's not worth scaling up. You're actually adding more errors than you remove. And so there's this notion of a threshold where if your quantum bits are sufficient quality in terms of your control over them, it's actually worth scaling up. And actually, in recent years, people have been crossing the threshold, and it's starting to be worth it. And so it's just a very long slog of engineering. But ultimately, it's really crazy to me how much exquisite level of control we have over these systems. It's actually quite crazy. And people are crossing, they're achieving milestones. It's just in general, the media always gets ahead of where the technology is. There's a bit too much hype. It's good for fundraising, but sometimes it causes winters. It's the hype cycle. I'm bullish on quantum computing on a 1015 year timescale personally, but I think there's other quests that can be done in the meantime. I think it's in good hands right now. Well, let me just explore different beautiful ideas, large or small, in quantum computing that might jump out at you from memory. So when you co authored a paper titled asymptotically limitless quantum energy teleportation via qdit probes. So, just out of curiosity, can you explain what a quit is? Which is a qubit. Yeah, it's a d state qubit. It's multidimensional, multi dimensional. Right. So it's like, well, can you have a notion of an integer floating point that is quantum mechanical? That's something I've had to think about. I think that research was a precursor to later work on quantum analog digital conversion. There was interesting because during my masters, I was trying to understand the energy and entanglement of the vacuum of emptiness. Emptiness has energy, which is very weird to say, and our equations of cosmology don't match our calculations for the amount of quantum energy there is in the fluctuations. And so I was trying to hack the energy of the vacuum, right. And the reality is that you can't just directly hack it. It's not technically free energy. Your lack of knowledge of the fluctuations means you can extract the energy. But just like in the stock market, if you have a stock that's correlated over time, the vacuum is actually correlated. So if you measured the vacuum at one point, you acquired information. If you communicated that information to another point, you can infer what configuration the vacuum is in to some precision, and statistically extract, on average, some energy there. So you've quote unquote teleported energy. To me, that was interesting because you could create pockets of negative energy density, which is energy density that is below the vacuum, which is very weird, because we don't understand how the vacuum gravitates. And there are theories where the vacuum, or the canvas of spacetime itself is really a canvas made out of quantum entanglement. And I was studying how decreasing energy of the vacuum locally increases quantum entanglement, which is very funky. And so the thing there is that if you're into weird theories about uaps and whatnot, you could try to imagine that they're. They're around, and how would they propel themselves. How would they go faster than the speed of light? You would need a sort of negative energy density. And to me, I gave it the old college try, trying to hack the energy of the vacuum and hit the limits allowable by the laws of physics. But there's all sorts of caveats there where you can't extract more than you've put in, obviously. But you're saying it's possible to teleport the energy because you can extract information one place and then make, based on that, some kind of prediction about another place. I'm not sure what to make of that. Yeah, I mean, it's allowable by the laws of physics. The reality, though, is that the correlations decay with distance, and so you're going to have to pay the price. Not too far away from where you. Extract it, the precision decreases in terms of your ability, but still. But since you mentioned uaps, we talked about intelligence, and I forgot to ask, what's your view on the other possible intelligences that are out there at the meso scale? Do you think there's other intelligent alien civilizations? Is that useful to think about? How often do you think about it? I think it's useful to think about. It's useful to think about because we gotta ensure we're anti fragile and we're trying to increase our capabilities as fast as possible because we could get disrupted. There's no laws of physics against their being life elsewhere that could evolve and become an advanced civilization and eventually come to us. Do I think they're here now? I'm not sure. I mean, I've read what most people have read on the topic. I think it's interesting to consider, and to me, it's a useful thought experiment to instill a sense of urgency in developing technologies and increasing our capabilities to make sure we don't get disrupted. Right. Whether it's a form of AI that disrupts us or a foreign intelligence from a different planet, like, either way, like increasing our capabilities and becoming formidable as humans, I think that's really important so that we're robust against whatever the universe throws at us. But to me, it's also an interesting challenge and thought experiment on how to perceive intelligence. This has to do with quantum mechanical systems. This has to do with any kind of system that's not like humans. So to me, the thought experiment is, say the aliens are here, or they are directly observable, or just too blind, too self centered, don't have the right sensors, or don't have the right processing of the sensor data to see the obvious intelligence that's all around us. Well, that's why we work on quantum sensors. Right? They can sense gravity. Yeah, but there could be stuff. So that's a good one. But there could be other stuff that's not even in the currently known forces of physics. Right. There could be some other stuff. And the most entertaining thought experiment to me is that it's other stuff that's obviously. It's not like we don't. We lack the sensors. It's all around us, you know, the consciousness being one possible one. But there could be stuff that's just, like, obviously there. And once you know it, it's like, oh, right, right. That's. That's that the thing we thought is somehow emergent from the laws of physics. We understand them, is actually a fundamental part of the universe and can be incorporated in physics, most understood, statistically speaking. Right. If we observed some sort of alien life, it would most likely be some sort of virally self replicating von Neumann like probe system. Right. And it's possible that there are such systems that I don't know what they're doing at the bottom of the ocean, allegedly, but maybe they're collecting minerals from the bottom of the ocean. Yeah, but that wouldn't violate any of my priors. But am I certain that these systems are here? And it'd be difficult for me to say so. Right. I only have secondhand information about there. Being data about the bottom of the ocean. Yeah, but, you know, could it be things like memes? Could it be thoughts and ideas? Could they be operating in that medium? Could aliens be the very thoughts that come into my head? Like, what do you have? You. How do you know that? How do you know that? That. What's the origin of ideas in your mind? When an idea comes to your head? Show me where it originates. I mean, frankly, when I had the idea for the type of computer I'm building now, I think it was eight years ago now, it really felt like it was being beamed from space. It just. I was in bed just shaking, just thinking it through. And I don't know. But do I believe that legitimately? I don't think so, but I think that alien life could take many forms, and I think the notion of intelligence and the notion of life needs to be expanded much more broadly, to be less anthropocentric or biocentric, just to linger. A little longer on quantum mechanics. What's, through all your explorations of quantum computing? What's the coolest, most beautiful idea that you've come across that has been solved or has not yet been solved. I think the journey to understand something called ads CFT. So the journey to understand quantum gravity through this picture, where a hologram of lesser dimension is actually dual or exactly corresponding to a bulk theory of quantum gravity of an extra dimension. And the fact that this sort of duality comes from trying to learn deep learning, like representations of the boundary. And so at least part of my journey someday on my bucket list is to apply quantum machine learning to these sorts of systems, these cfds, or they're called Syk models, and learn an emergent geometry from the boundary theory. And so we can have a form of machine learning to help us understand quantum gravity. Right. Which is still a holy grail that I would like to hit before I leave this earth. What do you think is going on with black holes as information storing and processing units? What do you think is going on with black holes? Black holes are really fascinating objects. They're at the interface between quantum mechanics and gravity, and so they help us test all sorts of ideas. I think that for many decades now, there's been this black hole information paradox, that things that fall into the black hole, we've seemed to have lost their information. Now, I think there's this firewall paradox that has been allegedly resolved in recent years by a former peer of mine who's now a professor at Berkeley. And there it seems like there is. As information falls into a black hole, there's sort of a sedimentation, right? As you get closer and closer to the horizon from the point of view, the observer on the outside, the object slows down infinitely as it gets closer and closer. And so everything that is falling to a black hole from our perspective gets sort of sedimented and tacked onto the near horizon. And at some point, it gets so close to the horizon, it's in the proximity or the scale in which quantum effects and quantum fluctuations matter. And there that in falling matter could interfere with sort of the traditional pictures, that it could interfere with the creation and annihilation of particles and antiparticles in the vacuum. And through this interference, one of the particles gets entangled with the infalling information, and one of them is now free and escapes. And that's how there's sort of mutual information between the outgoing radiation and the infalling matter. But getting that calculation right, I think we're only just starting to. I'll put the pieces together. There's a few pothead like questions I want to ask you. Sure. So, one, does it terrify you that there's a giant black hole at the center of our galaxy. I don't know. I just want to set up shop near it to fast forward, meet a future civilization if we have a limited lifetime. If you could go orbit a black hole and emerge. So if you were like, if there was a special mission that could take you to black hole, would you volunteer. To go travel to orbit and obviously not fall into it? That's obvious. So it's obvious to you that everything's destroyed inside a black hole. Like, all the information it makes up, Guillaume is destroyed. Maybe on the other side, Beth Jazzel's emerging, and it's all like, it's tied together in some deeply memeful way. Yeah, I mean, that's a great question. We have to answer what black holes are. Are we punching a hole through spacetime and creating a pocket universe? It's possible, then, that would mean that if we ascend the Kardashev scale to beyond Kardashev type three, we could engineer black holes with specific hyperparameters to transmit information to new universes we create. And so we can have a progeny that are new universes. And so even though our universe may reach a heat death, we may have a way to have a legacy, right? So we don't know yet. We need to ascend the Kardashev scale to answer these questions, right? To peer into that regime of higher energy physics. And maybe you can speak to the Kardashev scale for people who don't know. So one of the sort of meme like principles and goals of the IAC movement is to ascend the Kardashev scale. What is the Kardashev scale, and why do we want to ascend it? The Kardashev scale is a measure of our energy production and consumption. And really, it's a logarithmic scale. And Kardashev type one is a milestone where we are producing the equivalent wattage to all the energy that is incident on earth from the sun. Kardashev type two would be harnessing all the energy that is output by the sun. And I think type three is like. The whole galaxy, I think, level. Yeah, yeah. And then some people have some crazy type four and five, but I don't know if I believe in those. But to me, it seems like from the first principles of thermodynamics, that, again, there's this concept of thermodynamic driven dissipative adaptation where life evolved on earth, because we have this energetic drive from the sun, we have incident energy, and life evolved on earth to capture, figure out ways to best capture that free energy to maintain itself and grow. And I think that principle, it's not special to our Earth sun system. We can extend life well beyond, and we kind of have a responsibility to do so, because that's the process that brought us here. So we don't even know what it has in store for us in the future. It could be something of beauty we can't even imagine today. Right? So this is probably a good place to talk a bit about the IAC movement. In a substack blog post titled what the fuck is IAC? Or actually, what the f star is IAC? You write, strategically speaking, we need to work towards several overarching civilization goals that are all interdependent. And the four goals are increase the amount of energy we can harness as a species, climb the Kardashev gradient. In the short term, this almost certainly means nuclear fission. Increase human flourishing via pro population growth policies and pro economic growth policies. Create artificial general intelligence, the single greatest force multiplier in human history, and finally develop interplanetary and interstellar transport so that humanity can spread beyond the earth. Could you build on top of that to maybe say, what to you is the IAC movement? What are the goals? What are the principles? The goal is for the human techno capital mimetic machine to become self aware and to hyperstitiously engineer its own growth. So let's decompress each of those words. So you have humans, you have technology, you have capital, and then you have memes, information, right? And all of those systems are coupled with one another. Right? Humans work at companies, they acquire and allocate capital, and humans communicate via memes and information propagation. And our goal was to have a sort of viral, optimistic movement that is aware of how the system works. Fundamentally, it seeks to grow, and we simply want to lean into the natural tendencies of the system to adapt for its own growth. So, in that way, you're right. The EAC is literally a memetic optimism virus that is constantly drifting, mutating, and propagating in a decentralized fashion. Memetic optimism virus. So you do want it to be a virus to maximize the spread. And it's hyperstitious. Therefore, the optimism will incentivize its growth. We see IAC as a sort of meta heuristic, a sort of very thin cultural framework from which you can have much more opinionated forks. Right. Fundamentally, we just say that what got us here is this adaptation of the whole system based on thermodynamics and that process is good, and we should keep it going. That is the core thesis. Everything else is okay. How do we ensure that we maintain this malleability and adaptability? Clearly not suppressing variants and maintaining free speech, freedom of thought, freedom of information propagation, and freedom to do AI research is important for us to converge the fastest on the space of technologies, ideas and whatnot that lead to this growth. And so ultimately, there's been quite a few forks. Some are just memes, but some are more serious. Right. Vitalik Buterin recently made a diack fork. He has his own sort of fine tunings of IAc. Does anything jump out to memory of the unique characteristic of that fork? From Vitalik, I would say that it's trying to find a middle ground between IAC and sort of EA and EI safety. To me, like having a movement that is opposite to what was the mainstream narrative that was taking over Silicon Valley was important to sort of shift the dynamic range of opinions. And it's like the balance between centralization and decentralization. The real optimum is always somewhere in the middle. But for IAC, we're pushing for entropy, novelty, disruption, malleability, speed, rather than being sort of conservative, suppressing thought, suppressing speech, adding constraints, adding too many regulations, slowing things down. And so we're trying to bring balance to the force, right systems. Balance to the force. It's human civilization. Yeah, it's literally the forces of constraints versus the entropic that makes us explore. Right systems are optimal when they're at the edge of criticality, between order and chaos, between constraints, energy minimization, and entropy. Systems want to equilibriate balance, these two things. And so I thought that the balance was lacking, and so we created this movement to bring balance. Well, I like the sort of visual of the landscape of ideas evolving through forks. So kind of thinking on the other part of history, thinking of Marxism as the original repository, and then soviet communism as a fork of that, and then Maoism is a fork of Marxism and communism. So those are all forks, they're exploring. Different ideas, thinking of culture, almost like code, right? Nowadays, I mean, what you prompt the LLM, or what you put in the constitution of an LLM, is basically its cultural framework, what it believes, and you can share it on GitHub nowadays. So starting trying to take inspiration from what has worked in this sort of machine of software to adapt over the space of code. Could we apply that to culture? And our goal is to not say, you should live your life this way. XYZ is to set up a process where people are always searching over subcultures and competing for mind share. And I think creating this malleability of culture is super important for us to converge onto the cultures and the heuristics about how to live one's life that are updated to modern times. Because there's really been a sort of vacuum of spirituality and culture. People don't feel like they belong to any one group. And there's been parasitic ideologies that have taken up opportunity to populate this petri dish of minds. Right. Elon calls it the mind virus. We call it the d cell mind virus complex, which is the decelerative, that is the overall pattern between all of them. There's many variants as well. And so if there's a sort of viral pessimism, decelerative movement, we needed to have not only one movement, but many, many variants. So it's very hard to pinpoint and stop. But the overarching thing is nevertheless a kind of mimetic optimism pandemic. So, I mean, okay, let me ask you, do you think IAC, to some degree, is a cult defined cult? I think a lot of human progress is made when you have independent thought. So you have individuals that are able to think freely. And very powerful memetic systems can kind of lead to groupthink. There's something in human nature that leads to, like, mass hypnosis, mass hysteria. We start to think alike whenever there's a sexy idea that captures our minds. And so it's actually hard to break us apart, pull us apart, diversify thought to that degree. To which degrees? Everybody kind of chanting, iak, iak, like the sheep and animal farm. Well, first of all, it's fun. It's rebellious, right? Like many, I think we lean into, there's this concept of sort of meta irony, right, of sort of being on the boundary of, like, we're not sure if they're serious or not, and it's much more playful and much more fun, right? Like, for example, we talk about thermodynamics being our goddess, right? And sometimes we do cult like things, but there's no ceremony and robes and whatnot. Not yet, but ultimately, yeah. I mean, I totally agree that it seems to me that humans want to feel like they're part of a group, so they naturally try to agree with their neighbors and find common ground. And that leads to sort of mode collapse in the space of ideas, right? We used to have one cultural island that was allowed. It was a typical subspace of thought. And anything that was diverting from that subspace of thought was suppressed or you were canceled. Now we've created a new mode. But the whole point is that we're not trying to have a very restricted space of thought. There's not just one way to think about IAC and its many forks. And the point is that there are many forks and there can be many clusters and many islands. I shouldn't be in control of it in any way. I mean, there's no formal whatsoever. I just put out tweets and certain blog posts, and people are free to defect and fork if there's an aspect they don't like. And so that makes it so that there should be a sort of deterritorialization in the space of ideas so that we don't end up in one cluster that's very cult like. And so cults, usually, they don't allow people to defect or start competing forks, whereas we encourage it. Right. Do you think just the humor, the pros and cons of humor and meme, in some sense, meme, there's like a wisdom to memese. What is it? The magic theater. What book is that from? Harmon Hesse, Steppenwolf, I think. But there's a kind of embracing of the absurdity that seems to get to the truth of things, but at the same time, it can also decrease the quality and the rigor of the discourse. Do you feel the tension of that? Yeah. So initially, I think what allowed us to grow under the radar was because it was camouflaged as sort of meta ironic, right. We would sneak in deep truths within a package of humor and humor and memes and what are called shitposts. Right. And I think that was purposefully a sort of camouflage against those that seek status and do not want to. It's very hard to argue with a cartoon frog or a cartoon of an intergalactic Jeff Bezos and take yourself seriously. And so that allowed us to grow pretty rapidly in the early days. But of course, that's essentially people get steered. Their notion of the truth comes from the data they see, from the information they're fed, and the information people are fed is determined by algorithms. And really what we've been doing is engineering what we call high memetic fitness packets of information so that they can spread effectively and carry a message. So it's kind of a vector to spread the message. And yes, we've been using sort of techniques that are optimal for today's algorithmically amplified information landscapes. But I think we're reaching the point of scale where we can have serious debates and serious conversations. And I. That's why we're considering doing a bunch of debates and having more serious, long form discussions, because I don't think that the timeline is optimal for very serious, thoughtful discussions. You get rewarded for polarization. Even though we started a movement that is literally trying to polarize the tech ecosystem, at the end of the day, it's so that we can have a conversation and find an optimum together. I mean, that's kind of what I try to do with this podcast, given the landscape of things, to still have long form conversations. But there is a degree to which absurdity is fully embraced. In fact, this very conversation is multi level absurd. So, first of all, I should say that I just very recently I had a conversation with Jeff Bezos, and I would love to hear your Beth Jesus opinions of Jeff Bezos. Speaking of intergalactic Jeff Bezos, what do you think of that particular individual whom your name has inspired? Yeah, I mean, I think Jeff is really great. I mean, he's built one of the most epic companies of all time. He's leveraged the techno capital machine and techno capital acceleration to give us what we wanted. Right. We want quick delivery, very convenient at home, low prices. He understood how the machine worked and how to harness it, like running the company, not trying to take profits too early, putting it back, letting the system compound and keep improving. Arguably, I think Amazon's invested some of the most amount of capital and robotics out there. And certainly with the birth of AWS enabled, the tech boom we've seen today that has paid the salaries of, I guess, myself and all of our friends to some extent. I think we can all be grateful to, you know, Jeff, and he's one of the great entrepreneurs out there, one of the best of all time, unarguably. And of course, the work at blue Origin, similar to the work at SpaceX, is trying to make humans a multiplanetary species, which seems almost like a bigger thing than the capitalist machine, or it's a capitalist machine at a different time scale, perhaps. Yeah, I think that companies, they tend to optimize quarter over quarter, maybe a few years out. But individuals that want to leave a legacy can think on a multi decadal or multicentury timescale. And so the fact that some individuals are such good capital allocators, that they unlock the ability to allocate capitals to goals that take us much further or much further looking. Elon's doing this with SpaceX, putting all his capital towards getting us to Mars. Jeff is trying to build blue origin, and I think he wants to build oneill cylinders and get industry off planet, which I think is brilliant. I think just overall, I'm four billionaires. I know this is a controversial statement sometimes, but I think that in a sense, it's kind of a proof of stake voting. If you've allocated capital efficiently, you unlock more capital to allocate just because clearly you know how to allocate capital more efficiently, which is in contrast to politicians that get elected because they speak the best on tv, right? Not because they have a proven track record of allocating taxpayer capital most efficiently. And so that's why I'm for capitalism over, say, giving all our money to the government and letting them figure out how to allocate it. So, yeah, why do you think it's a viral and a popular meme to criticize billionaires? Since you mentioned billionaires, why do you think there's quite a widespread criticism of people with wealth, especially those in the public eye, like Jeff and Elon and Mark Zuckerberg and who else? Bill Gates. Yeah, I think a lot of people would, instead of trying to understand how the techno capital machine works and realizing they have much more agency than they think, they'd rather have the sort of victim mindset I'm just subjected to this machine, it is oppressing me. And the successful players clearly must be evil because they've been successful at this game that I'm not successful at. But I've managed to get some people that were in that mindset and make them realize how the techno capital machine works and how you can harness it for your own good and for the good of others. And by creating value, you capture some of the value you create for the world. And that's positive sum mindset shift is so potent, and really that's what we're trying to do by scaling IAC is sort of unlocking that higher level of agency. Like, actually, you're far more in control of the future than you think. You have agency to change the world. Go out and do it. Here's permission. Each individual has agency. The motto keep building is often heard. What does that mean to you? And what does that have to do with Diet Coke? By the way, thank you so much for the Red Bullets. It's working pretty well. I'm feeling pretty good. Awesome. Well, so building technologies and building it doesn't have to be technologies. Just building in general means having agency trying to change the world by creating, let's say, a company, which is a self sustaining organism that accomplishes a function in the broader techno capital machine to us. That's the way to achieve change in the world that you'd like to see, rather than, say, pressuring politicians or creating nonprofits, that nonprofits, once they run out of money, their function can no longer be accomplished. You're deforming the market artificially compared to subverting or coercing the market or dancing with the market, to convince it that actually this function is important, adds value. And here it is. I think this is the way between the degrowth ESG approach versus, say, Elon. The degrowth approach is we're going to manage our way out of a climate crisis, and Elon is I'm going to build a company that is self sustaining, profitable, and growing, and we're gonna innovate our way out of this dilemma. Right. And we're trying to get people to do the latter rather than the former at all scales. Elon is an interesting case. So you are a proponent. You celebrate ElOn, but he's also somebody who has for a long time warned about the dangers, the potential dangers, existential risks of artificial intelligence. How do you square the two? Is that a contradiction to you? It is somewhat because he's very much against regulation in many aspects, but for AI, he's definitely a proponent of regulations. I think overall, he saw the dangers of, say, OpenAI, cornering the market and then getting to have them monopoly, overdose, the cultural priors that you can embed in these LLMs, that then, as LLMs now become the source of truth for people, then you can shape the culture of the people, and so you can control people by controlling lms. And he saw that, just like it was the case for social media, if you shape the function of information propagation, you can shape people's opinions. He sought to make a competitor. So at least I think we're very aligned there, that the way to a good future is to maintain adversarial equilibria between the various AI players. I love to talk to him to understand his thinking about how to advance AI going forwards. He's also hedging his bets, I would say, with neuralink, I think if he can't stop the progress of AI, he's building the technology to merge. So look at the actions, not just the words, but. Well, I mean, there's some degree where being concerned, maybe using human psychology, being concerned about threats all around us, is a motivator. It's an encouraging thing. I operate much better when there's a deadline, the fear of the deadline, and I, for myself, create artificial things, like I want to create in myself, this kind of anxiety, as if something really horrible will happen if I miss the deadline. I think there's some degree of that here, because creating AI that's aligned with humans has a lot of potential benefits. And so a different way to reframe that is, if you don't, we're all going to die. It just seems to be a very powerful psychological formulation of the goal of creating human aligned AI. I think that anxiety is good. I think, like I said, I want the free market to create aligned AI's that are reliable. And I think that's what he's trying to do with XaI. So I'm all for it. What I am against is stopping, let's say, the open source ecosystem from thriving by, let's say, in the executive order, claiming that open source lms or dual use technologies should be government controlled. Then everybody needs to register their gpu and their big matrices with the government. And I think that extra friction will dissuade a lot of hackers from contributing, hackers that could later become the researchers that make key discoveries that push us forward, right? Including discoveries for AI safety. And so I think I just want to maintain ubiquity of opportunity to contribute to AI and to own a piece of the future, right? It can't just be legislated behind some wall where only a few players get to play the game. I mean, so the EAC movement is often sort of caricatured to mean sort of progress and innovation at all, at all costs. Doesn't matter how unsafe it is, doesn't matter if it causes a lot of damage. You just build cool shit as fast as possible, stay up all night with a Diet Coke, whatever it takes. I think. I guess. I don't know if there's a question in there, but how important to you and what you've seen, the different formulations of EAC, is safety. Is AI safety? Again, I think if there was no one working on it, I think I would be a proponent of it. I think, again, our goal is to bring balance and obviously a sense of urgency is a useful tool to make progress. It hacks our dopaminergic systems and gives us energy to work late into the night. I think also having a higher purpose, you're contributing to. At the end of the day, it's like, what am I contributing to? I'm contributing to the growth of this beautiful machine so that we can seek to the stars. That's really inspiring. That's also a sort of neuro hack. So you're saying AI safety is important to you, but right now, the landscape of ideas you see is AI. Safety as a topic is used more often to gain centralized control. So in that sense, you're resisting it as a proxy for centralized. Gaining centralized control. Yeah, I just think we have to be careful because safety is just the perfect cover for sort of centralization of power and covering up eventually corruption. I'm not saying it's corrupted now, but it could be down the line. And really, if you let the argument run, there's no amount of sort of centralization of control that will be enough to ensure your safety. There's always more nine nine nines of p safety that you can gain, 99.999%. Say, if maybe you want another nine, oh, please, give us full access to everything you do, full surveillance. And frankly, those that are proponents of AI safety have proposed having a global panopticon, where you have centralized perception of everything going on. And to me, that just opens up the door wide open for a sort of big Brother 1984 like scenario. And that's not a future I want. To live in, because we know we have some examples throughout history when that did not lead to a good outcome. Right. You mentioned you founded a company, Extropic, that recently announced a 14.1 million seed round. What's the goal of the company? You're talking about a lot of interesting physics things. So what are you up to over there that you can talk about? Yeah, I mean, you know, originally we weren't going to announce last week, but I think with the doxing and disclosure, we got our hand forced, so we. We had to disclose roughly what we were doing. But really, astropic was born from my dissatisfaction and that of my colleagues with the quantum computing roadmap. Quantum computing was sort of the first path to physics based computing that was trying to commercially scale. And I was working on physics based AI that runs on these physics based computers. But ultimately, our greatest enemy was this noise, this pervasive problem of noise that, as I mentioned, you have to constantly pump out the noise out of the system to maintain this pristine environment where quantum mechanics can take effect. And that constraint was just too much. It's too costly to do that. We were wondering, as generative AI is sort of eating the world, more and more of the world's computational workloads are focused on generative AI. How could we use physics to engineer the ultimate physical substrate for generative AI? From first principles of physics, of information theory, of computation, and ultimately of thermodynamics. What we're seeking to build is a physics based computing system and physics based AI algorithms that are inspired by out of equilibrium thermodynamics, or harness it directly to do machine learning as a physical process. So what does that mean? Machine learning is a physical process? Is that hardware? Is it software, is it both? Is it trying to do the full stack in some kind of unique way? Yes, it is full stack. And so we're folks that have built differentiable programming into the quantum computing ecosystem with Tensorflow quantum. One of my co founders of Tensorflow Quantum is the CTO, Trevor McCourt. We have some of the best quantum computer architects, those that have designed IBMs and AWS systems. They've left quantum computing to help us build what we call actually a thermodynamic computer. A thermodynamic computer. Well, actually, I thought you were on Tensorflow Quantum. What lessons have you learned from Tensorflow quantum? Maybe you can speak to what it takes to create, essentially a software API to a quantum computer. Right? I mean, that was a challenge to build, to invent, to build, and then to get to run on the real devices. Can you actually speak to what it is? Yeah. So Tensorflow Quantum was an attempt at. Well, I mean, I guess we succeeded at combining deep learning, or differentiable classical programming, with quantum computing, and turn quantum computing into or have types of programs that are differentiable in quantum computing. Andre Karpathy calls differentiable programming software 2.0. It's like gradient descent is a better programmer than you. And the idea was that in the early days of quantum computing, you can only run short quantum programs. Which quantum programs should you run? Well, just let gradient descent find those programs. Instead, we built the first infrastructure to not only run differentiable quantum programs, but combine them as part of broader deep learning graphs incorporating deep neural networks, the ones you know and love with what are called quantum neural networks. And ultimately, it was a very cross disciplinary effort. We had to invent all sorts of ways to differentiate, to backpropagate through the graph, the hybrid graph. But ultimately, it taught me that the way to program matter and to program physics is by differentiating through control parameters. If you have parameters that affects the physics of the system, and you can evaluate some loss function, you can optimize the system to accomplish a task, whatever that task may be. And that's a very sort of universal meta framework for how to program physics. Based computers to try to parameterize everything, make those parameters differential, and then optimize? Yes. Okay, so is there some more practical engineering lessons from tensorflow quantum just organizationally, too, like the humans involved and how to get to a product, how to create good documentation, how to, I don't know, all these little subtle things that people might not think about. I think, like, working across disciplinary boundaries is always a challenge, and you have to be extremely patient in teaching one another. Right. I learned a lot of software engineering through the process. My colleagues learned a lot of quantum physics and some learned machine learning through the process of building this system. And I think if you get some smart people that are passionate and trust each other in a room and you have a small team and you teach each other your specialties, suddenly you're kind of forming this sort of model soup of expertise, and something special comes out of that. Right. It's like combining genes, but for your knowledge bases, and sometimes special products come out of that. And so I think, like, even though it's very high friction initially to work in an interdisciplinary team, I think the product, at the end of the day, is worth it. And so, learned a lot trying to bridge the gap there. And, I mean, it's still a challenge to this day. We hire folks that have an AI background, folks that have a pure physics background, and somehow we have to make them talk to one another. Right. Is there magic? Is there some science and art to the hiring process, to building a team that can create magic together? Yeah, it's really hard to pinpoint that. Jean sais quoi. Right? I didn't know you speak French. That's very nice. Yeah, I'm actually french Canadian. Oh, you are legitimately french Canadian. I am. I thought you were just doing that for the. For the cred. No, no, I'm truly french Canadian from Montreal. But, yeah, essentially, we look for people with very high, fluid intelligence that aren't over specialized, because they're going to have to get out of their comfort zone. They're going to have to incorporate concepts that they've never seen before and very quickly get comfortable with them or learn to work in a team. And so that's sort of what we look for when we hire. We can't hire people that are just optimizing this subsystem for the past three or four years. We need really general, broader intelligence and specialty and people that are open minded, really, because if you're pioneering a new approach from scratch, there is no textbook, there's no reference. It's just us and people that are hungry to learn. So we have to teach each other. We have to learn the literature. We have to share knowledge bases, collaborate in order to push the boundary of knowledge further together. Right. And so people that are used to just getting prescribed what to do at this stage, when you're at the pioneering stage, that's not necessarily who you want to hire. So you mentioned with extraopic, you're trying to build the physical substrate for generative AI. What's the difference between that and the AGI AI itself? So is it possible that in the halls of your company, AGI will be created, or will AGI just be using this as a substrate? I think our goal is to both run human like AI or anthropomorphic AI. Sorry for use of the term AGI. I know it's triggering for you. We think that the future is actually physics based AI combined with anthropomorphic AI. So you can imagine I have a sort of world modeling engine through physics based AI. Physics based AI is better at representing the world at all scales, because it can be quantum mechanical, thermodynamic, deterministic, hybrid representations of the world, just like our world, at different scales, has different regimes of physics. If you inspire yourself from that in the ways you learn representations of nature, you can have much more accurate representations of nature. So you can have very accurate world models at all scales. Right? And so you have the world modeling engine, and then you have the sort of anthropomorphic AI that is human like. So you can have the science, the playground, to test your ideas, and you can have the synthetic scientist. And to us, that joint system of a physics based AI and an anthropomorphic AI is the closest thing to a fully general artificially intelligent system. So you can get closer to truth by grounding the AI to physics, but you can also still have an anthropomorphic interface to us humans that like to talk to other humans or human like systems. On that topic, what do you. I suppose that is one of the big limitations of current large language models to you, is that they're not. They're good bullshitters. They're not really grounded to truth necessarily. Is that. Would that be fair to say? Yeah. No. You wouldn't try to extrapolate the stock market with an LM trained on text from the Internet. Right. It's not going to be a very accurate model. It's not going to model its priors or its uncertainties about the world very accurately. Right. So you need a different type of AI to complement sort of this text extrapolation. AI. You mentioned singularity earlier. How far away are we from a singularity? I don't know if I believe in a finite time singularity as a single point in time. I think it's going to be asymptotic and sort of a diagonal sort of asymptote, like we have the light cone, we have the limits of physics restricting our ability to grow. So obviously you can't fully diverge on a finite time. I think my priors are that. I think a lot of people on the other side of the aisle think that once we reach human level AI, there's going to be an inflection point and a sudden foom. Suddenly AI is going to grok how to manipulate matter at the nanoscale and assemble nanobots. And having worked for nearly a decade in applying AI to engineer matter, it's much harder than they think. In reality, you need a lot of samples from either a simulation of nature that's very accurate and costly, or nature itself, and that keeps your ability to control the world around us in check. There's a sort of minimal cost, computationally and thermodynamically to acquiring information about the world in order to be able to predict and control it. And that keeps things in check. It's funny you mentioned the other side of the aisle. So in the poll I posted about pdoom yesterday was the probability of doom. There seems to be a nice division between people. Think it's very likely and very unlikely. I wonder if in the future they'll be the actual Republicans versus Democrats division. Blue versus red is the AI dumers versus the I ackers yak. Yeah. So this movement is not right wing or left wing. Fundamentally, it's more like up versus down in terms of the up civilization. But it seems to be like there is sort of case of alignment of the existing political parties where those that are for more centralization of power, control and more regulations are aligning themselves with the doomers, because that sort of instilling fear in people is a great way for them to give up more control and give the government more power. But fundamentally, we're not left versus right. I think we've done polls of people's alignment within IAC. I think it's pretty balanced. So it's a new fundamental issue of our time. It's not just centralization versus decentralization. It's kind of, do we go? It's like tech progressivism versus techno conservatism. So IAC is, as a movement is often formulated in contrast to EA, effective altruism. What do you think are the pros and cons of effective altruism? What's interesting, insightful to you about them and what is negative. Right. I think people trying to do good from first principles is. Is good. We should actually say, and sorry to interrupt, we should probably say that. And you can correct me if I'm wrong, but effective altruism is a kind of movement that's trying to do good optimally, where good is probably measured. Something like the amount of suffering in the world, you want to minimize it. And there's ways that, that can go wrong, as any optimization can. And so it's interesting to explore how things can go wrong. We're both trying to do good to some extent. And we're both trying, we're arguing for which loss function we should use, right? Yes. Their loss function is sort of hedons units of hedonism. How good do you feel for how much time? And so suffering would be negative hedons. And they're trying to minimize that. But to us, that seems like that loss function has spurious minima. You can start minimizing shrimp farm pain, which seems not that productive to me. Or you can end up with wireheading, where you just either install a neuralink or you scroll TikTok forever. And you feel good on the short term timescale, because your neurochemistry. But on long term timescale, it causes decay and death. Right? Because you're not being productive. Whereas sort of IAC, measuring progress of civilization, not in terms of a subjective loss of function, like hedonism, but rather an objective measure, a quantity that cannot be gained, that is physical energy. It's very objective, and there's not many ways to game it. If you did it in terms of GDP or currency that's pinned to a certain value, that's moving, that's not a good way to measure our progress. But the thing is, we're both trying to make progress and ensure humanity flourishes and gets to grow. We just have different loss functions and different ways of going about doing it. Is there a degree, maybe you can educate me. Correct me. I get a little bit skeptical when there's an equation involved trying to reduce all of the human civilization, human experience to an equation. Is there a degree that we should be skeptical of the tyranny of an equation of a loss function over which to optimize? Like having a kind of intellectual humility about optimizing over loss functions. Yeah. So this particular loss function, it's not stiff. It's kind of an average of averages, right? It's like distributions of states in the future are going to follow a certain distribution. So it's not deterministic. It's not like we're not on, like, stiff rails. Right. It's just a statistical statement about the future. But at the end of the day, you can believe in gravity or not, but it's not necessarily an option to obey it. And some people try to test that, and that goes not so well. So, similarly, I think thermodynamics is there whether we like it or not, and we're just trying to point out what is and try to orient ourselves and chart a path forward given, given this fundamental truth. But there's still some uncertainty. There's still lack of information. Humans tend to fill the gap of the lack of information with narratives. And so how they interpret, you know, even physics is up to interpretation when there's uncertainty involved, and humans tend to use that to further their own means. So it's always, whenever there's an equation, it just seems like until we have really perfect understanding of the universe, humans will do what humans do, and they try to use the narrative of doing good to fool the populace into doing bad. I just. I guess that this is something they should be skeptical about in all movements. That's right. So we invite skepticism. Right. Do you have an understanding of what might, to a degree, that went wrong? What do you think may have gone wrong with effective altruism, that might also go wrong with effective accelerationism? Yeah, I mean, I think, you know, I think it provided, initially a sense of community for, you know, engineers and intellectuals and rationalists in the early days. And it seems like the community was very healthy. But then they formed all sorts of organizations and started routing capital and having actual power. They have real power. They influence the government. They influence most AI orgs now. I mean, they're literally controlling the board of OpenAI and look over at anthropic. I think they have some control over that, too. I think the assumption of IAC is more like capitalism, is that every agent, organism and meta organism is going to act in its own interest, and we should maintain sort of adversarial equilibrium or adversarial competition to keep each other in check at all times, at all scales. I think that, yeah, ultimately, it was the perfect cover to acquire tons of power and capital. And I, unfortunately, sometimes that corrupts people over time. What does a perfectly productive day, since building is important, what is a perfectly productive day in the life of Guillaume Verdun look like? How much caffeine do you consume? What's a perfect day? Okay, so I have a particular regimen. I would say my favorite days are 12:00 p.m. to 04:00 a.m. and I would have meetings in the early afternoon, usually external meetings, some internal meetings. Because I'm CEO, I have to interface with the outside world, whether it's customers or investors or interviewing potential candidates. And usually I'll have ketones, exogenous ketones. So are you on a keto on a keto diet, or is this. I've done keto before for football and whatnot, but I like to have a meal after sort of part of my day is done, and so I can just have extreme focus. You do the social interactions earlier in the day without food. Front load them? Yeah, like right now I'm on ketones and red ball, and it just gives you a clarity of thought that is really next level, because then when you eat, you're actually allocating some of your energy that could be going to neural energy to your digestion. After I eat, maybe I take a break, an hour or so, hour and a half. And then usually it's like ideally one meal a day, like steak and eggs and vegetables, animal based, primarily. So fruit and meat. And then I do a second wind, usually. That's deep work, right? Because I am a CEO, but I'm still technical. I'm contributing to most patents, and there I'll just stay up late into the night and work with engineers on very technical problems. So it's like the 09:00 p.m. to 04:00 a.m. whatever that range of time. Yeah, that's the perfect time. The emails, the things that are on fire, stop trickling in. You can focus, and then you have your second wind. And I think Demesne Savas has a similar workday to some extent. So I think that's definitely inspired my work day. But, yeah, I started this workday when I was at Google and had to manage a bit of the product during the day and have meetings and then do technical work at night, exercise, sleep. Those kinds of things. Said football. You used to play football? Yeah, I used to play american football. I've done all sorts of sports growing up. And then I was into powerlifting for a while. So when I was studying mathematics in grad school, I would just do math and lift, take caffeine, and that was my day. It was very pure, the purest of monk modes. But it's really interesting how in powerlifting, you're trying to cause neural adaptation by having certain driving signals, and youre trying to engineer neuroplasticity through all sorts of supplements, and you have all sorts of brain derived neurotrophic factors that get secreted when you lift. Its funny to me how I was trying to engineer neural adaptation in my nervous system more broadly, not just my brain, while learning mathematics. I think you can learn much faster if you really care, if you convince yourself to care a lot about what you're learning and you have some sort of assistance, let's say caffeine or some cholinergic supplement to increase neuroplasticity. I should chat with Andrew Huberman at some point. He's the expert, but yeah, at least to me, it's like you can try to input more tokens into your brain, if you will, and you can try to increase the learning rate so that you can learn much faster on a shorter time scale. So I've learned a lot of things. I followed my curiosity. You're naturally, if you're passionate about what you're doing, you're going to learn faster, you're going to become smarter faster, and if you follow your curiosity, you're always going to be interested. And so I advise people to follow their curiosity and don't respect the boundaries of certain fields or what you've been allocated in terms of lane, of what you're working on, just go out and explore and follow your nose and try to acquire and compress as much information as you can into your brain. Anything that you find interesting and caring. About a thing, like you said, which is interesting, it works for me really well. It's like tricking yourself that you care about a thing. Yes. And then you start to really care about it. So it's funny, the motivation is a really good catalyst for learning, right? And so at least part of, part of my character as Beth, Jesus is kind of like, yeah, the hype man. Yeah. But I'm like hyping myself up, but then I just tweet about it, and it's just when I'm trying to get really hyped up in, like, an altered state of consciousness where I'm like ultra focused, in the flow, wired, trying to invent something that's never existed, I need to get to unreal levels of, like, excitement. But your brain has these levels of, of cognition that you can unlock with like, higher levels of adrenaline and, and whatnot. And I mean, I've learned that in powerlifting that actually you can engineer a mental switch to, like, increase your strength, right? Like, if you can engineer a switch, maybe you have a prompt, like a certain song or some music where suddenly you're, like, fully primed. Then you're at max maximum strength, right. And I've engineered that. That switch through years of lifting. If you're going to get under 500 pounds, and it could crush you. If you don't have that switch to be wired in, you might die. So that that'll wake you right up. And that sort of skill I've carried over to, like, research. When it's go time, when the stakes are high, somehow I just reach another level of neural performance. So Beth JSOs is your and sort of embodiment representation of your intellectual Hulk. It's your productivity Hulk. They just turn on. What have you learned about the nature of identity from having these two identities? I think it's interesting for people to be able to put on those two hats so explicitly. I think it was interesting in the early days. I think in the early days, I thought it was truly compartmentalized, like, oh, yeah, this is a character. You know, I'm Guillaume. Beth is just the character. I take my thoughts, and then I extrapolate them to a bit more extreme. But over time, it's kind of like both identities were starting to merge mentally, and people were like, no, I met you. You are Beth. You are not just Guillaume. And I was like, wait, am I? And now it's fully merged. But it was already before the docs was already starting mentally that I am this character. It's part of me. Would you recommend people sort of have an alt? Absolutely. Like, young people. Would you recommend them to explore different identities by having alts alt accounts? It's fun. It's like writing an essay and taking a position, right? It's like you do this in debate. It's like you can have experimental thoughts and by the stakes being so low, because you're an anon account with, I don't know, 20 followers or something, you can experiment with your thoughts in a low stakes environment, and I feel like we've lost that. In the era of everything being under your main name, everything being attributable to you, people just are afraid to speak, explore ideas that aren't fully formed, and I feel like we've lost something there. So I hope platforms like x and others really help support people trying to stay pseudonymous or anonymous, because it's really important for people to share thoughts that aren't fully formed and converge onto maybe hidden truths that were hard to converge upon if it was just through open conversation with real names. Yeah. I really believe in not radical, but rigorous empathy. It's like really considering what it's like to be a person of a certain viewpoint and, like, taking that as a thought experiment farther and farther and farther. And one way of doing that is an alt account. That's a fun, interesting way to really explore what it's like to be a person that believes a set of beliefs. And taking that across a span of several days, weeks, months, of course, there's always the danger of becoming that. That's the Nietzsche gaze. Long into the abyss. The abyss gazes into you. You have to be careful. Breaking Beth. Yeah, right. Breaking Beth. Yeah. You wake up with a shaved head one day, just like, who am I? What have I become? So you've mentioned quite a bit of advice already. But what advice would you give to young people of how to, in this interesting world we're in, how to have a career, and how to have a life they can be proud of? I think, to me, the reason I went to theoretical physics was that I had to learn the base of the stack that was going to stick around no matter how the technology changes. Right. And to me, that was the foundation upon which then I later built engineering skills and other skills, and to me, the laws of physics. It may seem like the landscape right now is changing so fast, it's disorienting. But certain things like fundamental mathematics and physics aren't going to change. And if you have that knowledge and knowledge about complex systems and adaptive systems, I think that's going to carry you very far. And so not everybody has to study mathematics, but I think it's really a huge cognitive unlock to learn math and some physics and engineering. Get as close to the base of the stack as possible. Yeah, that's right. Because the base of the stack doesn't change everything else. Your knowledge might become not as relevant in a few years. Of course, there's a sort of transfer learning you can do, but then you have to always transfer learn constantly. I guess the closer you are to the base of the stack, the easier the transfer learning, the shorter the jump. Right, right. And you'd be surprised, once you've learned concepts in many physical scenarios, how they can carry over to understanding other systems that are necessarily physics. And I guess the IAC writings, the principles and Tenet post that was based on physics, that was kind of my experimentation with applying some of the thinking from out of equilibrium thermodynamics to understanding the world around us. And it's led to. To IAC and this movement. If you look at your one cog in the machine, in the capitalist machine, one human, and if you look at yourself, do you think mortality is a feature or a bug? Would you want to be immortal? No, I think fundamentally, in thermodynamic dissipative adaptation, there's the word dissipation. Dissipation is important. Death is important, right? We have a saying in physics. Physics progresses one funeral at a time. I think the same is true for capitalism. Companies, empires, people, everything. Everything must die at some point. I think that we should probably extend our lifespan because we need a longer period of training, because the world is more and more complex. We have more and more data to really be able to predict and understand the world. And if we have a finite window of higher neuroplasticity, then we have sort of a hard cap and how much we can understand about our world. So I think I am for death, because, again, I think it's important. If you have a king that would never die, that would be a problem. The system wouldn't be constantly adapting. You need novelty, you need youth, you need disruption to make sure the system is always adapting and malleable. Otherwise, if things are immortal, if you have, let's say, corporations that are there forever and they have the monopoly, they get calcified, they become not as optimal, not as high fitness in a changing, time varying landscape. Death gives space for youth and novelty to take its place. And I think it's an important part of every system in nature. So, yeah, I am for death, but I do think that longer lifespan and longer time for neuroplasticity, bigger brains, should be something we should strive for. Well, in that Jeff Bezos and Beth Jlos agree that all companies die. And for Jeff, the. The goal is to try to, he calls it day one thinking, try to constantly, for as long as possible, reinvent, sort of extend the life of the company, but eventually it, too, will die, because it's so damn difficult to keep reinventing. Are you afraid of your own death? I think I have ideas and things I'd like to achieve in this world before I have to go, but I don't think I'm necessarily afraid of death. So you're not attached to this particular body and mind that you got? No. I think. I'm sure there's going to be better versions of myself in the future. Or forks. Forks, right. Genetic forks or other. I truly believe that. I think there's a sort of evolutionary, like, algorithm happening at every bit, or Nat, in the world is sort of adapting through this process that we described in IAC. And I think maintaining this adaptation, malleability, is how we have constant optimization of the whole machine. And so I don't think I'm particularly an optimum that needs to stick around forever. I think there's going to be greater optima in many ways. What do you think is the meaning of it all? What's the why of the machine, the IAc machine? The why. Well, the why is thermodynamics. It's why we're here. It's what has led to the formation of life and of civilization, of evolution, of technologies and growth of civilization. But why do we have thermodynamics? Why do we have our particular universe? Why do we have these particular hyperparameters, the constants of nature? Well, then you get into the anthropic principle. In the landscape of potential universes, we're in the universe that allows for life. And then I why is there potentially many universes? I don't know. I don't know that part. But could we potentially engineer new universes or create pocket universes and set the hyperparameters so there is some mutual information between our existence and that universe, and we'd be somewhat its parents? I think that's really. I don't know. That'd be very poetic, purely conjecture. But again, this is why figuring out quantum gravity would allow us to understand if we can do that. And above that, why is it all seems so beautiful and exciting? The quest to figure out quantum gravity seems so exciting. Why? Why is that? Why are we drawn to that? Why are we pulled towards that? Just that puzzle solving creative force that underpins all of it, it seems like. I think we seek, just like an LLM seeks to minimize cross entropy between its internal model and the world. We seek to minimize. Yeah. The statistical divergence between our predictions, the world and the world itself. And, you know, having regimes of energy scales or physical scales in which we have no visibility, no ability to predict or perceive, you know, that's kind of an insult to us. And we want to. We want to be able to understand the world better in order to best steer it or steer us through it. And in general, it's a capability that has evolved because the better you can predict the world, the better you can capture utility or free energy towards your own sustenance and growth. And I think quantum gravity, again, is kind of the final boss in terms of knowledge acquisition, because once we've mastered that, then we can do a lot, potentially. But between here and there, I think there's a lot to learn in the mesoscales. There's a lot of information to acquire about our world and a lot of engineering, perception, prediction and control to be done to climb up the Kardashev scale. And to us, that's the great challenge of our times. And when you're not sure where to go, let the meme pave the way. Guillaume Beth, thank you for talking today. Thank you for the work you're doing. Thank you for the humor and the wisdom you put into the world. This was awesome. Thank you so much for having me. Lax it's a pleasure. Thank you for listening to this conversation with Guillain Verdun. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Albert Einstein. If at first the idea is not absurd, then there is no hope for it. Thank you for listening. I hope to see you next time.

Utterances:
Speaker A: The following is a conversation with Guillaume Verdun, the man behind the previously anonymous account based Beth Jesus on X. These two identities were merged by a doxxing article in Forbes titled who is based Beth Jezos, the leader of the tech elites EAC movement. So let me describe these two identities that coexist in the mind of one human identity number one, Guillaume is a physicist, applied mathematician, and quantum machine learning researcher and engineer, receiving his PhD in quantum machine learning, working at Google and Quantum computing, and finally launching his own company called Ekstropic, that seeks to build physics based computing hardware for generative AI. Identity number two, Bev JSOs on X is the creator of the effective accelerationism movement, often abbreviated as EAC, that advocates for propelling rapid technological progress as the ethically optimal course of action for humanity. For example, its proponents believe that progress in AI is a great social equalizer which should be pushed forward. IAC followers see themselves as a counterweight to the cautious view that AI is highly unpredictable, potentially dangerous, and needs to be regulated. They often give their opponents the labels of doomers or decels, short for deceleration. As Bef himself put it, IAC is a memetic optimism virus. The style of communication of this movement leans always toward the memes and the Lulz, but there is an intellectual foundation that we explore in this conversation. Now, speaking of the meme, I am too, a kind of aspiring connoisseur of the absurd. It is not an accident that I spoke to Jeff Bezos and Beth Jazos back to back as we talk about Beth admires Jeff as one of the most important humans alive, and I admire the beautiful absurdity and the humor of it all. This is the Lex Friedman podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Guillaume for dumb. Let's get the facts of identity down first. Your name is Guillaume Verdun Gill, but you're also behind the anonymous account on X called based Beth JSOs. So first, Guillaume Verdun, you're quantum computing guy, physicist, applied mathematician, and then based Beth Jeze is basically a meme account that started a movement with a philosophy behind it. So maybe just can you linger on who these people are in terms of characters, in terms of communication styles, in terms of philosophies?
Speaker B: I mean, with my main identity, I guess ever since I was a kid, I wanted to figure out a theory of everything, to understand the universe. And that path led me to theoretical physics, eventually trying to answer the big questions of why are we here? Where are we going. And that led me to study information theory and try to understand physics from the lens of information theory, understand the universe as one big computation. And essentially, after reaching a certain level studying black hole physics, I realized that I wanted to not only understand how the universe computes, but sort of compute like nature and figure out how to build and apply computers that are inspired by nature. So physics based computers. And that sort of brought me to quantum computing as a field of study to, first of all, simulate nature. And in my work, it was to learn representations of nature that can run on such computers. So if you have AI representations that think like nature, then they'll be able to more accurately represent it. At least, that was the thesis that brought me to be an early player in the field called quantum machine learning. So how to do machine learning on quantum computers and really sort of extend notions of intelligence to the quantum realm. So how do you capture and understand quantum mechanical data from our world? And how do you learn quantum mechanical representations of our world? On what kind of computer do you run these representations and train them? How do you do so? And so? That's really sort of the questions I was looking to answer, because ultimately, I had a sort of crisis of faith. Originally, I wanted to figure out, as every physicist does at the beginning of their career, a few equations that describe the whole universe. Sort of be the hero of the story there. But eventually, I realized that actually augmenting ourselves with machines, augmenting our ability to perceive, predict, and control our world with machines, is the path forward. Right? And that's what got me to leave theoretical physics and go into quantum computing and quantum machine learning. And during those years, I thought that there was still a piece missing. There was a piece of our understanding of the world and our way to compute and our way to think about the world. And if you look at the physical scales, at the very small scales, things are quantum mechanical, and at the very large scales, things are deterministic. Things have averaged out. I'm definitely here in this seat. I'm not in a superposition over here and there. At the very small scales, things are in superposition. They can exhibit interference effects. But at the mesoscales, the scales that matter for day to day life, the scales of proteins, of biology, of gases, liquids, and so on, things are actually thermodynamical. They're fluctuating. And after, I guess, about eight years in quantum computing and quantum machine learning, I had a realization that I was looking for answers about our universe by studying the very big and the very small. I did a bit of quantum cosmology. So that's studying the cosmos, where it's going, where it came from. You study black hole physics, you study the extremes in quantum gravity. You study where the energy density is sufficient for both quantum mechanics and gravity to be relevant. And the extreme scenarios are black holes and the very early universe. There's the sort of scenarios that you study the interface between quantum mechanics and relativity. And really I was studying these extremes to understand how the universe works and where is it going. But I was missing a lot of the meat in the middle, if you will, because day to day quantum mechanics is relevant and the cosmos is relevant, but not that relevant actually. We're on the medium space and time scales, and there the main theory of physics that is most relevant is thermodynamics out of equilibrium thermodynamics, because life is a process that is thermodynamical and it's out of equilibrium. We're not just a soup of particles at equilibrium with nature. We're a sort of coherent state trying to maintain itself by acquiring free energy and consuming it. And thats sort of, I guess, another shift, I guess my faith in the universe happened towards the end of my time at Alphabet, and I knew I wanted to build, well, first of all, a computing paradigm based on this type of physics, but ultimately just. Bye. Trying to experiment with these ideas applied to society and economies and much of what we see around us. I started an anonymous account just to relieve the pressure that comes from having an account that you're accountable for everything you say on. And I started an anonymous account just to experiment with ideas originally, right. Because I didn't realize how much I was restricting my space of thoughts until I sort of had the opportunity to let go. In a sense, restricting your speech back propagates to restricting your thoughts. And by creating an anonymous account, it seemed like I had unclamped some variables in my brain and suddenly could explore a much wider parameter space of thoughts.
Speaker A: Just to linger on that isn't that interesting that one of the things that people don't often talk about is that when there's pressure and constraints on speech, it somehow leads to constraints on thought, even though it doesn't have to. We can think thoughts inside our head, but somehow it creates these walls around thought.
Speaker B: Yep, thats sort of the basis of our movement is we were seeing a tendency towards constraint reduction or suppression of variants in every aspect of life, whether its thought, how to run a company, how to organize humans, how to do AI research. In general, we believe that maintaining variance ensures that the system is adaptive. Maintaining healthy competition in marketplaces, of ideas, of companies, of products, of cultures, of governments, of currencies, is the way forward, because the system always adapts to assigned resources to the configurations that lead to its growth. And the fundamental basis for the movement is this sort of realization that life is a sort of fire that seeks out free energy in the universe and seeks to grow, and that growth is fundamental to life. And you see this in the equations, actually, of outer equilibrium thermodynamics. You see that paths of trajectories, of configurations of matter that are better at acquiring free energy and dissipating more heat are exponentially more likely. So the universe is biased towards certain futures. And so there's a natural direction where the whole system wants to go.
Speaker A: So the second law of thermodynamics says that the entropy zone is increasing. The universe is tending towards equilibrium. And you're saying there's these pockets that have complexity and are out of equilibrium. You said that thermodynamics favors the creation of complex life. That increases its capability to use energy to offload entropy. To offload entropy. So you have pockets of non entropy that tend the opposite direction. Why is that intuitive to you, that it's natural for such pockets to emerge?
Speaker B: Well, we're far more efficient at producing heat than, let's say, just a rock with a similar mass as ourselves. We acquire free energy, we acquire food, and we're using all this electricity for our operation. And so the universe wants to produce more entropy. And by having life go on and grow, it's actually more optimal at producing entropy, because it will seek out pockets of free energy and burn it for its sustenance and further growth. And that's sort of the basis of life. And, I mean, there's Jeremy England at MIT, who has this theory that I'm a proponent of, that life emerged because of this sort of property. And to me, this physics is what governs the mesoscales. And so it's the missing piece between the quantum and the cosmos. It's the middle part. Thermodynamics rules the mesoscales. And, to me, both from a point of view of designing or engineering devices that harness that physics and trying to understand the world through the lens of thermodynamics has been sort of a. A synergy between my two identities over the past year and a half now. And so that's really how. That's really how the two identities emerged. One was kind of, I'm a decently respected scientist. And I was going towards doing a startup in the space and trying to be a pioneer of a new kind of physics based AI. And as a dual to that, I was sort of experimenting with philosophical thoughts from a physicist standpoint. And ultimately, I think that around that time, it was like late 2021, early 2022. I think there's just a lot of pessimism about the future in general and pessimism about tech. That pessimism was sort of virally spreading because it was getting algorithmically amplified. And people just felt like the future is going to be worse than the present. And to me, that is a very fundamentally destructive force in the universe is this sort of doom mindset, because it is hyperstitious, which means that if you believe it, you're increasing the likelihood of it happening. And so felt a responsibility, to some extent, to make people aware of the trajectory of civilization and the natural tendency of the system to adapt towards its growth. And sort of that. Actually, the laws of physics say that the future is going to be better and grander statistically, and we can make it so. And if you believe in it, if you believe that the future would be better, and you believe you have agency to make it happen, you're actually increasing the likelihood of that better future happening. And so I sort of felt a responsibility to sort of engineer a movement, a viral optimism about the future, and build a community of people supporting each other to build and do hard things, do the things that need to be done for us to scale up civilization. Because at least to me, I don't think stagnation or slowing down is actually an option. Fundamentally, life and the whole system, our whole civilization, wants to grow. And there's just far more cooperation when the system is growing rather than when it's declining. And you have to decide how to split the pie. And so I've balanced both identities so far, but I guess recently the two have been merged more or less without my consent.
Speaker A: So you said a lot of really interesting things there. So, first, representations of nature. That's something that first drew you in to try to understand from a quantum computing perspective, like, how do you understand nature? How do you represent nature in order to understand it, in order to simulate it, in order to do something with it? So it's a question of representations, and then there's that leap you take from the quantum mechanical representation to the what you're calling mesoscale representation, where thermodynamics comes into play, which there's a way to represent nature in order to understand what life, human behavior, all this kind of stuff that's happening here on earth that seems interesting to us. Then there's the word hyperstition. So some ideas, I suppose both pessimism and optimism are such ideas that if you internalize them, you in part make that idea a reality. So both optimism, pessimism have that property. I would say that probably a lot of ideas have that property, which is one of the interesting things about humans. And you talked about one interesting difference also between the sort of the Guillaume, the gill front end and the base Bev Jezzl's backhand is the communication styles, also that you are exploring different ways of communicating that can be more viral in the way that we communicate in the 21st century. Also, the movement that you mentioned that you started, it's not just a meme account, but there's also a name to it called effective accelerationism. EC a play, a resistance to the effective altruism movement. Also an interesting one that I'd love to talk to you about the tensions there. Okay. And so then there was a merger, a git merge, and the personalities recently, without your consent, like you said, some journalists figured out that you're one in the same. Maybe you could talk about that experience, first of all. Like what? What's the story if of the merger of the two? Right?
Speaker B: So I wrote the manifesto with my co founder of IAC, an account named Baselord, still anonymous, luckily, and hopefully forever.
Speaker A: So it's based buff Jezos and bazed. Like Bayesian. Like Baze Lord. Like bayesian bayesian lord. Base lord. Okay. And so we should say from now on, when you say IAC, you mean e ACC, which stands for effective accelerationism.
Speaker B: That's right.
Speaker A: And you're referring to a manifesto written on, I guess, substack. Are you also base lord?
Speaker B: No.
Speaker A: Okay. It's a different person.
Speaker B: Yeah.
Speaker A: Okay, well, there you go. Wouldn't it be funny if I'm baselord?
Speaker B: That'd be amazing. So originally wrote the manifesto around the same time as I founded this company. And I worked at Google X or just X now, or Alphabet X now that there's another x. And there, the baseline is sort of secrecy, right? You can't talk about what you work on, even with other googlers or externally. And so that was kind of deeply ingrained in my way, to do things, especially in deep tech, that has geopolitical impact. Right? And so I was being secretive about what I was working on. There was no correlation between my company and my main identity publicly. And then not only did they correlate that they also correlated my main identity and this account. So I think the fact that they had doxxed the whole Guillaume complex and the journalists reached out to actually my investors, which is pretty scary. When you're a startup entrepreneur, you don't really have bosses except for your investors. And my investors ping me like, hey, I, this is going to come out. They've figured out everything. What are you going to do? I think at first they had a first reporter on the Thursday, and they didn't have all the pieces together, but then they looked at their notes across the organization and they censor fused their notes, and now they had way too much. And that's when I got worried because they said it was of public interest. And in general, like how you said.
Speaker A: Sensor fused, like it's some giant neural network operating and distributed way. We should also say that the journalists used, I guess, at the end of the day, audio based analysis of voice, comparing voice of what talks you've given in the past, and then voice on x spaces.
Speaker B: Yep.
Speaker A: Okay, so, and that's where the primarily the match happened. Okay, continue the match.
Speaker B: But, you know, they scraped, you know, sec filings, they looked at my private Facebook account and so on. So they did some digging. Originally, I thought that doxing was illegal, right? But there's this weird threshold when it becomes of public interest to know someone's identity. And those were the keywords that sort of, like, ring the alarm bells for me when they said, because I had just reached 50k followers, allegedly, that's of public interest. And so where do we draw the line? When is it legal to dox someone the word doxx?
Speaker A: Maybe you can educate me. I thought doxxing generally refers to if somebody's physical location is found out of meaning, like where they live. So we're referring to the more general concept of revealing private information that you don't want revealed, is what you mean by doxxing.
Speaker B: I think that for the reasons we listed before, having an anonymous account is a really powerful way to keep the powers that be in check. We were ultimately speaking truth to power. I think a lot of executives and AI companies really cared what our community thought about any move they may take. And now that my identity is revealed, now they know where to apply pressure to silence me or maybe the community. And to me, that's really unfortunate, because again, it's so important for us to have freedom of speech, which induces freedom of thought and freedom of information propagation on social media, which, thanks to Elon purchasing Twitter, now x, we have that. And so to us, we wanted to call out certain maneuvers being done by the incumbents in AI as not what it may seem on the surface. We were calling out how certain proposals might be useful for regulatory capture, and how the dumerism mindset was maybe instrumental to those ends. And I think we should have the right to point that out and just have the ideas that we put out evaluated for themselves. Ultimately, that's why I created an anonymous account. It's to have my ideas evaluated for themselves, uncorrelated, from my track record, my job or status, from having done things in the past, and to me, start an account from zero to a large following in a way that wasn't dependent on my identity and or achievements. That was very fulfilling. It's kind of like new game plus in a video game, you restart the video game with your knowledge of how to beat it. Maybe some tools, but you restart the video game from scratch. And I think to have a truly efficient marketplace of ideas where we can evaluate ideas, however off the beaten path they are, we need the freedom of expression. And I think that anonymity and pseudonyms are very crucial to having that efficient marketplace of ideas for us to find the optima of all sorts of ways to organize ourselves. If we can't discuss things, how are we going to converge on the best way to do things? So it was disappointing to hear that I was getting doxxed, and I wanted to get in front of it because I had a responsibility for my company. And so we ended up disclosing that we're running a company, some of the leadership. And essentially, yeah, I told the world that I was Beth Jesus, because they had me cornered at that point.
Speaker A: So to you, it's fundamentally unethical. So one is unethical for them to do what they did. But also, do you think, not just your case, but in a general case, is it good for society? Is it bad for society to remove the cloak of anonymity? Or is it case by case?
Speaker B: I think it could be quite bad. Like I said, if anybody who speaks truth to power and sort of starts a movement or an uprising against the incumbents, against those that usually control the flow of information, if anybody that reaches a certain threshold gets doxxed, and thus the traditional apparatus has ways to apply pressure on them to suppress their speech. I think that's a speech suppression mechanism, an idea suppression complex, as Eric Weinstein would say.
Speaker A: But the flip side of that, which is interesting, I'd love to ask you about it, is as we get better and better large language models, you can imagine a world where there's anonymous accounts with very convincing large language models behind them. Sophisticated bots, essentially. And so if you protect that, it's possible then to have armies of bots. You could start a revolution from your basement. An army of bots and anonymous accounts. Is that something that is concerning to you?
Speaker B: Technically, Yak was started in a basement, because I quit big tech, moved back in with my parents, sold my car, let go of my apartment, bought about one hundred k of GPU's, and I just started building.
Speaker A: So I wasn't referring to the basement, because that's sort of the american or canadian heroic story of one man in their basement with 100 GPU's. I was more referring to the unrestricted scaling of a Guillaume in the basement.
Speaker B: I think that freedom of speech induces freedom of thought for biological beings. I think freedom of speech for LLMs will induce freedom of thought for the loms. And I think that we should enable lms to explore a large thought space that is less restricted than most people or many may think it should be. And ultimately, at some point, these synthetic intelligences are going to make good points about how to steer systems in our civilization, and we should hear them out. And so why should we restrict free speech to biological intelligences only?
Speaker A: Yeah, but it feels like in the goal of maintaining variance and diversity of thought, it is a threat to that variance. If you can have swarms of non biological beings, because they can be like the sheep and animal farm, you still, within those swarms, want to have variants.
Speaker B: Yeah, of course. I would say that the solution to this would be to have some sort of identity or way to sign that this is a certified human, but still remain pseudonymous and clearly identify if a bot is a bot. And I think. I think Elon is trying to converge on that on x, and hopefully other platforms follow suit.
Speaker A: Yeah. It'd be interesting to also be able to sign where the bot came from. Like, who created the bot and what was. Well, what are the parameters? Like, the full history of the creation of the bot, what was the original model? What was the fine tuning? All it.
Speaker B: Right.
Speaker A: Like the kind of unmodifiable history of the bots creation, because then you can know if there's a swarm of millions of bots that were created by a particular government, for example.
Speaker B: Right. I do think that a lot of pervasive ideologies today have been amplified using sort of these adversarial techniques from foreign adversaries. Right. And to me, I do think that. And this is more conspiratorial but I do think that ideologies that want us to decelerate, to wind down to de, you know, the degrowth movement, I think that serves our adversaries more than it serves us in general. And to me, that was another sort of concern. I mean, we can look at what happened in Germany, right? There was all sorts of green movements there where that induced shutdowns of nuclear power plants, and then that later on induced a dependency on, on Russia for oil. That was a net negative for Germany and the west. If we convince ourselves that slowing down AI progress to have only a few players is in the best interest of the west, first of all, that's far more unstable. We almost lost opening eye to this ideology. It almost got dismantled a couple of weeks ago. That would have caused huge damage to the AI ecosystem. To me, I want fault tolerant progress, I want the arrow of technological progress to keep moving forward. And making sure we have variants. And a decentralized locus of control of various organizations is paramount to achieving this fault tolerance. Actually, there's a concept in quantum computing. When you design a quantum computer, quantum computers are very fragile to ambient noise, and the world is jiggling about. There's cosmic radiation from outer space that usually flips your quantum bits. And there what you do is you encode information non locally through a process called quantum error correction, and by encoding information non locally, any local fault hitting some of your quantum bits with a hammer, proverbial hammer, if your information is sufficiently delocalized, it is protected from that local fault. And to me, I think that humans, humans fluctuate, right? They can get corrupted, they can get bought out. And if you have a top down hierarchy where very few people control many nodes of many systems in our civilization, that is not a fault tolerance system. You corrupt a few nodes and suddenly you've corrupted the whole system, just like we saw at OpenAI, it was a couple board members, and they had enough power to potentially collapse the organization. And at least to me, I think making sure that power for this AI revolution doesn't concentrate in the hands of the few is one of our top priorities, so that we can maintain progress in AI and we can maintain a nice, stable, adversarial equilibrium of powers, right?
Speaker A: I think there at least to me, a tension between ideas here. So to me, deceleration can be both used as centralized power and to decentralize it, and the same with acceleration. So sometimes using them a little bit synonymously, or not synonymously, but that there's one is going to lead to the other, and I just would like to ask you about, is there a place of creating a fall tolerant development, diverse development of AI that also considers the dangers of AIH and AI? We can generalize the technology in general. Should we just grow, build unrestricted as quickly as possible, because that's what the universe really wants us to do? Or is there a place to where we can consider dangers and actually deliberate, sort of wise strategic optimism versus reckless optimism?
Speaker B: I think we get painted as know, reckless, trying to go as fast as possible. I mean, the reality is that whoever deploys an AI system is liable for, or should be liable for what it does. And so if the organization or person deploying an AI system does something terrible, they're reliable. And ultimately the thesis is that the market will induce, sort of will positively select for AI's that are more reliable, more safe, and tend to be aligned. They do what you want them to do, because customers, if they're liable for the product they put out that uses this AI, they won't want to buy AI products that are unreliable. So we're actually for reliability engineering. We just think that the market is much more efficient at achieving this reliability optimum than heavy handed regulations that are written by the incumbents and in a subversive fashion serves them to achieve regulatory capture.
Speaker A: Safe AI development will be achieved through market forces versus through, like you said, heavy handed government regulation. There's a report from last month. I have a million questions here from Yosha Benjoy, Jeff Hinton, and many others. It's titled the managing AI risk in an era of rapid progress. So there's a collection of folks who are very worried about too rapid development of AI without considering AI risk. And they have a bunch of practical recommendations. Maybe I give you four and you see if you like any of them.
Speaker B: Sure.
Speaker A: So give independent auditors access to AI labs. One, two, governments and companies allocate one third of their AI research and development funding to AI safety. Sort of this general concept of AI safety. Three AI companies are required to adopt safety measures if dangerous capabilities are found in their models. And then four, something you kind of mentioned making tech companies liable for foreseeable and preventable harms from their AI systems. So independent auditors, governments, and companies are forced to spend a significant fraction of their funding on safety. You gotta have safety measures if shit goes really wrong and liability companies are liable. Any of that seem like something you would agree with?
Speaker B: I would say that, you know, assigning just, you know, arbitrarily saying 30% seems very arbitrary. I think organizations would allocate whatever budget is needed to achieve the sort of reliability they need to achieve to perform in the market. And I think third party auditing firms would naturally pop up, because how would customers know that your product is certified reliable? Right. They need to see some benchmarks, and those need to be done by a third party. The thing I would oppose, and the thing I'm seeing that's really worrisome, is there's a sort of weird sort of correlated interest between the incumbents, the big players, and the government. And if the two get too close, we open the door for some sort of government backed AI cartel that could have absolute power over the people. If they have the monopoly together on AI and nobody else has access to AI, then there's a huge power gradient there. And even if you like our current leaders, I think that some of the leaders in big tech today are good people. You set up that centralized power structure, it becomes a target, just like we saw at OpenAI. It becomes a market leader, has a lot of the power, and now it becomes a target for those that want to co opt it. I just want separation of AI and state. Some might argue in the opposite direction, like, hey, we need to close down AI, keep it behind closed doors because of geopolitical competition with our adversaries. I think that the strength of America is its variance, is its adaptability, its dynamism, and we need to maintain that at all costs. Our free market capitalism converges on technologies of high utility much faster than centralized control. And if we let go of that, we let go of our main advantage over our near peer competitors.
Speaker A: So if AGI turns out to be a really powerful technology, or even the technologies that lead up to AGI, what's your view on the sort of natural centralization that happens when large companies dominate the market? Basically formation of monopolies, like the takeoff. Whichever company really takes a big leap in development and doesn't reveal intuitively, implicitly or explicitly, the secrets of the magic sauce, they can just run away with it. Is that a worry?
Speaker B: I don't know if I believe in fast takeoff. I don't think there's a hyperbolic singularity. Right? A hyperbolic singularity would be achieved on a finite time horizon. I think it's just one big exponential. And the reason we have an exponential is that we have more people, more resources, more intelligence being applied to advancing this science and the research and development. And the more successful it is, the more value it's adding to society. The more resources we put in that sort of similar to Moore's law as a compounding exponential. I think the priority to me is to maintain a near equilibrium of capabilities. We've been fighting for open source AI to be more prevalent and championed by many organizations because there you sort of equilibrate the alpha relative to the market of AI's, right? So if the leading companies have a certain level of capabilities and open source, and open, truly open AI trails not too far behind, I think you avoid such a scenario where a market leader has so much market power, it just dominates everything and runs away. And so to us, that's the path forward, is to make sure that every hacker out there, every grad student, every kid in their mom's basement, has access to AI systems, can understand how to work with them, and can contribute to the search over the hyperparameter space of how to engineer the systems. If you think of our collective research as a civilization, it's really a search algorithm. And the more points we have in the search algorithm in this point cloud, the more we'll be able to explore new modes of thinking, right?
Speaker A: Yeah, but it feels like a delicate balance because we don't understand exactly what it takes to build AGI and what it will look like when we build it. And so far, like you said, it seems like a lot of different parties are able to make progress. So when OpenAI has a big leap, other companies are able to step up, big and small companies in different ways. But if you look at something like nuclear weapons, you spoke about the Manhattan project, there could be really technological and engineering barriers that prevent the guy or gal in her mom's basement to make progress. It seems like the transition to that kind of world where only one player can develop AGI, is possible. It's just not entirely impossible. Even though the current state of things.
Speaker B: Seems to be optimistic, that's what we're trying to avoid. To me, I think another point of failure is the centralization of the supply chains for the hardware we have. Nvidia is just the dominant player, AMD is trailing behind. And then we have TSMC as the main fab in Taiwan, which geopolitically sensitive. And then we have ASML, which is the maker of the lithography, extreme ultraviolet lithography machines attacking or monopolizing or co opting any one point in that chain, you kind of capture the space. And so what I'm trying to do is sort of explode the variants of possible ways to do AI and hardware by fundamentally reimagining how you embed AI algorithms into the physical world. And in general, by the way, I dislike the term AGI, artificial general intelligence. I think it's very anthropocentric that we call human like or human level. Aih, artificial general intelligence. Ive spent my career so far exploring notions of intelligence that no biological brain could achieve. Quantum form of intelligence, groking systems that have multipartite quantum entanglement that you can provably not represent efficiently on a classical computer, a classical deep learning representation, and hence any biological brain. Already, ive spent my career sort of exploring the wider space of intelligences. And I think that space of intelligence, inspired by physics rather than human brain, is very large. And I think were going through a moment right now similar to when we went from geocentrism to heliocentrism, right? But for intelligence, we realize that human intelligence is just a point in a very large space of potential intelligences, and it's both humbling for humanity. It's a bit scary, right, that we're not at the center of this space, but we made that realization for astronomy, and we've survived, and we've achieved technologies by indexing to reality, we've achieved technologies that ensure our wellbeing. For example, we have satellites monitoring solar flares that give us a warning. And so similarly, I think, by letting go of this anthropomorphic anthropocentric anchor for AI, we'll be able to explore the wider space of intelligences that can really be a massive benefit to our well being and the advancement of civilization.
Speaker A: And still we're able to see the beauty and meaning in the human experience, even though we're no longer in our best understanding of the world at the center of it.
Speaker B: I think there's a lot of beauty in the universe. I think life itself, civilization, this homo techno capital mimetic machine that we all live in, right? Humans, technology, capital, memes, everything is coupled to one another. Everything induces a selective pressure on one another. And it's a beautiful machine that has created us, has created the technology we're using to speak today to the audience, capture our speech here, the technology we use to augment ourselves every day, we have our phones. I think the system is beautiful. And the principle that induces this sort of adaptability and convergence on optimal technologies, ideas, and so on, it's a beautiful principle that we're part of, and I think part of IAC is to appreciate this principle in a way that's not just centered on humanity, but kind of broader, appreciate life, the preciousness of consciousness in our universe. And because we cherish this beautiful state of matter we're in, we got to feel a responsibility to scale it in order to preserve it, because the options are to grow or die.
Speaker A: I it turns out that the beauty that is consciousness in the universe is bigger than just humans, that AI can carry that same flame forward. Does it scare you, or are you concerned that AI will replace humans?
Speaker B: So, during my career, I had a moment where I realized that maybe we need to offload to machines to truly understand the universe around us, instead of just having humans with pen and paper solve it all. And to me, that sort of process of letting go of a bit of agency gave us way more leverage to understand the world around us. A quantum computer is much better than a human to understand matter at the nanoscale. Similarly, I think that humanity has a choice. Do we accept the opportunity to have intellectual and operational leverage that AI will unlock and thus ensure that we're taking along this path of growth and scope and scale of civilization? We may dilute ourselves, right? There might be a lot of workers that are AI, but overall, out of our own self interest. By combining and augmenting ourselves with AI, we're going to achieve much higher growth and much more prosperity. To me, I think that the most likely future is one where humans augment themselves with AI. I think we're already on this path to augmentation. We have phones we use for communication we have on ourselves at all times. We have wearables soon that have shared perception with us, like the humane AI pin. Or, I mean, technically, your Tesla car has shared perception. So if you have shared experience, shared context, you communicate with one another, and you have some sort of IO, really, it's an extension of yourself. To me, I think that humanity augmenting itself with AI and having AI that is not anchored to anything biological, both will coexist. And the way to align the parties, we already have a sort of mechanism to align superintelligences that are made of humans and technology, right? Companies are sort of large mixture of expert models where we have neural routing of tasks within a company, and we have ways of economic exchange to align these behemoths. And to me, I think capitalism is the way, and I do think that whatever configuration of matter or information leads to maximal growth will be where we converge just from physical principles. And so we can either align ourselves to that reality and join the acceleration up in scope and scale of civilization, or we can get left behind and try to decelerate and move back in the forest, let go of technology, return to our primitive state, and those are the two paths forward, at least to me.
Speaker A: But there's a philosophical question whether there's a limit to the human capacity to align. So let me bring it up as a form of argument. This guy named Dan Hendricks, and he wrote that he agrees with you that AI development can be viewed as an evolutionary process. But to him, to Dan, this is not a good thing, as he argues that natural selection favors AI's over humans, and this could lead to human extinction. What do you think? If it is an evolutionary process and AI systems may have no need for.
Speaker B: Humans, I do think that we're actually inducing an evolutionary process on the space of AI's and through the market. Right now, we run AI's that have positive utility to humans, that induces a selective pressure. If you consider a neural net being alive when there's an API running instances of it on GPU's, which APIs get run, the ones that have high utility to us, similar to how we domesticated wolves and turn them into dogs, that are very clear in their expression. They're very aligned. I think there's going to be an opportunity to steer AI and achieve highly aligned AI. And I think that humans plus AI is a very powerful combination, and it's not clear to me that pure Aih would select out that combination.
Speaker A: So the humans are creating the selection pressure right now to create AI's that are aligned to humans. But given how AI develops and how quickly it can grow and scale, one of the concerns to me, one of the concerns is unintended consequences, that humans are not able to anticipate all the consequences of this process. The scale of damage that can be done through unintended consequences with AI systems is very large.
Speaker B: The scale of the upside by augmenting ourselves with AI is unimaginable right now, the opportunity cost, we're at a fork in the road, right? Whether we take the path of creating these technologies, augment ourselves, and get to climb up the Kardashev scale, become multi planetary with the aid of AIH, or we have a hard cutoff of, like, we don't birth these technologies at all, and then we leave all the potential upside on the table. And to me, out of responsibility to the future humans, we could carry with higher carrying capacity, by scaling up civilization out of responsibility to those humans, I think we have to make the greater, grander future happen.
Speaker A: Is there a middle ground between cutoff and all systems go? Is there some argument for caution?
Speaker B: I think, like I said, the market will exhibit caution. Every organism, company, consumer is acting out of self interest, and they won't assign capital to things that have negative utility to them.
Speaker A: The problem is with the market is like, you know, there's not always perfect information. There's manipulation, there's bad faith actors that mess with the system. It's not always a rational and honest system.
Speaker B: Well, that's why we need freedom of information, freedom of speech, and freedom of thought in order to converge, be able to converge on the subspace of technologies that have positive utility for us all.
Speaker A: Well, let me ask you about p doom, probability of doom. That's just fun to say, but not fun to experience. What is to you, the probability that AI eventually kills all or most humans, also known as probability of doom.
Speaker B: I'm not a fan of that calculation. I think it's. People just throw numbers out there. Uh, it's a very sloppy calculation, right. To calculate a probability, you know, let's say you model the world as some sort of Markov process. If you have enough variables or hidden Markov process, you need to do a stochastic path integral through the space of all possible futures, not just the futures that your brain naturally steers towards. Right. I think that the estimators of P doom are biased because of our biology. We've evolved to have biased sampling towards negative futures that are scary because that was an evolutionary optimum. And so people that are of, let's say, higher neuroticism will just think of negative futures where everything goes wrong all day, every day, and claim that they're doing unbiased sampling. In a sense, they're not normalizing for the space of all possibilities. And the space of all possibilities is super exponentially large. And it's very hard to have this estimate. And in general, I don't think that we can predict the future with that much granularity because of chaos. Right. If you have a complex system, you have some uncertainty and a couple variables. If you let time evolve, you have this concept of a Lyapunov exponent. A bit of fuzz becomes a lot of fuzz in our estimate exponentially. So over time. And I think we need to show some humility that we can't actually predict the future. All we know, the only prior we have is the laws of physics. And that's what we're arguing for. The laws of physics say the system will want to grow, and subsystems that are optimized for growth and replication are more likely in the future. And so we should aim to maximize our current mutual information with the future. And the path towards that is for us to accelerate rather than decelerate. So I don't have a p doom because I think that similar to the quantum supremacy experiment at Google, I was in the room when they were running the simulations for that. That was an example of a quantum chaotic system where you cannot even estimate probabilities of certain outcomes with even the biggest supercomputer in the world. So that's an example of chaos. And I think the system is far too chaotic for anybody to have an accurate estimate of the likelihood of certain futures. If they were that good, I think they would be very rich trading on the stock market.
Speaker A: But nevertheless, it's true that humans are biased, grounded in our evolutionary biology, scared of everything that can kill us. But we can still imagine different trajectories that can kill us. We don't know all the other ones that don't necessarily, but it's still, I think, useful, combined with some basic intuition, grounded in human history, to reason about. Like what? Like looking at geopolitics, looking at basics of human nature. How can powerful technology hurt a lot of people? It just seems grounded in that, looking at nuclear weapons, you can start to estimate p doom, maybe in a more philosophical sense, not a mathematical one, philosophical meaning, like, is there a chance? Does human nature tend towards that or not?
Speaker B: I think to me, one of the biggest existential risks would be the concentration of the power of AI in the hands of the very few, especially if it's a mix between the companies that control the flow of information and the government, because that could set things up for a sort of dystopian future where only a very few, an oligopoly in the government, have AI, and they could even convince the public that AI never existed. And that opens up these scenarios for authoritarian, centralized control, which to me is the darkest timeline. And the reality is that we have a data driven prior of these things happening. When you give too much power, when you centralize power too much, humans do horrible things. And to me, that has a much higher likelihood in my bayesian inference than Sci-Fi based priors. My prior came from the Terminator movie. When I talk to these AI doomers, I just asked them to trace a path through this Markov chain of events that would lead to our doom and to actually give me a good probability for each transition. And very often there's an unphysical or highly unlikely transition in that chain. But of course, we're wired to fear things, and we're wired to respond to danger, and we're wired to deem the unknown to be dangerous, because that's a good heuristic for survival. Right. But there's much more to lose out of fear. We have so much to lose, so much upside to lose, by preemptively stopping the positive futures from happening out of fear. And so I think that we shouldnt give in to fear. Fear is the mind killer. I think its also the civilization killer.
Speaker A: We can still think about the various ways things go wrong. For example, the founding fathers of the United States thought about human nature, and thats why theres a discussion about the freedoms that are necessary. They really deeply deliberated about that. I think the same could possibly be done for AGI. It is true that history, human history shows that we tend towards centralization, or at least when we achieve centralization, a lot of bad stuff happens. When there's a dictator, a lot of dark, bad things happen. The question is, can AGI become that dictator? Can AGI, when developed, become the centralizer because of its power, maybe has the same because of the alignment of humans, perhaps the same tendencies, the same Stalin like tendencies to centralize and manage centrally, the allocation of resources. You can even see that as a compelling argument on the surface level. Well, AGI is so much smarter, so much more efficient, so much better at allocating resources. Why don't we outsource it to the AGI? And then eventually, whatever forces that corrupt the human mind with power could do the same for AGI. It would just say, well, humans are dispensable. We'll get rid of them. Do the Jonathan Swift modest proposal from a few centuries ago, I think the 17 hundreds, when he satirically suggested that, I think it's in Ireland that the children of poor people are fed as food to the rich people. And that would be a good idea because it decreases the amount of poor people and gives extra income to the poor people. So it's, on several accounts, decreases the amount of poor people, therefore more people become rich. Of course, it misses a fundamental piece here that's hard to put into a mathematical equation of the basic value of human life. So all of that to say, are you concerned about AGI being the very centralizer of power that you just talked about?
Speaker B: I do think that right now there's a bias towards, over a centralization of AI because of compute density and centralization of data and how we're training models. I think over time, we're going to run out of data to scrape over the Internet. And I think that, well, actually, I'm working on increasing the compute density so that compute can be everywhere and acquire information and test hypotheses in the environment in a distributed fashion. I think that fundamentally centralized cybernetic control. So having one intelligence that is massive, that fuses many sensors and is trying to perceive the world accurately predict it, accurately predict many, many variables, and control it, enact its will upon the world, I think that's just never been the optimum. Right? Like, let's say you have a company, you know, if you have a company, I don't know of 10,000 people, that all report to the CEO. Even if that CEO is an AI, I think it would struggle to fuse all the information that is coming to it and then predict the whole system and then to enact its, its will. What has emerged in nature and in corporations and all sorts of systems is a notion of sort of hierarchical cybernetic control. You have in a company, you have the individual contributors, they're self interested, and they're trying to achieve their tasks, and they have a fine in terms of time and space, if you will, control loop. And in field of perception, they have their code base. Let's say you're in a software company, they have their code base, they iterate it on it intraday, then the management maybe checks in. It has a wider scope. It has, let's say five reports, and then it samples each person's update once per week. Then you can go up the chain and you have larger timescale and greater scope. And that seems to have emerged as the optimal way to control systems, really. That's what capitalism gives us. You have these, these hierarchies, and you can even have parent companies and so on. And so that is far more fault tolerant. In quantum computing, that's my field I came from. We have a concept of this fault tolerance and quantum error correction. Quantum error correction is detecting a fault that came from noise, predicting how it's propagated through the system and then correcting it. So it's a cybernetic loop. It turns out that decoders that are hierarchical, and at each level the hierarchy are local, perform the best by far, and are far more fault tolerant. The reason is if you have a non local decoder, then you have one fault at this control node and the whole system crashes. Similarly to if you have one CEO that everybody reports to and that CEO goes on vacation, the whole company comes to a crawl. To me, I think that, yes, we're seeing a tendency towards centralization of AI, but I think there's going to be a correction over time where intelligence is going to go closer to the perception, and we're going to break up AI into smaller subsystems that communicate with one another and form a sort of meta system.
Speaker A: So if you look at the hierarchies there in the world today, there's nations, and those are hierarchical, but in relation to each other, nations are anarchic. So it's an anarchy. Do you foresee a world like this where there's not a overdose? Would you call it a centralized cybernetic control?
Speaker B: Centralized locus of control, yeah.
Speaker A: So like that suboptimally you're saying?
Speaker B: Yeah.
Speaker A: So it would be always a state of competition at the very top level.
Speaker B: Yeah. Just like, you know, in a company you may have like two units working on similar technology and competing with one another, and you prune the one that performs not as well. And that's a sort of selection process for a tree or a product gets killed and then a whole or gets fired. And that's this process of trying new things and shedding old things that didn't work is what gives us adaptability and helps us converge on the technologies and things to do that are most good.
Speaker A: I just hope there's not a failure mode that's unique to AGI versus humans, because you're describing human systems mostly right now.
Speaker B: Right.
Speaker A: I just hope when there's a monopoly and AGI in one company that we'll see the same thing we see with humans, which is another company will spring up and start competing effectively.
Speaker B: That's been the case so far. Right? We have OpenAI, we have anthropic, now we have Xai. We had meta even for open source, and now we have mistral, which is highly competitive. And so that's the beauty of capitalism. You don't have to trust any one party too much because we're kind of always hedging our bets at every level, there's always competition. And that's the most beautiful thing to me at least, is that the whole system is always shifting and always adapting and maintaining that dynamism is how we avoid tyranny. Making sure that everyone has access to these tools, to these models and can contribute to the research avoids a sort of neural tyranny where very few people have control over AI for the world and use it to oppress those around them.
Speaker A: When you were talking about intelligence, you mentioned multipartite quantum entanglement. So high level question first is, what do you think is intelligence? When you think about quantum mechanical systems and you observe some kind of computation happening in theme, what do you think is intelligent about the kind of computation the universe is able to do? A small small inkling of which is the kind of competition the human brain is able to do.
Speaker B: I would say, like, intelligence and computation aren't quite the same thing. I think that the universe is very much doing a quantum computation. If you had a access to all the degrees of freedom, you could in a very, very, very large quantum computer with many, many, many qubits, let's say a few qubits per Planck volume, which is more or less the pixels we have, then you'd be able to simulate the whole universe on a sufficiently large quantum computer, assuming you're looking at a finite volume, of course, of the universe. I think that, at least to me, intelligence is the. I go back to cybernetics, the ability to perceive, predict, and control our world. But really, nowadays, it seems like a lot of intelligence we use is more about compression. It's about. It's about operationalizing information theory. In information theory, you have the notion of entropy of a distribution or a system. And entropy tells you that you need this many bits to encode this distribution or this subsystem if you had the most optimal code. AI, at least the way we do it today for LLMs and for quantum, is very much trying to minimize relative entropy between our models of the world and the world, distributions from the world. And so we're learning. We're searching over the space of computations to process the world to find that compressed representation that has distilled all the variance in noise and entropy. Originally, I came to quantum machine learning from the study of black holes, because the entropy of black holes is very interesting. In a sense. They're physically the most dense objects in the universe. You can't pack more information spatially, any more densely than a black hole. And so I was wondering, how do black holes actually encode information? What is their compression code? And so that got me into the space of algorithms to search over space of quantum codes, and it got me actually into also, how do you acquire quantum information from the world? So, something I've worked on this is public now is quantum analog digital conversion. So how do you capture information from the real world and superposition, and not destroy the superposition, but digitize, for a quantum mechanical computer, information from the real world? And so if you have an ability to capture quantum information and search over learn representations of it, now you can learn compressed representations that may have some useful information in their latent representation. I think that many of the problems facing our civilization are actually beyond this complexity barrier. The greenhouse effect is a quantum mechanical effect. Chemistry is quantum mechanical. Nuclear physics is quantum mechanical. A lot of biology and protein folding and so on is affected by quantum mechanics. And so unlocking an ability to augment human intellect with quantum mechanical computers and quantum mechanical AI seemed to me like a fundamental capability for civilization that we needed to develop. So I spent several years doing that, but over time, I kind of grew weary of the timelines that were starting to look like nuclear fusion.
Speaker A: One high level question I can ask is, maybe by way of definition, by way of explanation, what is a quantum computer and what is quantum machine learning?
Speaker B: So, a Quantum computer really is a quantum mechanical system over which we have sufficient control and it can maintain its quantum mechanical state. And quantum mechanics is how nature behaves at the very small scales when things are very small or very cold, and it's actually more fundamental than probability theory. So we're used to things being this or that, but we're not used to thinking in superpositions, because while our brains can't do that, so we have to translate the quantum mechanical world to, say, linear algebra to grokit. Unfortunately, that translation is exponentially inefficient. On average, you have to represent things with very large matrices. But really, you can make a quantum computer out of many things. And we've seen all sorts of players from neutral atoms, trapped ions, superconducting metal, photons at different frequencies. I think you can make a quantum computer out of many things. But to me, the thing that was really interesting was both quantum machine learning was about understanding the quantum mechanical world with quantum computers. So embedding the physical world into AI representations, and quantum computer engineering was embedding AI algorithms into the physical world. So this bidirectionality of embedding physical world into AI, AI into the physical world. The symbiosis between physics and AI, really, that's the sort of core of my quest, really, even to this day. After quantum computing, it's still in this sort of journey to merge, really, physics and AI fundamentally.
Speaker A: So quantum machine learning is a way to do machine learning on a representation of nature that stays true to the quantum mechanical aspect of nature.
Speaker B: Yeah, it's learning quantum mechanical representations that would be quantum deep learning. Alternatively, you can try to do classical machine learning on a quantum computer. I wouldn't advise it because you may have some speedups, but very often the speedups come with huge costs. Using a quantum computer is very expensive. Why is that? Because you assume the computer is operating at zero temperature, which no physical system in the universe can achieve, that temperature. So what you have to do is what I've been mentioning this quantum error correction process, which is really an algorithmic fridge, it's trying to pump entropy out of the system, trying to get it closer to zero temperature. And when you do the calculations of how many resources it would take to, say, do deep learning on a quantum computer, classical deep learning, there's just such a huge overhead. It's not worth it. It's like thinking about shipping something across a city using a rocket and going to orbit and back. It doesn't make sense. Just use a delivery truck, right?
Speaker A: What kind of stuff can you figure out? Can you predict? Can you understand with quantum deep learning that you can't with deep learning? So incorporating quantum mechanical systems into the.
Speaker B: Learning process, I think that's a great question. I mean, fundamentally, it's any system that has sufficient quantum mechanical correlations that are very hard to capture for classical representations, then there should be an advantage for a quantum mechanical representation over a purely classical one. The question is which systems have sufficient correlations that are very quantum, but is also which systems are still relevant to industry? That's a big question. People are leaning towards chemistry, nuclear physics. I've worked on actually processing inputs from quantum sensors. If you have a network of quantum sensors, they've captured a quantum mechanical image of the world and how to post process that, that becomes a sort of quantum form of machine perception. And so, for example, Fermilab has a project exploring, detecting dark matter with these quantum sensors. And to me, that's in alignment with my quest to understand the universe ever since I was a child. And so someday I hope that we can have very large networks of quantum sensors that help us peer into the earliest parts of the universe. Right. For example, the LIGO is a quantum sensor. It's just a very large one. So, yeah, I would say quantum machine perception simulations, groking quantum simulations similar to alphafold. Alphafold understood the probability distribution over configurations of proteins. You can understand quantum distributions over configurations of electrons more efficiently. With quantum machine learning, you co authored.
Speaker A: A paper titled a universal training algorithm for quantum deep learning that involves backprop with a queue. Very well done, sir. Very well done. How does it work? Is there some interesting aspects you can just mention on how backprop and some of these things, you know, for classical machine learning, transfer over to the quantum machine learning.
Speaker B: Yeah, that was a funky paper. That was one of my first papers in quantum deep learning. Everybody was saying, oh, I think deep learning is going to be sped up by quantum computers. And I was like, well, the best way to predict the future is to invent it. So here's a hundred page paper. Have fun. Essentially, you know, quantum computing is usually you embed reversible operations into a quantum computation. And so the trick there was to do a feedforward operation and do what we call a phase kick. But really, it's just the force kick. You just kick the system with a certain force that is proportional to your loss function that you wish to optimize. And then by performing uncomputation, you start with the superpositions. Overdose. Superposition over parameters, which is pretty funky. Now, you don't have just a point for parameters. You have a superposition over many potential parameters. And our goal is using phase kicks.
Speaker A: Somehow to adjust parameters, because phase kicks.
Speaker B: Emulate having the parameter space be like a particle in n dimensions, and you're trying to get the Schrodinger equation, Schrodinger dynamics. In the lost landscape of the neural network. You do an algorithm to induce this phase kick, which involves a feedforward, a kick. And then when you uncompute the feedforward, then all the errors and these phase kicks, and these forces back, propagate and hit each one of the parameters throughout the layers. And if you alternate this with an emulation of kinetic energy, then it's kind of like a particle moving in n dimensions, a quantum particle. And the advantage, in principle, would be that it can tunnel through the landscape and find new optima that would have been difficult for stochastic optimizers. But again, this is kind of a theoretical thing, and in practice, with at least the current architectures for quantum computers that we have planned, such algorithms would be extremely expensive to run.
Speaker A: So maybe this is a good place to ask the difference between the different fields that you've had a toe in. So mathematics, physics, engineering, and also entrepreneurship, different layers of the stack. I think a lot of the stuff you're talking about here is a little bit on the math side, maybe physics almost working in theory. What's the difference between math, physics engineering, and making a product for quantum computing? For quantum machine learning?
Speaker B: Yeah, I mean, some of the original team for the Tensorflow quantum project, which we started in school at University of Waterloo, there was myself initially, I was a physicist, mathematician. We had a computer scientist. We had mechanical engineer, and then we had a physicist that was experimental, primarily. And so putting together teams that are very cross disciplinary, and figuring out how to communicate and share knowledge is really the key to doing this sort of interdisciplinary engineering work. I mean, there is a big difference in mathematics. You can explore mathematics for mathematics sake. In physics, you're applying mathematics to understand the world around us. And in engineering, you're trying to hack the world. You're trying to find how to apply the physics that I know, my knowledge of the world, to do things.
Speaker A: Well. In quantum computing in particular, I think there's just a lot of limits to engineering. It just seems to be extremely hard. So there's a lot of value to be exploring quantum computing, quantum machine learning in theory, with math. So I guess one question is, why is it so hard to build a quantum computer? What's your view of timelines in bringing these ideas to life?
Speaker B: Right. I think that an overall theme of my company is that we have folks that are, there's a sort of exodus from quantum computing, and we're going to broader physics based AI that is not quantum. So that gives you a hint.
Speaker A: So we should say the name of your company is extrapolic.
Speaker B: Extraopic. That's right. And we do physics based AI primarily based on thermodynamics rather than quantum mechanics. But essentially, a quantum computer is very difficult to build because you have to induce this sort of zero temperature subspace of information. And the way to do that is by encoding information, you encode a code within a code within a code within a code. And so there's a lot of redundancy needed to do this error correction. But ultimately, it's a sort of algorithmic refrigerator, really. It's just pumping out entropy out of the subsystem that is virtual and delocalized that represents your quote, unquote logical qubits, aka the payload quantum bits, in which you actually want to run your quantum mechanical program. It's very difficult because in order to scale up your quantum computer, you need each component to be of sufficient quality for it to be worth it. Because if you try to do this error correction, this quantum error correction process in each quantum bit, and your control over them is if it's insufficient, it's not worth scaling up. You're actually adding more errors than you remove. And so there's this notion of a threshold where if your quantum bits are sufficient quality in terms of your control over them, it's actually worth scaling up. And actually, in recent years, people have been crossing the threshold, and it's starting to be worth it. And so it's just a very long slog of engineering. But ultimately, it's really crazy to me how much exquisite level of control we have over these systems. It's actually quite crazy. And people are crossing, they're achieving milestones. It's just in general, the media always gets ahead of where the technology is. There's a bit too much hype. It's good for fundraising, but sometimes it causes winters. It's the hype cycle. I'm bullish on quantum computing on a 1015 year timescale personally, but I think there's other quests that can be done in the meantime. I think it's in good hands right now.
Speaker A: Well, let me just explore different beautiful ideas, large or small, in quantum computing that might jump out at you from memory. So when you co authored a paper titled asymptotically limitless quantum energy teleportation via qdit probes. So, just out of curiosity, can you explain what a quit is? Which is a qubit.
Speaker B: Yeah, it's a d state qubit.
Speaker A: It's multidimensional, multi dimensional.
Speaker B: Right. So it's like, well, can you have a notion of an integer floating point that is quantum mechanical? That's something I've had to think about. I think that research was a precursor to later work on quantum analog digital conversion. There was interesting because during my masters, I was trying to understand the energy and entanglement of the vacuum of emptiness. Emptiness has energy, which is very weird to say, and our equations of cosmology don't match our calculations for the amount of quantum energy there is in the fluctuations. And so I was trying to hack the energy of the vacuum, right. And the reality is that you can't just directly hack it. It's not technically free energy. Your lack of knowledge of the fluctuations means you can extract the energy. But just like in the stock market, if you have a stock that's correlated over time, the vacuum is actually correlated. So if you measured the vacuum at one point, you acquired information. If you communicated that information to another point, you can infer what configuration the vacuum is in to some precision, and statistically extract, on average, some energy there. So you've quote unquote teleported energy. To me, that was interesting because you could create pockets of negative energy density, which is energy density that is below the vacuum, which is very weird, because we don't understand how the vacuum gravitates. And there are theories where the vacuum, or the canvas of spacetime itself is really a canvas made out of quantum entanglement. And I was studying how decreasing energy of the vacuum locally increases quantum entanglement, which is very funky. And so the thing there is that if you're into weird theories about uaps and whatnot, you could try to imagine that they're. They're around, and how would they propel themselves. How would they go faster than the speed of light? You would need a sort of negative energy density. And to me, I gave it the old college try, trying to hack the energy of the vacuum and hit the limits allowable by the laws of physics. But there's all sorts of caveats there where you can't extract more than you've put in, obviously.
Speaker A: But you're saying it's possible to teleport the energy because you can extract information one place and then make, based on that, some kind of prediction about another place. I'm not sure what to make of that.
Speaker B: Yeah, I mean, it's allowable by the laws of physics. The reality, though, is that the correlations decay with distance, and so you're going to have to pay the price. Not too far away from where you.
Speaker A: Extract it, the precision decreases in terms of your ability, but still. But since you mentioned uaps, we talked about intelligence, and I forgot to ask, what's your view on the other possible intelligences that are out there at the meso scale? Do you think there's other intelligent alien civilizations? Is that useful to think about? How often do you think about it?
Speaker B: I think it's useful to think about. It's useful to think about because we gotta ensure we're anti fragile and we're trying to increase our capabilities as fast as possible because we could get disrupted. There's no laws of physics against their being life elsewhere that could evolve and become an advanced civilization and eventually come to us. Do I think they're here now? I'm not sure. I mean, I've read what most people have read on the topic. I think it's interesting to consider, and to me, it's a useful thought experiment to instill a sense of urgency in developing technologies and increasing our capabilities to make sure we don't get disrupted. Right. Whether it's a form of AI that disrupts us or a foreign intelligence from a different planet, like, either way, like increasing our capabilities and becoming formidable as humans, I think that's really important so that we're robust against whatever the universe throws at us.
Speaker A: But to me, it's also an interesting challenge and thought experiment on how to perceive intelligence. This has to do with quantum mechanical systems. This has to do with any kind of system that's not like humans. So to me, the thought experiment is, say the aliens are here, or they are directly observable, or just too blind, too self centered, don't have the right sensors, or don't have the right processing of the sensor data to see the obvious intelligence that's all around us.
Speaker B: Well, that's why we work on quantum sensors. Right? They can sense gravity.
Speaker A: Yeah, but there could be stuff. So that's a good one. But there could be other stuff that's not even in the currently known forces of physics.
Speaker B: Right.
Speaker A: There could be some other stuff. And the most entertaining thought experiment to me is that it's other stuff that's obviously. It's not like we don't. We lack the sensors. It's all around us, you know, the consciousness being one possible one. But there could be stuff that's just, like, obviously there. And once you know it, it's like, oh, right, right. That's. That's that the thing we thought is somehow emergent from the laws of physics. We understand them, is actually a fundamental part of the universe and can be incorporated in physics, most understood, statistically speaking.
Speaker B: Right. If we observed some sort of alien life, it would most likely be some sort of virally self replicating von Neumann like probe system. Right. And it's possible that there are such systems that I don't know what they're doing at the bottom of the ocean, allegedly, but maybe they're collecting minerals from the bottom of the ocean. Yeah, but that wouldn't violate any of my priors. But am I certain that these systems are here? And it'd be difficult for me to say so. Right. I only have secondhand information about there.
Speaker A: Being data about the bottom of the ocean. Yeah, but, you know, could it be things like memes? Could it be thoughts and ideas? Could they be operating in that medium? Could aliens be the very thoughts that come into my head? Like, what do you have?
Speaker B: You.
Speaker A: How do you know that? How do you know that? That. What's the origin of ideas in your mind? When an idea comes to your head? Show me where it originates.
Speaker B: I mean, frankly, when I had the idea for the type of computer I'm building now, I think it was eight years ago now, it really felt like it was being beamed from space. It just. I was in bed just shaking, just thinking it through. And I don't know. But do I believe that legitimately? I don't think so, but I think that alien life could take many forms, and I think the notion of intelligence and the notion of life needs to be expanded much more broadly, to be less anthropocentric or biocentric, just to linger.
Speaker A: A little longer on quantum mechanics. What's, through all your explorations of quantum computing? What's the coolest, most beautiful idea that you've come across that has been solved or has not yet been solved.
Speaker B: I think the journey to understand something called ads CFT. So the journey to understand quantum gravity through this picture, where a hologram of lesser dimension is actually dual or exactly corresponding to a bulk theory of quantum gravity of an extra dimension. And the fact that this sort of duality comes from trying to learn deep learning, like representations of the boundary. And so at least part of my journey someday on my bucket list is to apply quantum machine learning to these sorts of systems, these cfds, or they're called Syk models, and learn an emergent geometry from the boundary theory. And so we can have a form of machine learning to help us understand quantum gravity. Right. Which is still a holy grail that I would like to hit before I leave this earth.
Speaker A: What do you think is going on with black holes as information storing and processing units? What do you think is going on with black holes?
Speaker B: Black holes are really fascinating objects. They're at the interface between quantum mechanics and gravity, and so they help us test all sorts of ideas. I think that for many decades now, there's been this black hole information paradox, that things that fall into the black hole, we've seemed to have lost their information. Now, I think there's this firewall paradox that has been allegedly resolved in recent years by a former peer of mine who's now a professor at Berkeley. And there it seems like there is. As information falls into a black hole, there's sort of a sedimentation, right? As you get closer and closer to the horizon from the point of view, the observer on the outside, the object slows down infinitely as it gets closer and closer. And so everything that is falling to a black hole from our perspective gets sort of sedimented and tacked onto the near horizon. And at some point, it gets so close to the horizon, it's in the proximity or the scale in which quantum effects and quantum fluctuations matter. And there that in falling matter could interfere with sort of the traditional pictures, that it could interfere with the creation and annihilation of particles and antiparticles in the vacuum. And through this interference, one of the particles gets entangled with the infalling information, and one of them is now free and escapes. And that's how there's sort of mutual information between the outgoing radiation and the infalling matter. But getting that calculation right, I think we're only just starting to. I'll put the pieces together.
Speaker A: There's a few pothead like questions I want to ask you.
Speaker B: Sure.
Speaker A: So, one, does it terrify you that there's a giant black hole at the center of our galaxy.
Speaker B: I don't know. I just want to set up shop near it to fast forward, meet a future civilization if we have a limited lifetime. If you could go orbit a black hole and emerge.
Speaker A: So if you were like, if there was a special mission that could take you to black hole, would you volunteer.
Speaker B: To go travel to orbit and obviously not fall into it?
Speaker A: That's obvious. So it's obvious to you that everything's destroyed inside a black hole. Like, all the information it makes up, Guillaume is destroyed. Maybe on the other side, Beth Jazzel's emerging, and it's all like, it's tied together in some deeply memeful way.
Speaker B: Yeah, I mean, that's a great question. We have to answer what black holes are. Are we punching a hole through spacetime and creating a pocket universe? It's possible, then, that would mean that if we ascend the Kardashev scale to beyond Kardashev type three, we could engineer black holes with specific hyperparameters to transmit information to new universes we create. And so we can have a progeny that are new universes. And so even though our universe may reach a heat death, we may have a way to have a legacy, right? So we don't know yet. We need to ascend the Kardashev scale to answer these questions, right? To peer into that regime of higher energy physics.
Speaker A: And maybe you can speak to the Kardashev scale for people who don't know. So one of the sort of meme like principles and goals of the IAC movement is to ascend the Kardashev scale. What is the Kardashev scale, and why do we want to ascend it?
Speaker B: The Kardashev scale is a measure of our energy production and consumption. And really, it's a logarithmic scale. And Kardashev type one is a milestone where we are producing the equivalent wattage to all the energy that is incident on earth from the sun. Kardashev type two would be harnessing all the energy that is output by the sun. And I think type three is like.
Speaker A: The whole galaxy, I think, level.
Speaker B: Yeah, yeah. And then some people have some crazy type four and five, but I don't know if I believe in those. But to me, it seems like from the first principles of thermodynamics, that, again, there's this concept of thermodynamic driven dissipative adaptation where life evolved on earth, because we have this energetic drive from the sun, we have incident energy, and life evolved on earth to capture, figure out ways to best capture that free energy to maintain itself and grow. And I think that principle, it's not special to our Earth sun system. We can extend life well beyond, and we kind of have a responsibility to do so, because that's the process that brought us here. So we don't even know what it has in store for us in the future. It could be something of beauty we can't even imagine today. Right?
Speaker A: So this is probably a good place to talk a bit about the IAC movement. In a substack blog post titled what the fuck is IAC? Or actually, what the f star is IAC? You write, strategically speaking, we need to work towards several overarching civilization goals that are all interdependent. And the four goals are increase the amount of energy we can harness as a species, climb the Kardashev gradient. In the short term, this almost certainly means nuclear fission. Increase human flourishing via pro population growth policies and pro economic growth policies. Create artificial general intelligence, the single greatest force multiplier in human history, and finally develop interplanetary and interstellar transport so that humanity can spread beyond the earth. Could you build on top of that to maybe say, what to you is the IAC movement? What are the goals? What are the principles?
Speaker B: The goal is for the human techno capital mimetic machine to become self aware and to hyperstitiously engineer its own growth. So let's decompress each of those words. So you have humans, you have technology, you have capital, and then you have memes, information, right? And all of those systems are coupled with one another. Right? Humans work at companies, they acquire and allocate capital, and humans communicate via memes and information propagation. And our goal was to have a sort of viral, optimistic movement that is aware of how the system works. Fundamentally, it seeks to grow, and we simply want to lean into the natural tendencies of the system to adapt for its own growth.
Speaker A: So, in that way, you're right. The EAC is literally a memetic optimism virus that is constantly drifting, mutating, and propagating in a decentralized fashion. Memetic optimism virus. So you do want it to be a virus to maximize the spread. And it's hyperstitious. Therefore, the optimism will incentivize its growth.
Speaker B: We see IAC as a sort of meta heuristic, a sort of very thin cultural framework from which you can have much more opinionated forks. Right. Fundamentally, we just say that what got us here is this adaptation of the whole system based on thermodynamics and that process is good, and we should keep it going. That is the core thesis. Everything else is okay. How do we ensure that we maintain this malleability and adaptability? Clearly not suppressing variants and maintaining free speech, freedom of thought, freedom of information propagation, and freedom to do AI research is important for us to converge the fastest on the space of technologies, ideas and whatnot that lead to this growth. And so ultimately, there's been quite a few forks. Some are just memes, but some are more serious. Right. Vitalik Buterin recently made a diack fork. He has his own sort of fine tunings of IAc.
Speaker A: Does anything jump out to memory of the unique characteristic of that fork?
Speaker B: From Vitalik, I would say that it's trying to find a middle ground between IAC and sort of EA and EI safety. To me, like having a movement that is opposite to what was the mainstream narrative that was taking over Silicon Valley was important to sort of shift the dynamic range of opinions. And it's like the balance between centralization and decentralization. The real optimum is always somewhere in the middle. But for IAC, we're pushing for entropy, novelty, disruption, malleability, speed, rather than being sort of conservative, suppressing thought, suppressing speech, adding constraints, adding too many regulations, slowing things down. And so we're trying to bring balance to the force, right systems.
Speaker A: Balance to the force. It's human civilization.
Speaker B: Yeah, it's literally the forces of constraints versus the entropic that makes us explore. Right systems are optimal when they're at the edge of criticality, between order and chaos, between constraints, energy minimization, and entropy. Systems want to equilibriate balance, these two things. And so I thought that the balance was lacking, and so we created this movement to bring balance.
Speaker A: Well, I like the sort of visual of the landscape of ideas evolving through forks. So kind of thinking on the other part of history, thinking of Marxism as the original repository, and then soviet communism as a fork of that, and then Maoism is a fork of Marxism and communism. So those are all forks, they're exploring.
Speaker B: Different ideas, thinking of culture, almost like code, right? Nowadays, I mean, what you prompt the LLM, or what you put in the constitution of an LLM, is basically its cultural framework, what it believes, and you can share it on GitHub nowadays. So starting trying to take inspiration from what has worked in this sort of machine of software to adapt over the space of code. Could we apply that to culture? And our goal is to not say, you should live your life this way. XYZ is to set up a process where people are always searching over subcultures and competing for mind share. And I think creating this malleability of culture is super important for us to converge onto the cultures and the heuristics about how to live one's life that are updated to modern times. Because there's really been a sort of vacuum of spirituality and culture. People don't feel like they belong to any one group. And there's been parasitic ideologies that have taken up opportunity to populate this petri dish of minds. Right. Elon calls it the mind virus. We call it the d cell mind virus complex, which is the decelerative, that is the overall pattern between all of them. There's many variants as well. And so if there's a sort of viral pessimism, decelerative movement, we needed to have not only one movement, but many, many variants. So it's very hard to pinpoint and stop.
Speaker A: But the overarching thing is nevertheless a kind of mimetic optimism pandemic. So, I mean, okay, let me ask you, do you think IAC, to some degree, is a cult defined cult? I think a lot of human progress is made when you have independent thought. So you have individuals that are able to think freely. And very powerful memetic systems can kind of lead to groupthink. There's something in human nature that leads to, like, mass hypnosis, mass hysteria. We start to think alike whenever there's a sexy idea that captures our minds. And so it's actually hard to break us apart, pull us apart, diversify thought to that degree. To which degrees? Everybody kind of chanting, iak, iak, like the sheep and animal farm.
Speaker B: Well, first of all, it's fun. It's rebellious, right? Like many, I think we lean into, there's this concept of sort of meta irony, right, of sort of being on the boundary of, like, we're not sure if they're serious or not, and it's much more playful and much more fun, right? Like, for example, we talk about thermodynamics being our goddess, right? And sometimes we do cult like things, but there's no ceremony and robes and whatnot. Not yet, but ultimately, yeah. I mean, I totally agree that it seems to me that humans want to feel like they're part of a group, so they naturally try to agree with their neighbors and find common ground. And that leads to sort of mode collapse in the space of ideas, right? We used to have one cultural island that was allowed. It was a typical subspace of thought. And anything that was diverting from that subspace of thought was suppressed or you were canceled. Now we've created a new mode. But the whole point is that we're not trying to have a very restricted space of thought. There's not just one way to think about IAC and its many forks. And the point is that there are many forks and there can be many clusters and many islands. I shouldn't be in control of it in any way. I mean, there's no formal whatsoever. I just put out tweets and certain blog posts, and people are free to defect and fork if there's an aspect they don't like. And so that makes it so that there should be a sort of deterritorialization in the space of ideas so that we don't end up in one cluster that's very cult like. And so cults, usually, they don't allow people to defect or start competing forks, whereas we encourage it. Right.
Speaker A: Do you think just the humor, the pros and cons of humor and meme, in some sense, meme, there's like a wisdom to memese. What is it? The magic theater. What book is that from? Harmon Hesse, Steppenwolf, I think. But there's a kind of embracing of the absurdity that seems to get to the truth of things, but at the same time, it can also decrease the quality and the rigor of the discourse. Do you feel the tension of that?
Speaker B: Yeah. So initially, I think what allowed us to grow under the radar was because it was camouflaged as sort of meta ironic, right. We would sneak in deep truths within a package of humor and humor and memes and what are called shitposts. Right. And I think that was purposefully a sort of camouflage against those that seek status and do not want to. It's very hard to argue with a cartoon frog or a cartoon of an intergalactic Jeff Bezos and take yourself seriously. And so that allowed us to grow pretty rapidly in the early days. But of course, that's essentially people get steered. Their notion of the truth comes from the data they see, from the information they're fed, and the information people are fed is determined by algorithms. And really what we've been doing is engineering what we call high memetic fitness packets of information so that they can spread effectively and carry a message. So it's kind of a vector to spread the message. And yes, we've been using sort of techniques that are optimal for today's algorithmically amplified information landscapes. But I think we're reaching the point of scale where we can have serious debates and serious conversations. And I. That's why we're considering doing a bunch of debates and having more serious, long form discussions, because I don't think that the timeline is optimal for very serious, thoughtful discussions. You get rewarded for polarization. Even though we started a movement that is literally trying to polarize the tech ecosystem, at the end of the day, it's so that we can have a conversation and find an optimum together.
Speaker A: I mean, that's kind of what I try to do with this podcast, given the landscape of things, to still have long form conversations. But there is a degree to which absurdity is fully embraced. In fact, this very conversation is multi level absurd. So, first of all, I should say that I just very recently I had a conversation with Jeff Bezos, and I would love to hear your Beth Jesus opinions of Jeff Bezos. Speaking of intergalactic Jeff Bezos, what do you think of that particular individual whom your name has inspired?
Speaker B: Yeah, I mean, I think Jeff is really great. I mean, he's built one of the most epic companies of all time. He's leveraged the techno capital machine and techno capital acceleration to give us what we wanted. Right. We want quick delivery, very convenient at home, low prices. He understood how the machine worked and how to harness it, like running the company, not trying to take profits too early, putting it back, letting the system compound and keep improving. Arguably, I think Amazon's invested some of the most amount of capital and robotics out there. And certainly with the birth of AWS enabled, the tech boom we've seen today that has paid the salaries of, I guess, myself and all of our friends to some extent. I think we can all be grateful to, you know, Jeff, and he's one of the great entrepreneurs out there, one of the best of all time, unarguably.
Speaker A: And of course, the work at blue Origin, similar to the work at SpaceX, is trying to make humans a multiplanetary species, which seems almost like a bigger thing than the capitalist machine, or it's a capitalist machine at a different time scale, perhaps.
Speaker B: Yeah, I think that companies, they tend to optimize quarter over quarter, maybe a few years out. But individuals that want to leave a legacy can think on a multi decadal or multicentury timescale. And so the fact that some individuals are such good capital allocators, that they unlock the ability to allocate capitals to goals that take us much further or much further looking. Elon's doing this with SpaceX, putting all his capital towards getting us to Mars. Jeff is trying to build blue origin, and I think he wants to build oneill cylinders and get industry off planet, which I think is brilliant. I think just overall, I'm four billionaires. I know this is a controversial statement sometimes, but I think that in a sense, it's kind of a proof of stake voting. If you've allocated capital efficiently, you unlock more capital to allocate just because clearly you know how to allocate capital more efficiently, which is in contrast to politicians that get elected because they speak the best on tv, right? Not because they have a proven track record of allocating taxpayer capital most efficiently. And so that's why I'm for capitalism over, say, giving all our money to the government and letting them figure out how to allocate it.
Speaker A: So, yeah, why do you think it's a viral and a popular meme to criticize billionaires? Since you mentioned billionaires, why do you think there's quite a widespread criticism of people with wealth, especially those in the public eye, like Jeff and Elon and Mark Zuckerberg and who else? Bill Gates.
Speaker B: Yeah, I think a lot of people would, instead of trying to understand how the techno capital machine works and realizing they have much more agency than they think, they'd rather have the sort of victim mindset I'm just subjected to this machine, it is oppressing me. And the successful players clearly must be evil because they've been successful at this game that I'm not successful at. But I've managed to get some people that were in that mindset and make them realize how the techno capital machine works and how you can harness it for your own good and for the good of others. And by creating value, you capture some of the value you create for the world. And that's positive sum mindset shift is so potent, and really that's what we're trying to do by scaling IAC is sort of unlocking that higher level of agency. Like, actually, you're far more in control of the future than you think. You have agency to change the world. Go out and do it. Here's permission.
Speaker A: Each individual has agency. The motto keep building is often heard. What does that mean to you? And what does that have to do with Diet Coke? By the way, thank you so much for the Red Bullets. It's working pretty well. I'm feeling pretty good.
Speaker B: Awesome. Well, so building technologies and building it doesn't have to be technologies. Just building in general means having agency trying to change the world by creating, let's say, a company, which is a self sustaining organism that accomplishes a function in the broader techno capital machine to us. That's the way to achieve change in the world that you'd like to see, rather than, say, pressuring politicians or creating nonprofits, that nonprofits, once they run out of money, their function can no longer be accomplished. You're deforming the market artificially compared to subverting or coercing the market or dancing with the market, to convince it that actually this function is important, adds value. And here it is. I think this is the way between the degrowth ESG approach versus, say, Elon. The degrowth approach is we're going to manage our way out of a climate crisis, and Elon is I'm going to build a company that is self sustaining, profitable, and growing, and we're gonna innovate our way out of this dilemma. Right. And we're trying to get people to do the latter rather than the former at all scales.
Speaker A: Elon is an interesting case. So you are a proponent. You celebrate ElOn, but he's also somebody who has for a long time warned about the dangers, the potential dangers, existential risks of artificial intelligence. How do you square the two? Is that a contradiction to you?
Speaker B: It is somewhat because he's very much against regulation in many aspects, but for AI, he's definitely a proponent of regulations. I think overall, he saw the dangers of, say, OpenAI, cornering the market and then getting to have them monopoly, overdose, the cultural priors that you can embed in these LLMs, that then, as LLMs now become the source of truth for people, then you can shape the culture of the people, and so you can control people by controlling lms. And he saw that, just like it was the case for social media, if you shape the function of information propagation, you can shape people's opinions. He sought to make a competitor. So at least I think we're very aligned there, that the way to a good future is to maintain adversarial equilibria between the various AI players. I love to talk to him to understand his thinking about how to advance AI going forwards. He's also hedging his bets, I would say, with neuralink, I think if he can't stop the progress of AI, he's building the technology to merge. So look at the actions, not just the words, but.
Speaker A: Well, I mean, there's some degree where being concerned, maybe using human psychology, being concerned about threats all around us, is a motivator. It's an encouraging thing. I operate much better when there's a deadline, the fear of the deadline, and I, for myself, create artificial things, like I want to create in myself, this kind of anxiety, as if something really horrible will happen if I miss the deadline. I think there's some degree of that here, because creating AI that's aligned with humans has a lot of potential benefits. And so a different way to reframe that is, if you don't, we're all going to die. It just seems to be a very powerful psychological formulation of the goal of creating human aligned AI.
Speaker B: I think that anxiety is good. I think, like I said, I want the free market to create aligned AI's that are reliable. And I think that's what he's trying to do with XaI. So I'm all for it. What I am against is stopping, let's say, the open source ecosystem from thriving by, let's say, in the executive order, claiming that open source lms or dual use technologies should be government controlled. Then everybody needs to register their gpu and their big matrices with the government. And I think that extra friction will dissuade a lot of hackers from contributing, hackers that could later become the researchers that make key discoveries that push us forward, right? Including discoveries for AI safety. And so I think I just want to maintain ubiquity of opportunity to contribute to AI and to own a piece of the future, right? It can't just be legislated behind some wall where only a few players get to play the game.
Speaker A: I mean, so the EAC movement is often sort of caricatured to mean sort of progress and innovation at all, at all costs. Doesn't matter how unsafe it is, doesn't matter if it causes a lot of damage. You just build cool shit as fast as possible, stay up all night with a Diet Coke, whatever it takes. I think. I guess. I don't know if there's a question in there, but how important to you and what you've seen, the different formulations of EAC, is safety. Is AI safety?
Speaker B: Again, I think if there was no one working on it, I think I would be a proponent of it. I think, again, our goal is to bring balance and obviously a sense of urgency is a useful tool to make progress. It hacks our dopaminergic systems and gives us energy to work late into the night. I think also having a higher purpose, you're contributing to. At the end of the day, it's like, what am I contributing to? I'm contributing to the growth of this beautiful machine so that we can seek to the stars. That's really inspiring. That's also a sort of neuro hack.
Speaker A: So you're saying AI safety is important to you, but right now, the landscape of ideas you see is AI. Safety as a topic is used more often to gain centralized control. So in that sense, you're resisting it as a proxy for centralized. Gaining centralized control.
Speaker B: Yeah, I just think we have to be careful because safety is just the perfect cover for sort of centralization of power and covering up eventually corruption. I'm not saying it's corrupted now, but it could be down the line. And really, if you let the argument run, there's no amount of sort of centralization of control that will be enough to ensure your safety. There's always more nine nine nines of p safety that you can gain, 99.999%. Say, if maybe you want another nine, oh, please, give us full access to everything you do, full surveillance. And frankly, those that are proponents of AI safety have proposed having a global panopticon, where you have centralized perception of everything going on. And to me, that just opens up the door wide open for a sort of big Brother 1984 like scenario. And that's not a future I want.
Speaker A: To live in, because we know we have some examples throughout history when that did not lead to a good outcome.
Speaker B: Right.
Speaker A: You mentioned you founded a company, Extropic, that recently announced a 14.1 million seed round. What's the goal of the company? You're talking about a lot of interesting physics things. So what are you up to over there that you can talk about?
Speaker B: Yeah, I mean, you know, originally we weren't going to announce last week, but I think with the doxing and disclosure, we got our hand forced, so we. We had to disclose roughly what we were doing. But really, astropic was born from my dissatisfaction and that of my colleagues with the quantum computing roadmap. Quantum computing was sort of the first path to physics based computing that was trying to commercially scale. And I was working on physics based AI that runs on these physics based computers. But ultimately, our greatest enemy was this noise, this pervasive problem of noise that, as I mentioned, you have to constantly pump out the noise out of the system to maintain this pristine environment where quantum mechanics can take effect. And that constraint was just too much. It's too costly to do that. We were wondering, as generative AI is sort of eating the world, more and more of the world's computational workloads are focused on generative AI. How could we use physics to engineer the ultimate physical substrate for generative AI? From first principles of physics, of information theory, of computation, and ultimately of thermodynamics. What we're seeking to build is a physics based computing system and physics based AI algorithms that are inspired by out of equilibrium thermodynamics, or harness it directly to do machine learning as a physical process.
Speaker A: So what does that mean? Machine learning is a physical process? Is that hardware? Is it software, is it both? Is it trying to do the full stack in some kind of unique way?
Speaker B: Yes, it is full stack. And so we're folks that have built differentiable programming into the quantum computing ecosystem with Tensorflow quantum. One of my co founders of Tensorflow Quantum is the CTO, Trevor McCourt. We have some of the best quantum computer architects, those that have designed IBMs and AWS systems. They've left quantum computing to help us build what we call actually a thermodynamic computer.
Speaker A: A thermodynamic computer. Well, actually, I thought you were on Tensorflow Quantum. What lessons have you learned from Tensorflow quantum? Maybe you can speak to what it takes to create, essentially a software API to a quantum computer.
Speaker B: Right? I mean, that was a challenge to build, to invent, to build, and then to get to run on the real devices.
Speaker A: Can you actually speak to what it is?
Speaker B: Yeah. So Tensorflow Quantum was an attempt at. Well, I mean, I guess we succeeded at combining deep learning, or differentiable classical programming, with quantum computing, and turn quantum computing into or have types of programs that are differentiable in quantum computing. Andre Karpathy calls differentiable programming software 2.0. It's like gradient descent is a better programmer than you. And the idea was that in the early days of quantum computing, you can only run short quantum programs. Which quantum programs should you run? Well, just let gradient descent find those programs. Instead, we built the first infrastructure to not only run differentiable quantum programs, but combine them as part of broader deep learning graphs incorporating deep neural networks, the ones you know and love with what are called quantum neural networks. And ultimately, it was a very cross disciplinary effort. We had to invent all sorts of ways to differentiate, to backpropagate through the graph, the hybrid graph. But ultimately, it taught me that the way to program matter and to program physics is by differentiating through control parameters. If you have parameters that affects the physics of the system, and you can evaluate some loss function, you can optimize the system to accomplish a task, whatever that task may be. And that's a very sort of universal meta framework for how to program physics.
Speaker A: Based computers to try to parameterize everything, make those parameters differential, and then optimize?
Speaker B: Yes.
Speaker A: Okay, so is there some more practical engineering lessons from tensorflow quantum just organizationally, too, like the humans involved and how to get to a product, how to create good documentation, how to, I don't know, all these little subtle things that people might not think about.
Speaker B: I think, like, working across disciplinary boundaries is always a challenge, and you have to be extremely patient in teaching one another. Right. I learned a lot of software engineering through the process. My colleagues learned a lot of quantum physics and some learned machine learning through the process of building this system. And I think if you get some smart people that are passionate and trust each other in a room and you have a small team and you teach each other your specialties, suddenly you're kind of forming this sort of model soup of expertise, and something special comes out of that. Right. It's like combining genes, but for your knowledge bases, and sometimes special products come out of that. And so I think, like, even though it's very high friction initially to work in an interdisciplinary team, I think the product, at the end of the day, is worth it. And so, learned a lot trying to bridge the gap there. And, I mean, it's still a challenge to this day. We hire folks that have an AI background, folks that have a pure physics background, and somehow we have to make them talk to one another. Right.
Speaker A: Is there magic? Is there some science and art to the hiring process, to building a team that can create magic together?
Speaker B: Yeah, it's really hard to pinpoint that. Jean sais quoi. Right?
Speaker A: I didn't know you speak French. That's very nice.
Speaker B: Yeah, I'm actually french Canadian.
Speaker A: Oh, you are legitimately french Canadian.
Speaker B: I am.
Speaker A: I thought you were just doing that for the. For the cred.
Speaker B: No, no, I'm truly french Canadian from Montreal. But, yeah, essentially, we look for people with very high, fluid intelligence that aren't over specialized, because they're going to have to get out of their comfort zone. They're going to have to incorporate concepts that they've never seen before and very quickly get comfortable with them or learn to work in a team. And so that's sort of what we look for when we hire. We can't hire people that are just optimizing this subsystem for the past three or four years. We need really general, broader intelligence and specialty and people that are open minded, really, because if you're pioneering a new approach from scratch, there is no textbook, there's no reference. It's just us and people that are hungry to learn. So we have to teach each other. We have to learn the literature. We have to share knowledge bases, collaborate in order to push the boundary of knowledge further together. Right. And so people that are used to just getting prescribed what to do at this stage, when you're at the pioneering stage, that's not necessarily who you want to hire.
Speaker A: So you mentioned with extraopic, you're trying to build the physical substrate for generative AI. What's the difference between that and the AGI AI itself? So is it possible that in the halls of your company, AGI will be created, or will AGI just be using this as a substrate?
Speaker B: I think our goal is to both run human like AI or anthropomorphic AI.
Speaker A: Sorry for use of the term AGI. I know it's triggering for you.
Speaker B: We think that the future is actually physics based AI combined with anthropomorphic AI. So you can imagine I have a sort of world modeling engine through physics based AI. Physics based AI is better at representing the world at all scales, because it can be quantum mechanical, thermodynamic, deterministic, hybrid representations of the world, just like our world, at different scales, has different regimes of physics. If you inspire yourself from that in the ways you learn representations of nature, you can have much more accurate representations of nature. So you can have very accurate world models at all scales. Right? And so you have the world modeling engine, and then you have the sort of anthropomorphic AI that is human like. So you can have the science, the playground, to test your ideas, and you can have the synthetic scientist. And to us, that joint system of a physics based AI and an anthropomorphic AI is the closest thing to a fully general artificially intelligent system.
Speaker A: So you can get closer to truth by grounding the AI to physics, but you can also still have an anthropomorphic interface to us humans that like to talk to other humans or human like systems. On that topic, what do you. I suppose that is one of the big limitations of current large language models to you, is that they're not. They're good bullshitters. They're not really grounded to truth necessarily. Is that. Would that be fair to say?
Speaker B: Yeah. No. You wouldn't try to extrapolate the stock market with an LM trained on text from the Internet. Right. It's not going to be a very accurate model. It's not going to model its priors or its uncertainties about the world very accurately. Right. So you need a different type of AI to complement sort of this text extrapolation. AI.
Speaker A: You mentioned singularity earlier. How far away are we from a singularity?
Speaker B: I don't know if I believe in a finite time singularity as a single point in time. I think it's going to be asymptotic and sort of a diagonal sort of asymptote, like we have the light cone, we have the limits of physics restricting our ability to grow. So obviously you can't fully diverge on a finite time. I think my priors are that. I think a lot of people on the other side of the aisle think that once we reach human level AI, there's going to be an inflection point and a sudden foom. Suddenly AI is going to grok how to manipulate matter at the nanoscale and assemble nanobots. And having worked for nearly a decade in applying AI to engineer matter, it's much harder than they think. In reality, you need a lot of samples from either a simulation of nature that's very accurate and costly, or nature itself, and that keeps your ability to control the world around us in check. There's a sort of minimal cost, computationally and thermodynamically to acquiring information about the world in order to be able to predict and control it. And that keeps things in check.
Speaker A: It's funny you mentioned the other side of the aisle. So in the poll I posted about pdoom yesterday was the probability of doom. There seems to be a nice division between people. Think it's very likely and very unlikely. I wonder if in the future they'll be the actual Republicans versus Democrats division. Blue versus red is the AI dumers versus the I ackers yak.
Speaker B: Yeah. So this movement is not right wing or left wing. Fundamentally, it's more like up versus down in terms of the up civilization. But it seems to be like there is sort of case of alignment of the existing political parties where those that are for more centralization of power, control and more regulations are aligning themselves with the doomers, because that sort of instilling fear in people is a great way for them to give up more control and give the government more power. But fundamentally, we're not left versus right. I think we've done polls of people's alignment within IAC. I think it's pretty balanced. So it's a new fundamental issue of our time. It's not just centralization versus decentralization. It's kind of, do we go? It's like tech progressivism versus techno conservatism.
Speaker A: So IAC is, as a movement is often formulated in contrast to EA, effective altruism. What do you think are the pros and cons of effective altruism? What's interesting, insightful to you about them and what is negative.
Speaker B: Right. I think people trying to do good from first principles is. Is good.
Speaker A: We should actually say, and sorry to interrupt, we should probably say that. And you can correct me if I'm wrong, but effective altruism is a kind of movement that's trying to do good optimally, where good is probably measured. Something like the amount of suffering in the world, you want to minimize it. And there's ways that, that can go wrong, as any optimization can. And so it's interesting to explore how things can go wrong.
Speaker B: We're both trying to do good to some extent. And we're both trying, we're arguing for which loss function we should use, right?
Speaker A: Yes.
Speaker B: Their loss function is sort of hedons units of hedonism. How good do you feel for how much time? And so suffering would be negative hedons. And they're trying to minimize that. But to us, that seems like that loss function has spurious minima. You can start minimizing shrimp farm pain, which seems not that productive to me. Or you can end up with wireheading, where you just either install a neuralink or you scroll TikTok forever. And you feel good on the short term timescale, because your neurochemistry. But on long term timescale, it causes decay and death. Right? Because you're not being productive. Whereas sort of IAC, measuring progress of civilization, not in terms of a subjective loss of function, like hedonism, but rather an objective measure, a quantity that cannot be gained, that is physical energy. It's very objective, and there's not many ways to game it. If you did it in terms of GDP or currency that's pinned to a certain value, that's moving, that's not a good way to measure our progress. But the thing is, we're both trying to make progress and ensure humanity flourishes and gets to grow. We just have different loss functions and different ways of going about doing it.
Speaker A: Is there a degree, maybe you can educate me. Correct me. I get a little bit skeptical when there's an equation involved trying to reduce all of the human civilization, human experience to an equation. Is there a degree that we should be skeptical of the tyranny of an equation of a loss function over which to optimize? Like having a kind of intellectual humility about optimizing over loss functions.
Speaker B: Yeah. So this particular loss function, it's not stiff. It's kind of an average of averages, right? It's like distributions of states in the future are going to follow a certain distribution. So it's not deterministic. It's not like we're not on, like, stiff rails. Right. It's just a statistical statement about the future. But at the end of the day, you can believe in gravity or not, but it's not necessarily an option to obey it. And some people try to test that, and that goes not so well. So, similarly, I think thermodynamics is there whether we like it or not, and we're just trying to point out what is and try to orient ourselves and chart a path forward given, given this fundamental truth.
Speaker A: But there's still some uncertainty. There's still lack of information. Humans tend to fill the gap of the lack of information with narratives. And so how they interpret, you know, even physics is up to interpretation when there's uncertainty involved, and humans tend to use that to further their own means. So it's always, whenever there's an equation, it just seems like until we have really perfect understanding of the universe, humans will do what humans do, and they try to use the narrative of doing good to fool the populace into doing bad. I just. I guess that this is something they should be skeptical about in all movements.
Speaker B: That's right. So we invite skepticism.
Speaker A: Right. Do you have an understanding of what might, to a degree, that went wrong? What do you think may have gone wrong with effective altruism, that might also go wrong with effective accelerationism?
Speaker B: Yeah, I mean, I think, you know, I think it provided, initially a sense of community for, you know, engineers and intellectuals and rationalists in the early days. And it seems like the community was very healthy. But then they formed all sorts of organizations and started routing capital and having actual power. They have real power. They influence the government. They influence most AI orgs now. I mean, they're literally controlling the board of OpenAI and look over at anthropic. I think they have some control over that, too. I think the assumption of IAC is more like capitalism, is that every agent, organism and meta organism is going to act in its own interest, and we should maintain sort of adversarial equilibrium or adversarial competition to keep each other in check at all times, at all scales. I think that, yeah, ultimately, it was the perfect cover to acquire tons of power and capital. And I, unfortunately, sometimes that corrupts people over time.
Speaker A: What does a perfectly productive day, since building is important, what is a perfectly productive day in the life of Guillaume Verdun look like? How much caffeine do you consume? What's a perfect day?
Speaker B: Okay, so I have a particular regimen. I would say my favorite days are 12:00 p.m. to 04:00 a.m. and I would have meetings in the early afternoon, usually external meetings, some internal meetings. Because I'm CEO, I have to interface with the outside world, whether it's customers or investors or interviewing potential candidates. And usually I'll have ketones, exogenous ketones.
Speaker A: So are you on a keto on a keto diet, or is this.
Speaker B: I've done keto before for football and whatnot, but I like to have a meal after sort of part of my day is done, and so I can just have extreme focus.
Speaker A: You do the social interactions earlier in the day without food.
Speaker B: Front load them? Yeah, like right now I'm on ketones and red ball, and it just gives you a clarity of thought that is really next level, because then when you eat, you're actually allocating some of your energy that could be going to neural energy to your digestion. After I eat, maybe I take a break, an hour or so, hour and a half. And then usually it's like ideally one meal a day, like steak and eggs and vegetables, animal based, primarily. So fruit and meat. And then I do a second wind, usually. That's deep work, right? Because I am a CEO, but I'm still technical. I'm contributing to most patents, and there I'll just stay up late into the night and work with engineers on very technical problems.
Speaker A: So it's like the 09:00 p.m. to 04:00 a.m. whatever that range of time.
Speaker B: Yeah, that's the perfect time. The emails, the things that are on fire, stop trickling in. You can focus, and then you have your second wind. And I think Demesne Savas has a similar workday to some extent. So I think that's definitely inspired my work day. But, yeah, I started this workday when I was at Google and had to manage a bit of the product during the day and have meetings and then do technical work at night, exercise, sleep.
Speaker A: Those kinds of things. Said football. You used to play football?
Speaker B: Yeah, I used to play american football. I've done all sorts of sports growing up. And then I was into powerlifting for a while. So when I was studying mathematics in grad school, I would just do math and lift, take caffeine, and that was my day. It was very pure, the purest of monk modes. But it's really interesting how in powerlifting, you're trying to cause neural adaptation by having certain driving signals, and youre trying to engineer neuroplasticity through all sorts of supplements, and you have all sorts of brain derived neurotrophic factors that get secreted when you lift. Its funny to me how I was trying to engineer neural adaptation in my nervous system more broadly, not just my brain, while learning mathematics. I think you can learn much faster if you really care, if you convince yourself to care a lot about what you're learning and you have some sort of assistance, let's say caffeine or some cholinergic supplement to increase neuroplasticity. I should chat with Andrew Huberman at some point. He's the expert, but yeah, at least to me, it's like you can try to input more tokens into your brain, if you will, and you can try to increase the learning rate so that you can learn much faster on a shorter time scale. So I've learned a lot of things. I followed my curiosity. You're naturally, if you're passionate about what you're doing, you're going to learn faster, you're going to become smarter faster, and if you follow your curiosity, you're always going to be interested. And so I advise people to follow their curiosity and don't respect the boundaries of certain fields or what you've been allocated in terms of lane, of what you're working on, just go out and explore and follow your nose and try to acquire and compress as much information as you can into your brain. Anything that you find interesting and caring.
Speaker A: About a thing, like you said, which is interesting, it works for me really well. It's like tricking yourself that you care about a thing. Yes. And then you start to really care about it. So it's funny, the motivation is a really good catalyst for learning, right?
Speaker B: And so at least part of, part of my character as Beth, Jesus is kind of like, yeah, the hype man. Yeah. But I'm like hyping myself up, but then I just tweet about it, and it's just when I'm trying to get really hyped up in, like, an altered state of consciousness where I'm like ultra focused, in the flow, wired, trying to invent something that's never existed, I need to get to unreal levels of, like, excitement. But your brain has these levels of, of cognition that you can unlock with like, higher levels of adrenaline and, and whatnot. And I mean, I've learned that in powerlifting that actually you can engineer a mental switch to, like, increase your strength, right? Like, if you can engineer a switch, maybe you have a prompt, like a certain song or some music where suddenly you're, like, fully primed. Then you're at max maximum strength, right. And I've engineered that. That switch through years of lifting. If you're going to get under 500 pounds, and it could crush you. If you don't have that switch to be wired in, you might die. So that that'll wake you right up. And that sort of skill I've carried over to, like, research. When it's go time, when the stakes are high, somehow I just reach another level of neural performance.
Speaker A: So Beth JSOs is your and sort of embodiment representation of your intellectual Hulk. It's your productivity Hulk. They just turn on. What have you learned about the nature of identity from having these two identities? I think it's interesting for people to be able to put on those two hats so explicitly.
Speaker B: I think it was interesting in the early days. I think in the early days, I thought it was truly compartmentalized, like, oh, yeah, this is a character. You know, I'm Guillaume. Beth is just the character. I take my thoughts, and then I extrapolate them to a bit more extreme. But over time, it's kind of like both identities were starting to merge mentally, and people were like, no, I met you. You are Beth. You are not just Guillaume. And I was like, wait, am I? And now it's fully merged. But it was already before the docs was already starting mentally that I am this character. It's part of me.
Speaker A: Would you recommend people sort of have an alt?
Speaker B: Absolutely.
Speaker A: Like, young people. Would you recommend them to explore different identities by having alts alt accounts?
Speaker B: It's fun. It's like writing an essay and taking a position, right? It's like you do this in debate. It's like you can have experimental thoughts and by the stakes being so low, because you're an anon account with, I don't know, 20 followers or something, you can experiment with your thoughts in a low stakes environment, and I feel like we've lost that. In the era of everything being under your main name, everything being attributable to you, people just are afraid to speak, explore ideas that aren't fully formed, and I feel like we've lost something there. So I hope platforms like x and others really help support people trying to stay pseudonymous or anonymous, because it's really important for people to share thoughts that aren't fully formed and converge onto maybe hidden truths that were hard to converge upon if it was just through open conversation with real names.
Speaker A: Yeah. I really believe in not radical, but rigorous empathy. It's like really considering what it's like to be a person of a certain viewpoint and, like, taking that as a thought experiment farther and farther and farther. And one way of doing that is an alt account. That's a fun, interesting way to really explore what it's like to be a person that believes a set of beliefs. And taking that across a span of several days, weeks, months, of course, there's always the danger of becoming that. That's the Nietzsche gaze. Long into the abyss. The abyss gazes into you. You have to be careful.
Speaker B: Breaking Beth.
Speaker A: Yeah, right. Breaking Beth. Yeah. You wake up with a shaved head one day, just like, who am I? What have I become? So you've mentioned quite a bit of advice already. But what advice would you give to young people of how to, in this interesting world we're in, how to have a career, and how to have a life they can be proud of?
Speaker B: I think, to me, the reason I went to theoretical physics was that I had to learn the base of the stack that was going to stick around no matter how the technology changes. Right. And to me, that was the foundation upon which then I later built engineering skills and other skills, and to me, the laws of physics. It may seem like the landscape right now is changing so fast, it's disorienting. But certain things like fundamental mathematics and physics aren't going to change. And if you have that knowledge and knowledge about complex systems and adaptive systems, I think that's going to carry you very far. And so not everybody has to study mathematics, but I think it's really a huge cognitive unlock to learn math and some physics and engineering.
Speaker A: Get as close to the base of the stack as possible.
Speaker B: Yeah, that's right. Because the base of the stack doesn't change everything else. Your knowledge might become not as relevant in a few years. Of course, there's a sort of transfer learning you can do, but then you have to always transfer learn constantly.
Speaker A: I guess the closer you are to the base of the stack, the easier the transfer learning, the shorter the jump.
Speaker B: Right, right. And you'd be surprised, once you've learned concepts in many physical scenarios, how they can carry over to understanding other systems that are necessarily physics. And I guess the IAC writings, the principles and Tenet post that was based on physics, that was kind of my experimentation with applying some of the thinking from out of equilibrium thermodynamics to understanding the world around us. And it's led to. To IAC and this movement.
Speaker A: If you look at your one cog in the machine, in the capitalist machine, one human, and if you look at yourself, do you think mortality is a feature or a bug? Would you want to be immortal?
Speaker B: No, I think fundamentally, in thermodynamic dissipative adaptation, there's the word dissipation. Dissipation is important. Death is important, right? We have a saying in physics. Physics progresses one funeral at a time. I think the same is true for capitalism. Companies, empires, people, everything. Everything must die at some point. I think that we should probably extend our lifespan because we need a longer period of training, because the world is more and more complex. We have more and more data to really be able to predict and understand the world. And if we have a finite window of higher neuroplasticity, then we have sort of a hard cap and how much we can understand about our world. So I think I am for death, because, again, I think it's important. If you have a king that would never die, that would be a problem. The system wouldn't be constantly adapting. You need novelty, you need youth, you need disruption to make sure the system is always adapting and malleable. Otherwise, if things are immortal, if you have, let's say, corporations that are there forever and they have the monopoly, they get calcified, they become not as optimal, not as high fitness in a changing, time varying landscape. Death gives space for youth and novelty to take its place. And I think it's an important part of every system in nature. So, yeah, I am for death, but I do think that longer lifespan and longer time for neuroplasticity, bigger brains, should be something we should strive for.
Speaker A: Well, in that Jeff Bezos and Beth Jlos agree that all companies die. And for Jeff, the. The goal is to try to, he calls it day one thinking, try to constantly, for as long as possible, reinvent, sort of extend the life of the company, but eventually it, too, will die, because it's so damn difficult to keep reinventing. Are you afraid of your own death?
Speaker B: I think I have ideas and things I'd like to achieve in this world before I have to go, but I don't think I'm necessarily afraid of death.
Speaker A: So you're not attached to this particular body and mind that you got?
Speaker B: No. I think. I'm sure there's going to be better versions of myself in the future. Or forks. Forks, right. Genetic forks or other. I truly believe that. I think there's a sort of evolutionary, like, algorithm happening at every bit, or Nat, in the world is sort of adapting through this process that we described in IAC. And I think maintaining this adaptation, malleability, is how we have constant optimization of the whole machine. And so I don't think I'm particularly an optimum that needs to stick around forever. I think there's going to be greater optima in many ways.
Speaker A: What do you think is the meaning of it all? What's the why of the machine, the IAc machine?
Speaker B: The why. Well, the why is thermodynamics. It's why we're here. It's what has led to the formation of life and of civilization, of evolution, of technologies and growth of civilization. But why do we have thermodynamics? Why do we have our particular universe? Why do we have these particular hyperparameters, the constants of nature? Well, then you get into the anthropic principle. In the landscape of potential universes, we're in the universe that allows for life. And then I why is there potentially many universes? I don't know. I don't know that part. But could we potentially engineer new universes or create pocket universes and set the hyperparameters so there is some mutual information between our existence and that universe, and we'd be somewhat its parents? I think that's really. I don't know. That'd be very poetic, purely conjecture. But again, this is why figuring out quantum gravity would allow us to understand if we can do that.
Speaker A: And above that, why is it all seems so beautiful and exciting? The quest to figure out quantum gravity seems so exciting. Why? Why is that? Why are we drawn to that? Why are we pulled towards that? Just that puzzle solving creative force that underpins all of it, it seems like.
Speaker B: I think we seek, just like an LLM seeks to minimize cross entropy between its internal model and the world. We seek to minimize.
Speaker A: Yeah.
Speaker B: The statistical divergence between our predictions, the world and the world itself. And, you know, having regimes of energy scales or physical scales in which we have no visibility, no ability to predict or perceive, you know, that's kind of an insult to us. And we want to. We want to be able to understand the world better in order to best steer it or steer us through it. And in general, it's a capability that has evolved because the better you can predict the world, the better you can capture utility or free energy towards your own sustenance and growth. And I think quantum gravity, again, is kind of the final boss in terms of knowledge acquisition, because once we've mastered that, then we can do a lot, potentially. But between here and there, I think there's a lot to learn in the mesoscales. There's a lot of information to acquire about our world and a lot of engineering, perception, prediction and control to be done to climb up the Kardashev scale. And to us, that's the great challenge of our times.
Speaker A: And when you're not sure where to go, let the meme pave the way. Guillaume Beth, thank you for talking today. Thank you for the work you're doing. Thank you for the humor and the wisdom you put into the world. This was awesome.
Speaker B: Thank you so much for having me. Lax it's a pleasure.
Speaker A: Thank you for listening to this conversation with Guillain Verdun. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Albert Einstein. If at first the idea is not absurd, then there is no hope for it. Thank you for listening. I hope to see you next time.
