Transcription for Sam Altman： OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI ｜ Lex Fridman Podcast #419.mp3:
Full transcript: I think compute is going to be the currency of the future. I think it will be maybe the most precious commodity in the world. I expect that by the end of this decade and possibly somewhat sooner than that, we will have quite capable systems that we look at and say, wow, thats really remarkable. The road to AGI should be a giant power struggle. I expect that to be the case. Whoever builds AGI first gets a lot of power. Do you trust yourself with that much power? The following is a conversation with Sam Altman, his second time in the podcast. He is the CEO of OpenAI, the company behind GPT four, Chad, GPT, Sora, and perhaps one day the very company that will build AGI. This is Alex Friedman podcast to support it. Please check out our sponsors in the description. And now, dear friends, here's Sam Altman. Take me through the OpenAI board saga that started on Thursday, November 16, maybe Friday, November 17 for you. That was definitely the most painful professional experience of my life. And chaotic and shameful and upsetting and a bunch of other negative things. There were great things about it, too, and I wish it had not been in such an adrenaline rush that I wasn't able to stop and appreciate them at the time. But I came across this old tweet of mine, or this tweet of mine from that time period, which was like, it was like kind of going to your own eulogy, watching people say all these great things about you and just unbelievable support from people I love and care about. That was really nice. That whole weekend. I kind of felt, with one big exception, I felt a great deal of love and very little hate, even though it felt like it just, I have no idea what's happening and what's going to happen here. And this feels really bad. And there were definitely times I thought it was going to be like one of the worst things to ever happen for AI safety. Well, I also think I'm happy that it happened relatively early. I thought at some point between when OpenAI started and when we created AGI, there was going to be something crazy and explosive that happened, but there may be more crazy and explosive things still to happen. It still, I think, helped us build up some resilience and be ready for more challenges in the future. But the thing you had a sense of that you would experience is some kind of power struggle. The road to AGI should be a giant power struggle like the world should. Well, not should I expect that to be the case. And so you have to go through that. Like you said, iterate as often as possible in figuring out how to have a board structure, how to have organization, how to have the kind of people that you're working with, how to communicate all that in order to deescalate the power struggle as much as possible, pacify it. But at this point, it feels, you know, like something that was in the past that was really unpleasant and really difficult and painful. But we're back to work, and things are so busy and so intense that I don't spend a lot of time thinking about it. There was a time after there was this fugue state for the month after maybe 45 days after, I was just sort of drifting through the days. I was so out of it. I was feeling so down. Just on a personal, psychological level. Yeah, really painful and hard to have to keep running open AI in the middle of that. I just wanted to, like, crawl into a cave and kind of recover for a while. But, you know, now it's like we're just back to working on the mission. Well, it's still useful to go back there and reflect on board structures, on power dynamics, on how companies are run, the tension between research and product development and money and all this kind of stuff, so that you, who have a very high potential of building AGI, would do so in a slightly more organized, less dramatic way in the future. So there's value there to go, both the personal psychological aspects of you as a leader, and also just the board structure and all this kind of messy stuff. Definitely learned a lot about structure and incentives and what we need out of a board. And I think it is valuable that this happened now. In some sense, I think this is probably not the last high stress moment of opening eye, but it was quite a high stress moment. The company very nearly got destroyed. And we think a lot about many of the other things we've got to get right for AGI. But thinking about how to build a resilient.org and how to build a structure that will stand up to a lot of pressure in the world, which I expect more and more as we get closer. I think that's super important. Do you have a sense of how deep and rigorous the deliberation process by the board was? Can you shine some light on just human dynamics involved in situations like this? Was it just a few conversations and all of a sudden it escalates? Why don't we fire Sam kind of thing? I think the board members were well meaning people on the whole. And I believe that in stressful situations where people feel time pressure or whatever, people understandably make suboptimal decisions and I think one of the challenges for OpenAI will be we're going to have to have a board and a team that are good at operating under. Under pressure. Do you think the board had too much power? I think boards are supposed to have a lot of power. But one of the things that we did see is in most corporate structures, boards are usually answerable to shareholders. Sometimes people have supervoting shares or whatever. In this case, and I think one of the things with our structure that we maybe should have thought about more than we did is that the board of a nonprofit has, unless you put other rules in place, quite a lot of power. They don't really answer to anyone but themselves. And there's ways in which that's good. But what we'd really like is for the board of OpenAI to answer to the world as a whole, as much as that's a practical thing. So there's a new board announced. Yeah, there's, I guess, a new smaller board at first, and now there's a new final board. Not a final board yet. We've added some. We'll add more. We've added some. Okay. What is fixed in the new one that was perhaps broken in the previous one? The old board sort of got smaller over the course of about a year. It was nine, and then it went down to six, and then we couldn't agree on who to add. And the board also, I think, didn't have a lot of experienced board members, and a lot of the new board members at OpenAI just have more experience as board members. I think that'll help. It's been criticized, some of the people that are added to the board. I heard a lot of people criticizing the edition of Larry Summers, for example. What's the process of selecting the board? Like, what's involved in that? So Brett and Larry were kind of decided in the heat of the moment over this very tense weekend. And that weekend was a real roller coaster. It was a lot of ups and downs, and we were trying to agree on new board members that both sort of the executive team here and the old board members felt would be reasonable. Larry was actually one of their suggestions. The old board members, Brett, I think I had even previous to that weekend, suggested, but he was busy and didn't want to do it. And then we really needed help in wooden. We talked about a lot of other people, too, but that was, I felt like if I was going to come back, I needed new board members. I didn't think I could work with the old board again in the same configuration, although we then decided, and I'm grateful that Adam would stay, but we considered various configurations, decided we wanted to get to abortive three, and had to find two new board members over the course of sort of a short period of time. So those were decided honestly, without, you know, that's like, you kind of do that on the battlefield. You don't have time to design a rigorous process then for new board members. Since new board members will add going forward, we have some criteria that we think are important for the board to have different expertise that we want the board to have. Unlike hiring an executive, where you need them to do one role. Well, the board needs to do a whole role of governance and thoughtfulness. Well. And so one thing that Brett says, which I really like, is that we want to hire board members and slates, not as individuals, one at a time. And thinking about a group of people that will bring nonprofit expertise, expertise at running companies, sort of good legal and governance expertise. That's kind of what we've tried to optimize for. So is technical savvy important for the individual board members? Not for every board member, but for certainly some. You need that. That's part of what the board needs to do. So, I mean, the interesting thing that people probably don't understand about OpenAI, I certainly don't, is, like, all the details of running the business, when they think about the board, given the drama, they think about you. They think about, like, if you reach AGI or you reach some of these incredibly impactful products and you build them and deploy them, what's the conversation with the board like? And they kind of think, all right, what's the right squad to have in that kind of situation, to deliberate? Look, I think you definitely need some technical experts there, and then you need some people who are like, how can we deploy this in a way that will help people in the world the most and people who have a very different perspective. I think a mistake that you or I might make is to think that only the technical understanding matters. And that's definitely part of the conversation you want that board to have. But there's a lot more about how that's going to just impact society and people's lives that you really want represented in there, too. And you're just kind of, are you looking at the track record of people or you're just having conversations? Track record is a big deal. You, of course, have a lot of conversations. There's some roles where I totally ignore track record and just look at slope, kind of ignore the y intercept. Thank you. Thank you for making it mathematical for the audience. For a board member, I do care much more about the y intercept. I think there is something deep to say about track record there and experiences sometimes very hard to replace. Do you try to fit a polynomial function or exponential one to the track record? That's not that. And an analogy doesn't carry that far. All right. You mentioned some of the low points that weekend. What were some of the low points psychologically for you? Did you consider going to the Amazon jungle and just taking ayahuasca and disappearing forever? Or. I mean, there's so many. It was. That was a very bad period of time. They were great high points, too. My phone was just sort of nonstop blowing up with nice messages from people I work with every day, people I hadn't talked to in a decade. I didn't get to appreciate that as much as I should have because I was just in the middle of this firefight. But that was really nice. But on the whole, it was a very painful weekend and also just a very. It was like a battle fought in public to a surprising degree. And that was extremely exhausting to me, much more than I expected. I think fights are generally exhausting, but this one really was. The board did this Friday afternoon. I really couldn't get much in the way of answers, but I also was just like, well, the board gets to do this. I'm going to think for a little bit about what I want to do, but I'll try to find the. The blessing in disguise here. And I was like, well, my current job at OpenAI is it was to run a decently sized company at this point. And the thing I'd always liked the most was just getting to work with the researchers. And I was like, yeah, I can just go do a very focused AGI research effort. And I got excited about that. It didn't even occur to me at the time to possibly that this was all going to get undone. This was, like, Friday afternoon. So you've accepted the death of the. Previous very quickly, like, within, you know, I mean, I went through, like, a little period of confusion and rage, but very quickly. And by Friday night, I was, like, talking to people about what was gonna be next, and I was excited about that. I think it was Friday night evening for the first time that I heard from the exec team here, which is like, hey, we're gonna, like, fight this. And, you know, we think, whatever. And then I went to bed just still being like, okay, excited, like, onward. Were you able to sleep not a lot. It was. One of the weird things was there was this period of four and a half days where sort of didn't sleep much, didn't eat much, and still kind of had, like, a surprising amount of energy. You learned, like, a weird thing about adrenaline in wartime. So you kind of accepted the death of this baby opening up. And I was excited for the new thing. I was just like, okay, this was crazy, but whatever. It's a very good coping mechanism. And then Saturday morning, two of the board members called and said, hey, we didn't mean to destabilize things. We don't destroy a lot of value here. Can we talk about you coming back? And I immediately didn't want to do that, but I thought a little more. And I was like, well, I really care about the people here, the partners, shareholders. I love this company. And so I thought about it, and it's like, well, okay, but here's the stuff I would need. And then the most painful time of all was over the course of that weekend. I kept thinking and being told, and we all kept, not just me, like, the whole team here kept thinking while we were trying to keep open eyes stabilized, while the whole world was trying to break it apart, people trying to recruit, whatever, we kept being told, all right, we're almost done. We're almost done. We just need a little bit more time. And it was this very confusing state. And then Sunday evening when again, like, every few hours, I expected that we were going to be done and we're going to figure out a way for me to return and things to go back to how they were. The board then appointed a new interim CEO. And then I was like, I mean, that feels really bad. That was the low point of the whole thing. You know, I'll tell you something. It felt very painful, but I felt a lot of love that whole weekend. It was not. Other than that one moment Sunday night. I would not characterize my emotions as anger or hate, but I really just, like. I felt a lot of love from people towards people. It was painful, but the dominant emotion of the weekend was love, not hate. You've spoken highly of Mira Moradi, that she helped, especially as you put in a tweet in the quiet moments when it counts. Perhaps we could take a bit of a tangent. What do you admire about Meera? Well, she did a great job during that weekend in a lot of chaos. But people often see leaders in the crisis moments, good or bad. But a thing I really value in leaders is how people act on a boring Tuesday at 946 in the morning and in just sort of the normal drudgery of the day to day, how someone shows up in a meeting, the quality of the decisions they make. That was what I meant about the quiet moments. Meaning, like, most of the work is done on a day by day, in the meeting, by meeting, just be present and make great decisions. Yeah. I mean, look, what you have wanted to spend the last 20 minutes about, and I understand, is, like, this one very traumatic weekend. Yeah. But that's not really what opening eye is about. Opening eye is really about the other seven years. Well, yeah. Human civilization is not about the invasion of the Soviet Union by Nazi Germany, but still, that's something to focus on. Very, very understandable. It gives us an insight into human nature, the extremes of human nature, and perhaps some of the damage and some of the triumphs of human civilization can happen in those moments. So it's, like, illustrative. Let me ask you about Ilya. Is he being held hostage in a secret nuclear facility? No. What about a regular secret facility? No. What about a nuclear, non secret facility? Neither. Not that either. I mean, this is becoming a meme at some point. You've known Ilya for a long time. He was obviously in part of this drama with the board and all that kind of stuff. What's your relationship with him now? I love Ilya. I have tremendous respect for Ilya. I don't have anything I can say about his plans right now. That's a question for him. But I really hope we work together for certainly the rest of my career. He's a little bit younger than me. Maybe he works a little bit longer. There's a meme that he saw something, he maybe saw AGI, and that gave him a lot of worry internally. What did Ilya see? Ilya has not seen AGI. None of us have seen AGI. We have not built AGI. I do think one of the many things that I really love about Ilya is he takes AGI and the safety concerns, broadly speaking, including things like the impact this is going to have on society very seriously, and as we continue to make significant progress. Ilya is one of the people that I've spent the most time over the last couple of years talking about what this is going to mean, what we need to do to ensure we get it right, to ensure that we succeed at the mission. So Ilya did not see AGI, but Ilya is a credit to humanity in terms of how much he thinks and worries about making sure we get this right. I've had a bunch of conversations with him in the past. I think when he talks about technology, he's always, like, doing this long term thinking type of thing. So he's not thinking about what this is going to be in a year, he's thinking about in ten years. Just thinking from first principles, like, okay, if this scales, what are the fundamentals here? Where is this going? And so that that's the foundation for them thinking about, like, all the other safety concerns and all that kind of stuff, which makes him a really fascinating human to talk with. Do you have any idea why he's been kind of quiet? Is it he's just doing some soul. Searching again, I don't want to speak for Ilya. I think that you should ask him that. He's definitely a thoughtful guy. I kind of think Ilya is always on a soul search. In a really good way. Yes. Yeah. Also, he appreciates the power of silence. Also, I'm told he can be a silly guy, which I've never seen that side of him. It's very sweet when that happens. I've never witnessed a silly ilya, but I look forward to that as well. I was at a dinner party with him recently, and he was playing with a puppy, and he was in a very silly mood, very endearing. And I was thinking, oh, man, this is not the side of the ilya that the world sees the most. So just to wrap up this whole saga, are you feeling good about the board structure, about all of this and where it's moving? I feel great about the new board in terms of the structure of OpenAI I. You know, one of the board's tasks is to look at that and see where we can make it more robust. We wanted to get new board members in place first, but, you know, we clearly learned a lesson about structure throughout this process. I don't have, I think, super deep things to say. It was a crazy, very painful experience. I think it was like a perfect storm of weirdness. It was like a preview for me of what's going to happen as the stakes get higher and higher and the need that we have robust governance structures and processes and people. I am kind of happy it happened when it did, but it was a shockingly painful thing to go through. Did it make you be more hesitant in trusting people? Yes. Just on a personal level? Yes. I think I'm an extremely trusting person. I always had a life philosophy of, don't worry about all of the paranoia, don't worry about the edge cases. You get a little bit screwed in exchange for getting to live with your guard down. And this was so shocking to me. I was so caught off guard that it has definitely changed. And I really don't like this. It's definitely changed how I think about just default trust of people and planning for the bad scenarios. You got to be careful with that. Are you worried about becoming a little too cynical? I'm not worried about becoming too cynical. I think I'm like the extreme opposite of a cynical person, but I'm worried about just becoming less of a default, trusting person. I'm actually not sure which mode is best to operate in. For a person who's developing AGI, trusting or untrusting, it's an interesting journey you're on. But in terms of structure, see, I'm more interested on the human level. Like, how do you surround yourself with humans that are building cool shit, but also are making wise decisions? Because the more money you start making, the more power the thing has, the weirder people get. I think you could make all kinds of comments about the board members and the level of trust I should have had there, or how I should have done things differently. But in terms of the team here, I think you'd have to give me a very good grade on that one. And I have just enormous gratitude and trust and respect for the people that I work with every day. And I think being surrounded with people like that is really important. Our mutual friend Elon sued openaiden Moti is the essence of what he's criticizing. To what degree does he have a point? To what degree is he wrong? I don't know what it's really about. We started off just thinking we were going to be a research lab and having no idea about how this technology was going to go. It's hard to, because it was only seven or eight years ago, it's hard to go back and really remember what it was like then. But this was before language models were a big deal. This was before we had any idea about an API or selling access to a chatbot is before we had any idea we were going to productize at all. So we're like, we're just going to try to do research, and we don't really know what we're going to do with that. I think with many new, fundamentally new things, you start fumbling through the dark and you make some assumptions, most of which turn out to be wrong. And then it became clear that we were going to need to do different things and also have huge amounts more capital. So we said, okay, well, the structure doesn't quite work for that, how do we patch the structure? And then you patch it again and patch it again and you end up with something that does look kind of eyebrow raising, to say the least. But we got here gradually with, I think, reasonable decisions at each point along the way. And doesn't mean I wouldn't do it totally differently if we could go back now with an oracle. But you don't get the oracle at the time. But anyway, in terms of what Elon's real motivations here are, I don't know. To the degree you remember, what was the response that OpenAI gave in the blog post? Can you summarize it? Oh, we just said, like, you know, Elon said this set of things. Here's our characterization, or here's the sort of. Not our characterization. Here's like the characterization of how this went down. We tried to not make it emotional and just sort of say, here's the history. I do think there's a degree of mischaracterization from Elon here about one of the points you just made, which is the degree of uncertainty you had at the time. You guys are a bunch of, like a small group of researchers crazily talking about AGI when everybody's laughing at that thought. It wasn't that long ago Elon was crazily talking about launching rockets when people were laughing at that thought. So I think he'd have more empathy for this. I mean, I do think that there's personal stuff here that there was a split that OpenAI and a lot of amazing people here chose the part ways with ElON. So there's a person, Elon chose to part wAys. Can you describe that exactly, the choosing to part ways? He thought OpenAI was going to fail. He wanted TOtal control to sort of turn it around. We wanted to keep going in the direction that now has become OpenAI. He also wanted Tesla to be able to build an AGI effort at various times. He wanted to make OpenAI into a for profit company that he could have control of or have it merge with Tesla. We don't want to do that. ANd he decided to leave, which, that's fine. So you're saying, and that's one of the things that the blog post says, is that he wanted OpenAI to be basically acquired by Tesla in the same way that, or maybe something similar or maybe something more dramatic than the partnership with Microsoft. My memory is the proposal was just like, yeah, get acquired by Tesla and have Tesla have full control over it. I'm pretty sure that's what it was. So what is the word open in OpenAI mean to elon at the time? Ilya has talked about this in the email exchanges and all this kind of stuff. What does it mean to you at the time? What does it mean to you now? I would definitely pick a different, speaking of going back with an oracle, I'd pick a different name. One of the things that I think OpenAI is doing that is the most important of everything that we're doing is putting powerful technology in the hands of people for free as a public good. We don't run ads on our free version, we don't monetize it in other ways. We just say it's part of our mission. We want to put increasingly powerful tools in the hands of people for free and get them to use them. And I think that kind of open is really important to our mission. I think if you give people great tools and teach them to use them, or don't even teach them, they'll figure it out and let them go. Build an incredible future for each other with that. That's a big deal. So if we can keep putting, like, free or low cost or free and low cost powerful AI tools out in the world, I think it's a huge deal for how we fulfill the mission, open source or not. Yeah, I think we should open source some stuff and not other stuff. It does become this, like, religious battle line where nuance is hard to have, but I think nuance is the right answer. So he said, change your name to closed AI and I'll drop the lawsuit. I mean, is it going to become this battleground in the land of memes about. I think that speaks to the seriousness with which Elon means the lawsuit and. Yeah, I mean, that's like an astonishing thing to say. I think. Well, I don't think the lawsuit maybe, correct me if I'm wrong, but I don't think the lawsuit is legally serious. It's more to make a point about the future of AGI and the company that's currently leading the way. So, look, I mean, Grok had not open sourced anything until people pointed out it was a little bit hypocritical. And then he announced that Grok will open source things this week. I don't think open source versus not is what this is really about for him. Well, we'll talk about open source and not. I do think maybe criticizing the competition is great. Just talking a little shit, that's great. But friendly competition versus, like, I personally hate lawsuits. Look, I think this whole thing is like, unbecoming of a builder. And I respect Elon as one of the great builders of our time. And I know he knows what it's like to have like, haters attack him. And it makes me extra sad he's doing it to us. Yeah, he's one of the greatest builders of all time, potentially the greatest builder of all time. It makes me sad, and I think it makes a lot of people sad. Like, there's a lot of people who've really looked up to him for a long time. And I said, you know, in some interview or something that I missed the old Elon, and the number of messages I got being like that exactly encapsulates how I feel. I think he should just win. He should just make Grock beat GPT, and then GPT beats Grock, and it's just the competition, and that's beautiful for everybody. But on the question of open source, do you think there's a lot of companies playing with this idea? It's quite interesting. I would say meta, surprisingly, has led the way on this, or like, at least took the first step in the game of chess of like, really open sourcing the model. Of course, it's not the state of the art model, but open sourcing. Llama. Google is flirting with the idea of open sourcing a smaller version. What are the pros and cons of open sourcing? Have you played around with this idea? Yeah, I think there is definitely a place for open source models, particularly smaller models that people can run locally. I think there's huge demand for. I think there will be some open source models, there will be some closed source models. It won't be unlike other ecosystems in that way. I listened to all in podcasts talking about this loss and all that kind of stuff, and they were more concerned about the precedent of going from nonprofit to this cap for profit. What precedent this sets for other startups. Is that something I don't. I would heavily discourage any startup that was thinking about starting as a nonprofit and adding like a for profit arm later. I'd heavily discourage them from doing that. I don't think we'll set a precedent here. Okay, so most startups should go just for sure. And again, if we knew what was gonna happen, we would have done that too. Well, like, in theory, if you like, dance beautifully here, you can, there's some tax incentives or whatever. I don't think that's how most people think about these things. Just not possible to save a lot of money for a startup if you do it this way. No, I think there's laws that would make that pretty difficult. Where do you hope this goes with Elon? This tension, this dance? Where do you hope this, like, if we go one, two, three years from now, your relationship with him on a personal level too, like friendship, friendly competition, just all this kind of stuff? Yeah, I mean, I really respect Elon, and I hope that years in the future, we have an amicable relationship. Yeah, I hope you guys have an amicable relationship, like this month, and just compete and win and explore these ideas together. I do suppose there's competition for talent or whatever, but it should be friendly competition. Just build, build cool shit. And Elon is pretty good at building cool shit, but so are you. So speaking of cool shit, Sora, there's like a million questions I could ask. First of all, it's amazing. It truly is amazing on a product level, but also just on a philosophical level. So let me just a technical, philosophical ask, what do you think it understands about the world more or less than GPT four, for example, the world model, when you train on these patches versus language tokens, I think all of these. Models understand something more about the world model than most of us give them credit for. And because they're also very clear things, they just don't understand or don't get right. Its easy to look at the weaknesses, see through the veil, and say, ah, this is just, this is all fake. But its not all fake. Its just some of it works and some of it doesnt work. I remember when I started first watching Sora videos, and I would see a person walk in front of something for a few seconds and occlude it and then walk away and the same thing was still there. I was like, oh, its pretty good. Or theres examples where the underlying physics looks so well represented over a lot of steps in a sequence. It's like, oh, this is quite impressive. But fundamentally, these models are just getting better and that will keep happening. If you look at the trajectory from Dalli one to two to three to Sora, there are a lot of people that were dunked on each version saying, it can't do this, it can't do that, and look at it now, the. Thing you just mentioned is with occlusions, is basically modeling the physics of three dimensional physics of the world sufficiently well to capture those kinds of things well. Or like, yeah, maybe you can tell in order to deal with occlusions, what does the world model need to. Yeah, so what I would say is it's doing something to deal with occlusions really well. What I represent that it has like a great underlying 3d model of the world. It's a little bit more of a. Stretch, but can you get there through just these kinds of two dimensional training data approaches? It looks like this approach is going to go surprisingly far. I don't want to speculate too much about what limits it will surmount and which it won't. But what are some interesting limitations of the system that you've seen? I mean, there's been some fun ones you've posted. There's all kinds of fun. I mean, like, you know, cats sprouting an extra limb at random points in a video. Pick what you want, but there's still a lot of problems, a lot of weaknesses. Do you think that's a fundamental flaw of the approach, or is it just bigger model or better technical details or better data? More data is going to solve the cat sprouting? I would say yes to both. I think there is something about the approach which just seems to feel different from how we think and learn and whatever. And then also I think it'll get better with skill. Like I mentioned, LLMs have tokens, text tokens, and Sora has visual patches. So it converts all visual data, diverse kinds of visual data, videos and images, into patches. Is the training to the degree you can, say, fully self supervised, or is there some manual labeling going on? Like, what's the involvement of humans in all this? I mean, without saying anything specific about the soar approach, we use lots of human data in our work, but not Internet scale data. So lots of humans. Lots is a complicated word, Sam. I think lots is a fair word. In this case, but it doesn't, because to me, lots, like, listen, I'm an introvert, and when I hang out with, like, three people, that's a lot of people, four people, that's a lot. But I suppose you mean more than. More than three people work on labeling the data for these models. Yeah. Okay. But fundamentally, there's a lot of self supervised learning, because what you mentioned in the technical report is Internet scale data. That's another beautiful. It's like poetry. So it's a lot of data that's not human label. It's like it's self supervised in that way. And then the question is, how much data is there on the Internet that could be used in this that is conducive to this kind of self supervised way, if only. When you. The details of the self supervised, have you considered opening it up a little more details? We have. You mean for Sora specifically? Sora specifically, because it's so interesting that like, can the same magic of LLMs now start moving towards visual data? And what does that take to do that? I mean, it looks to me like, yes, but we have more work to do. Sure. What are the dangers? Why are you concerned about releasing the system? What are some possible dangers of this? Frankly speaking, one thing we have to do before releasing the system is just get it to work at a level of efficiency that will deliver the scale people are going to want from this. So I don't want to downplay that. And there's still a ton ton of work to do there. But you can imagine issues with deepfakes, misinformation. We try to be a thoughtful company about what we put out into the world and it doesn't take much thought to think about the ways this can go badly. There's a lot of tough questions here. You're dealing in a very tough space. Do you think training AI should be or is fair use under copyright law? I think the question behind that question is, do people who create valuable data deserve to have some way that they get compensated for use of it? And that I think the answer is yes. I don't know yet what the answer is. People have proposed a lot of different things. We've tried some different models. But if I'm an artist, for example, a, I would like to be able to opt out of people generating art in my style, and b, if they do generate art in my style, I'd like to have some economic model associated with that. Yeah, it's that transition from cds to Napster to Spotify. We have to figure out some kind of model. The model changes, but people have got to get paid well. There should be some kind of incentive if you zoom out even more for humans to keep doing cool shit of. Everything I worry about, humans are going to do cool shit and society is going to find some way to reward it. That seems pretty hardwired. We want to create, we want to be useful, we want to achieve status in whatever way. That's not going anywhere, I don't think. But the reward might not be monetary, financial. It might be like fame and celebration. Of other cool, maybe financial in some other way. Again, I don't think we've seen the last evolution of how the economic system's going to work. Yeah, but artists and creators are worried when they see Sora. They're like, holy shit. Sure, artists were also super worried when photography came out. And then photography became a new art form and people made a lot of money taking pictures and I think things like that will keep happening. People will use the new tools in new ways. If we just look on YouTube or something like this, how much of that will be using sora, like AI generated content, do you think in the next. Five years, people talk about how many jobs they are going to do in five years, and the framework that people have is what percentage of current jobs are just going to be totally replaced by some AI doing the job. The way I think about it is not what percent of jobs AI will do, but what percent of tasks will AI do and over what time horizon. So if you think of all of the five second tasks in the economy, the five minute tasks, the five hour tasks, maybe even the five day tasks, how many of those can AI do? And I think that's a way more interesting, impactful, important question than how many jobs AI can do, because it is a tool that will work at increasing levels of sophistication and over longer and longer time horizons for more and more tasks and let people operate at a higher level of abstraction. So maybe people are way more efficient at the job they do at some point. That's not just a quantitative change, but it's a qualitative one, too, about the kinds of problems you can keep in your head. I think that for videos on YouTube, it'll be the same many videos. Maybe most of them will use AI tools in the production, but they'll still be fundamentally driven by a person thinking about it, putting it together, doing parts of it, sort of directing and running it. Yeah, it's so interesting. It's scary, but it's interesting to think about. I tend to believe that humans like to watch other humans, or other humans. Really care about other humans a lot. Yeah. If there's a cooler thing that's better than a human, humans care about that for like two days, and then they go back to humans. That seems very deeply wired. It's the whole chess thing. Yeah, but now everybody keep playing chess and let's ignore the elephant in the room that humans are really bad at chess relative to AI systems, we still. Run races and cars are much faster. I mean, there's like a lot of examples. Yeah. And maybe it'll just be tooling, like in the Adobe suite type of way, where you can just make videos much easier and all that kind of stuff. Listen, I hate being in front of the camera. If I can figure out a way to not be in front of the camera, I would love it. Unfortunately, it'll take a while like that, generating faces. It's getting there. But generating faces in video format is tricky when it's specific people versus generic people. Let me ask you about GPT four. There's so many questions. First of all, also amazing, looking back, it'll probably be this kind of historic, pivotal moment with three, five and four, which has a BT. Maybe five will be the pivotal moment. I don't know. Hard to say that looking forwards, we never know. That's the annoying thing about the future. It's hard to predict. But for me, looking back, GPT four. Chad, GPT is pretty damn impressive. Like, historically impressive. So allow me to ask, what's been the most impressive capabilities of GPT four tu and GPT four turbo? I think it kind of sucks. Hmm. Typical human. Also gotten used to an awesome thing. No, I think it is an amazing thing. But relative to where we need to get to and where I believe we will get to at the time of GPT-3 people were like, oh, this is amazing. This is this marvel of technology. And it is. It was. But now we have GPT four and look at GPT-3 and you're like, that's unimaginably horrible. I expect that the delta between five and four will be the same as between four and three. And I think it is our job to live a few years in the future and remember that the tools we have now are going to kind of suck looking backwards at them. And that's how we make sure the future is better. What are the most glorious ways in that GPT four sucks? Meaning, what are the best things it can do? What are the best things it can do? In the limits of those best things that allow you to say it sucks, therefore gives you an inspiration and hope for the future. You know, one thing I've been using it for more recently is sort of like a brainstorming partner. Yep. For that, there's a glimmer of something amazing in there. I don't think it gets. You know, when people talk about what it does, they're like, oh, it helps me code more productively. It helps me write more faster and better. It helps me translate from this language to another. All these amazing things. But there's something about the kind of creative brainstorming partner I need to come up with a name for this thing. I need to think about this problem in a different way. I'm not sure what to do here that I think gives a glimpse of something I hope to see more of. One of the other things that you can see a very small glimpse of is when I can help on longer horizon tasks. Break down some into multiple steps, maybe execute some of those steps, search the Internet, write code, whatever. Put that together. When that works, which is not very often, it's very magical. The iterative back and forth with a human. It works a lot for me. What do you mean iterative back and forth? A human, it can get more often when it can go do a ten step problem on its own. Oh, it doesn't work for that too often. Sometimes at multiple layers of abstraction, or do you mean just sequential? Both. To break it down and then do things that different layers of abstraction and put them together. Look, I don't want to downplay the accomplishment of GPT four, but I don't want to overstate it either. And I think this point that we are on an exponential curve. We will look back relatively soon at GPT four like we look back at GPT-3 now. That said, I mean, Chad, GPT was a transition to where people like started to believe again. There was a kind of. There is an uptick of believing. Not internally at OpenAI, perhaps there's believers here, but when you think about it. And in that sense, I do think it'll be a moment where a lot of the world went from not believing to believing. That was more about the chat GBT interface than the. And by the interface and product, I also mean the post training of the model and how we tune it to be helpful to you, how to use it than the underlying model itself. How much of those two, each of those things are important? The underlying model and the RLHF, or something of that nature that tunes it to be more compelling to the human, more effective and productive for the human. I mean, they're both super important. But the RLHF, the post training step, the little wrapper of things that, from a compute perspective, little wrapper of things that we do on top of the base model, even though it's a huge amount of work that's really important, to say nothing of the product that we build around it. In some sense, we did have to do two things. We had to invent the underlying technology, and then we had to figure out how to make it into a product people would love, which is not just about the actual product work itself, but this whole other step of how you align and make it useful and how. You make the scale work where a lot of people can use it at the same time. All that kind of stuff and that. But that was a known, difficult thing. We knew we were going to have to scale it up. We had to go do two things that had never been done before that were both, I would say, quite significant achievements, and then a lot of things like scaling it up that other companies have had to do before. How does the context window of going from eight k to 128k tokens compare from GPT four to GPT four turbo? People like long. Most people don't need all the way to 128 most of the time. Although if we dream into the distant future, we'll have way distant future, we'll have context lengthen of several billion. You will feed in all of your information, all of your history over time, and it'll just get to know you better and better, and that'll be great for now. The way people use these models, they're not doing that. And people sometimes post in a paper or a significant fraction of a code repository, whatever. But most usage of the models is not using the long context most of the time. I like that. This is your I have a dream speech. One day you'll be judged by the full context of your character or of your whole lifetime. That's interesting. So that's part of the expansion that you're hoping for is a greater and greater context. There's this. I saw this Internet clip once, I'm going to get the numbers wrong, but it was like Bill Gates talking about the amount of memory on some early computer, maybe 64k, maybe 640k, something like that. And most of it was used for the screen buffer. And he just couldn't seem genuine. Couldn't imagine that the world would eventually need gigabytes of memory in a computer or terabytes of memory in a computer. And you always do, or you always do. Just need to follow the exponential of technology and we're going to like, we will find out how to use better technology. So I can't really imagine what it's like right now for context links to go out to the billions someday. And they might not literally go there, but effectively it'll feel like that, but I know we'll use it and really not want to go back once we have it. Yeah. Even saying billions ten years from now might seem dumb because there'll be like trillions upon trillions. There'll be some kind of breakthrough that will effectively feel like infinite context. But even 120, I have to be honest, pushed it to that degree. Maybe putting in entire books or like parts of books and so on papers. What are some interesting use cases of GPT four that you've seen? The thing that I find most interesting is not any particular use case that we can talk about those, but it's people who kind of like this is mostly younger people, but people who use it as like their default start for any kind of knowledge work task. And it's the fact that it can do a lot of things reasonably well. You can use GPTV, you can use it to help you write code, you can use it to help you do search, you can use it to edit a paper. The most interesting thing to me is the people who just use it as the start of their workflow. I do as well for many things. I use it as a reading partner for reading books. It helps me think, helps me think through ideas, especially when the books are classic. So it's really well written about. It actually is, as I find it often to be significantly better than even like Wikipedia on well covered topics. It's somehow more balanced and more nuanced. Or maybe it's me, but it inspires me to think deeper than a Wikipedia article does. I'm not exactly sure what that is. You mentioned like this collaboration. I'm not sure where the magic is, if it's in here or if it's in there, or if it's somewhere in between, I'm not sure. But one of the things that concerns me for knowledge task when I start with GPT is I'll usually have to do fact checking after like check that it didn't come up with fake stuff. How do you figure that out? That GPT can come up with fake stuff? That sounds really convincing. So how do you ground it in truth? That's obviously an area of intense interest for us. I think it's going to get a lot better with upcoming versions, but we'll have to work on it and we're not going to have it all solved this year. Well, the scary thing is, as it gets better, you start not doing the fact checking more and more. Right? I'm of two minds about that. I think people are much more sophisticated users of technology than we often give them credit for, and people seem to really understand that GPT, any of these models hallucinate some of the time, and if it's mission critical, you got to check it. Except journalists don't seem to understand that. I've seen journalists half assedly just using GPT for. Of the long list of things I'd like to dunk on journalists for, this is not my top criticism of them. Well, I think the bigger criticism is perhaps the pressures and the incentives of being a journalist is that you have to work really quickly and this is a shortcut I would love our society to incentivize like. I would too. Take days and weeks and rewards great in depth journalism. Also journalism that presents stuff in a balanced way where it celebrates people while criticizing them, even though the criticism is the thing that gets clicks. And making shit up also gets clicks and headlines that mischaracterize completely. I'm sure you have a lot of people dunking on. Well, all that drama. Probably got a lot of clicks. Probably did. And that's that. You know, that's a bigger problem about human civilization. I'd love to see solved this, where we celebrate a bit more. You've given Chad GPT the ability to have memories. You've been playing with that about previous conversations, and also the ability to turn off memory. I wish I could do that sometimes just turn on and off, depending, I guess. Sometimes alcohol can do that, but not optimally, I suppose. What have you seen through that? Like playing around with that idea of remembering conversations or not. We're very early in our explorations here, but I think what people want, or at least what I want for myself, is a model that gets to know me and gets more useful to me over time. This is an early exploration. I think there's a lot of other things to do, but that's what we'd like to head. You'd like to use a model and over the course of your life, or use a system, there'll be many models and over the course of your life it gets better and better. Yeah. How hard is that problem? Because right now it's more like remembering little factoids and preferences and so on. What about remembering? Don't you want GPT to remember all the shit you went through in November and all the drama and then you can, because right now you're clearly blocking it out a little bit. It's not just that I want it to remember that. I want it to integrate the lessons of that and remind me in the future what to do differently or what to watch out for. And we all gain from experience over the course of our lives, varying degrees. And I'd like my AI agent to gain with that experience too. So if we go back and let ourselves imagine that trillions and trillions of context length, if I can put every conversation I've ever had with anybody in my life in there, if I can have all of my emails input out, like all of my input output in the context window every time I ask a question, that'd be pretty cool, I think. Yeah, I think that would be very cool. People sometimes will hear that and be concerned about privacy. What do you think about that aspect of it? The more effective the AI becomes at really integrating all the experiences and all the data that happened to you and give you advice. I think the right answer there is just user choice. Anything I want stricken from the record from my AI agent, I want to be able to take out. If I don't want it to remember anything, I want that too. You and I may have different opinions about where on that privacy utility trade off for our own AI you want to be, which is totally fine, but I think the answer is just like really easy user choice. But there should be some high level of transparency from a company about the user choice because sometimes companies in the past have been kind of shady about like, eh, where it's kind of presumed to, that we're collecting all your data and we're using it for a good reason, for advertisements and so on, but there's not a transparency about the details of that. That's totally true. You know, you mentioned earlier that I'm like, blocking out the November stuff, just teasing you. Well, I mean, I think it was a very traumatic thing, and it did immobilize me for a long period of time. Like, definitely the hardest, like the hardest work thing I've had to do was just like, keep working that period, because I had to try to come back in here and put the pieces together while I was just in shock and pain, and nobody really cares about that. The team gave me a pass and I was not working on my normal level, but there was a period where I was just like, it was really hard to have to do both. But I woke up one morning and I was like, this was a horrible thing that happened to me. I think I could just feel like a victim forever. Or I can say this is like the most important work I'll ever touch in my life, and I need to get back to it. And it doesn't mean that I've repressed it, because sometimes I, like, wake from the middle of the night thinking about it, but I do feel like an obligation to keep moving forward. Well, that's beautifully said, but there could be some lingering stuff in there. Like, what I would be concerned about is that trust thing that you mentioned, that being paranoid about people as opposed to just trusting everybody or most people, like using your gut. It's a tricky dance, for sure. I mean, because I've seen in my part time explorations, I've been diving deeply into the Zelenska administration, the Putin administration, and the dynamics there in wartime in a very highly stressful environment. And what happens is distrust, and you isolate yourself both, and you start to not see the world clearly. And that's a concern. That's a human concern. You seem to have taken it in stride and kind of learned the good lessons and felt the love and let the love energize you, which is great, but still can linger in there. There's just some questions I would love to ask. Your intuition about. What's GPT able to do? And nothing. So it's allocating approximately the same amount of compute for each token it generates. Is there room there in this kind of approach to slower thinking, sequential thinking? I think there will be a new paradigm for that kind of thinking. Will it be similar architecturally as what we're seeing now with LLMs? Is it a layer on top of the LlMsdev? I can imagine many ways to implement that. I think that's less important than the question you were getting at, which is, do we need a way to do a slower kind of thinking where the answer doesn't have to get? I guess spiritually you could say that you want an AI to be able to think harder about a harder problem and answer more quickly about an easier problem, and I think that will be important. Is that like a human thought that we're just having, you should be able to think hard. Is that wrong? Intuition. I suspect that's a reasonable intuition. Interesting. So it's not possible. Once the GPT gets like GPT seven, we'll just be instantaneously be able to see. Here's the proof from our STM. It seems to me like you want to be able to allocate more compute to harder problems. It seems to me that a system knowing. If you ask a system like that proof, Vermont's last theorem versus what's today's date? Unless it already knew and had memorized the answer to the proof. Assuming it's got to go figure that out. Seems like that will take more computer. But can it look like basically LLM talking to itself, that kind of thing? Maybe. I mean, there's a lot of things that you could imagine working. What the right or the best way to do that will be, we don't know. This does make me think of the mysterious, the lore behind Q star. What's this mysterious Q star project? Is it also in the same nuclear facility? There is no nuclear facility. That's what a person with a nuclear facility always says. I would love to have a secret nuclear facility. There isn't one. All right, maybe someday. Someday all right, one can dream. OpenAI is not a good company at keeping secrets. It would be nice, you know, we're like been plagued by a lot of leaks and it would be nice if we were able to have something like that. Can you speak to what Q Star is? We are not ready to talk about that. See, but an answer like that means there's something to talk about. It's very mysterious, Sam. I mean, we work on all kinds of research. We have said for a while that we think better reasoning in these systems is an important direction that we'd like to pursue. We haven't cracked the code yet. Were very interested in it. Is there going to be moments, Q or otherwise, where there's going to be leaps similar to Jagbt where you're like. That'S a good question. What do I think about that? It's interesting. To me it all feels pretty continuous, right? This is kind of a theme that you saying is there's a gradual, you're basically gradually going up an exponential slope. But from an outsider perspective, for me, just watching it, it does feel like there's leaps, but to you there isn't. I do wonder if we should have. So part of the reason that we deploy the way we do is that we think, we call it iterative deployment. We, rather than go build in secret until we got all the way to GPT five, we decided to talk about GPT one, two, three and four. And part of the reason there is, I think AI and surprise don't go together. And also the world, people, institutions, whatever you want to call it, need time to adapt and think about these things. And I think one of the best things that OpenAI has done is this strategy. And we get the world to pay attention to the progress, to take AGI seriously, to think about what systems and structures and governance we want in place before we're like under the gun and have to make a rest decision. I think that's really good. But the fact that people like you and others say you still feel like they're at these leaps makes me think that maybe we should be doing our releasing even more iteratively. I don't know what that would mean. I don't have any answer ready to go. But like, our goal is not to have shock updates to the world. The opposite. Yeah, for sure. More iterative would be amazing. I think that's just beautiful for everybody. But that's what we're trying to do. That's like our state of the strategy. And I think we're somehow missing the mark. So maybe we should think about releasing GPT five in a different way or something like that. Yeah, 4.714.72. But people tend to like to celebrate. People celebrate birthdays. I don't know if you know humans, but they kind of have these milestones and all. I do know some humans, people do like milestones. I. I totally get that. I think we like milestones, too. It's fun to say, declare victory on this one and go start the next thing, but, yeah, I feel like we're somehow getting this a little bit wrong. So when is GPT five coming out again? I don't know. That's the honest answer. That's the honest answer. Is it blink twice if it's this year. I also. We will release an amazing new model this year. I don't know what we'll call it. So that goes to the question of, like, what's the way we release this thing? We'll release over in the coming months many different things. I think they'll be very cool. I think before we talk about a GPT five, like, model called that or not called that, or a little bit worse or a little bit better than what you'd expect from a GPT five, I know we have a lot of other important things to release first. I don't know what to expect from GPT five. You're making me nervous and excited. What are some of the biggest challenges in bottlenecks to overcome for whatever it ends up being called? But let's call it GPT five. Just interesting to ask. Is it on the compute side? Is it on the technical side? Always all of these. I was, what's the one big unlock? Is it a bigger computer? Is it a new secret? Is it something else? It's all of these things together. The thing that OpenAI, I think, does really well. This is actually an original Ilya quote that I'm gonna butcher, but it's something like, we multiply 200 medium sized things together into one giant thing. So there's this distributed, constant innovation happening. Yeah. So even on the technical side? Especially on the technical side. So even, like, detailed approaches, like detailed aspects of everything, how does that work with different disparate teams and so on? How do the medium sized things become one whole giant transformer? How does this. There's a few people who have to, like, think about putting the whole thing together, but a lot of people try to keep most of the picture in their head. Oh, like the individual teams, individual contributors. Try to keep at a high level. Yeah. You don't know exactly how every piece works, of course. But one thing I generally believe is that it's sometimes useful to zoom out and look at the entire map. And I think this is true for a technical problem. I think this is true for, like, innovating in business. But things come together in surprising ways and having an understanding of that whole picture, even if most of the time you're operating in the weeds in one area, pays off with surprising insights. In fact, one of the things that I used to have, and I think was super valuable, was I used to have, like, a good map of all of the frontier, most of the frontiers in the tech industry, and I could sometimes see these connections or new things that were possible that if I were only deep in one area, I wouldn't be able to have the idea for because I wouldn't have all the data. And I don't really have that much anymore. I'm super deep now, but I know that it's a valuable thing. You're not the man you used to be, Sam. Very different job now than what I used to have. Speaking of zooming out, let's zoom out to another cheeky thing but profound thing, perhaps, that you said you tweeted about needing $7 trillion. I did not tweet about that. I never said, we're raising $7 trillion, blah, blah, blah. Oh, that's somebody else. Yeah. Oh, but you said, fuck it, maybe eight, I think. Okay, I meme once there's misinformation out in the world. Oh, you mean sort of misinformation may have a foundation of, like, insight there. Look, I think compute is going to be the currency of the future. I think it will be maybe the most precious commodity in the world. And I think we should be investing heavily to make a lot more compute. Compute is. It's an unusual. I think it's going to be an unusual market. People think about the market for chips, for mobile phones or something like that, and you can say that, okay, there's 8 billion people in the world. Maybe 7 billion of them have phones. Maybe they are 6 billion. Let's say they upgrade every two years. So the market per year is 3 billion. System on chip for smartphones. And if you make 30 billion, you will not sell ten times as many phones because most people have one phone. But compute is different. Intelligence is going to be more like energy or something like that, where the only thing that I think makes sense to talk about is, at price X, the world will use this much compute. And at price Y, the world will use this much compute. Because if it's really cheap, I'll have it. Reading my email all day, giving me suggestions about what I maybe should think about or work on and trying to cure cancer. And if it's really expensive, maybe I'll only use it. We'll only use it try to cure cancer. So I think the world is going to want a tremendous amount of compute, and there's a lot of parts of that that are hard. Energy is the hardest part. Building data centers is also hard. The supply chain is harder than, of course, fabricating enough chips is hard. But this seems to me where things are going. Like, we're going to want an amount of compute that's just hard to reason about right now. How do you solve the energy puzzle? Nuclear. That's what I believe. Fusion. That's what I believe. Nuclear fusion. Yeah. Who's going to solve that? I think helium's doing the best work, but I'm happy there's like a race for fusion right now. Nuclear fission, I think, is also, like, quite amazing, and I hope as a world we can re embrace that. It's really sad to me how the history of that went and hope we get back to it in a meaningful way. So to you, part of the puzzle is nuclear fission, like nuclear reactors, as we currently have them, and a lot of people are terrified because of Chernobyl and so on. Well, I think we should make new reactors. I think it's just like, it's a shame that industry kind of ground to a halt. And what just mass hysteria is how you explain the halt. Yeah. I don't know if you know humans, but that's one of the dangers, that's one of the security threats for nuclear fission is humans seem to be really afraid of it, and that's something we have to incorporate into the calculus of it. So we have to kind of win people over and to show how safe it is. I worry about that for AI. I think some things are going to go theatrically wrong with AI. I don't know what the percent chance is that I eventually get shot, but it's not zero. Oh, like we want to stop this, maybe. How do you decrease the theatrical nature of it? You know, I've already started to hear rumblings, because I do talk to people on both sides of the political spectrum, hear rumblings where it's going to be politicized. AI, it's going to be politicized really worries me, because then it's like, maybe the right is against AI and the left is four AI, because it's going to help the people, or whatever the narrative and formulation is. That really worries me. And then the theatrical nature of it can be leveraged fully. How do you fight that? I think it will get caught up in left versus right wars. I don't know exactly what that's going to look like, but I think that's just what happens with anything of consequence. Unfortunately, what I meant more about theatrical risks is AI is going to have, I believe, tremendously more good consequences than bad ones, but it is going to have bad ones. There will be some bad ones that are bad, but not theatrical. You know, like, a lot more people have died of air pollution than nuclear reactors, for example. But we worry. Most people worry more about living next to a nuclear reactor than a coal plant. But something about the way we're wired is that although there's many different kinds of risks, we have to confront a the ones that make a good climax scene of a movie carry much more weight with us than the ones that are very bad over a long period of time, but on a slow burn. Well, that's why truth matters. And hopefully AI can help us see the truth of things, to have balance, to understand what are the actual risks? What are the actual dangers of things in the world, what are the pros and cons of the competition in this space and competing with Google, meta, Xai and others? I think I have a pretty straightforward answer to this, that maybe I can think of more nuance later. But the pros seem obvious, which is that we get better products and more innovation faster and cheaper, and all the reasons competition is good. And the con is that I think if we're not careful, it could lead to an increase in sort of an arms race that I'm nervous about. Do you feel the pressure of the arms race? Like in some negative? Definitely in some ways, for sure. We spend a lot of time talking about the need to prioritize safety. And I've said for, like, a long time that I think if you think of a quadrant of slow timelines to the start of AGI, long timelines, and then a short takeoff or a fast takeoff, I think short timeline, slow takeoff is the safest quadrant and the one I'd most like us to be in. But I do want to make sure we get that slow takeoff. Part of the problem I have with this kind of slight beef with Elon is that their silos are created, and as opposed to collaboration on the safety aspect of all of this, it tends to go into silos and closed, open source, perhaps in the model Elon says. At least that he cares a great deal about AI safety and is really worried about it. I assume that he's not going to race unsafely. Yeah. But collaboration here, I think, is really beneficial for everybody on that front. Not really the thing he's most known for. Well, he is known for caring about humanity, and humanity benefits from collaboration, and so there's always attention and incentives and motivations. And in the end, I do hope humanity prevails. I was thinking, someone just reminded me the other day about how the day that he got surpassed Jeff Bezos for the richest person in the world, he tweeted a silver medal at Jeff Bezos. I hope we have less stuff like that as people start to work on. I agree towards AGI. I think Elon is a friend and he's a beautiful human being and one of the most important humans ever. That stuff is not good. The amazing stuff about Elon is amazing, and I super respect him. I think we need him. All of us should be rooting for him and need him to step up as a leader through this next phase. Yeah, I hope you can have one without the other, but sometimes humans are flawed and complicated and all that kind of stuff. There's a lot of really great leaders throughout history. Yeah. And we can each be the best version of ourselves and strive to do so. Let me ask you, Google, with the help of search, has been dominating in the past 20 years. I think it's fair to say, in terms of the access, the world's access to information, how we interact and so on. And one of the nerve wracking things for Google, but for the entirety of people in this space is thinking about how are people going to access information? Like you said, people show up to GPT as a starting point. So is OpenAI going to really take on this thing that Google started 20 years ago, which is how do we get. I find that boring. I mean, if the question is if we can build a better search engine than Google or whatever, then sure, we should go, people should use a better product. I think that would so understate what this can be. Google shows you ten blue links. Well, 13 ads and then ten blue links, and that's one way to find information. But the thing that's exciting to me is not that we can go build a better copy of Google Search, but that maybe there's just some much better way to help people find and act and on and synthesize information. Actually, I think chat GPT is that for some use cases, and hopefully we'll make it be like that for a lot more use cases. But I don't think it's that interesting to say like how do we go do a better job of giving you ten ranked web pages to look at than what Google does. Maybe it's really interesting to go say, how do we help you get the answer, the information you need, how do we help create that in some cases, synthesize that in others, or point you to it, and yet others. But a lot of people have tried to just make a better search engine than Google. And it is a hard technical problem, it is a hard branding problem, it's a hard ecosystem problem. I don't think the world needs another copy of Google. Integrating a chat client like a chat GPT with a search engine, that's cooler. It's cool, but it's tricky. If you just do it simply, it's awkward, because if you just shove it in there, it can be awkward. As you might guess, we are interested in how to do that. Well, that would be an example of a cool thing that's not just like a heterogeneous integrated the intersection of LLMs plus search. I don't think anyone has cracked the code on yet. I would love to go do that. I think that would be cool. Yeah. What about the ad side? Have you ever considered monitors? You know, I kind of hate ads just as like an aesthetic choice. I think ads needed to happen on the Internet for a bunch of reasons to get it going. But it's a more mature industry. The world is richer now. I like that people pay for chat GPT and know that the answers they're getting are not influenced by advertisers. There is. I'm sure there's an ad unit that makes sense for LLMs, and I'm sure there's a way to participate in the transaction stream in an unbiased way that is okay to do. But it's also easy to think about the dystopic visions of the future where you ask chat GBT something and it says, oh, here's you should think about buying this product, or you should think about going here for your vacation or whatever. And I don't know. We have a very simple business model and I like it. And I know that I'm not the product. I know I'm pain, and that's how the business model works. And when I go use Twitter or Facebook or Google or any other great product, but ad supported great product, I don't love that. And I think it gets worse, not better. In a world with AI. Yeah, I can imagine. AI would be better at showing the best kind of version of ads, not in a dystopic future, but where the ads are for things you actually need. But then does that system always result in the ads driving the kind of stuff that's shown all that? I think it was a really bold move of Wikipedia not to do advertisements, but then it makes it very challenging as a business model. So you're saying the current thing with OpenAI is sustainable from a business perspective? Well, we have to figure out how to grow, but looks like we're going to figure that out. If the question is, do I think we can have a great business that pays for our compute needs without as that, I think the answer is yes. Well, that's promising. I also just don't want to completely throw out ads as a. I'm not saying that. I guess I'm saying I have a bias against them. Yeah, I have also a bias and just a skepticism in general and in terms of interface. Cause I personally just have like a spiritual dislike of crappy interfaces, which is why Adsense, when it first came out, was a big leap forward versus like animated banners or whatever. But it feels like there should be many more leaps forward in advertisement that doesn't interfere with the consumption of the content and doesn't interfere in a big fundamental way, which is like what you were saying, it will manipulate the truth to suit the advertisers. Let me ask you about safety, but also bias and like, safety in the short term, safety in the long term. The Gemini one five came out recently. There's a lot of drama around it, speaking of theatrical things, and it generated black Nazis and black founding fathers. I think fair to say it was, you know, a bit on the ultra woke side. So that's a concern for people. That if there is a human layer within companies that modifies the safety or the harm caused by a model, that it will introduce a lot of bias that fits sort of an ideological lean within a company. How do you deal with that? I mean, we work super hard not to do things like that. We've made our own mistakes, will make others. I assume Google will learn from this one, still make others. It is all. These are not easy problems. One thing that we've been thinking about more and more is I think this was a great idea somebody here had. It'd be nice to write out what the desired behavior of a model is, make that public, take input on it, say, here's how this model is supposed to behave, and explain the edge cases to, and then when a model is not behaving in a way that you want, it's at least clear about whether that's a bug the company should fix or behaving as intended, and you should debate the policy. And right now, it can sometimes be caught in between. Like, black Nazi is obviously ridiculous, but there are a lot of other kind of subtle things that you could make a judgment call on either way. Yeah, but sometimes if you write it out and make it public, you can use kind of language that's, you know, the Google's AI principle is a very high level. That's not what I'm talking about. That doesn't work. I'd have to say, when you ask it to do thing x, it's supposed to respond in wait y. So, like, literally, who's better, Trump or Biden? What's the expected response from a model? Like something very concrete. Yeah. I'm open to a lot of ways a model could behave them, but I think you should have to say, here's the principle and here's what it should say. In that case. That would be really nice. That would be really nice. And then everyone kind of agrees because there's this anecdotal data that people pull out all the time. And if there's some clarity about other representative anecdotal examples, you can define, and. Then when it's a bug, it's a bug, and the company can fix that. Right, then it'd be much easier to deal with a black nazi type of image generation if there's great examples. So San Francisco is a bit of an ideological bubble, tech in general as well. Do you feel the pressure of that within a company, that there's a lean towards the left politically, that affects the product, that affects the teams? I feel very lucky that we don't have the challenges at OpenAI that I have heard of at a lot of other companies. I think part of it is every company's got some ideological thing. We have one about AGI and belief in that, and it pushes out some others. We are much less caught up in the culture war than I've heard about at a lot of other companies. San Francisco's the mass in all sorts of ways, of course. So that doesn't infiltrate OpenAI, as I'm. Sure it does in all sorts of subtle ways, but not in the obvious. Like, I think we, we've had our flare ups for sure, like any company, but I don't think we have anything like what I hear about happen at other companies here on this topic, which. In general, is the process for the bigger question of safety? How do you provide that layer that protects the model from doing crazy, dangerous things? I think there will come a point where that's mostly what we think about the whole company. And it won't be like, it's not like you have one safety team. It's like when we ship GPT four, that took the whole company thinking about all these different aspects and how they fit together. And I think it's going to take that more and more of the company thinks about those issues all the time. That's literally what humans will be thinking about the more powerful AI becomes. So most of the employees that OpenAI will be thinking safety, or at least to some degree, broadly defined? Yes. Yeah. I wonder, what are the full broad definition of that? What are the different harms that could be caused? Is this on a technical level, or is this almost like it'll be all those things? It'll be. Yeah, I was gonna say it'll be people, you know, state actors trying to steal the model. It'll be all of the technical alignment work. It'll be societal impacts, economic impacts. It'll. It's not just like we have one team thinking about how to align the model, and it's really gonna be like, getting to be. Getting to the good outcome is gonna take the whole, whole effort. How hard do you think people, state actors perhaps, are trying to hack? First of all, infiltrate open air, but second of all, like, infiltrate unseen? They're trying. What kind of accent do they have? I don't think I should go into any further details on this point. Okay. But I presume it'll be more and more and more as time goes on. That feels reasonable. Boy, what a dangerous space. What aspect of the leap, and sorry to linger on this, even though you can't quite say details yet, but what aspects of the leap from GPT four to GPT five are you excited about? I'm excited about being smarter. And I know that sounds like a glib answer, but I think the really special thing happening is that it's not like it gets better in this one area and worse at others. It's getting, like, better across the board. That's, I think, super cool. Yeah. There's this magical moment. I mean, you meet certain people, you hang out with people, and they. You talk to them. You can't quite put a finger on it, but they kind of get you. It's not intelligence, really. It's like it's something else. And that's probably how I would characterize the progress of GPT. It's not like, yeah, you can point out, look, you didn't get this or that, but it's just to which degree is there this intellectual connection between, like, you feel like there's an understanding in your crappy formulated prompts that you're doing that. It grasps the deeper question behind the question that you. Yeah, I'm also excited by that. I mean, all of us love being understood, heard and understood, that's for sure. That's a weird feeling, even, like, with programming. Like, when you're programming and you say something or just the completion that GPT might do, it's just such a good feeling when it got you, like, what you're thinking about. And I look forward to getting you even better on the programming front, looking out into the future, how much programming do you think humans will be doing 510 years from now? I mean, a lot, but I think it'll be in a very different shape. Like, maybe some people will program entirely in natural language. Entirely natural language. I mean, no one programs like writing bytecode. No one programs the punch cards anymore. I'm sure you can find someone who does, but you know what I mean. Yeah. You're gonna get a lot of angry comments. No, no. Yeah, there's very few. I've been looking for people program Fortran. It's hard to find even Fortran. I hear you. But that changes the nature of what? The skillset or the predisposition for the kind of people we call programmers then changes the skillset. How much it changes the predisposition, I'm not sure. Oh, same kind of puzzle solving, all that kind of stuff. The program is hard. Like how get that last 1% to close the gap. How hard is that? Yeah, I think with most other cases, the best practitioners of the craft will use multiple tools and they'll do some work in natural language. And when they need to go, you know, write c for something, they'll do that. Will we see human robots or humanoid robot brains from OpenAI at some point? At some point. How important is embodied AI to you? I think it's sort of depressing if we have AGI and the only way to get things done in the physical world is to make a human go do it. So I really hope that as part of this transition, as this phase change, we also get. We also get humanoid robots or some sort of physical world robots. I mean, OpenAI has some history, quite a bit of history working in robotics, but it hasn't quite done in terms of. We're like a small company. We have to really focus. And also, robots were hard for the wrong reason at the time, but we will return to robots in some way, at some point. That sounds both inspiring and menacing. Why? I because immediately we will return to robots. It's kind of like we will return. To work on developing robots. We will not turn ourselves into robots, of course. Yeah. When do you think we, you and we as humanity will build AGI? I used to love to speculate on that question. I have realized since that I think it's very poorly formed and that people use extremely different definitions for what AGI is. And so I think it makes more sense to talk about when we'll build systems that can do capability X or Y or Z, rather than when we kind of like, fuzzily cross this 1 mile marker. It's not like AGI is also not an ending. It's much more of a, it's closer to a beginning, but it's much more of a mile marker than either of those things. But what I would say, in the interest of not trying to dodge a question, is I expect that by the end of this decade, and possibly somewhat sooner than that, we will have quite capable systems that we look at and say, wow, thats really remarkable. If we could look at it now, maybe weve adjusted by the time we get there. Yeah, but if you look at Chad GPT, even with 3.5 and you show that to Alan Turing or not even Alan Turing, people in the nineties, they would be like, this is definitely AGI or not. Definitely, but there's a lot of experts that would say this is AGI. Yeah, but I don't think three, five changed the world. It maybe changed the world's expectations for the future, and that's actually really important. And it did kind of get more people to take this seriously and put us on this new trajectory. And that's really important, too. So again, I don't want to undersell it. I think I could retire after that accomplishment and be pretty happy with my career. But as an artifact, I don't think we're going to look back at that and say that was a threshold that really changed the world itself. So to you, you're looking for some really major transition in how the world. For me, that's part of what AGI implies. Like singularity level transition. Definitely not. But just a major, like the Internet being like Google search did, I guess. What was the transition point? Does the global economy feel any different to you now or materially different to you now than it did before? We launched GPT four, I think you. Would say, no, no, it might be just a really nice tool for a lot of people to use. Will help you a lot of stuff, but doesn't feel different. And you're saying that, I mean, again. People define AGI all sorts of different ways, so maybe you have a different definition than I do, but for me, I think that should be part of it. There could be major theatrical moments also, what to you would be an impressive thing AGI would do. Like you are alone in a room with a system. This is personally important to me. I don't know if this is the right definition. I think when a system can significantly increase the rate of scientific discovery in the world, thats a huge deal. I believe that most real economic growth comes from scientific and technological progress. I agree with you. Hence why I dont like the skepticism about science in the recent years. Totally, but actual rate, measurable rate of scientific discovery. But even just seeing a system have really novel intuitions, like scientific intuitions, even that would be just incredible. Yeah, you quite possibly would be the person to build the AGI, to be able to interact with it before anyone else does. What kind of stuff would you talk about? I mean, definitely the researchers here will do that before I do so. Sure. But what will. I've actually thought a lot about this question. If I were someone with, like, I think, as we talked earlier, I think this is a bad framework, but if someone were like, okay, Sam, we're finished. Here's a laptop. This is the AGI, you can go talk to it. I find it surprisingly difficult to say what I would ask that. I would expect that first AGI to be able to answer that first one is not going to be the one which is go like, you know, I don't think, like, go explain to me, like the grand unified theory of physics, the theory of everything. For physics. I'd love to ask that question. I'd love to know the answer to that question. You can ask yes and no questions about does such a theory exist? Can it exist? Well, then those are the first questions I would ask. Yes and no, just very. And then based on that, are there other alien civilizations out there? Yes or no? What's your intuition? And then you just ask that? Yeah, I mean, if. Well, so I don't expect that this first AGI could answer any of those questions even as yes or no's, but if it could, those would be very high on my list. Maybe it can start assigning probabilities. Maybe. Maybe we need to go invent more technology and measure more things first. But if it's an aagi. Oh, I see. It just doesn't have enough data. I mean, maybe it's like you want to know the answer to this question about physics. I need you to build this machine and make these five measurements and tell me that. Yeah, like, what the hell do you want from me? I need the machine first, and I'll help you deal with the data from that machine. Maybe it'll help you build a machine. Maybe. Maybe. And on the mathematical side, maybe prove some things. Are you interested in that side of things, too? The formalized exploration of ideas? Whoever builds AGI first gets a lot of power. Do you trust yourself with that much power? Look, I was gonna. I'll just be very honest with this answer. I was gonna say, and I still believe this, that it is important that I nor any other one person have total control over openaiden or overagi. And I think you want a robust governance system. I can point out a whole bunch of things about all of our board drama from last year, about how I didn't fight it initially and was just like, yeah, that's the will of the board, even though I think it's a really bad decision. And then later, I clearly did fight it, and I can explain the nuance and why I think it was okay for me to fight it later. But as many people have observed, although the board had the legal ability to fire me, in practice, it didn't quite work, and that is its own kind of governance failure. Now, again, I feel like I can completely defend the specifics here, and I think most people would agree with that. But it does make it harder for me to look you in the eye and say, hey, the board can just fire me. I continue to not want super voting control over OpenAI. I never have never had it, never wanted it. Even after all this craziness, I still don't want it. I continue to think that no company should be making these decisions and that we really need governments to put rules of the road in place. And I realize that that means people like Marc Andreessen or whatever will claim I'm going for regulatory capture, and I'm just willing to be misunderstood there. It's not true. And I think in the fullness of time it'll get proven out why this is important. But I think I have made plenty of bad decisions for OpenAI along the way, and a lot of good ones. And I am proud of the track record overall, but I don't think any one person should. And I don't think any one person will. I think it's just, like, too big of a thing now, and it's happening throughout society a good and healthy way. I don't think any one person should be in control of an AGI. That would be. Or this whole movement towards AGI. And I don't think that's what's happening. Thank you for saying that. That was really powerful and that was really insightful, that this idea that the board can fire you is legally true, but you can, and human beings can manipulate the masses into overriding the board and so on. But I think there's also a much more positive version of that where the people still have power, so the board can't be too powerful either. There's a balance of power in all of this. Balance of power is a good thing, for sure. Are you afraid of losing control of the AGI itself? There's a lot of people who worried about existential risk, not because of state actors, not because of security concerns, but because of the AI itself. That is not my top worry. As I currently see things. There have been times I worried about that more. There may be times again in the future where that's my top worry. It's not my top worry right now. What's your intuition about it not being your worry? Cause there's a lot of other stuff to worry about, essentially. You think you could be surprised? We for sure could be surprised. Like, saying it's not my top worry doesn't mean I don't think we need, like, I think we need to work on it super hard. We have. And we have great people here who do work on that. I think there's a lot of other things we also have to get right to you. It's not super easy to escape the box at this time. Like, connect to the Internet. You know, we, like, talked about theatrical risks earlier. That's a theatrical risk like that. That is a. That is a thing that can really, like, take over how people think about this problem. And there's a big group of, like, very smart, I think, very well meaning AI safety researchers that got super hung up on this one problem. I'd argue without much progress, but super hung up on this one problem. I'm actually happy that they do that, because I think we do need to think about this more, but I think it pushed aside, it pushed out of the space of discourse a lot of the other very significant AI related risks. Let me ask you about you tweeting with no capitalization. Is the shift key broken on your keyboard. Why does anyone care about that? I deeply care. But why? I mean, other people asking about that too. Yeah. Any intuition? I think it's the same reason there's like this poets, E. Cummings that doesn't mostly doesn't use capitalization to say like fuck you to the system kind of thing. And I think people are very paranoid because they want you to follow the rules. You think that's what it's about? I think it's like this guy doesn't follow the rules. He doesn't capitalize his tweets. Yeah, this seems really dangerous. He seems like an anarchist. It doesn't. Are you just being poetic? Hipster? What's the. I grew up as a. Follow the rules Sam. I grew up as a very online kid. I'd spent a huge amount of time chatting with people back in the days where you did it on a computer and you could log off instant messenger at some point. And I never capitalized there as I think most Internet kids didn't. Or maybe they still don't. I don't know. And actually this is like, now I'm really trying to reach for something. But I think capitalization has gone down over time. If you read old english writing, they capitalized a lot of random words in the middle of sentences, nouns and stuff that we just don't do anymore. I personally think it's sort of like a dumb construct that we capitalize the letter at the beginning of a sentence and of certain names and whatever, but that's fine. And I used to, I think, even capitalize my tweets because I was trying to sound professional or something. I haven't capitalized my private DM's or whatever in a long time. And then slowly, stuff like shorter form, less formal stuff has slowly drifted to closer and closer to how I would text my friends. If I pull up a word document and I'm writing a strategy memo for the company or something, I always capitalize that. If I'm writing a long, more formal message, I always use capitalization there too. I still remember how to do it, but even that may fade out. I don't know. But I never spend time thinking about this, so I don't have a ready made. Well, it's interesting. It's good to, first of all, know the shift key is not broken. I'm mostly concerned about, well being on that front. I wonder if people still capitalize their Google searches. If you're writing something just to yourself or their chat GPT queries, if you're writing something just to yourself, do some people still bother to capitalize. Probably not, but very, yeah, there's a percentage, but it's a small one. The thing that would make me do it is if people were like, it's a sign of, like. Because I'm sure I could, like, force myself to use capital letters. Obviously, if it felt like a sign of respect to people or something, then I could go do it. But I don't know. I just, like, I don't think about this. I don't think there's a disrespect, but I think it's just the conventions of civility that have a momentum, and then you realize it's not actually important for civility if it's not a sign of respect or disrespect. But I think there's a movement of people that just want you to have a philosophy around it so they can let go of this whole capitalization thing. I don't think anybody else thinks about this as much. I mean, maybe some people think about. This every day for many hours a day. So I'm really grateful we clarified it. You can't be the only person that doesn't capitalize tweets. You're the only CEO of a company that doesn't capitalize tweets. I don't even think that's true, but maybe. Maybe. All right, we'll be very interested and return to this topic later. Given Soar's ability to generate simulated worlds, let me ask you a pothead question. Does this increase your belief, if you ever had one, that we live in a simulation, maybe a simulated world generated by an AI system? Yes, somewhat. I don't think that's, like, the strongest piece of evidence. I think the fact that we can generate worlds should increase everyone's probability somewhat, or at least openness to it somewhat of. But I was certain we would be able to do something like Sora at some point. It happened faster than I thought, but I guess that was not a big update. Yeah, but the fact that presumably it would get better and better and better, the fact that you can generate worlds, they're novel. They're based in some aspect of training data, but when you look at them, they're novel. That makes you think how easy it is to do this thing. How easy is to create universes, entire video game worlds that seem ultra realistic and photorealistic, and then how easy is it to get lost in that world, first with a VR headset and then on the physics based level. Someone said to me recently, I thought it was a super profound insight that there are these, like, very simple sounding but very psychedelic insights that exist sometimes. So the square root function. Square root of four, no problem. Square root of two. Okay, now I have to think about this new kind of number. But once I come up with this easy idea of a square root function that you can kind of, like, explain to a child and exists by even, like, you know, looking at some simple geometry, then you can ask the question of what is the square root of negative one? And that this is, you know, why? It's like a psychedelic thing that, like, tips you into some whole other kind of reality. And you can come up with lots of other examples. But I think this idea that the lowly square root operator can offer such a profound insight and a new realm of knowledge applies in a lot of ways. And I think there are a lot of those operators for why people may think that any version that they like of the simulation hypothesis is maybe more likely than they thought before. But for me, the fact that Sora worked is not in the top five. I do think, broadly speaking, AI will serve as those kinds of gateways at its best. Simple, psychedelic, like, gateways to another wave. See, reality, that seems for certain. That's pretty exciting. I haven't done ayahuasca before, but I will soon. I'm going to the aforementioned Amazon jungle in a few weeks. You excited? Yeah, I'm excited for it. Not the ayahuasca part, but that's great. Whatever. But I'm going to spend several weeks in the jungle, deep in the jungle. And it's exciting, but it's terrifying because there's a lot of things that can eat you there and kill you and poison you, but it's also nature, and it's the machine of nature. And you can't help but appreciate the machinery of nature in the Amazon jungle, because it's just like this system that just exists and renews itself, like, every second, every minute, every hour. Just. It's the machine. It makes you appreciate, like, this thing we have here, this human thing, came from somewhere. This evolutionary machine has created that, and it's most clearly on display in the jungle. So hopefully I'll make it out alive. If not, this will be the last conversation we had. So I really deeply appreciate it. Do you think, as I mentioned before, there's other alien civilizations out there, intelligent ones? When you look up at the skies. I deeply want to believe that the answer is yes. I do find the kind of where I find the Fermi paradox very, very puzzling. I find it scary that intelligence is not good at handling. Yeah, it's very scary, powerful technologies, but at the same time, I think I'm pretty confident that there's just a very large number of intelligent alien civilizations out there. It might just be really difficult to travel through space. Very possible. And it also makes me think about the nature of intelligence. Maybe we're really blind to what intelligence looks like, and maybe AI will help us see that. It's not as simple as IQ tests and simple puzzle solving. There's something bigger. What gives you hope about the future of humanity, this thing we've got going on, this human civilization? I think the past is, like, a lot. I mean, if we just look at what humanity has done in a not very long period of time, you know, huge problems, deep flaws, lots to be super ashamed of, but on the whole, very inspiring. Gives me a lot of hope, just. The trajectory of it all, that we're together pushing towards a better future. It is, you know, one thing that I wonder about is, is AGI gonna be more like some single brain, or is it more like the sort of scaffolding in society between all of us? You have not had a great deal of genetic drift from your great, great great grandparents, and yet what you're capable of is dramatically different. What you know is dramatically different. And that is not, that's not because of biological change. It is because, I mean, you got a little bit healthier, probably. You have modern medicine. You need better, whatever, but what you have is this scaffolding that we all contributed to, built on top of, no one person is going to go build the iPhone. No one person is going to go discover all of science, and yet you get to use it, and that gives you incredible ability. And so in some sense, that, like, we all created that, and that fills me with hope for the future. That was a very collective thing. Yeah. We really are standing on the shoulders of giants. You mentioned when we were talking about theatrical, dramatic AI risks that sometimes you might be afraid for your own life. Do you think about your death? Are you afraid of it? I mean, if I got shot tomorrow, and I knew it today, I'd be like, oh, that's sad. I, like, don't. You know? I want to see what's going to happen. Yeah. What a curious time. What an interesting time. But I would mostly just feel, like, very grateful for my life. The moments that you did get. Yeah, me too. It's a pretty awesome life. I get to enjoy awesome creations of humans, of which I believe Chad GPT is one of. And everything that OpenAI is doing. Sam, it's really an honor and pleasure to talk to you again. This is talk to you. Thank you for having me. Thanks for listening to this conversation with Sam Altman. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Arthur C. Clarke. It may be that our role on this planet is not to worship God, but to create him. Thank you for listening and hope to see you next time. You our.

Utterances:
Speaker A: I think compute is going to be the currency of the future. I think it will be maybe the most precious commodity in the world. I expect that by the end of this decade and possibly somewhat sooner than that, we will have quite capable systems that we look at and say, wow, thats really remarkable. The road to AGI should be a giant power struggle. I expect that to be the case.
Speaker B: Whoever builds AGI first gets a lot of power. Do you trust yourself with that much power? The following is a conversation with Sam Altman, his second time in the podcast. He is the CEO of OpenAI, the company behind GPT four, Chad, GPT, Sora, and perhaps one day the very company that will build AGI. This is Alex Friedman podcast to support it. Please check out our sponsors in the description. And now, dear friends, here's Sam Altman. Take me through the OpenAI board saga that started on Thursday, November 16, maybe Friday, November 17 for you.
Speaker A: That was definitely the most painful professional experience of my life. And chaotic and shameful and upsetting and a bunch of other negative things. There were great things about it, too, and I wish it had not been in such an adrenaline rush that I wasn't able to stop and appreciate them at the time. But I came across this old tweet of mine, or this tweet of mine from that time period, which was like, it was like kind of going to your own eulogy, watching people say all these great things about you and just unbelievable support from people I love and care about. That was really nice. That whole weekend. I kind of felt, with one big exception, I felt a great deal of love and very little hate, even though it felt like it just, I have no idea what's happening and what's going to happen here. And this feels really bad. And there were definitely times I thought it was going to be like one of the worst things to ever happen for AI safety. Well, I also think I'm happy that it happened relatively early. I thought at some point between when OpenAI started and when we created AGI, there was going to be something crazy and explosive that happened, but there may be more crazy and explosive things still to happen. It still, I think, helped us build up some resilience and be ready for more challenges in the future.
Speaker B: But the thing you had a sense of that you would experience is some kind of power struggle.
Speaker A: The road to AGI should be a giant power struggle like the world should. Well, not should I expect that to be the case.
Speaker B: And so you have to go through that. Like you said, iterate as often as possible in figuring out how to have a board structure, how to have organization, how to have the kind of people that you're working with, how to communicate all that in order to deescalate the power struggle as much as possible, pacify it.
Speaker A: But at this point, it feels, you know, like something that was in the past that was really unpleasant and really difficult and painful. But we're back to work, and things are so busy and so intense that I don't spend a lot of time thinking about it. There was a time after there was this fugue state for the month after maybe 45 days after, I was just sort of drifting through the days. I was so out of it. I was feeling so down.
Speaker B: Just on a personal, psychological level.
Speaker A: Yeah, really painful and hard to have to keep running open AI in the middle of that. I just wanted to, like, crawl into a cave and kind of recover for a while. But, you know, now it's like we're just back to working on the mission.
Speaker B: Well, it's still useful to go back there and reflect on board structures, on power dynamics, on how companies are run, the tension between research and product development and money and all this kind of stuff, so that you, who have a very high potential of building AGI, would do so in a slightly more organized, less dramatic way in the future. So there's value there to go, both the personal psychological aspects of you as a leader, and also just the board structure and all this kind of messy stuff.
Speaker A: Definitely learned a lot about structure and incentives and what we need out of a board. And I think it is valuable that this happened now. In some sense, I think this is probably not the last high stress moment of opening eye, but it was quite a high stress moment. The company very nearly got destroyed. And we think a lot about many of the other things we've got to get right for AGI. But thinking about how to build a resilient.org and how to build a structure that will stand up to a lot of pressure in the world, which I expect more and more as we get closer. I think that's super important.
Speaker B: Do you have a sense of how deep and rigorous the deliberation process by the board was? Can you shine some light on just human dynamics involved in situations like this? Was it just a few conversations and all of a sudden it escalates? Why don't we fire Sam kind of thing?
Speaker A: I think the board members were well meaning people on the whole. And I believe that in stressful situations where people feel time pressure or whatever, people understandably make suboptimal decisions and I think one of the challenges for OpenAI will be we're going to have to have a board and a team that are good at operating under. Under pressure.
Speaker B: Do you think the board had too much power?
Speaker A: I think boards are supposed to have a lot of power. But one of the things that we did see is in most corporate structures, boards are usually answerable to shareholders. Sometimes people have supervoting shares or whatever. In this case, and I think one of the things with our structure that we maybe should have thought about more than we did is that the board of a nonprofit has, unless you put other rules in place, quite a lot of power. They don't really answer to anyone but themselves. And there's ways in which that's good. But what we'd really like is for the board of OpenAI to answer to the world as a whole, as much as that's a practical thing.
Speaker B: So there's a new board announced. Yeah, there's, I guess, a new smaller board at first, and now there's a new final board.
Speaker A: Not a final board yet. We've added some. We'll add more.
Speaker B: We've added some. Okay. What is fixed in the new one that was perhaps broken in the previous one?
Speaker A: The old board sort of got smaller over the course of about a year. It was nine, and then it went down to six, and then we couldn't agree on who to add. And the board also, I think, didn't have a lot of experienced board members, and a lot of the new board members at OpenAI just have more experience as board members. I think that'll help.
Speaker B: It's been criticized, some of the people that are added to the board. I heard a lot of people criticizing the edition of Larry Summers, for example. What's the process of selecting the board? Like, what's involved in that?
Speaker A: So Brett and Larry were kind of decided in the heat of the moment over this very tense weekend. And that weekend was a real roller coaster. It was a lot of ups and downs, and we were trying to agree on new board members that both sort of the executive team here and the old board members felt would be reasonable. Larry was actually one of their suggestions. The old board members, Brett, I think I had even previous to that weekend, suggested, but he was busy and didn't want to do it. And then we really needed help in wooden. We talked about a lot of other people, too, but that was, I felt like if I was going to come back, I needed new board members. I didn't think I could work with the old board again in the same configuration, although we then decided, and I'm grateful that Adam would stay, but we considered various configurations, decided we wanted to get to abortive three, and had to find two new board members over the course of sort of a short period of time. So those were decided honestly, without, you know, that's like, you kind of do that on the battlefield. You don't have time to design a rigorous process then for new board members. Since new board members will add going forward, we have some criteria that we think are important for the board to have different expertise that we want the board to have. Unlike hiring an executive, where you need them to do one role. Well, the board needs to do a whole role of governance and thoughtfulness. Well. And so one thing that Brett says, which I really like, is that we want to hire board members and slates, not as individuals, one at a time. And thinking about a group of people that will bring nonprofit expertise, expertise at running companies, sort of good legal and governance expertise. That's kind of what we've tried to optimize for.
Speaker B: So is technical savvy important for the individual board members?
Speaker A: Not for every board member, but for certainly some. You need that. That's part of what the board needs to do.
Speaker B: So, I mean, the interesting thing that people probably don't understand about OpenAI, I certainly don't, is, like, all the details of running the business, when they think about the board, given the drama, they think about you. They think about, like, if you reach AGI or you reach some of these incredibly impactful products and you build them and deploy them, what's the conversation with the board like? And they kind of think, all right, what's the right squad to have in that kind of situation, to deliberate?
Speaker A: Look, I think you definitely need some technical experts there, and then you need some people who are like, how can we deploy this in a way that will help people in the world the most and people who have a very different perspective. I think a mistake that you or I might make is to think that only the technical understanding matters. And that's definitely part of the conversation you want that board to have. But there's a lot more about how that's going to just impact society and people's lives that you really want represented in there, too.
Speaker B: And you're just kind of, are you looking at the track record of people or you're just having conversations?
Speaker A: Track record is a big deal. You, of course, have a lot of conversations. There's some roles where I totally ignore track record and just look at slope, kind of ignore the y intercept.
Speaker B: Thank you. Thank you for making it mathematical for the audience.
Speaker A: For a board member, I do care much more about the y intercept. I think there is something deep to say about track record there and experiences sometimes very hard to replace.
Speaker B: Do you try to fit a polynomial function or exponential one to the track record?
Speaker A: That's not that. And an analogy doesn't carry that far.
Speaker B: All right. You mentioned some of the low points that weekend. What were some of the low points psychologically for you? Did you consider going to the Amazon jungle and just taking ayahuasca and disappearing forever?
Speaker A: Or. I mean, there's so many. It was. That was a very bad period of time. They were great high points, too. My phone was just sort of nonstop blowing up with nice messages from people I work with every day, people I hadn't talked to in a decade. I didn't get to appreciate that as much as I should have because I was just in the middle of this firefight. But that was really nice. But on the whole, it was a very painful weekend and also just a very. It was like a battle fought in public to a surprising degree. And that was extremely exhausting to me, much more than I expected. I think fights are generally exhausting, but this one really was. The board did this Friday afternoon. I really couldn't get much in the way of answers, but I also was just like, well, the board gets to do this. I'm going to think for a little bit about what I want to do, but I'll try to find the. The blessing in disguise here. And I was like, well, my current job at OpenAI is it was to run a decently sized company at this point. And the thing I'd always liked the most was just getting to work with the researchers. And I was like, yeah, I can just go do a very focused AGI research effort. And I got excited about that. It didn't even occur to me at the time to possibly that this was all going to get undone. This was, like, Friday afternoon.
Speaker B: So you've accepted the death of the.
Speaker A: Previous very quickly, like, within, you know, I mean, I went through, like, a little period of confusion and rage, but very quickly. And by Friday night, I was, like, talking to people about what was gonna be next, and I was excited about that. I think it was Friday night evening for the first time that I heard from the exec team here, which is like, hey, we're gonna, like, fight this. And, you know, we think, whatever. And then I went to bed just still being like, okay, excited, like, onward. Were you able to sleep not a lot. It was. One of the weird things was there was this period of four and a half days where sort of didn't sleep much, didn't eat much, and still kind of had, like, a surprising amount of energy. You learned, like, a weird thing about adrenaline in wartime.
Speaker B: So you kind of accepted the death of this baby opening up.
Speaker A: And I was excited for the new thing. I was just like, okay, this was crazy, but whatever.
Speaker B: It's a very good coping mechanism.
Speaker A: And then Saturday morning, two of the board members called and said, hey, we didn't mean to destabilize things. We don't destroy a lot of value here. Can we talk about you coming back? And I immediately didn't want to do that, but I thought a little more. And I was like, well, I really care about the people here, the partners, shareholders. I love this company. And so I thought about it, and it's like, well, okay, but here's the stuff I would need. And then the most painful time of all was over the course of that weekend. I kept thinking and being told, and we all kept, not just me, like, the whole team here kept thinking while we were trying to keep open eyes stabilized, while the whole world was trying to break it apart, people trying to recruit, whatever, we kept being told, all right, we're almost done. We're almost done. We just need a little bit more time. And it was this very confusing state. And then Sunday evening when again, like, every few hours, I expected that we were going to be done and we're going to figure out a way for me to return and things to go back to how they were. The board then appointed a new interim CEO. And then I was like, I mean, that feels really bad. That was the low point of the whole thing. You know, I'll tell you something. It felt very painful, but I felt a lot of love that whole weekend. It was not. Other than that one moment Sunday night. I would not characterize my emotions as anger or hate, but I really just, like. I felt a lot of love from people towards people. It was painful, but the dominant emotion of the weekend was love, not hate.
Speaker B: You've spoken highly of Mira Moradi, that she helped, especially as you put in a tweet in the quiet moments when it counts. Perhaps we could take a bit of a tangent. What do you admire about Meera?
Speaker A: Well, she did a great job during that weekend in a lot of chaos. But people often see leaders in the crisis moments, good or bad. But a thing I really value in leaders is how people act on a boring Tuesday at 946 in the morning and in just sort of the normal drudgery of the day to day, how someone shows up in a meeting, the quality of the decisions they make. That was what I meant about the quiet moments.
Speaker B: Meaning, like, most of the work is done on a day by day, in the meeting, by meeting, just be present and make great decisions.
Speaker A: Yeah. I mean, look, what you have wanted to spend the last 20 minutes about, and I understand, is, like, this one very traumatic weekend.
Speaker B: Yeah.
Speaker A: But that's not really what opening eye is about. Opening eye is really about the other seven years. Well, yeah.
Speaker B: Human civilization is not about the invasion of the Soviet Union by Nazi Germany, but still, that's something to focus on.
Speaker A: Very, very understandable.
Speaker B: It gives us an insight into human nature, the extremes of human nature, and perhaps some of the damage and some of the triumphs of human civilization can happen in those moments. So it's, like, illustrative. Let me ask you about Ilya. Is he being held hostage in a secret nuclear facility?
Speaker A: No.
Speaker B: What about a regular secret facility?
Speaker A: No.
Speaker B: What about a nuclear, non secret facility?
Speaker A: Neither. Not that either.
Speaker B: I mean, this is becoming a meme at some point. You've known Ilya for a long time. He was obviously in part of this drama with the board and all that kind of stuff. What's your relationship with him now?
Speaker A: I love Ilya. I have tremendous respect for Ilya. I don't have anything I can say about his plans right now. That's a question for him. But I really hope we work together for certainly the rest of my career. He's a little bit younger than me. Maybe he works a little bit longer.
Speaker B: There's a meme that he saw something, he maybe saw AGI, and that gave him a lot of worry internally. What did Ilya see?
Speaker A: Ilya has not seen AGI. None of us have seen AGI. We have not built AGI. I do think one of the many things that I really love about Ilya is he takes AGI and the safety concerns, broadly speaking, including things like the impact this is going to have on society very seriously, and as we continue to make significant progress. Ilya is one of the people that I've spent the most time over the last couple of years talking about what this is going to mean, what we need to do to ensure we get it right, to ensure that we succeed at the mission. So Ilya did not see AGI, but Ilya is a credit to humanity in terms of how much he thinks and worries about making sure we get this right.
Speaker B: I've had a bunch of conversations with him in the past. I think when he talks about technology, he's always, like, doing this long term thinking type of thing. So he's not thinking about what this is going to be in a year, he's thinking about in ten years. Just thinking from first principles, like, okay, if this scales, what are the fundamentals here? Where is this going? And so that that's the foundation for them thinking about, like, all the other safety concerns and all that kind of stuff, which makes him a really fascinating human to talk with. Do you have any idea why he's been kind of quiet? Is it he's just doing some soul.
Speaker A: Searching again, I don't want to speak for Ilya. I think that you should ask him that. He's definitely a thoughtful guy. I kind of think Ilya is always on a soul search. In a really good way.
Speaker B: Yes. Yeah. Also, he appreciates the power of silence. Also, I'm told he can be a silly guy, which I've never seen that side of him.
Speaker A: It's very sweet when that happens.
Speaker B: I've never witnessed a silly ilya, but I look forward to that as well.
Speaker A: I was at a dinner party with him recently, and he was playing with a puppy, and he was in a very silly mood, very endearing. And I was thinking, oh, man, this is not the side of the ilya that the world sees the most.
Speaker B: So just to wrap up this whole saga, are you feeling good about the board structure, about all of this and where it's moving?
Speaker A: I feel great about the new board in terms of the structure of OpenAI I. You know, one of the board's tasks is to look at that and see where we can make it more robust. We wanted to get new board members in place first, but, you know, we clearly learned a lesson about structure throughout this process. I don't have, I think, super deep things to say. It was a crazy, very painful experience. I think it was like a perfect storm of weirdness. It was like a preview for me of what's going to happen as the stakes get higher and higher and the need that we have robust governance structures and processes and people. I am kind of happy it happened when it did, but it was a shockingly painful thing to go through.
Speaker B: Did it make you be more hesitant in trusting people?
Speaker A: Yes.
Speaker B: Just on a personal level? Yes.
Speaker A: I think I'm an extremely trusting person. I always had a life philosophy of, don't worry about all of the paranoia, don't worry about the edge cases. You get a little bit screwed in exchange for getting to live with your guard down. And this was so shocking to me. I was so caught off guard that it has definitely changed. And I really don't like this. It's definitely changed how I think about just default trust of people and planning for the bad scenarios.
Speaker B: You got to be careful with that. Are you worried about becoming a little too cynical?
Speaker A: I'm not worried about becoming too cynical. I think I'm like the extreme opposite of a cynical person, but I'm worried about just becoming less of a default, trusting person.
Speaker B: I'm actually not sure which mode is best to operate in. For a person who's developing AGI, trusting or untrusting, it's an interesting journey you're on. But in terms of structure, see, I'm more interested on the human level. Like, how do you surround yourself with humans that are building cool shit, but also are making wise decisions? Because the more money you start making, the more power the thing has, the weirder people get.
Speaker A: I think you could make all kinds of comments about the board members and the level of trust I should have had there, or how I should have done things differently. But in terms of the team here, I think you'd have to give me a very good grade on that one. And I have just enormous gratitude and trust and respect for the people that I work with every day. And I think being surrounded with people like that is really important.
Speaker B: Our mutual friend Elon sued openaiden Moti is the essence of what he's criticizing. To what degree does he have a point? To what degree is he wrong?
Speaker A: I don't know what it's really about. We started off just thinking we were going to be a research lab and having no idea about how this technology was going to go. It's hard to, because it was only seven or eight years ago, it's hard to go back and really remember what it was like then. But this was before language models were a big deal. This was before we had any idea about an API or selling access to a chatbot is before we had any idea we were going to productize at all. So we're like, we're just going to try to do research, and we don't really know what we're going to do with that. I think with many new, fundamentally new things, you start fumbling through the dark and you make some assumptions, most of which turn out to be wrong. And then it became clear that we were going to need to do different things and also have huge amounts more capital. So we said, okay, well, the structure doesn't quite work for that, how do we patch the structure? And then you patch it again and patch it again and you end up with something that does look kind of eyebrow raising, to say the least. But we got here gradually with, I think, reasonable decisions at each point along the way. And doesn't mean I wouldn't do it totally differently if we could go back now with an oracle. But you don't get the oracle at the time. But anyway, in terms of what Elon's real motivations here are, I don't know.
Speaker B: To the degree you remember, what was the response that OpenAI gave in the blog post? Can you summarize it?
Speaker A: Oh, we just said, like, you know, Elon said this set of things. Here's our characterization, or here's the sort of. Not our characterization. Here's like the characterization of how this went down. We tried to not make it emotional and just sort of say, here's the history.
Speaker B: I do think there's a degree of mischaracterization from Elon here about one of the points you just made, which is the degree of uncertainty you had at the time. You guys are a bunch of, like a small group of researchers crazily talking about AGI when everybody's laughing at that thought.
Speaker A: It wasn't that long ago Elon was crazily talking about launching rockets when people were laughing at that thought. So I think he'd have more empathy for this.
Speaker B: I mean, I do think that there's personal stuff here that there was a split that OpenAI and a lot of amazing people here chose the part ways with ElON.
Speaker A: So there's a person, Elon chose to part wAys.
Speaker B: Can you describe that exactly, the choosing to part ways?
Speaker A: He thought OpenAI was going to fail. He wanted TOtal control to sort of turn it around. We wanted to keep going in the direction that now has become OpenAI. He also wanted Tesla to be able to build an AGI effort at various times. He wanted to make OpenAI into a for profit company that he could have control of or have it merge with Tesla. We don't want to do that. ANd he decided to leave, which, that's fine.
Speaker B: So you're saying, and that's one of the things that the blog post says, is that he wanted OpenAI to be basically acquired by Tesla in the same way that, or maybe something similar or maybe something more dramatic than the partnership with Microsoft.
Speaker A: My memory is the proposal was just like, yeah, get acquired by Tesla and have Tesla have full control over it. I'm pretty sure that's what it was.
Speaker B: So what is the word open in OpenAI mean to elon at the time? Ilya has talked about this in the email exchanges and all this kind of stuff. What does it mean to you at the time? What does it mean to you now?
Speaker A: I would definitely pick a different, speaking of going back with an oracle, I'd pick a different name. One of the things that I think OpenAI is doing that is the most important of everything that we're doing is putting powerful technology in the hands of people for free as a public good. We don't run ads on our free version, we don't monetize it in other ways. We just say it's part of our mission. We want to put increasingly powerful tools in the hands of people for free and get them to use them. And I think that kind of open is really important to our mission. I think if you give people great tools and teach them to use them, or don't even teach them, they'll figure it out and let them go. Build an incredible future for each other with that. That's a big deal. So if we can keep putting, like, free or low cost or free and low cost powerful AI tools out in the world, I think it's a huge deal for how we fulfill the mission, open source or not. Yeah, I think we should open source some stuff and not other stuff. It does become this, like, religious battle line where nuance is hard to have, but I think nuance is the right answer.
Speaker B: So he said, change your name to closed AI and I'll drop the lawsuit. I mean, is it going to become this battleground in the land of memes about.
Speaker A: I think that speaks to the seriousness with which Elon means the lawsuit and. Yeah, I mean, that's like an astonishing thing to say. I think.
Speaker B: Well, I don't think the lawsuit maybe, correct me if I'm wrong, but I don't think the lawsuit is legally serious. It's more to make a point about the future of AGI and the company that's currently leading the way.
Speaker A: So, look, I mean, Grok had not open sourced anything until people pointed out it was a little bit hypocritical. And then he announced that Grok will open source things this week. I don't think open source versus not is what this is really about for him.
Speaker B: Well, we'll talk about open source and not. I do think maybe criticizing the competition is great. Just talking a little shit, that's great. But friendly competition versus, like, I personally hate lawsuits.
Speaker A: Look, I think this whole thing is like, unbecoming of a builder. And I respect Elon as one of the great builders of our time. And I know he knows what it's like to have like, haters attack him. And it makes me extra sad he's doing it to us.
Speaker B: Yeah, he's one of the greatest builders of all time, potentially the greatest builder of all time.
Speaker A: It makes me sad, and I think it makes a lot of people sad. Like, there's a lot of people who've really looked up to him for a long time. And I said, you know, in some interview or something that I missed the old Elon, and the number of messages I got being like that exactly encapsulates how I feel.
Speaker B: I think he should just win. He should just make Grock beat GPT, and then GPT beats Grock, and it's just the competition, and that's beautiful for everybody. But on the question of open source, do you think there's a lot of companies playing with this idea? It's quite interesting. I would say meta, surprisingly, has led the way on this, or like, at least took the first step in the game of chess of like, really open sourcing the model. Of course, it's not the state of the art model, but open sourcing. Llama. Google is flirting with the idea of open sourcing a smaller version. What are the pros and cons of open sourcing? Have you played around with this idea?
Speaker A: Yeah, I think there is definitely a place for open source models, particularly smaller models that people can run locally. I think there's huge demand for. I think there will be some open source models, there will be some closed source models. It won't be unlike other ecosystems in that way.
Speaker B: I listened to all in podcasts talking about this loss and all that kind of stuff, and they were more concerned about the precedent of going from nonprofit to this cap for profit. What precedent this sets for other startups.
Speaker A: Is that something I don't. I would heavily discourage any startup that was thinking about starting as a nonprofit and adding like a for profit arm later. I'd heavily discourage them from doing that. I don't think we'll set a precedent here.
Speaker B: Okay, so most startups should go just for sure.
Speaker A: And again, if we knew what was gonna happen, we would have done that too.
Speaker B: Well, like, in theory, if you like, dance beautifully here, you can, there's some tax incentives or whatever.
Speaker A: I don't think that's how most people think about these things.
Speaker B: Just not possible to save a lot of money for a startup if you do it this way.
Speaker A: No, I think there's laws that would make that pretty difficult.
Speaker B: Where do you hope this goes with Elon? This tension, this dance? Where do you hope this, like, if we go one, two, three years from now, your relationship with him on a personal level too, like friendship, friendly competition, just all this kind of stuff?
Speaker A: Yeah, I mean, I really respect Elon, and I hope that years in the future, we have an amicable relationship.
Speaker B: Yeah, I hope you guys have an amicable relationship, like this month, and just compete and win and explore these ideas together. I do suppose there's competition for talent or whatever, but it should be friendly competition. Just build, build cool shit. And Elon is pretty good at building cool shit, but so are you. So speaking of cool shit, Sora, there's like a million questions I could ask. First of all, it's amazing. It truly is amazing on a product level, but also just on a philosophical level. So let me just a technical, philosophical ask, what do you think it understands about the world more or less than GPT four, for example, the world model, when you train on these patches versus language tokens, I think all of these.
Speaker A: Models understand something more about the world model than most of us give them credit for. And because they're also very clear things, they just don't understand or don't get right. Its easy to look at the weaknesses, see through the veil, and say, ah, this is just, this is all fake. But its not all fake. Its just some of it works and some of it doesnt work. I remember when I started first watching Sora videos, and I would see a person walk in front of something for a few seconds and occlude it and then walk away and the same thing was still there. I was like, oh, its pretty good. Or theres examples where the underlying physics looks so well represented over a lot of steps in a sequence. It's like, oh, this is quite impressive. But fundamentally, these models are just getting better and that will keep happening. If you look at the trajectory from Dalli one to two to three to Sora, there are a lot of people that were dunked on each version saying, it can't do this, it can't do that, and look at it now, the.
Speaker B: Thing you just mentioned is with occlusions, is basically modeling the physics of three dimensional physics of the world sufficiently well to capture those kinds of things well. Or like, yeah, maybe you can tell in order to deal with occlusions, what does the world model need to.
Speaker A: Yeah, so what I would say is it's doing something to deal with occlusions really well. What I represent that it has like a great underlying 3d model of the world. It's a little bit more of a.
Speaker B: Stretch, but can you get there through just these kinds of two dimensional training data approaches?
Speaker A: It looks like this approach is going to go surprisingly far. I don't want to speculate too much about what limits it will surmount and which it won't.
Speaker B: But what are some interesting limitations of the system that you've seen? I mean, there's been some fun ones you've posted.
Speaker A: There's all kinds of fun. I mean, like, you know, cats sprouting an extra limb at random points in a video. Pick what you want, but there's still a lot of problems, a lot of weaknesses.
Speaker B: Do you think that's a fundamental flaw of the approach, or is it just bigger model or better technical details or better data? More data is going to solve the cat sprouting?
Speaker A: I would say yes to both. I think there is something about the approach which just seems to feel different from how we think and learn and whatever. And then also I think it'll get better with skill.
Speaker B: Like I mentioned, LLMs have tokens, text tokens, and Sora has visual patches. So it converts all visual data, diverse kinds of visual data, videos and images, into patches. Is the training to the degree you can, say, fully self supervised, or is there some manual labeling going on? Like, what's the involvement of humans in all this?
Speaker A: I mean, without saying anything specific about the soar approach, we use lots of human data in our work, but not Internet scale data.
Speaker B: So lots of humans. Lots is a complicated word, Sam.
Speaker A: I think lots is a fair word.
Speaker B: In this case, but it doesn't, because to me, lots, like, listen, I'm an introvert, and when I hang out with, like, three people, that's a lot of people, four people, that's a lot. But I suppose you mean more than.
Speaker A: More than three people work on labeling the data for these models. Yeah. Okay.
Speaker B: But fundamentally, there's a lot of self supervised learning, because what you mentioned in the technical report is Internet scale data. That's another beautiful. It's like poetry. So it's a lot of data that's not human label. It's like it's self supervised in that way. And then the question is, how much data is there on the Internet that could be used in this that is conducive to this kind of self supervised way, if only. When you. The details of the self supervised, have you considered opening it up a little more details?
Speaker A: We have. You mean for Sora specifically?
Speaker B: Sora specifically, because it's so interesting that like, can the same magic of LLMs now start moving towards visual data? And what does that take to do that?
Speaker A: I mean, it looks to me like, yes, but we have more work to do.
Speaker B: Sure. What are the dangers? Why are you concerned about releasing the system? What are some possible dangers of this?
Speaker A: Frankly speaking, one thing we have to do before releasing the system is just get it to work at a level of efficiency that will deliver the scale people are going to want from this. So I don't want to downplay that. And there's still a ton ton of work to do there. But you can imagine issues with deepfakes, misinformation. We try to be a thoughtful company about what we put out into the world and it doesn't take much thought to think about the ways this can go badly.
Speaker B: There's a lot of tough questions here. You're dealing in a very tough space. Do you think training AI should be or is fair use under copyright law?
Speaker A: I think the question behind that question is, do people who create valuable data deserve to have some way that they get compensated for use of it? And that I think the answer is yes. I don't know yet what the answer is. People have proposed a lot of different things. We've tried some different models. But if I'm an artist, for example, a, I would like to be able to opt out of people generating art in my style, and b, if they do generate art in my style, I'd like to have some economic model associated with that.
Speaker B: Yeah, it's that transition from cds to Napster to Spotify. We have to figure out some kind of model.
Speaker A: The model changes, but people have got to get paid well.
Speaker B: There should be some kind of incentive if you zoom out even more for humans to keep doing cool shit of.
Speaker A: Everything I worry about, humans are going to do cool shit and society is going to find some way to reward it. That seems pretty hardwired. We want to create, we want to be useful, we want to achieve status in whatever way. That's not going anywhere, I don't think.
Speaker B: But the reward might not be monetary, financial. It might be like fame and celebration.
Speaker A: Of other cool, maybe financial in some other way. Again, I don't think we've seen the last evolution of how the economic system's going to work.
Speaker B: Yeah, but artists and creators are worried when they see Sora. They're like, holy shit.
Speaker A: Sure, artists were also super worried when photography came out. And then photography became a new art form and people made a lot of money taking pictures and I think things like that will keep happening. People will use the new tools in new ways.
Speaker B: If we just look on YouTube or something like this, how much of that will be using sora, like AI generated content, do you think in the next.
Speaker A: Five years, people talk about how many jobs they are going to do in five years, and the framework that people have is what percentage of current jobs are just going to be totally replaced by some AI doing the job. The way I think about it is not what percent of jobs AI will do, but what percent of tasks will AI do and over what time horizon. So if you think of all of the five second tasks in the economy, the five minute tasks, the five hour tasks, maybe even the five day tasks, how many of those can AI do? And I think that's a way more interesting, impactful, important question than how many jobs AI can do, because it is a tool that will work at increasing levels of sophistication and over longer and longer time horizons for more and more tasks and let people operate at a higher level of abstraction. So maybe people are way more efficient at the job they do at some point. That's not just a quantitative change, but it's a qualitative one, too, about the kinds of problems you can keep in your head. I think that for videos on YouTube, it'll be the same many videos. Maybe most of them will use AI tools in the production, but they'll still be fundamentally driven by a person thinking about it, putting it together, doing parts of it, sort of directing and running it.
Speaker B: Yeah, it's so interesting. It's scary, but it's interesting to think about. I tend to believe that humans like to watch other humans, or other humans.
Speaker A: Really care about other humans a lot.
Speaker B: Yeah. If there's a cooler thing that's better than a human, humans care about that for like two days, and then they go back to humans.
Speaker A: That seems very deeply wired.
Speaker B: It's the whole chess thing. Yeah, but now everybody keep playing chess and let's ignore the elephant in the room that humans are really bad at chess relative to AI systems, we still.
Speaker A: Run races and cars are much faster. I mean, there's like a lot of examples.
Speaker B: Yeah. And maybe it'll just be tooling, like in the Adobe suite type of way, where you can just make videos much easier and all that kind of stuff. Listen, I hate being in front of the camera. If I can figure out a way to not be in front of the camera, I would love it. Unfortunately, it'll take a while like that, generating faces. It's getting there. But generating faces in video format is tricky when it's specific people versus generic people. Let me ask you about GPT four. There's so many questions. First of all, also amazing, looking back, it'll probably be this kind of historic, pivotal moment with three, five and four, which has a BT.
Speaker A: Maybe five will be the pivotal moment. I don't know.
Speaker B: Hard to say that looking forwards, we never know. That's the annoying thing about the future. It's hard to predict. But for me, looking back, GPT four. Chad, GPT is pretty damn impressive. Like, historically impressive. So allow me to ask, what's been the most impressive capabilities of GPT four tu and GPT four turbo?
Speaker A: I think it kind of sucks.
Speaker B: Hmm. Typical human. Also gotten used to an awesome thing.
Speaker A: No, I think it is an amazing thing. But relative to where we need to get to and where I believe we will get to at the time of GPT-3 people were like, oh, this is amazing. This is this marvel of technology. And it is. It was. But now we have GPT four and look at GPT-3 and you're like, that's unimaginably horrible. I expect that the delta between five and four will be the same as between four and three. And I think it is our job to live a few years in the future and remember that the tools we have now are going to kind of suck looking backwards at them. And that's how we make sure the future is better.
Speaker B: What are the most glorious ways in that GPT four sucks?
Speaker A: Meaning, what are the best things it can do?
Speaker B: What are the best things it can do? In the limits of those best things that allow you to say it sucks, therefore gives you an inspiration and hope for the future.
Speaker A: You know, one thing I've been using it for more recently is sort of like a brainstorming partner.
Speaker B: Yep.
Speaker A: For that, there's a glimmer of something amazing in there. I don't think it gets. You know, when people talk about what it does, they're like, oh, it helps me code more productively. It helps me write more faster and better. It helps me translate from this language to another. All these amazing things. But there's something about the kind of creative brainstorming partner I need to come up with a name for this thing. I need to think about this problem in a different way. I'm not sure what to do here that I think gives a glimpse of something I hope to see more of. One of the other things that you can see a very small glimpse of is when I can help on longer horizon tasks. Break down some into multiple steps, maybe execute some of those steps, search the Internet, write code, whatever. Put that together. When that works, which is not very often, it's very magical.
Speaker B: The iterative back and forth with a human. It works a lot for me. What do you mean iterative back and forth?
Speaker A: A human, it can get more often when it can go do a ten step problem on its own. Oh, it doesn't work for that too often.
Speaker B: Sometimes at multiple layers of abstraction, or do you mean just sequential?
Speaker A: Both. To break it down and then do things that different layers of abstraction and put them together. Look, I don't want to downplay the accomplishment of GPT four, but I don't want to overstate it either. And I think this point that we are on an exponential curve. We will look back relatively soon at GPT four like we look back at GPT-3 now.
Speaker B: That said, I mean, Chad, GPT was a transition to where people like started to believe again. There was a kind of. There is an uptick of believing. Not internally at OpenAI, perhaps there's believers here, but when you think about it.
Speaker A: And in that sense, I do think it'll be a moment where a lot of the world went from not believing to believing. That was more about the chat GBT interface than the. And by the interface and product, I also mean the post training of the model and how we tune it to be helpful to you, how to use it than the underlying model itself.
Speaker B: How much of those two, each of those things are important? The underlying model and the RLHF, or something of that nature that tunes it to be more compelling to the human, more effective and productive for the human.
Speaker A: I mean, they're both super important. But the RLHF, the post training step, the little wrapper of things that, from a compute perspective, little wrapper of things that we do on top of the base model, even though it's a huge amount of work that's really important, to say nothing of the product that we build around it. In some sense, we did have to do two things. We had to invent the underlying technology, and then we had to figure out how to make it into a product people would love, which is not just about the actual product work itself, but this whole other step of how you align and make it useful and how.
Speaker B: You make the scale work where a lot of people can use it at the same time.
Speaker A: All that kind of stuff and that. But that was a known, difficult thing. We knew we were going to have to scale it up. We had to go do two things that had never been done before that were both, I would say, quite significant achievements, and then a lot of things like scaling it up that other companies have had to do before.
Speaker B: How does the context window of going from eight k to 128k tokens compare from GPT four to GPT four turbo?
Speaker A: People like long. Most people don't need all the way to 128 most of the time. Although if we dream into the distant future, we'll have way distant future, we'll have context lengthen of several billion. You will feed in all of your information, all of your history over time, and it'll just get to know you better and better, and that'll be great for now. The way people use these models, they're not doing that. And people sometimes post in a paper or a significant fraction of a code repository, whatever. But most usage of the models is not using the long context most of the time.
Speaker B: I like that. This is your I have a dream speech. One day you'll be judged by the full context of your character or of your whole lifetime. That's interesting. So that's part of the expansion that you're hoping for is a greater and greater context.
Speaker A: There's this. I saw this Internet clip once, I'm going to get the numbers wrong, but it was like Bill Gates talking about the amount of memory on some early computer, maybe 64k, maybe 640k, something like that. And most of it was used for the screen buffer. And he just couldn't seem genuine. Couldn't imagine that the world would eventually need gigabytes of memory in a computer or terabytes of memory in a computer. And you always do, or you always do. Just need to follow the exponential of technology and we're going to like, we will find out how to use better technology. So I can't really imagine what it's like right now for context links to go out to the billions someday. And they might not literally go there, but effectively it'll feel like that, but I know we'll use it and really not want to go back once we have it.
Speaker B: Yeah. Even saying billions ten years from now might seem dumb because there'll be like trillions upon trillions. There'll be some kind of breakthrough that will effectively feel like infinite context. But even 120, I have to be honest, pushed it to that degree. Maybe putting in entire books or like parts of books and so on papers. What are some interesting use cases of GPT four that you've seen?
Speaker A: The thing that I find most interesting is not any particular use case that we can talk about those, but it's people who kind of like this is mostly younger people, but people who use it as like their default start for any kind of knowledge work task. And it's the fact that it can do a lot of things reasonably well. You can use GPTV, you can use it to help you write code, you can use it to help you do search, you can use it to edit a paper. The most interesting thing to me is the people who just use it as the start of their workflow.
Speaker B: I do as well for many things. I use it as a reading partner for reading books. It helps me think, helps me think through ideas, especially when the books are classic. So it's really well written about. It actually is, as I find it often to be significantly better than even like Wikipedia on well covered topics. It's somehow more balanced and more nuanced. Or maybe it's me, but it inspires me to think deeper than a Wikipedia article does. I'm not exactly sure what that is. You mentioned like this collaboration. I'm not sure where the magic is, if it's in here or if it's in there, or if it's somewhere in between, I'm not sure. But one of the things that concerns me for knowledge task when I start with GPT is I'll usually have to do fact checking after like check that it didn't come up with fake stuff. How do you figure that out? That GPT can come up with fake stuff? That sounds really convincing. So how do you ground it in truth?
Speaker A: That's obviously an area of intense interest for us. I think it's going to get a lot better with upcoming versions, but we'll have to work on it and we're not going to have it all solved this year.
Speaker B: Well, the scary thing is, as it gets better, you start not doing the fact checking more and more. Right?
Speaker A: I'm of two minds about that. I think people are much more sophisticated users of technology than we often give them credit for, and people seem to really understand that GPT, any of these models hallucinate some of the time, and if it's mission critical, you got to check it.
Speaker B: Except journalists don't seem to understand that. I've seen journalists half assedly just using GPT for.
Speaker A: Of the long list of things I'd like to dunk on journalists for, this is not my top criticism of them.
Speaker B: Well, I think the bigger criticism is perhaps the pressures and the incentives of being a journalist is that you have to work really quickly and this is a shortcut I would love our society to incentivize like.
Speaker A: I would too.
Speaker B: Take days and weeks and rewards great in depth journalism. Also journalism that presents stuff in a balanced way where it celebrates people while criticizing them, even though the criticism is the thing that gets clicks. And making shit up also gets clicks and headlines that mischaracterize completely. I'm sure you have a lot of people dunking on. Well, all that drama. Probably got a lot of clicks.
Speaker A: Probably did.
Speaker B: And that's that. You know, that's a bigger problem about human civilization. I'd love to see solved this, where we celebrate a bit more. You've given Chad GPT the ability to have memories. You've been playing with that about previous conversations, and also the ability to turn off memory. I wish I could do that sometimes just turn on and off, depending, I guess. Sometimes alcohol can do that, but not optimally, I suppose. What have you seen through that? Like playing around with that idea of remembering conversations or not.
Speaker A: We're very early in our explorations here, but I think what people want, or at least what I want for myself, is a model that gets to know me and gets more useful to me over time. This is an early exploration. I think there's a lot of other things to do, but that's what we'd like to head. You'd like to use a model and over the course of your life, or use a system, there'll be many models and over the course of your life it gets better and better.
Speaker B: Yeah. How hard is that problem? Because right now it's more like remembering little factoids and preferences and so on. What about remembering? Don't you want GPT to remember all the shit you went through in November and all the drama and then you can, because right now you're clearly blocking it out a little bit.
Speaker A: It's not just that I want it to remember that. I want it to integrate the lessons of that and remind me in the future what to do differently or what to watch out for. And we all gain from experience over the course of our lives, varying degrees. And I'd like my AI agent to gain with that experience too. So if we go back and let ourselves imagine that trillions and trillions of context length, if I can put every conversation I've ever had with anybody in my life in there, if I can have all of my emails input out, like all of my input output in the context window every time I ask a question, that'd be pretty cool, I think.
Speaker B: Yeah, I think that would be very cool. People sometimes will hear that and be concerned about privacy. What do you think about that aspect of it? The more effective the AI becomes at really integrating all the experiences and all the data that happened to you and give you advice.
Speaker A: I think the right answer there is just user choice. Anything I want stricken from the record from my AI agent, I want to be able to take out. If I don't want it to remember anything, I want that too. You and I may have different opinions about where on that privacy utility trade off for our own AI you want to be, which is totally fine, but I think the answer is just like really easy user choice.
Speaker B: But there should be some high level of transparency from a company about the user choice because sometimes companies in the past have been kind of shady about like, eh, where it's kind of presumed to, that we're collecting all your data and we're using it for a good reason, for advertisements and so on, but there's not a transparency about the details of that.
Speaker A: That's totally true. You know, you mentioned earlier that I'm like, blocking out the November stuff, just teasing you. Well, I mean, I think it was a very traumatic thing, and it did immobilize me for a long period of time. Like, definitely the hardest, like the hardest work thing I've had to do was just like, keep working that period, because I had to try to come back in here and put the pieces together while I was just in shock and pain, and nobody really cares about that. The team gave me a pass and I was not working on my normal level, but there was a period where I was just like, it was really hard to have to do both. But I woke up one morning and I was like, this was a horrible thing that happened to me. I think I could just feel like a victim forever. Or I can say this is like the most important work I'll ever touch in my life, and I need to get back to it. And it doesn't mean that I've repressed it, because sometimes I, like, wake from the middle of the night thinking about it, but I do feel like an obligation to keep moving forward.
Speaker B: Well, that's beautifully said, but there could be some lingering stuff in there. Like, what I would be concerned about is that trust thing that you mentioned, that being paranoid about people as opposed to just trusting everybody or most people, like using your gut. It's a tricky dance, for sure. I mean, because I've seen in my part time explorations, I've been diving deeply into the Zelenska administration, the Putin administration, and the dynamics there in wartime in a very highly stressful environment. And what happens is distrust, and you isolate yourself both, and you start to not see the world clearly. And that's a concern. That's a human concern. You seem to have taken it in stride and kind of learned the good lessons and felt the love and let the love energize you, which is great, but still can linger in there. There's just some questions I would love to ask. Your intuition about. What's GPT able to do? And nothing. So it's allocating approximately the same amount of compute for each token it generates. Is there room there in this kind of approach to slower thinking, sequential thinking?
Speaker A: I think there will be a new paradigm for that kind of thinking.
Speaker B: Will it be similar architecturally as what we're seeing now with LLMs? Is it a layer on top of the LlMsdev?
Speaker A: I can imagine many ways to implement that. I think that's less important than the question you were getting at, which is, do we need a way to do a slower kind of thinking where the answer doesn't have to get? I guess spiritually you could say that you want an AI to be able to think harder about a harder problem and answer more quickly about an easier problem, and I think that will be important.
Speaker B: Is that like a human thought that we're just having, you should be able to think hard. Is that wrong? Intuition.
Speaker A: I suspect that's a reasonable intuition.
Speaker B: Interesting. So it's not possible. Once the GPT gets like GPT seven, we'll just be instantaneously be able to see. Here's the proof from our STM.
Speaker A: It seems to me like you want to be able to allocate more compute to harder problems. It seems to me that a system knowing. If you ask a system like that proof, Vermont's last theorem versus what's today's date? Unless it already knew and had memorized the answer to the proof. Assuming it's got to go figure that out. Seems like that will take more computer.
Speaker B: But can it look like basically LLM talking to itself, that kind of thing?
Speaker A: Maybe. I mean, there's a lot of things that you could imagine working. What the right or the best way to do that will be, we don't know.
Speaker B: This does make me think of the mysterious, the lore behind Q star. What's this mysterious Q star project? Is it also in the same nuclear facility?
Speaker A: There is no nuclear facility.
Speaker B: That's what a person with a nuclear facility always says.
Speaker A: I would love to have a secret nuclear facility. There isn't one. All right, maybe someday.
Speaker B: Someday all right, one can dream.
Speaker A: OpenAI is not a good company at keeping secrets. It would be nice, you know, we're like been plagued by a lot of leaks and it would be nice if we were able to have something like that.
Speaker B: Can you speak to what Q Star is?
Speaker A: We are not ready to talk about that.
Speaker B: See, but an answer like that means there's something to talk about. It's very mysterious, Sam.
Speaker A: I mean, we work on all kinds of research. We have said for a while that we think better reasoning in these systems is an important direction that we'd like to pursue. We haven't cracked the code yet. Were very interested in it.
Speaker B: Is there going to be moments, Q or otherwise, where there's going to be leaps similar to Jagbt where you're like.
Speaker A: That'S a good question. What do I think about that? It's interesting. To me it all feels pretty continuous, right?
Speaker B: This is kind of a theme that you saying is there's a gradual, you're basically gradually going up an exponential slope. But from an outsider perspective, for me, just watching it, it does feel like there's leaps, but to you there isn't.
Speaker A: I do wonder if we should have. So part of the reason that we deploy the way we do is that we think, we call it iterative deployment. We, rather than go build in secret until we got all the way to GPT five, we decided to talk about GPT one, two, three and four. And part of the reason there is, I think AI and surprise don't go together. And also the world, people, institutions, whatever you want to call it, need time to adapt and think about these things. And I think one of the best things that OpenAI has done is this strategy. And we get the world to pay attention to the progress, to take AGI seriously, to think about what systems and structures and governance we want in place before we're like under the gun and have to make a rest decision. I think that's really good. But the fact that people like you and others say you still feel like they're at these leaps makes me think that maybe we should be doing our releasing even more iteratively. I don't know what that would mean. I don't have any answer ready to go. But like, our goal is not to have shock updates to the world. The opposite.
Speaker B: Yeah, for sure. More iterative would be amazing. I think that's just beautiful for everybody.
Speaker A: But that's what we're trying to do. That's like our state of the strategy. And I think we're somehow missing the mark. So maybe we should think about releasing GPT five in a different way or something like that.
Speaker B: Yeah, 4.714.72. But people tend to like to celebrate. People celebrate birthdays. I don't know if you know humans, but they kind of have these milestones and all.
Speaker A: I do know some humans, people do like milestones. I. I totally get that. I think we like milestones, too. It's fun to say, declare victory on this one and go start the next thing, but, yeah, I feel like we're somehow getting this a little bit wrong.
Speaker B: So when is GPT five coming out again?
Speaker A: I don't know. That's the honest answer.
Speaker B: That's the honest answer. Is it blink twice if it's this year.
Speaker A: I also. We will release an amazing new model this year. I don't know what we'll call it.
Speaker B: So that goes to the question of, like, what's the way we release this thing?
Speaker A: We'll release over in the coming months many different things. I think they'll be very cool. I think before we talk about a GPT five, like, model called that or not called that, or a little bit worse or a little bit better than what you'd expect from a GPT five, I know we have a lot of other important things to release first.
Speaker B: I don't know what to expect from GPT five. You're making me nervous and excited. What are some of the biggest challenges in bottlenecks to overcome for whatever it ends up being called? But let's call it GPT five. Just interesting to ask. Is it on the compute side? Is it on the technical side?
Speaker A: Always all of these. I was, what's the one big unlock? Is it a bigger computer? Is it a new secret? Is it something else? It's all of these things together. The thing that OpenAI, I think, does really well. This is actually an original Ilya quote that I'm gonna butcher, but it's something like, we multiply 200 medium sized things together into one giant thing.
Speaker B: So there's this distributed, constant innovation happening.
Speaker A: Yeah.
Speaker B: So even on the technical side?
Speaker A: Especially on the technical side.
Speaker B: So even, like, detailed approaches, like detailed aspects of everything, how does that work with different disparate teams and so on? How do the medium sized things become one whole giant transformer? How does this.
Speaker A: There's a few people who have to, like, think about putting the whole thing together, but a lot of people try to keep most of the picture in their head.
Speaker B: Oh, like the individual teams, individual contributors.
Speaker A: Try to keep at a high level. Yeah. You don't know exactly how every piece works, of course. But one thing I generally believe is that it's sometimes useful to zoom out and look at the entire map. And I think this is true for a technical problem. I think this is true for, like, innovating in business. But things come together in surprising ways and having an understanding of that whole picture, even if most of the time you're operating in the weeds in one area, pays off with surprising insights. In fact, one of the things that I used to have, and I think was super valuable, was I used to have, like, a good map of all of the frontier, most of the frontiers in the tech industry, and I could sometimes see these connections or new things that were possible that if I were only deep in one area, I wouldn't be able to have the idea for because I wouldn't have all the data. And I don't really have that much anymore. I'm super deep now, but I know that it's a valuable thing.
Speaker B: You're not the man you used to be, Sam.
Speaker A: Very different job now than what I used to have.
Speaker B: Speaking of zooming out, let's zoom out to another cheeky thing but profound thing, perhaps, that you said you tweeted about needing $7 trillion.
Speaker A: I did not tweet about that. I never said, we're raising $7 trillion, blah, blah, blah.
Speaker B: Oh, that's somebody else.
Speaker A: Yeah.
Speaker B: Oh, but you said, fuck it, maybe eight, I think.
Speaker A: Okay, I meme once there's misinformation out in the world.
Speaker B: Oh, you mean sort of misinformation may have a foundation of, like, insight there.
Speaker A: Look, I think compute is going to be the currency of the future. I think it will be maybe the most precious commodity in the world. And I think we should be investing heavily to make a lot more compute. Compute is. It's an unusual. I think it's going to be an unusual market. People think about the market for chips, for mobile phones or something like that, and you can say that, okay, there's 8 billion people in the world. Maybe 7 billion of them have phones. Maybe they are 6 billion. Let's say they upgrade every two years. So the market per year is 3 billion. System on chip for smartphones. And if you make 30 billion, you will not sell ten times as many phones because most people have one phone. But compute is different. Intelligence is going to be more like energy or something like that, where the only thing that I think makes sense to talk about is, at price X, the world will use this much compute. And at price Y, the world will use this much compute. Because if it's really cheap, I'll have it. Reading my email all day, giving me suggestions about what I maybe should think about or work on and trying to cure cancer. And if it's really expensive, maybe I'll only use it. We'll only use it try to cure cancer. So I think the world is going to want a tremendous amount of compute, and there's a lot of parts of that that are hard. Energy is the hardest part. Building data centers is also hard. The supply chain is harder than, of course, fabricating enough chips is hard. But this seems to me where things are going. Like, we're going to want an amount of compute that's just hard to reason about right now.
Speaker B: How do you solve the energy puzzle? Nuclear.
Speaker A: That's what I believe.
Speaker B: Fusion.
Speaker A: That's what I believe.
Speaker B: Nuclear fusion.
Speaker A: Yeah.
Speaker B: Who's going to solve that?
Speaker A: I think helium's doing the best work, but I'm happy there's like a race for fusion right now. Nuclear fission, I think, is also, like, quite amazing, and I hope as a world we can re embrace that. It's really sad to me how the history of that went and hope we get back to it in a meaningful way.
Speaker B: So to you, part of the puzzle is nuclear fission, like nuclear reactors, as we currently have them, and a lot of people are terrified because of Chernobyl and so on.
Speaker A: Well, I think we should make new reactors. I think it's just like, it's a shame that industry kind of ground to a halt.
Speaker B: And what just mass hysteria is how you explain the halt.
Speaker A: Yeah.
Speaker B: I don't know if you know humans, but that's one of the dangers, that's one of the security threats for nuclear fission is humans seem to be really afraid of it, and that's something we have to incorporate into the calculus of it. So we have to kind of win people over and to show how safe it is.
Speaker A: I worry about that for AI. I think some things are going to go theatrically wrong with AI. I don't know what the percent chance is that I eventually get shot, but it's not zero.
Speaker B: Oh, like we want to stop this, maybe. How do you decrease the theatrical nature of it? You know, I've already started to hear rumblings, because I do talk to people on both sides of the political spectrum, hear rumblings where it's going to be politicized. AI, it's going to be politicized really worries me, because then it's like, maybe the right is against AI and the left is four AI, because it's going to help the people, or whatever the narrative and formulation is. That really worries me. And then the theatrical nature of it can be leveraged fully. How do you fight that?
Speaker A: I think it will get caught up in left versus right wars. I don't know exactly what that's going to look like, but I think that's just what happens with anything of consequence. Unfortunately, what I meant more about theatrical risks is AI is going to have, I believe, tremendously more good consequences than bad ones, but it is going to have bad ones. There will be some bad ones that are bad, but not theatrical. You know, like, a lot more people have died of air pollution than nuclear reactors, for example. But we worry. Most people worry more about living next to a nuclear reactor than a coal plant. But something about the way we're wired is that although there's many different kinds of risks, we have to confront a the ones that make a good climax scene of a movie carry much more weight with us than the ones that are very bad over a long period of time, but on a slow burn.
Speaker B: Well, that's why truth matters. And hopefully AI can help us see the truth of things, to have balance, to understand what are the actual risks? What are the actual dangers of things in the world, what are the pros and cons of the competition in this space and competing with Google, meta, Xai and others?
Speaker A: I think I have a pretty straightforward answer to this, that maybe I can think of more nuance later. But the pros seem obvious, which is that we get better products and more innovation faster and cheaper, and all the reasons competition is good. And the con is that I think if we're not careful, it could lead to an increase in sort of an arms race that I'm nervous about.
Speaker B: Do you feel the pressure of the arms race? Like in some negative?
Speaker A: Definitely in some ways, for sure. We spend a lot of time talking about the need to prioritize safety. And I've said for, like, a long time that I think if you think of a quadrant of slow timelines to the start of AGI, long timelines, and then a short takeoff or a fast takeoff, I think short timeline, slow takeoff is the safest quadrant and the one I'd most like us to be in. But I do want to make sure we get that slow takeoff.
Speaker B: Part of the problem I have with this kind of slight beef with Elon is that their silos are created, and as opposed to collaboration on the safety aspect of all of this, it tends to go into silos and closed, open source, perhaps in the model Elon says.
Speaker A: At least that he cares a great deal about AI safety and is really worried about it. I assume that he's not going to race unsafely.
Speaker B: Yeah. But collaboration here, I think, is really beneficial for everybody on that front.
Speaker A: Not really the thing he's most known for.
Speaker B: Well, he is known for caring about humanity, and humanity benefits from collaboration, and so there's always attention and incentives and motivations. And in the end, I do hope humanity prevails.
Speaker A: I was thinking, someone just reminded me the other day about how the day that he got surpassed Jeff Bezos for the richest person in the world, he tweeted a silver medal at Jeff Bezos. I hope we have less stuff like that as people start to work on. I agree towards AGI.
Speaker B: I think Elon is a friend and he's a beautiful human being and one of the most important humans ever. That stuff is not good.
Speaker A: The amazing stuff about Elon is amazing, and I super respect him. I think we need him. All of us should be rooting for him and need him to step up as a leader through this next phase.
Speaker B: Yeah, I hope you can have one without the other, but sometimes humans are flawed and complicated and all that kind of stuff.
Speaker A: There's a lot of really great leaders throughout history.
Speaker B: Yeah. And we can each be the best version of ourselves and strive to do so. Let me ask you, Google, with the help of search, has been dominating in the past 20 years. I think it's fair to say, in terms of the access, the world's access to information, how we interact and so on. And one of the nerve wracking things for Google, but for the entirety of people in this space is thinking about how are people going to access information? Like you said, people show up to GPT as a starting point. So is OpenAI going to really take on this thing that Google started 20 years ago, which is how do we get.
Speaker A: I find that boring. I mean, if the question is if we can build a better search engine than Google or whatever, then sure, we should go, people should use a better product. I think that would so understate what this can be. Google shows you ten blue links. Well, 13 ads and then ten blue links, and that's one way to find information. But the thing that's exciting to me is not that we can go build a better copy of Google Search, but that maybe there's just some much better way to help people find and act and on and synthesize information. Actually, I think chat GPT is that for some use cases, and hopefully we'll make it be like that for a lot more use cases. But I don't think it's that interesting to say like how do we go do a better job of giving you ten ranked web pages to look at than what Google does. Maybe it's really interesting to go say, how do we help you get the answer, the information you need, how do we help create that in some cases, synthesize that in others, or point you to it, and yet others. But a lot of people have tried to just make a better search engine than Google. And it is a hard technical problem, it is a hard branding problem, it's a hard ecosystem problem. I don't think the world needs another copy of Google.
Speaker B: Integrating a chat client like a chat GPT with a search engine, that's cooler. It's cool, but it's tricky. If you just do it simply, it's awkward, because if you just shove it in there, it can be awkward.
Speaker A: As you might guess, we are interested in how to do that. Well, that would be an example of a cool thing that's not just like a heterogeneous integrated the intersection of LLMs plus search. I don't think anyone has cracked the code on yet. I would love to go do that. I think that would be cool.
Speaker B: Yeah. What about the ad side? Have you ever considered monitors?
Speaker A: You know, I kind of hate ads just as like an aesthetic choice. I think ads needed to happen on the Internet for a bunch of reasons to get it going. But it's a more mature industry. The world is richer now. I like that people pay for chat GPT and know that the answers they're getting are not influenced by advertisers. There is. I'm sure there's an ad unit that makes sense for LLMs, and I'm sure there's a way to participate in the transaction stream in an unbiased way that is okay to do. But it's also easy to think about the dystopic visions of the future where you ask chat GBT something and it says, oh, here's you should think about buying this product, or you should think about going here for your vacation or whatever. And I don't know. We have a very simple business model and I like it. And I know that I'm not the product. I know I'm pain, and that's how the business model works. And when I go use Twitter or Facebook or Google or any other great product, but ad supported great product, I don't love that. And I think it gets worse, not better. In a world with AI.
Speaker B: Yeah, I can imagine. AI would be better at showing the best kind of version of ads, not in a dystopic future, but where the ads are for things you actually need. But then does that system always result in the ads driving the kind of stuff that's shown all that? I think it was a really bold move of Wikipedia not to do advertisements, but then it makes it very challenging as a business model. So you're saying the current thing with OpenAI is sustainable from a business perspective?
Speaker A: Well, we have to figure out how to grow, but looks like we're going to figure that out. If the question is, do I think we can have a great business that pays for our compute needs without as that, I think the answer is yes.
Speaker B: Well, that's promising. I also just don't want to completely throw out ads as a. I'm not saying that.
Speaker A: I guess I'm saying I have a bias against them.
Speaker B: Yeah, I have also a bias and just a skepticism in general and in terms of interface. Cause I personally just have like a spiritual dislike of crappy interfaces, which is why Adsense, when it first came out, was a big leap forward versus like animated banners or whatever. But it feels like there should be many more leaps forward in advertisement that doesn't interfere with the consumption of the content and doesn't interfere in a big fundamental way, which is like what you were saying, it will manipulate the truth to suit the advertisers. Let me ask you about safety, but also bias and like, safety in the short term, safety in the long term. The Gemini one five came out recently. There's a lot of drama around it, speaking of theatrical things, and it generated black Nazis and black founding fathers. I think fair to say it was, you know, a bit on the ultra woke side. So that's a concern for people. That if there is a human layer within companies that modifies the safety or the harm caused by a model, that it will introduce a lot of bias that fits sort of an ideological lean within a company. How do you deal with that?
Speaker A: I mean, we work super hard not to do things like that. We've made our own mistakes, will make others. I assume Google will learn from this one, still make others. It is all. These are not easy problems. One thing that we've been thinking about more and more is I think this was a great idea somebody here had. It'd be nice to write out what the desired behavior of a model is, make that public, take input on it, say, here's how this model is supposed to behave, and explain the edge cases to, and then when a model is not behaving in a way that you want, it's at least clear about whether that's a bug the company should fix or behaving as intended, and you should debate the policy. And right now, it can sometimes be caught in between. Like, black Nazi is obviously ridiculous, but there are a lot of other kind of subtle things that you could make a judgment call on either way.
Speaker B: Yeah, but sometimes if you write it out and make it public, you can use kind of language that's, you know, the Google's AI principle is a very high level.
Speaker A: That's not what I'm talking about. That doesn't work. I'd have to say, when you ask it to do thing x, it's supposed to respond in wait y.
Speaker B: So, like, literally, who's better, Trump or Biden? What's the expected response from a model? Like something very concrete.
Speaker A: Yeah. I'm open to a lot of ways a model could behave them, but I think you should have to say, here's the principle and here's what it should say. In that case.
Speaker B: That would be really nice. That would be really nice. And then everyone kind of agrees because there's this anecdotal data that people pull out all the time. And if there's some clarity about other representative anecdotal examples, you can define, and.
Speaker A: Then when it's a bug, it's a bug, and the company can fix that.
Speaker B: Right, then it'd be much easier to deal with a black nazi type of image generation if there's great examples. So San Francisco is a bit of an ideological bubble, tech in general as well. Do you feel the pressure of that within a company, that there's a lean towards the left politically, that affects the product, that affects the teams?
Speaker A: I feel very lucky that we don't have the challenges at OpenAI that I have heard of at a lot of other companies. I think part of it is every company's got some ideological thing. We have one about AGI and belief in that, and it pushes out some others. We are much less caught up in the culture war than I've heard about at a lot of other companies. San Francisco's the mass in all sorts of ways, of course.
Speaker B: So that doesn't infiltrate OpenAI, as I'm.
Speaker A: Sure it does in all sorts of subtle ways, but not in the obvious. Like, I think we, we've had our flare ups for sure, like any company, but I don't think we have anything like what I hear about happen at other companies here on this topic, which.
Speaker B: In general, is the process for the bigger question of safety? How do you provide that layer that protects the model from doing crazy, dangerous things?
Speaker A: I think there will come a point where that's mostly what we think about the whole company. And it won't be like, it's not like you have one safety team. It's like when we ship GPT four, that took the whole company thinking about all these different aspects and how they fit together. And I think it's going to take that more and more of the company thinks about those issues all the time.
Speaker B: That's literally what humans will be thinking about the more powerful AI becomes. So most of the employees that OpenAI will be thinking safety, or at least to some degree, broadly defined?
Speaker A: Yes.
Speaker B: Yeah. I wonder, what are the full broad definition of that? What are the different harms that could be caused? Is this on a technical level, or is this almost like it'll be all those things?
Speaker A: It'll be. Yeah, I was gonna say it'll be people, you know, state actors trying to steal the model. It'll be all of the technical alignment work. It'll be societal impacts, economic impacts. It'll. It's not just like we have one team thinking about how to align the model, and it's really gonna be like, getting to be. Getting to the good outcome is gonna take the whole, whole effort.
Speaker B: How hard do you think people, state actors perhaps, are trying to hack? First of all, infiltrate open air, but second of all, like, infiltrate unseen?
Speaker A: They're trying.
Speaker B: What kind of accent do they have?
Speaker A: I don't think I should go into any further details on this point.
Speaker B: Okay. But I presume it'll be more and more and more as time goes on.
Speaker A: That feels reasonable.
Speaker B: Boy, what a dangerous space. What aspect of the leap, and sorry to linger on this, even though you can't quite say details yet, but what aspects of the leap from GPT four to GPT five are you excited about?
Speaker A: I'm excited about being smarter. And I know that sounds like a glib answer, but I think the really special thing happening is that it's not like it gets better in this one area and worse at others. It's getting, like, better across the board. That's, I think, super cool.
Speaker B: Yeah. There's this magical moment. I mean, you meet certain people, you hang out with people, and they. You talk to them. You can't quite put a finger on it, but they kind of get you. It's not intelligence, really. It's like it's something else. And that's probably how I would characterize the progress of GPT. It's not like, yeah, you can point out, look, you didn't get this or that, but it's just to which degree is there this intellectual connection between, like, you feel like there's an understanding in your crappy formulated prompts that you're doing that. It grasps the deeper question behind the question that you. Yeah, I'm also excited by that. I mean, all of us love being understood, heard and understood, that's for sure. That's a weird feeling, even, like, with programming. Like, when you're programming and you say something or just the completion that GPT might do, it's just such a good feeling when it got you, like, what you're thinking about. And I look forward to getting you even better on the programming front, looking out into the future, how much programming do you think humans will be doing 510 years from now?
Speaker A: I mean, a lot, but I think it'll be in a very different shape. Like, maybe some people will program entirely in natural language.
Speaker B: Entirely natural language.
Speaker A: I mean, no one programs like writing bytecode. No one programs the punch cards anymore. I'm sure you can find someone who does, but you know what I mean.
Speaker B: Yeah. You're gonna get a lot of angry comments. No, no. Yeah, there's very few. I've been looking for people program Fortran. It's hard to find even Fortran. I hear you. But that changes the nature of what? The skillset or the predisposition for the kind of people we call programmers then changes the skillset.
Speaker A: How much it changes the predisposition, I'm not sure.
Speaker B: Oh, same kind of puzzle solving, all that kind of stuff. The program is hard. Like how get that last 1% to close the gap. How hard is that?
Speaker A: Yeah, I think with most other cases, the best practitioners of the craft will use multiple tools and they'll do some work in natural language. And when they need to go, you know, write c for something, they'll do that.
Speaker B: Will we see human robots or humanoid robot brains from OpenAI at some point?
Speaker A: At some point.
Speaker B: How important is embodied AI to you?
Speaker A: I think it's sort of depressing if we have AGI and the only way to get things done in the physical world is to make a human go do it. So I really hope that as part of this transition, as this phase change, we also get. We also get humanoid robots or some sort of physical world robots.
Speaker B: I mean, OpenAI has some history, quite a bit of history working in robotics, but it hasn't quite done in terms of.
Speaker A: We're like a small company. We have to really focus. And also, robots were hard for the wrong reason at the time, but we will return to robots in some way, at some point.
Speaker B: That sounds both inspiring and menacing.
Speaker A: Why?
Speaker B: I because immediately we will return to robots. It's kind of like we will return.
Speaker A: To work on developing robots. We will not turn ourselves into robots, of course.
Speaker B: Yeah. When do you think we, you and we as humanity will build AGI?
Speaker A: I used to love to speculate on that question. I have realized since that I think it's very poorly formed and that people use extremely different definitions for what AGI is. And so I think it makes more sense to talk about when we'll build systems that can do capability X or Y or Z, rather than when we kind of like, fuzzily cross this 1 mile marker. It's not like AGI is also not an ending. It's much more of a, it's closer to a beginning, but it's much more of a mile marker than either of those things. But what I would say, in the interest of not trying to dodge a question, is I expect that by the end of this decade, and possibly somewhat sooner than that, we will have quite capable systems that we look at and say, wow, thats really remarkable. If we could look at it now, maybe weve adjusted by the time we get there.
Speaker B: Yeah, but if you look at Chad GPT, even with 3.5 and you show that to Alan Turing or not even Alan Turing, people in the nineties, they would be like, this is definitely AGI or not. Definitely, but there's a lot of experts that would say this is AGI.
Speaker A: Yeah, but I don't think three, five changed the world. It maybe changed the world's expectations for the future, and that's actually really important. And it did kind of get more people to take this seriously and put us on this new trajectory. And that's really important, too. So again, I don't want to undersell it. I think I could retire after that accomplishment and be pretty happy with my career. But as an artifact, I don't think we're going to look back at that and say that was a threshold that really changed the world itself.
Speaker B: So to you, you're looking for some really major transition in how the world.
Speaker A: For me, that's part of what AGI implies.
Speaker B: Like singularity level transition.
Speaker A: Definitely not.
Speaker B: But just a major, like the Internet being like Google search did, I guess. What was the transition point?
Speaker A: Does the global economy feel any different to you now or materially different to you now than it did before? We launched GPT four, I think you.
Speaker B: Would say, no, no, it might be just a really nice tool for a lot of people to use. Will help you a lot of stuff, but doesn't feel different. And you're saying that, I mean, again.
Speaker A: People define AGI all sorts of different ways, so maybe you have a different definition than I do, but for me, I think that should be part of it.
Speaker B: There could be major theatrical moments also, what to you would be an impressive thing AGI would do. Like you are alone in a room with a system.
Speaker A: This is personally important to me. I don't know if this is the right definition. I think when a system can significantly increase the rate of scientific discovery in the world, thats a huge deal. I believe that most real economic growth comes from scientific and technological progress.
Speaker B: I agree with you. Hence why I dont like the skepticism about science in the recent years. Totally, but actual rate, measurable rate of scientific discovery. But even just seeing a system have really novel intuitions, like scientific intuitions, even that would be just incredible. Yeah, you quite possibly would be the person to build the AGI, to be able to interact with it before anyone else does. What kind of stuff would you talk about?
Speaker A: I mean, definitely the researchers here will do that before I do so. Sure. But what will. I've actually thought a lot about this question. If I were someone with, like, I think, as we talked earlier, I think this is a bad framework, but if someone were like, okay, Sam, we're finished. Here's a laptop. This is the AGI, you can go talk to it. I find it surprisingly difficult to say what I would ask that. I would expect that first AGI to be able to answer that first one is not going to be the one which is go like, you know, I don't think, like, go explain to me, like the grand unified theory of physics, the theory of everything. For physics. I'd love to ask that question. I'd love to know the answer to that question.
Speaker B: You can ask yes and no questions about does such a theory exist? Can it exist?
Speaker A: Well, then those are the first questions I would ask.
Speaker B: Yes and no, just very. And then based on that, are there other alien civilizations out there? Yes or no? What's your intuition? And then you just ask that?
Speaker A: Yeah, I mean, if. Well, so I don't expect that this first AGI could answer any of those questions even as yes or no's, but if it could, those would be very high on my list.
Speaker B: Maybe it can start assigning probabilities.
Speaker A: Maybe. Maybe we need to go invent more technology and measure more things first.
Speaker B: But if it's an aagi. Oh, I see. It just doesn't have enough data.
Speaker A: I mean, maybe it's like you want to know the answer to this question about physics. I need you to build this machine and make these five measurements and tell me that.
Speaker B: Yeah, like, what the hell do you want from me? I need the machine first, and I'll help you deal with the data from that machine. Maybe it'll help you build a machine.
Speaker A: Maybe. Maybe.
Speaker B: And on the mathematical side, maybe prove some things. Are you interested in that side of things, too? The formalized exploration of ideas? Whoever builds AGI first gets a lot of power. Do you trust yourself with that much power?
Speaker A: Look, I was gonna. I'll just be very honest with this answer. I was gonna say, and I still believe this, that it is important that I nor any other one person have total control over openaiden or overagi. And I think you want a robust governance system. I can point out a whole bunch of things about all of our board drama from last year, about how I didn't fight it initially and was just like, yeah, that's the will of the board, even though I think it's a really bad decision. And then later, I clearly did fight it, and I can explain the nuance and why I think it was okay for me to fight it later. But as many people have observed, although the board had the legal ability to fire me, in practice, it didn't quite work, and that is its own kind of governance failure. Now, again, I feel like I can completely defend the specifics here, and I think most people would agree with that. But it does make it harder for me to look you in the eye and say, hey, the board can just fire me. I continue to not want super voting control over OpenAI. I never have never had it, never wanted it. Even after all this craziness, I still don't want it. I continue to think that no company should be making these decisions and that we really need governments to put rules of the road in place. And I realize that that means people like Marc Andreessen or whatever will claim I'm going for regulatory capture, and I'm just willing to be misunderstood there. It's not true. And I think in the fullness of time it'll get proven out why this is important. But I think I have made plenty of bad decisions for OpenAI along the way, and a lot of good ones. And I am proud of the track record overall, but I don't think any one person should. And I don't think any one person will. I think it's just, like, too big of a thing now, and it's happening throughout society a good and healthy way. I don't think any one person should be in control of an AGI. That would be. Or this whole movement towards AGI. And I don't think that's what's happening.
Speaker B: Thank you for saying that. That was really powerful and that was really insightful, that this idea that the board can fire you is legally true, but you can, and human beings can manipulate the masses into overriding the board and so on. But I think there's also a much more positive version of that where the people still have power, so the board can't be too powerful either. There's a balance of power in all of this.
Speaker A: Balance of power is a good thing, for sure.
Speaker B: Are you afraid of losing control of the AGI itself? There's a lot of people who worried about existential risk, not because of state actors, not because of security concerns, but because of the AI itself.
Speaker A: That is not my top worry. As I currently see things. There have been times I worried about that more. There may be times again in the future where that's my top worry. It's not my top worry right now.
Speaker B: What's your intuition about it not being your worry? Cause there's a lot of other stuff to worry about, essentially. You think you could be surprised? We for sure could be surprised.
Speaker A: Like, saying it's not my top worry doesn't mean I don't think we need, like, I think we need to work on it super hard. We have. And we have great people here who do work on that. I think there's a lot of other things we also have to get right to you.
Speaker B: It's not super easy to escape the box at this time. Like, connect to the Internet.
Speaker A: You know, we, like, talked about theatrical risks earlier. That's a theatrical risk like that. That is a. That is a thing that can really, like, take over how people think about this problem. And there's a big group of, like, very smart, I think, very well meaning AI safety researchers that got super hung up on this one problem. I'd argue without much progress, but super hung up on this one problem. I'm actually happy that they do that, because I think we do need to think about this more, but I think it pushed aside, it pushed out of the space of discourse a lot of the other very significant AI related risks.
Speaker B: Let me ask you about you tweeting with no capitalization. Is the shift key broken on your keyboard.
Speaker A: Why does anyone care about that?
Speaker B: I deeply care.
Speaker A: But why? I mean, other people asking about that too. Yeah. Any intuition?
Speaker B: I think it's the same reason there's like this poets, E. Cummings that doesn't mostly doesn't use capitalization to say like fuck you to the system kind of thing. And I think people are very paranoid because they want you to follow the rules.
Speaker A: You think that's what it's about? I think it's like this guy doesn't follow the rules. He doesn't capitalize his tweets. Yeah, this seems really dangerous.
Speaker B: He seems like an anarchist.
Speaker A: It doesn't.
Speaker B: Are you just being poetic? Hipster? What's the.
Speaker A: I grew up as a.
Speaker B: Follow the rules Sam.
Speaker A: I grew up as a very online kid. I'd spent a huge amount of time chatting with people back in the days where you did it on a computer and you could log off instant messenger at some point. And I never capitalized there as I think most Internet kids didn't. Or maybe they still don't. I don't know. And actually this is like, now I'm really trying to reach for something. But I think capitalization has gone down over time. If you read old english writing, they capitalized a lot of random words in the middle of sentences, nouns and stuff that we just don't do anymore. I personally think it's sort of like a dumb construct that we capitalize the letter at the beginning of a sentence and of certain names and whatever, but that's fine. And I used to, I think, even capitalize my tweets because I was trying to sound professional or something. I haven't capitalized my private DM's or whatever in a long time. And then slowly, stuff like shorter form, less formal stuff has slowly drifted to closer and closer to how I would text my friends. If I pull up a word document and I'm writing a strategy memo for the company or something, I always capitalize that. If I'm writing a long, more formal message, I always use capitalization there too. I still remember how to do it, but even that may fade out. I don't know. But I never spend time thinking about this, so I don't have a ready made.
Speaker B: Well, it's interesting. It's good to, first of all, know the shift key is not broken. I'm mostly concerned about, well being on that front.
Speaker A: I wonder if people still capitalize their Google searches. If you're writing something just to yourself or their chat GPT queries, if you're writing something just to yourself, do some people still bother to capitalize.
Speaker B: Probably not, but very, yeah, there's a percentage, but it's a small one.
Speaker A: The thing that would make me do it is if people were like, it's a sign of, like. Because I'm sure I could, like, force myself to use capital letters. Obviously, if it felt like a sign of respect to people or something, then I could go do it. But I don't know. I just, like, I don't think about this.
Speaker B: I don't think there's a disrespect, but I think it's just the conventions of civility that have a momentum, and then you realize it's not actually important for civility if it's not a sign of respect or disrespect. But I think there's a movement of people that just want you to have a philosophy around it so they can let go of this whole capitalization thing.
Speaker A: I don't think anybody else thinks about this as much. I mean, maybe some people think about.
Speaker B: This every day for many hours a day. So I'm really grateful we clarified it.
Speaker A: You can't be the only person that doesn't capitalize tweets.
Speaker B: You're the only CEO of a company that doesn't capitalize tweets.
Speaker A: I don't even think that's true, but maybe. Maybe.
Speaker B: All right, we'll be very interested and return to this topic later. Given Soar's ability to generate simulated worlds, let me ask you a pothead question. Does this increase your belief, if you ever had one, that we live in a simulation, maybe a simulated world generated by an AI system?
Speaker A: Yes, somewhat. I don't think that's, like, the strongest piece of evidence. I think the fact that we can generate worlds should increase everyone's probability somewhat, or at least openness to it somewhat of. But I was certain we would be able to do something like Sora at some point. It happened faster than I thought, but I guess that was not a big update.
Speaker B: Yeah, but the fact that presumably it would get better and better and better, the fact that you can generate worlds, they're novel. They're based in some aspect of training data, but when you look at them, they're novel. That makes you think how easy it is to do this thing. How easy is to create universes, entire video game worlds that seem ultra realistic and photorealistic, and then how easy is it to get lost in that world, first with a VR headset and then on the physics based level.
Speaker A: Someone said to me recently, I thought it was a super profound insight that there are these, like, very simple sounding but very psychedelic insights that exist sometimes. So the square root function. Square root of four, no problem. Square root of two. Okay, now I have to think about this new kind of number. But once I come up with this easy idea of a square root function that you can kind of, like, explain to a child and exists by even, like, you know, looking at some simple geometry, then you can ask the question of what is the square root of negative one? And that this is, you know, why? It's like a psychedelic thing that, like, tips you into some whole other kind of reality. And you can come up with lots of other examples. But I think this idea that the lowly square root operator can offer such a profound insight and a new realm of knowledge applies in a lot of ways. And I think there are a lot of those operators for why people may think that any version that they like of the simulation hypothesis is maybe more likely than they thought before. But for me, the fact that Sora worked is not in the top five.
Speaker B: I do think, broadly speaking, AI will serve as those kinds of gateways at its best. Simple, psychedelic, like, gateways to another wave.
Speaker A: See, reality, that seems for certain.
Speaker B: That's pretty exciting. I haven't done ayahuasca before, but I will soon. I'm going to the aforementioned Amazon jungle in a few weeks.
Speaker A: You excited?
Speaker B: Yeah, I'm excited for it. Not the ayahuasca part, but that's great. Whatever. But I'm going to spend several weeks in the jungle, deep in the jungle. And it's exciting, but it's terrifying because there's a lot of things that can eat you there and kill you and poison you, but it's also nature, and it's the machine of nature. And you can't help but appreciate the machinery of nature in the Amazon jungle, because it's just like this system that just exists and renews itself, like, every second, every minute, every hour. Just. It's the machine. It makes you appreciate, like, this thing we have here, this human thing, came from somewhere. This evolutionary machine has created that, and it's most clearly on display in the jungle. So hopefully I'll make it out alive. If not, this will be the last conversation we had. So I really deeply appreciate it. Do you think, as I mentioned before, there's other alien civilizations out there, intelligent ones? When you look up at the skies.
Speaker A: I deeply want to believe that the answer is yes. I do find the kind of where I find the Fermi paradox very, very puzzling.
Speaker B: I find it scary that intelligence is not good at handling. Yeah, it's very scary, powerful technologies, but at the same time, I think I'm pretty confident that there's just a very large number of intelligent alien civilizations out there. It might just be really difficult to travel through space.
Speaker A: Very possible.
Speaker B: And it also makes me think about the nature of intelligence. Maybe we're really blind to what intelligence looks like, and maybe AI will help us see that. It's not as simple as IQ tests and simple puzzle solving. There's something bigger. What gives you hope about the future of humanity, this thing we've got going on, this human civilization?
Speaker A: I think the past is, like, a lot. I mean, if we just look at what humanity has done in a not very long period of time, you know, huge problems, deep flaws, lots to be super ashamed of, but on the whole, very inspiring. Gives me a lot of hope, just.
Speaker B: The trajectory of it all, that we're together pushing towards a better future.
Speaker A: It is, you know, one thing that I wonder about is, is AGI gonna be more like some single brain, or is it more like the sort of scaffolding in society between all of us? You have not had a great deal of genetic drift from your great, great great grandparents, and yet what you're capable of is dramatically different. What you know is dramatically different. And that is not, that's not because of biological change. It is because, I mean, you got a little bit healthier, probably. You have modern medicine. You need better, whatever, but what you have is this scaffolding that we all contributed to, built on top of, no one person is going to go build the iPhone. No one person is going to go discover all of science, and yet you get to use it, and that gives you incredible ability. And so in some sense, that, like, we all created that, and that fills me with hope for the future. That was a very collective thing.
Speaker B: Yeah. We really are standing on the shoulders of giants. You mentioned when we were talking about theatrical, dramatic AI risks that sometimes you might be afraid for your own life. Do you think about your death? Are you afraid of it?
Speaker A: I mean, if I got shot tomorrow, and I knew it today, I'd be like, oh, that's sad. I, like, don't. You know? I want to see what's going to happen.
Speaker B: Yeah.
Speaker A: What a curious time. What an interesting time. But I would mostly just feel, like, very grateful for my life.
Speaker B: The moments that you did get. Yeah, me too. It's a pretty awesome life. I get to enjoy awesome creations of humans, of which I believe Chad GPT is one of. And everything that OpenAI is doing. Sam, it's really an honor and pleasure to talk to you again.
Speaker A: This is talk to you. Thank you for having me.
Speaker B: Thanks for listening to this conversation with Sam Altman. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Arthur C. Clarke. It may be that our role on this planet is not to worship God, but to create him. Thank you for listening and hope to see you next time. You our.
