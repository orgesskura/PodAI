Transcription for Rana el Kaliouby： Emotion AI, Social Robots, and Self-Driving Cars ｜ Lex Fridman Podcast #322.mp3:
Full transcript: There's a broader question here, right. As we build socially and emotionally intelligent machines, what does that mean about our relationship with them, and then more broadly, our relationship with one another, right? Because this machine is going to be programmed to be amazing at empathy by definition, right? It's going to always be there for you. It's not going to get bored. I don't know how I feel about that. I think about that. A lot of the following is a conversation with Rana L. Kolyubi, a pioneer in the field of emotion, recognition and human centric artificial intelligence. She is the founder of Effectiva, deputy CEO of Smarteye, author of Girl Decoded, and one of the most brilliant, kind, inspiring and fun human beings I've gotten the chance to talk to. This is Alex Friedman podcast to support it. Please check out our sponsors in the description. And now, dear friends, here's Rana L. Kalyubi. You grew up in the Middle east in Egypt. What is a memory from that time that makes you smile? Or maybe a memory that stands out as helping your mind take shape and helping you define yourself in this world? So the memory that stands out is, we used to live in my grandma's house. She used to have these mango trees in her garden, and in the summer, and so mango season was like July and August. And so in the summer she would invite all my aunts and uncles and cousins and, you know, like, it was just like, maybe there were like 20 or 30 people in the house, and she would cook all this amazing food. And us, the kids, we would like, go down the garden and we would like, pick all these mangoes. And I don't know, I think it's just the bringing people together like that always stuck with me. The warmth around the mango tree. Yeah, around the mango tree. And there's just like the joy, the joy of being together around food. And I'm a terrible cook, so I guess that didn't, that memory didn't translate to me kind of doing the same. I love hosting people. Do you remember colors, smells? Is that what, like what, how does memory work? Yeah. Like, what do you visualize? Do you visualize people's faces, smiles? Do, is there colors? Is there like a yemenite, a theme to the colors as it smells because of food involved? Yeah, I think that's a great question. So those egyptian mangoes, there's a particular type that I love, and it's called Dawesi mangoes. And they're kind of, you know, they're oval and they have a little red in them. So they're red and mango colored on the outside. So I remember that. Does red indicate, like, extra sweetness? Is that. Yes. That means, like, it's nicely sweet. Yeah, it's nice and ripe and stuff. Yeah. What's, like, a definitive food of Egypt? You know, there's like, these almost stereotypical foods in different parts of the world. Like, Ukraine invented borscht. Borscht is this beet soup with that you put sour cream on. See, it's not. I can't explain it that way. If you know, if you know, if you know what it is, I think, you know, it's delicious, but if I explain it, it's just not gonna sound delicious. Be a, like, beet soup. This doesn't make any sense, but that's kind of. And you probably have actually seen pictures of it. Cause it's one of the traditional foods in Ukraine, in Russia, in different parts of the slavic world. So that's. But it's become so cliche and stereotypical that you almost don't mention it. But it's still delicious. Like, I visited Ukraine, I eat that every single day. So do you make it yourself? How hard is it to make. No, I don't know. I think to make it, well, like anything. Like Italians, they say, well, tomato sauce is easy to make, but to make it right, that's like a generational skill. So anyway, is there something like that in Egypt? Is there a culture of food? There is. And actually, we have a similar kind of soup. It's called molochaya, and it's made of this green plant. It's like somewhere between spinach and kale. And you mince it, and then you cook it in chicken broth. And my grandma used to make, and my mom makes it really well, and I try to make it, but it's not as great. So we used to have that, and then we used to have it alongside stuffed pigeons. I'm pescetarian now, so I don't eat that anymore. But stuffed pigeons? Yeah, it was really yummy. It's the one thing I miss about, you know, now that I'm pescetarian and. I don't eat the stuffed pigeons. Yeah, the stuffed pigeons. What are they stuffed with? If that doesn't bother you too much to describe. No, no. It's stuffed with a lot of, like, just rice and. Oh, God, yeah. Rice. Yeah. So. And you also, you said that your first in your book, that your first computer was an Atari and Space Invaders was your favorite game. Is that when you first fell in love with computers, would you say, yeah, I would say. So. Video games or just the computer itself, just something about the machine. Oh, this thing. There's magic in here. Yeah. I think the magical moment is definitely like playing video games with my. I have two younger sisters, and we just, like, had fun together, like, playing games. But the other memory I have is my first code. The first code I wrote, I wrote. I drew a Christmas tree, and I'm Muslim. Right. So it's kind of, it was kind of funny that I, that I, that the first thing I did was, like, this Christmas tree. So. Yeah. And that's when I realized, wow, you can write code to do all sorts of, like, really cool stuff. I must have been like, six or seven at the time. So you can write programs and the programs do stuff for you. That's power, if you think about it. That's empowering. Hey. Hi. Yeah, I know what it is. I don't know. You see, like, I don't know if many people think of it that way when they first learn to program. They just love the puzzle of it. Like, ooh, this is cool. This is pretty. It's a Christmas tree, but, like, it's power. It is you eventually, I guess you couldn't at the time, but eventually, this thing, if it's interesting enough, if it's a pretty enough Christmas tree, it can be run by millions of people and bring them joy, like that little thing. And then because it's digital, it's easy to spread. So, like, you just created something that's easily spreadable to millions of people. Totally. It's hard to think that way when you're six. In the book, you write, I am who I am because I was raised by a particular set of parents, both modern and conservative, forward thinking, yet locked in tradition. I'm a Muslim, and I feel I'm stronger and more centered for it. I adhere to the values of my religion, even if I'm not as dutiful as I once was. And I am a new american, and I'm thriving on the energy, vitality, and entrepreneurial spirit of this great country. So let me ask you about your parents. What have you learned about life from them, especially when you were young? So both my parents, they're egyptian, but they moved to Kuwait right out. They actually, there's a cute story about how they met. So my dad taught coble in the seventies. Nice. And my mom decided to learn programming, so she signed up to take his Cobol programming class, and he tried to date her, and she was like, no, no, no, I don't date. And so he's like, okay, I'll propose. And that's how they got married. Whoa. Yeah, I know, right? Exactly right? That's really impressive. So those kobold guys know how to. How to impress a lady. So. So, yeah. So what have you learned from them? So, definitely grit. One of the core values in our family is just hard work. There were no slackers in our family. And that's something I've definitely. That's definitely stayed with me, both as a professional, but also in my personal life. But I also think my mom always used to. I don't know, it was like unconditional love. I just knew my parents would be there for me regardless of what I chose to do. I think that's very powerful. And they got tested on it because I kind of challenged. I challenged cultural norms, and I kind of took a different path, I guess, than what's expected of a woman in the Middle east. And they still love me, which is. I'm so grateful for that. When was a moment that was the most challenging for them? Which moment where they kind of had to come face to face with the fact that you're a bit of a rebel? I think the first big moment was when I. I had just gotten married, but I decided to go do my PhD at Cambridge University. And because my husband at the time, he's now my ex, ran a company in Cairo. He was going to stay in Egypt. So it was going to be a long distance relationship. And that's very unusual in the Middle east for a woman to just head out and kind of, you know, pursue her career. And so my dad, actually, my dad and my I. Parents in law both said, you know, we do not approve of you doing this, but now you're under the jurisdiction of your husband, so he can make the call. And luckily for me, he was supportive. He said, you know, this is your dream come true. You've always wanted to do a PhD. I'm going to support you. So I think that was the first time where, you know, I challenged the cultural norms. Was that scary? Oh, my God, yes, it was totally scary. What's the biggest culture shock from there to Cambridge to London? Well, that was also during, right around September 11, so everyone thought that there was going to be a third world war. It was really. And at the time, I used to wear the hijab, so I was very visibly muslim. And so my parents just were. They were afraid for my safety. But anyways, when I got to Cambridge, because I was so scared, I decided to take off my headscarf and wear a hat instead. So I just went to class wearing these, like, british hats, which was, in my opinion, actually worse than just showing up in a headscarf because it was just so awkward. Right. Like sitting in class with, like, all. These, trying to fit in. Yeah, yeah, yeah, yeah, yeah. So after a few weeks of doing that, I was like, to heck with that. I'm just gonna go back to wearing my headscarf. Yeah, you wore the hijab. So starting in 2000 and for twelve years after, so always whenever you're in public, you have to wear the head covering. Can you speak to that? To the hijab? Maybe your mixed feelings about it. Like, what does it represent in its best case? What does it represent in the worst case? Yeah, you know, I think there's a lot of. I guess I'll first start by saying I wore it voluntarily. I was not forced to wear it. And in fact, I was one of the very first women in my family to decide to put on the hijab. And my family thought it was really odd. Right? Like, there was, they were like, why do you want to put this on? And at its best, it's a sign of modesty, humility. It's like me wearing a suit. People are like, why are you wearing a suit? It's a step back into some kind of tradition, a respect for tradition of sorts. So you said because it's by choice, you're kind of free to make that choice, to celebrate a tradition of modesty. Exactly. And I actually made it my own. I remember I would really match the color of my head scarf with what I was wearing. It was a form of self expression. And at its best, I loved wearing it. I have a lot of questions around how we practice religion and religion. And, you know, and I think also it was a time where I was spending a lot of time going back and forth between the US and Egypt. And I started meeting a lot of people in the US who are just amazing people, very purpose driven people who have very strong core values, but they're not Muslim. That's okay. Right. And so that was when I just had a lot of questions. And politically also, the situation in Egypt was when the Muslim Brotherhood ran the country and I didn't agree with their ideology. It was at a time when I was going through a divorce. It was just the perfect storm of political personal conditions where I was like, this doesn't feel like me anymore. And it took a lot of courage to take it off because culturally, it's okay if you don't wear it, but it's really not okay to wear it and then take it off. But you're still. So you have to do that while still maintaining a deep core and pride in the origins in your origin story. Totally. So still being egyptian, still being a Muslim. Right. And being, I think, generally, like, faith driven, but, yeah. But what that means changes year by year for you. It's like a personal journey. Yeah, exactly. What would you say is the role of faith in that part of the world? Like, how do you see. You mention it a bit in the book, too. Yeah. I mean, I think there is something really powerful about just believing that there's a bigger force. You know, there's a kind of surrendering, I guess, that comes with religion. And you surrender and you have this deep conviction that it's gonna be okay. Right. Like, the universe is out to, like, do amazing things for you, and it's gonna be okay. And there's strength to that. Like, even when you're going through adversity, you just know that it's gonna work out. Yeah. It gives you, like, an inner peace, a calmness. Exactly, exactly. Yeah. That's faith in all the meanings of that word. Right. Faith that everything is going to be okay, and it is because time passes and time cures all things. It's like a calmness with the chaos of the world. And also there's like, a silver. I'm a true believer of this, that something at a specific moment in time can look like it's catastrophic and it's not what you wanted in life. Da da da da. But then time passes, and then you look back and there's a silver lining. Right? It maybe closed the door, but it opened a new door for you. And so I'm a true believer in that. That, you know, there's a silver lining in almost anything in life. You just have to have this, like. Yeah. Faith or conviction that it's going to work out. So. Such a beautiful way to see a shady feeling. So if you're. If you feel shitty about a current situation, I mean, it almost is always true. Unless it's the cliches thing of if it doesn't kill you, whatever doesn't kill you makes you stronger. It's. It does seem that over time, when you take a perspective on things, that the hardest moments and periods of your life are the most meaningful. Yeah. Yeah. So over time, you get to have that. Right. What about. Because you mentioned Kuwait? What about. Let me ask you about war. What's the role of war and peace? Maybe even the big love and hate in that part of the world, because it does seem to be a part of the world where there's turmoil. There was turmoil, there's still turmoil. It is so unfortunate, honestly. It's such a waste of human resources and human mind. Share. At the end of the day, we all kind of want the same things. We want a human connection. We want joy, we want to feel fulfilled. We want to feel, you know, a life of purpose. And I just find it baffling, honestly, that we are still having to grapple with that. I have a story to share about this. You know, I grew up, indeed, I'm egyptian American now, but, you know, originally from Egypt. And when I first got to Cambridge, it turned out my office mate, like, my PhD kind of, you know, ended up, you know, we ended up becoming friends. But she was from Israel, and we didn't know. Yeah, we didn't know how it was gonna be. Like, did you guys sit there just staring at each other for a bit? Actually, she. Cause I arrived before she did, and it turns out she emailed our PhD advisor and asked him if she thought it was gonna be okay. Yeah. Oh, this is around 911, too. Yeah. And Peter Robinson, our PhD advisor, was like, yeah. Just as an academic institution. Just show up. And we became super good friends. We were both new moms. Like, we both had our kids during our PhD. We were both doing artificial emotional intelligence. She was looking at speech, I was looking at the face. We just had. So the culture was so similar. Our jokes were similar. It was just. I was like, why on earth are our countries? Why is there all this war and tension? And I think it falls back to the narrative, right? If you change the narrative, like, whoever creates this narrative of war, I don't know. We should have women run the world. Yeah, that's one solution. The good women. Because there's also evil women in the world. True. Okay. But yes, yes, there could be less war if women ran the world. The other aspect is, doesn't matter. The gender, the people in power. You know, I get to see this with Ukraine and Russia, parts of the world around that conflict now, and that's happening in Yemen as well. And everywhere else, there's these narratives told by the leaders to the populace, and those narratives take hold, and everybody believes that, and they have a distorted view of the humanity on the other side. In fact, especially during war, you don't even see the people on the other side as human or as equal, intelligence or worth or value as you. You tell all kinds of narratives about them. Being Nazis or dumb or whatever narrative you want to weave around that or evil. But I think when you actually meet them face to face, you realize they're like the same. Exactly. Right. It's actually big shock for people to realize, like, that they've been essentially lied to within their country. And I kind of have faith that social media, as ridiculous it is to say or any kind of technology is able to bypass the walls that governments put up and connect people directly. And then you get to realize, ooh, like, people fall in love across different nations, religions, and so on. And that, I think, ultimately can cure a lot of our ills, especially sort of in person, just, I also think that if leaders met in person to have a conversation that would cure a lot of the ills of the, of the world, especially in private. Let me ask you about the women running, running the world. So gender does in part, perhaps shaped the landscape of just our human experience. So in what ways was it limiting? And in what ways was it empowering for you to be a woman in the Middle East? I think just kind of just going back to, like, my comment on, like, women running the world, I think it comes back to empathy, right. Which has been a common thread throughout my entire career. And it's this idea of human connection. Once you build common ground with a person or a group of people, you build trust, you build loyalty, you build friendship, and then you can turn that into behavior change and motivation and persuasion. So it's like empathy and emotions are just at the center of everything we do. And I think being from the Middle east, kind of this human connection is very strong. Like, we have this running joke that if you come to Egypt for a visit, people are gonna, we'll know everything about your life, like, right away. Right. I have no problems asking you about your personal life. There's no, like, no boundaries, really. No personal boundaries. In terms of getting to know people. We get emotionally intimate, like, very, very quickly. But I think people just get to know each other, like, authentically, I guess. You know, there isn't this, like, superficial level of getting to know people. You just try to get to know people, really. And empathy is a part of that totally. Because you can put yourself in this person's shoe and kind of. Yeah, imagine, you know, what challenges they're going through. And so I think I've definitely taken that with me. Generosity is another one, too. Like, just being generous with your time and love and attention and even with your wealth. Right. Even if you don't have a lot of it, you're still very generous, and. I think that's another enjoying the humanity of other people. And so do you think there's a useful difference between men and women in that aspect and empathy, or is doing these kind of big general groups, does that hinder progress? Yeah, I actually don't want to over generalize. I mean, some of the men I know are, like, the most empathetic humans. Yeah, I strive to be empathetic. Yeah, you're actually very empathetic. Yeah. So I don't want to over generalize. Although one of the researchers I worked with when I was at Cambridge, Professor Simon Baring Cohen, he's Sacha Baring Cohen's cousin. Yeah. And he runs the autism research center at Cambridge, and he's written multiple books on autism. And one of his theories is the empathy scale, like the systemizers and the empathizers. And there's a disproportionate amount of computer scientists and engineers who are systemizers and perhaps not great empathizers. And then, you know, there's. And there's more men in that bucket, I guess, than women, and then there's more women in the empathizer's bucket. So, again, not. Not to over generalize. I sometimes wonder about that. It's been frustrating to me how many, I guess, systemizers there are in the field of robotics. Yeah. It's actually encouraging to me because I care about, obviously, social robotics and because it's. It. There's more opportunity for people that are empathic. Exactly. I totally agree. Well, right, so it's nice. Yes. So every robotist I talk to, they don't see the. The human as interesting as, like, it is. It's not exciting. You want to avoid the human at all costs. It's a. It's a safety concern to be touching the human, which it is, but it's also an opportunity for a deep connection or collaboration or all that kind of stuff. And because most brilliant roboticists don't care about the human, it's an opportunity. In your case, it's a business opportunity, too, but in general, an opportunity to explore those ideas in this beautiful journey to Cambridge, to UK, and then to America. What's the moment or moments that were most transformational for you as a scientist and as a leader? So he became an exceptionally successful CEO, founder, researcher, scientist, and so on. Was there a phase shift there where, like, I can be somebody, I can really do something in this world? Yeah. So, actually, just kind of a little bit of background. So the reason why I moved from Cairo to Cambridge, UK, to do my PhD is because I had a very, you know, clear career plan. I was like, okay, I'll go abroad, get my PhD, gonna crush it in three or four years, come back to Egypt and teach. It was very clear, very well laid out. Was topic clear or. No? The topic. Well, I did my PhD around building artificial emotional intelligence and looking at it. No, but in your master plan, ahead of time, when you're sitting by the mango tree, did you know it's gonna be artificial intelligence? No, no, no, that I did not know. Although I think I kind of knew that I was going to be doing computer science, but I didn't know the specific area. But I love teaching. I mean, I still love teaching. So I just, yeah, I just wanted to go abroad, get a PhD, come back, teach. Why computer science? Can we just linger on that? Because you're such an empathic person who cares about emotion, humans and so on. Aren't computers cold and emotionless with changing that? Yeah, I know, but like, isn't that the. Or did you see computers as the having the capability to actually connect with humans? I think that was, like, my takeaway from my experience just growing up. Like, computers sit at the center of how we connect and communicate with one another. Right. Or technology in general. Like, I remember my first experience being away from my parents. We communicated with a fax machine. But thank goodness for the fax machine because we could send letters back and forth to each other. This was pre emails and stuff. So I think technology can be not just transformative in terms of productivity, et cetera. It actually does change how we connect with one another. Can I just defend the fax machine? There's something like the haptic feel because the email is all digital. There's something really nice, I still write letters to people. There's something nice about the haptic aspect of the fax machine because you still have to press, you still have to do something in the physical world to make this thing a reality. The sense of. Right. And then it comes out as a printout and you can actually touch it and read it. Yeah, there's something lost when it's just an email, obviously. I wonder how we can regain some of that in the digital world, which goes to the metaverse and all those kinds of things. We'll talk about it. Actually, do a question on that one. Do you still, do you have photo albums anymore? Do you still print photos? No, no, but I'm a minimalist. Okay. So it was one of the, one of the painful steps in my life. Was to scan all the photos and let go of them and then let go of all my books. You let go of your books? Yeah, switch to Kindle. Everything, kinda. So I thought. I thought, okay, think 30 years from now, nobody's gonna have books anymore. The technology of digital books gonna get better and better and better. Are you really gonna be the guy that's still romanticizing physical books? Are you gonna be the old man on the porch who's like, kids? Yes. So just get used to it. Cause it was, it felt. It still feels a little bit uncomfortable to read on a. On a Kindle. But get used to it like you always. I mean, I'm trying to learn new programming languages. Always. Like, with technology, you have to kind of challenge yourself to adapt to it. You know, I force myself to use TikTok. No, that thing doesn't need much forcing it pulls you in like a. Like a. Like the worst kind of. Or the best kind of drug anyway. Yeah. So. Yeah, but I do love haptic things. There's a magic to the haptic, even, like touchscreens. It's tricky to get right to. To get the experience of a button. Yeah. Anyway, what were we talking about? So, AI. So the journey, your whole plan was to come back to Cairo and teach, right? And then what, did the plan go wrong? Yeah, exactly right. And then I got to Cambridge and I fall in love with the idea of research, right. And kind of embarking on a path. Nobody's explored this path before. You're building stuff that nobody's built before, and it's challenging and it's hard and there's a lot of non believers. I just totally loved that. And at the end of my PhD, I think it's the meeting that changed the trajectory of my life. Professor Rosalind Picard, who's. She runs the affective computing group at the MIT media lab. I had read her book. You know, I was like, following all her research. Aka raze? Yes, aka Roz. And she was giving a talk at a pattern recognition conference in Cambridge and she had a couple of hours to kill, so she emailed the lab and she said, you know, if any students want to meet with me, like, just, you know, sign up here. And so I signed up for slot and I spent like the weeks leading up to it preparing for this meeting, and I want to show her a demo of my research and everything. And we met and we ended up hitting it off. Like, we totally clicked. And at the end of the meeting, she said, do you want to come work with me as a postdoc at MIT. And this is what I told her. I was like, okay, this would be a dream come true, but there's a husband waiting for me in Cairo. I kind of have to go back. Yeah. And she said, it's fine. Just commute. And I literally started commuting between Cairo and Boston. Yeah, it was a long commute. And I did that, like, every few weeks, I would, you know, hop on a plane and go to Boston. But that. That changed the trajectory of my life. There was no. I kind of outgrew my dreams. Right. I didn't want to go back to Egypt anymore and be faculty. Like, that was no longer my dream. I had a new dream. What was the. What was it like to be at MIT? What was that? Culture shock? You mean America in general, but also, I mean, Cambridge has its own culture, so what was MIT like? And what was America like? I think. I wonder if that's similar to your experience at MIT. I was just at the media lab in particular. I was just really impressed is not the right word. I didn't expect the openness to, like, innovation and the acceptance of taking a risk and failing. Like, failure isn't really accepted back in Egypt. Right. You don't want to fail. Like, there's a fear of failure, which I think has been hardwired in my brain. But you get to MIT and it's okay to start things, and if they don't work out, like it's okay, you pivot to another idea. And that kind of thinking was just very new to me. That's liberating. Well, media lab, for people who don't know MIT, media lab is its own beautiful thing, because they, I think, more than other places, MIT reach for big ideas. And, like, they try. I mean, I think. I mean, depending, of course, on who. But certainly with Roslyn, you try wild stuff. You try big things and crazy things, and also try to take things to completion so you can demo them. So always, always have a demo. Like, if you go. One of the sad things to me about robotics labs at MIT, and there's, like, over 30, I think, is like, usually when you show up to a robotics lab, there's not a single working robot. They're all broken. All the robots are broken, which is like the normal state of things because you're working on them. But it would be nice if we lived in a world where robotics labs had some robots functioning. One of my favorite moments that just sticks with me. I visited Boston Dynamics, and there was a, first of all, seeing so many spots so many legged robots in one place. I'm like, I'm home. But the. This is where I was built. The cool thing was just to see there was a random robot spot was walking down the hall. It's probably doing mapping, but it looked like he wasn't doing anything. And he was wearing he or she. I don't know, but it. Well, in my mind, they're people. They have a backstory. But this one in particular definitely has a backstory because he was wearing a cowboy hat. So I just saw a spot robot with a cowboy hat walking down the hall. And there was just this feeling like there's a life. Like he has a life. He probably has to commute back to his family at night. Like there's a. There's a feeling like there's life instilled in this robot and that's magical. I don't know, it was kind of inspiring to see. Did it say hello to. Did he say hello to you? No. There's a focused nature to the robot. No, no, listen, I love competence and focus and. Great. Like, he was not going to get distracted by the. The shallowness of small talk. There's a job to be done and he was doing it. So anyway, the fact that it was working is a beautiful thing. And I think media lab really prides itself on trying to always have a thing that's working. It could show off. Yes. We used to call it demo or die. Yeah. You could not like, show up with like PowerPoint or something. You actually had to have it working. You know what? My son, who is now 13, I don't know if this is still his lifelong goal or not, but when he was a little younger, his dream is to build an island that's just inhabited by robots. Like no humans. He just wants all these robots to be connecting and having fun and that's all. There you go. Does he have humanity? Does he have an idea of which robots he loves most? Is it. Is it roomba like robots? Is it humanoid robots, robot dogs? Or is it not clear yet? We used to have a jibo, which was one of the MIT media lab spin outs, and he used to love it. Thing with a giant head. Yes, it spins. Right, exactly. And rotate. And it's an eye. It has. Well, like, not glowing like. Right, right, exactly. It's like Hal 9000, but the friendly version. He loved that. And then he just loves. Yeah, he just. I think he loves all forms of robots, actually. So it embodied intelligence. Yes. I like, I personally like legged robots, especially anything that can wiggle its butt. No, that's not the definition of what I love, but that's just technically what I've been working on recently. Except I have a bunch of legged robots now in Austin, and I've been doing. I was. I've been trying to have them communicate affection with their body in different ways, just for art. So cool. For art, really, because I love the idea of walking around with the robots like as you would with the dog. I think it's inspiring to a lot of people, especially young people. Like, kids love kids love it. Parents, like adults are scared of robots. But kids don't have this kind of weird construction of the world that's full of evil. They love cool things. Yeah, I remember when Adam was in first grade, so he must have been like seven or so. I went into class with a whole bunch of robots and the emotion AI demo and da da. And I asked the kids, I was like, would you kids want to have a robot? You know, robot friend or robot companion? Everybody said yes. And they wanted it for all sorts of things, like to help them with their math homework and to, like, be a friend. So there's. It just struck me how there was no fear of robots. A lot of adults have that, like us versus them. Yeah, none of that. Of course you want to be very careful because you still have to look at the lessons of history and how robots can be used by the power centers of the world to abuse your rights and all that kind of stuff. But mostly it's good to enter anything new with an excitement and optimism. Speaking of Ras, what have you learned about science and life from Rosalind Picard? Oh, my God. I've learned so many things about life from Roz. I think the thing I learned the most is perseverance. When I first met Roz, we applied, and she invited me to be our postdoc. We applied for a grant to the National Science foundation to apply some of our research to autism. And we got back, we were rejected. Rejected? Yeah. And the reasoning was, the first time. You were rejected for fun. Yeah, yeah. And I basically, I just took the rejection to mean, okay, we're rejected, it's done. Like, end of story, right? And Roz was like, it's great news. They love the idea. They just don't think we can do it. So let's build it, show them, and then reapply. And it was that, oh, my God. That story totally stuck with me. And she's like that in every aspect of her life. She just does not take no for an answer. Reframe all negative feedback. As a challenge. As a challenge, yes. They like this. Yeah, it was a riot. What else about science in general, about how you see computers and also business and just everything about the world. She's a very powerful, brilliant woman like yourself. So is there some aspect of that, too? Yeah, I think Roz is actually also very faith driven. She has this deep belief and conviction. Yeah. In the good in the world and humanity. And I think that was meeting her and her family was definitely a defining moment for me, because that was when I was like, wow, you can be of a different background and religion and whatever, and you can still have the same core values. So that was, yeah, I'm grateful to her. Roz, if you're listening. Thank you. Yeah, she's great. She's been on this podcast before. I hope she'll be on the. I'm sure she'll be on again. You were the founder and CEO of Effectiva, which is a big company that was acquired by another big company, Smarteye, and you're now the deputy CEO of Smarteye. So you're a powerful leader, you're brilliant, you're brilliant scientist. A lot of people are inspired by you. What advice would you give, especially to young women, but people in general who dream of becoming a powerful leaders like yourself, in a world where perhaps in a world perhaps doesn't give them a clear, easy path to do so, whether we're talking about Egypt or elsewhere, you. Know, hearing you kind of describe me that way, kind of encapsulates, I think, what I think is the biggest challenge of all, which is believing in yourself. Right. I have had to, like, grapple with this, what I call now the Debbie Downer voice in my head that kind of basically is just chattering all the time. It's basically saying, oh, no, no, no, you can't do this. Like, you're not going to raise money. You can't start a company. Like, what business do you have? Like, starting a company or running a company or selling a company. Like, you name it, it's always like, and I think my biggest advice to not just women, but people who are taking a new path and they're not sure, is to not let yourself and let your thoughts be the biggest obstacle in your way. And I've had to really work on myself to not be my own biggest obstacle. So you got that negative voice. Yeah. Um, so is that. Am I the only one? I don't think I'm the only one. No, I have that negative voice. I'm not exactly sure if it's a bad thing or a good thing. I've been really torn about it because it's been a lifelong companion, so it's hard to know. It's kind of, um. It drives productivity and progress, but it can hold you back from taking big leaps. I think you. I. The best I can say is probably you have to somehow be able to control it. So turn it off when it's not useful and turn it on when it's useful. Like I have from almost like a third person perspective. Right. Somebody who's sitting there, like. Yeah. Like, because it is useful to be critical. Like, after I just gave a talk yesterday at MIT, and I was just, you know, there's so much love, and it was such an incredible experience. So many amazing people I got a chance to talk to. But, you know, afterwards, when I. When I went home and just took this long walk, it was mostly just negative thoughts about me. I don't like one basic stuff. Like, I don't deserve any of it. And so, second is like. Like, why did you. That was so dumb. You said this. That's so dumb. Like, you should have prepared that better. Why did you say this? But I think it's good to hear that voice out. All right. And, like, sit in that. And ultimately, I think you grow from that. Now, when you're making really big decisions about funding or starting a company or taking a leap to go to the UK or take a leap to go to America to work in media lab, though. Yeah. There's a. That's. You should be able to shut that off then, because you should have, like, this weird confidence, almost like faith that you said before, that everything's gonna work out. So take the leap of faith. Take the leap of faith. Despite all the negativity. I mean, there's some of that. You actually tweeted a really nice tweet thread. It says, quote, a year ago, a friend recommended I do daily affirmations. And I was skeptical, but I was going through major transitions in my life, so I gave it a shot and it set me on a journey of self acceptance and self love. So what was that? Like, maybe talk through this idea of affirmations and how that helped you? Yeah. Because really, like, I'm just like me. I'm a kind. I'd like to think of myself as a kind person in general, but I'm kind of mean to myself sometimes. Yeah. And so I've been doing journaling for almost ten years now. I use an app called Day one, and it's awesome. I just journal and I use it as an opportunity to almost have a conversation with the Debbie Downer voice in my. It's like a rebuttal, right? Like, Debbie Downer says, oh, my God. Like, you. You know, you won't be able to erase this round of funny. I'm like, okay, let's talk about it. I have a track record of doing x, y, and z. I think I can do this. And it's literally, like, so I wouldn't. I don't know that I can shut off the voice, but I can have a conversation with it, and it just. And I bring data to the table. Right. Nice. So, that was the journaling part, which I found very helpful, but the affirmation took it to a whole next level, and I just love it. I'm a year into doing this, and you literally wake up in the morning, and the first thing you do, I meditate first, and then I write my affirmations. And it's the energy I want to put out in the world that hopefully will come right back to me. So I always start with, my smile lights up the whole world. And I kid you not, people in the street will stop me and say, oh, my God, we love your smile. So, my affirmations will change depending on, you know, what's happening this day. Is it funny? I know. Don't judge. Don't judge. No, that's not what. Laughter's not judgment. It's just awesome. I mean, it's true. But you're saying affirmations somehow help kind of. What is it? They do work to remind you of the kind of person you are and the kind of person you want to be, which actually may be in reverse order, the kind of person you want to be, and that helps you become the. The kind of person you actually are. It's just. It's. It brings intentionality to, like, what you're doing. Right. And so, by the way, I was. Laughing because my affirmations, which I also do, are the opposite. Oh, you do? Oh, what do you. I don't have a. My smile lights up the affirmation. Maybe I should add that, because, like, I I have. I just. I have a. Oh, boy. I just. It's, uh. It's much more stoic, like, about focus, about this. Oh. But the joy, the emotion that you're just in, that little affirmation is beautiful. So maybe I should add that I'm. Like, focused on it. That's usually. That's a cool start. It's just, after all the, like, smiling, you're inspiring, playful, and joyful and all that. And then it's like, okay, I kick butt. Let's get shit done, right? Let's get you done. Affirmation. Okay, cool. So, like, what else is on there? What else is on there? Well, I'm a magnet for all sorts of things. So I'm an amazing people magnet. I attract awesome people into my universe. So that's an actual affirmation? Yes. That's great. Yeah. And that somehow manifests itself into working? I think so, yeah. Can you speak to why it feels good to do the affirmations? I honestly think it just grounds the day, and then it allows me to. Instead of just, like, being pulled back and forth, like, throughout the day, it just grounds me. I'm like, okay. Like, this thing happened. It's not exactly what I wanted it to be, but I'm patient. Or I'm, you know, I'm. I trust that the universe will do amazing things for me, which is one of my other consistent affirmations. Or I'm an amazing mom. Right. And so I can grapple with all the feelings of mom guilt that I have all the time. Or here's another one. I'm a love magnet. And I literally say, I will kind of picture the person that I'd love to end up with. And I write it all down and hasn't happened yet, but it. What are you. What are you picturing? This is Brad Pitt. Brad Pitt. Because that's what I picture. Okay. That's what you picture. Yeah. Holding hands, running together. No, no, more like fight club. That. The fight club. Brad Pitt, where he's, like, standing. All right. People will know anyway. I'm sorry. I'll get off on that. Do you have, like, when you're thinking about the. Being a love magnet in that way. Are you picturing specific people? Or is this almost, like, in the space of, like, energy? Right. As somebody who is smart and well accomplished and successful in their life, but they're generous and they're well traveled and they want to travel the world, things like that. Like, their head over heels into me is like, I know it sounds super silly, but it's literally what I write, and I believe it'll happen one day. Oh, you actually write, so you don't say it out loud? No, I write it. I write all my affirmations. I do the opposite. I say, interesting. Yeah. If I'm alone, I'll say it out loud. Yeah. Interesting. I should try that. I think it's. Which. What feels more powerful to you? To me, more powerful. Saying stuff feels more powerful. Yeah. Writing is. Writing feels like I'm losing. Losing the words. Like losing the power of the words. Maybe because I write slow. Do you hand write? No, I type. It's on this app. It's day one, basically. And I just. I can look. The best thing about it is I can look back and see like a year ago, what was I affirming? Right. So it's also changes over time. It hasn't like, changed a lot, but it. But the focus kind of changes over time. I got it. Yeah. I say the same exact thing over and over and over. Oh, you do? Okay. There's a comfort in the. In the sameness of it, actually. Let me jump around because. Let me ask you about. Because all this talk about Brad Pitt or maybe that's just going on inside my head. Let me ask about dating in general. You tweeted, are you based in Boston? In single question mark. And then you pointed to a startup singles night sponsored by Smile dating app. I mean, this is jumping around a little bit, but since you mentioned can AI help solve this dating love problem, what do you think this problem of connection that is part of the human condition, can AI help that you yourself are in the search affirming. Maybe that's what I should affirm. Like build an AI. Build an AI that finds love. I think. I think there must be a science behind that first moment you meet a person and you either have chemistry or you don't. Right. I guess that was the question I was asking. Would you put it brilliantly? Is that a science or an art? Ooh. I think there are like, there's actual chemicals that get exchanged when two people meet. Well, I don't know about that. I like how you're changing. Yeah. Changing your mind as we're describing it, but it feels that way. But it's what science shows us is sometimes we can explain with rigor the things that feel like magic. Right. So maybe you can remove all the magic. Maybe it's like. I honestly think, like I said, that Goodreads should be a dating app, which, like books. I wonder if you look at just like books or content you've consumed. I mean, that's essentially what YouTube does when it does recommendation. If you just look at your footprint of content consumed, if there's an overlap, but maybe interesting difference with an overlap that some. I'm sure this is a machine learning problem that's solvable. Like this person is very likely to be not only there to be chemistry in the short term, but a good lifelong partner to grow together, I bet you it's a good machine learning problem. We just need the data. Let's do it. Well, actually, I do think there's so much data about each of us that there ought to be a machine learning algorithm that can ingest all this data and basically say, I think the following ten people would be interesting connections for you. Right. And so smile dating app kind of took one particular angle, which is humor. It matches people based on their humor styles, which is one of the main ingredients of a successful relationship. Like, if you meet somebody and they can make you laugh, that's a good thing. And if you develop internal jokes, like inside jokes, and you're bantering, like, that's fun. Yeah. So I think, yeah, definitely. But, yeah, that's the number of and the rate of inside joke generation. You could probably measure that and then optimize it over the first few days. You can see. Right. And then we're just turning this into a machine learning problem. I love it. But for somebody like you, who's exceptionally successful and busy, is there, is there science to that aspect of dating? Is it tricky? Is there advice you can give? Oh, my God, I give the worst advice. Well, I can tell you, like, I have a spreadsheet. Spreadsheet. That's great. Is that a good or a bad thing? Do you regret the spreadsheet? Well, I don't know. What's the name of the spreadsheet? Is it love? It's the dating tracker. Dating tracker. It's very, like, love tracker. Yeah. And there's a rating system, I'm sure. Yeah. There's like weights and stuff. It's too close to home. Oh, is it? Do you also have. Well, I don't have a spreadsheet, but I would. Now that you say it, it seems like a good idea. Oh, no. Turning into data. I do wish that somebody else had a spreadsheet about me, if, you know, if it was like I said, like you said, convert, collect a lot of data about us in a way that's privacy preserving, that I own the data, I can control it, and then use that data to find, I mean, not just romantic love, but collaborators, friends, all that kind of stuff. It seems like the data is there. That's the problem social networks are trying to solve. But I think they're doing a really poor job. Even Facebook tried to get into a dating app business, and I think there's so many components to running a successful. The company that connects human beings. And part of that is, you know, having engineers that care about the human side right. As you know extremely well, it's not. It's not easy to find those. But you don't also don't want just people that care about the human. They also have to be good engineers. So it's like you have to find this beautiful mix. And for some reason, just empirically speaking, it's. People have not done a good job of that, of building companies like that. It must mean that it's a difficult problem to solve. Dating apps. It seems difficult. Okay, Cupid, Tinder, all those kind of stuff. They seem to find. Of course they work, but they seem to not work as well as I would imagine is possible. Like. Right. With data, wouldn't you be able to find better human connection? It's like arrange marriages on steroids, essentially. Right? Arranged by machine learning algorithm. Arranged by machine learning algorithm. But not a superficial one. I think a lot of the dating apps out there are just so superficial. They're just matching on high level criteria that aren't ingredients for successful partnership. But you know what's missing, though, too? I don't know how to fix that. The serendipity piece of it. Like, how do you engineer serendipity? Like, this random, like, chance encounter, and then you fall in love with the person. Like, I don't know how a dating app can do that. So there has to be a little bit of randomness. Maybe every 10th match is just a, you know. Yeah. Somebody that the algorithm wouldn't have necessarily recommended, but it's. It allows for a little bit of. Well, it can also, you know, it can also trick you into thinking and serendipity by, like, somehow showing you a tweet of a person that he thinks you'll match well with, but do it accidentally as part of another search. Right. And, like, you just notice it, like. And then you get. You go down a rabbit hole and you connect them and outside the app to, like. So you connect with this person outside the app somehow. So it's just. It creates that moment of meaning. Of course, you have to think of from an app perspective how you can turn that into a business. But I think ultimately, a business that helps people find love in any way. Like, that's what Apple was about. Create products that people love. That's beautiful. I mean, that's. You got to make money somehow. If you help people fall in love personally with the product, find self love or another human being, you're going to make money. You're going to figure out a way to make money. I just feel like the dating apps often will optimize for something else than love. It's the same with social networks. They optimize for engagement as opposed to a deep, meaningful connection that's ultimately grounded in personal growth, you as a human being growing and all that kind of stuff. Let me do a pivot to a dark topic, which you open the book with. Yeah. A story. Because I'd like to talk to you about just emotion and artificial intelligence. And I think this is a good story to start to think about emotional intelligence. You open the book with a story of a central Florida man, Jamel Dunn, who was drowning and drowned while five teenagers watched and laughed, saying things like, you're gonna die. And when Jamel disappeared below the surface of the water, one of them said, he just died, and the others laughed. What does this incident teach you about human nature and the response to it, perhaps? Yeah, I mean, I think this is a really, really, really sad story, and it highlights what I believe is a real problem in our world today. It's an empathy crisis. Yeah. We are living through an empathy crisis. Empathy crisis. Yeah. And, I mean, we've talked about this throughout our conversation. We dehumanize each other, and unfortunately, yes, technology is bringing us together, but in a way, it's just dehumanizing. It's creating this, like. Yeah. Dehumanizing of the other. And I think that's a huge problem. The good news is, I think the solution could be technology based. Like, I think if we rethink the way we design and deploy our technologies, we can solve parts of this problem. But I worry about it. I mean, even with my son, a lot of his interactions are computer mediated, and I just question what that's doing to his empathy skills and, you know, his ability to really connect with people. So you think. You think it's not possible to form empathy through a digital medium? I think it is, but we have to be thoughtful about, because the way we engage face to face, which is what we're doing right now. Right. There's the nonverbal signals, which are a majority of how we communicate. It's like 90% of how we communicate is your facial expressions. I'm saying something, and you're nodding your head now, and that creates a feedback loop. And if you break that and now. I have anxiety about it. Right. Poor Lex. Oh, boy. I am not scrutinizing your facial expressions during this interview. Right. I am. Look normal. Look human. Yeah. Nod head. Yeah. Nod head. In agreement. If Rana says yes, then nod head. Else, don't do it too much, because it might be at the wrong time, and then it will send the wrong signal. Oh, God. Make eye contact sometimes. Cause humans appreciate that. All right. Anyway. Okay. Yeah, but something about the. Especially when you say mean things in person, you get to see the pain of the other person. Exactly. But if you're tweeting it at a person and you have no idea how it's gonna land, you're more likely to do that on social media than you are in face to face conversations. So. What do you think is more important, Eq or IQ? Eq. Being emotional intelligence. In terms of in what makes us human. I think emotional intelligence is what makes us human. It's how we connect with one another. It's how we build trust. It's how we make decisions. Right. Like, your emotions drive kind of what you had for breakfast, but also where you decide to live and what you want to do for the rest of your life. I think emotions are underrated. So emotional intelligence isn't just about the effective expression of your own emotions. It's about sensitivity and empathy to other people's emotions and that sort of being able to effectively engage in a dance of emotions with other people. Yeah, I like that explanation. I like that kind of. Yeah. Thinking about it as a dance, because it is really about that. It's about sensing what state the other person's in and using that information to decide on how you're going to react. And I think it can be very powerful. Like, people who are the best, most persuasive, most persuasive leaders in the world tap into, you know, they have. If you have higher eq, you're more likely to be able to motivate people to change their behaviors. So. So it can be very powerful. On a more kind of technical, maybe philosophical level, you've written that emotion is universal. It seems that sort of, like Chomsky says, language is universal. There's a bunch of other stuff, like cognition. Consciousness seems a lot of us have these aspects. So the human mind generates all this. And so what do you think is the. They all seem to be like echoes of the same thing. What do you think emotion is exactly like? How deep does it run? Is it a surface level thing that we display to each other? Is it just another form of language or something deep within? I think it's really deep. It's how we started with memory. I think emotions play a really important. Yeah, emotions play a very important role in how we encode memories. Right. Our memories are often encoded, almost indexed by emotions. Yeah. It's at the core of how our decision making engine is also heavily influenced by our emotions. So emotions is part of cognition? Totally. It's intermixed into the whole thing? Yes, absolutely. And, in fact, when you take it away, people are unable to make decisions. They're really paralyzed. Like, they can't go about their daily or their personal or professional lives. So. It does seem like there's probably some interesting interweaving of emotion and consciousness. I wonder if it's possible to have, like, if they're next door neighbors somehow or if they're actually flatmates. I don't. It feels like the. The hard problem of consciousness, where it's some. It feels like something to experience the thing. Like red feels like red. And it's, you know, when you eat a mango, it's sweet. The taste, the sweetness that. It feels like something to experience that sweetness, that whatever generates emotions. But then, like, see, I feel like emotion is part of communication. It's very much about communication. And then that means it's also deeply connected to language. But then probably human intelligence is deeply connected to the collective intelligence between humans. It's not just a standalone thing. So the whole thing is really connected. So emotion is connected to language. Language is connected to intelligence, and then intelligence connected consciousness, and consciousness is connected to emotion. The whole thing is that it's a beautiful mess. So. Can I comment on the emotions being a communication mechanism? Because I think there are two facets of our emotional experiences. One is communication. Right? Like, we use emotions, for example, facial expressions or other nonverbal cues to connect with other human beings and with other beings in the world, right? But even if it's not a communication context, we still experience emotions, and we still process emotions, and we still leverage emotions to make decisions and to learn and, you know, to experience life. So it isn't always just about communication. And we learned that very early on in our kind of our work at affectiva, one of the very first applications we brought to market Washington, understanding how people respond to content, right? So if they're watching this video of ours, like, are they interested? Are they inspired? Are they bored to death? And so we watched their facial expressions, and we had, we weren't sure if people would express any emotions if they were sitting alone. Like, if you're in your bed at night watching a Netflix tv series, would we still see any emotions on your face? And we were surprised that, yes, people still emote, even if they're alone, even if you're in your car driving around, you're singing along the song, and you're joyful. We'll see these expressions. So it's not just about communicating with another person. It sometimes really is just about experiencing the world. And first of all, I wonder if some of that is because we develop our intelligence and our emotional intelligence by communicating with other humans. And so when other humans disappear from the picture, we're still kind of a virtual human. The code still runs, basically. Yeah, the code still runs. But you also kind of. Yeah, you're still. There's, like, virtual humans. You don't have to think of it that way, but there's a kind of. When you, like, chuckle, like. Yeah, like you're. You're kind of chuckling to a virtual human. I mean, it's possible that the the code is the. Has to have another human there. Because if you just grew up alone, I wonder if emotion will still be there in this visual form. So I. Yeah, I I wonder. But anyway, the, uh, what can you tell from the human face about what's going on inside? So that's the problem that effectiva first tackled, which is using computer vision, using machine learning, to try to detect stuff about the human face, as many things as possible and convert them into a prediction of categories of emotion. Anger, happiness, all that kind of stuff. How hard is that problem? Extremely hard. It's very, very hard because there is no one to one mapping between a facial expression and your internal state. There just isn't. There's this oversimplification of the problem where it's something like, if you are smiling, then you are happy. If you do a brow furrow, then you're angry. If you do an eyebrow raise, then you're surprised. Just think about it for a moment. You could be smiling for a whole host of reasons. You could also be happy and not be smiling. Right. You could furrow your eyebrows because you're angry or you're confused about something or you're constipated. So I think this over simplistic approach to inferring emotion from a facial expression is really dangerous. The solution is to incorporate as many contextual signals as you can, right? So if, for example, I'm driving a car and you can see me, like, nodding my head and my eyes are closed and the blinking rate is changing, I'm probably falling asleep at the wheel, right? It doesn't, because you know the context, you understand what the person's doing. So I think or add additional channels like voice or gestures or even physiological sensors. But I think it's very dangerous to just take this over. Simplistic approach of, yeah, smile equals happy. If you're able to, in a high resolution way, specify the context, there's certain things that are going to be somewhat reliable signals of something like drowsiness or happiness or stuff like that. I mean, when people are watching Netflix, that problem, that's a really compelling idea that you can kind of, at least in aggregate, highlight. Like, which part was boring, which part was exciting? How hard was that problem that was. On the scale of, like, difficulty? I think that's one of the easier problems to solve because it's a relatively constrained environment you have somebody sitting in front of. Initially, we started with, like, a device in front of you, like a laptop. And then we graduated to doing this on a mobile phone, which is a lot harder just because of, you know, from a computer vision perspective, the profile view of the face can be a lot more challenging. We had to figure out lighting conditions because usually people are watching content literally in their bedrooms at night. Lights are dimmed. Yeah. I mean, if you're standing, it's probably going to be the looking up the nostril view. Yeah. Nobody looks good at. I've seen data sets from that perspective. It's like, this is not a good look for anyone. Or if you're laying in bed at night, what is it? Side view or something, and half your face is on a pillow. Actually, I would love to have data about, like, how people watch stuff in bed at night. Like, do they prop there? Is it a pillow? Like, I'm sure there's a lot of. Interesting dynamics from a health and well being perspective. Right? Like, it's like, oh, you're not machine. Learning perspective, but yes, but also. Yeah, yeah. Once you have that data, you can start making all kinds of inference about health and stuff like that. Interesting. Yeah. There was an interesting thing when I was at Google that we were. It's called active authentication, where you want to be able to unlock your phone without using a password. So it would face, but also other stuff, like the way you take a phone out of the pocket. So that kind of data, to use the multimodal with machine learning, to be able to identify that it's you, or likely to be you, likely not to be you, that allows you to not always have to enter the password. That was the idea. Funny thing about that is, I just want to tell a small anecdote, is because it was all male engineers, except. So my boss, our boss, who's still one of my favorite humans, was a woman, Regina Dugan. Oh, my God, I love her. She's best. She's awesome. She's the best. So. But anyway, and there was one female, brilliant female engineer on the team, and she was the one that actually highlights the fact that women often don't have pockets, right? It was like, whoa, that was not even a category in the code of, like, wait a minute, you can take the phone out of some other place than your pocket. So, anyway, that's a funny thing. When you're considering people laying in bed, watching a phone, you have to consider if there. You have to, you know, diversity in all its forms, depending on the problem. Depending on the context. Yeah, actually, this is, like, a very important. I think this is, you know, you probably get this all the time. Like, people are worried that AI is going to take over humanity and, like, get rid of all the humans in the world. I'm like, actually, that's not my biggest concern. My biggest concern is that we are building bias into these systems, and then they're, like, deployed at large and at scale. And before you know it, you're kind of accentuating the bias that exists in society. And. Yeah, I'm not, you know, I know people. It's very important to worry about that. But the worry is an emergent phenomena to me, which is a very good one, because I think these systems are actually, by encoding the data that exists, they're revealing the bias in society, therefore teaching us what the bias is. Therefore, we can now improve that bias within the system. So they're almost like putting a mirror to ourselves. Totally. So I'm not. We have to be open to looking at the mirror, though. You have to be right. Open to scrutinizing the data. If you just take it as ground. Or you don't even have to look at the. I mean, yes, the data is how you fix it, but then you just look at the behavior of the system, and so you realize, holy crap, this thing is kind of racist, right? Like, why is that? And then you look at the days like, okay. And then you start to realize that I think that's a much more effective way to be introspective as a society than through sort of political discourse. Like AI, kind of, because people are easy. People are for some reason, more productive and rigorous in criticizing AI than they're criticizing each other. So I think this is just a nice method for studying society and see which way progress lies. Anyway, what we're talking about, you're watching the problem of watching Netflix in bed or elsewhere and seeing which parts are exciting, which parts are boring. You're saying that's relatively constrained because, you know, you have a captive audience and you kind of know the context. And one thing you said that was really key is the aggregate. You're doing this in aggregate, right? Like we're looking at aggregated response of people. And so when you see a peak, say a smile peek, they're probably smiling or laughing at something that's in the content. So that was one of the first problems we were able to solve. And when we see the smile peak, it doesn't mean that these people are internally happy. They're just laughing at content. So it's important to call it for. What it is, but still really, really useful data. I wonder how that compares to what YouTube and other places will use is obviously they don't have in, for the most case, they don't have that kind of data, but they have the data of when people tune out, drop off. And I think that's in aggregate for YouTube, at least a pretty powerful signal. I worry about what that leads to because looking at like, youtubers that are kind of really care about views and, you know, tried to maximize the number of views, I think they, when they say that the video should be constantly interesting, which seems like a good goal, I feel like that leads to this manic pace of a video. Like the idea that I would speak at the current speed that I'm speaking, I don't know. And that every moment has to be engaging, right? Engaging, yeah. I think there's value to silence. There's value to the boring bits. I mean, all, some of the greatest movies ever, some of the greatest stories ever told me. They're boring bits, seemingly boring bits. I don't know. I wonder about that. Of course, it's not that the human face can capture that either. It's just giving an extra signal. You have to really, I don't know, you have to really collect deeper, long term data about what was meaningful to people. When they think 30 days from now, what they still remember, what moved them, what changed them, what helped them grow, that kind of stuff. You know, what would be a really interesting, I don't know if there are any researchers out there who are doing this type of work. Wouldn't it be so cool to tie your emotional expressions while you're, say, listening to a podcast interview and then go, you know, and then 30 days later interview people and say, hey, what do you remember? You've watched this 30 days ago, like, what stuck with you? And then see if there's any. There ought to be, maybe there ought to be some correlation between these emotional experiences and. Yeah, what you, what stays with you, huh? So the. The one guy listening now on the beach in Brazil, please record a video of yourself listening to this and send it to me. And then I'll interview 30 days from now. It'll be statistically significant. But, you know. Yeah, yeah, I think that's really fascinating. I think that's. That kind of holds the key to a future where entertainment or content is both entertaining and, I don't know, makes you better. Empowering in some way. So figuring out, like, showing people stuff that entertains them, but also they're happy they watched 30 days from now because they've become a better person because of it. Well, you know. Okay, not to riff on this topic for too long, but I have two children, right. And I see my role as a parent as, like, a chief opportunity officer. Like, I am responsible for exposing them to all sorts of things in the world, and. But often I have no idea of knowing, like, what stuck. Like, what was, you know, is this actually going to be transformative, you know, for them ten years down the line? And I wish there was a way to quantify these experiences. Like, are they. I can tell in. In the moment if they're engaging, right? I can tell. But it's really hard to know if they're going to remember them ten years from now or if it's going to. Yeah, that one is weird, because it seems like kids remember the weirdest things. I've seen parents do incredible stuff for their kids, and they don't remember any of that. They remember some tiny, small, sweet thing a parent did. Right? Like, some. I took you to, like, this amazing. Country, whatever, and then they'll be like some, like, stuffed toy you got or some. Or the new PlayStation or something, or some. Some silly little thing. So I think they just like that they were designed that way. They want to mess with your head. But definitely kids are very impacted by, it seems like sort of negative events. So minimizing the number of negative events is important, but not too much. Right. You can't just like, you know, there's still discipline and challenge and all those kinds of things. So. So, yeah, I mean, I'm definitely. When I have kids, I'm gonna drive them out into the woods. Okay. And then they have to survive and figure out how to make their way back home, like, 20 miles out. Okay. Yeah. And after that, we can go for ice cream. Anyway, I'm working on this whole parenting thing. I haven't figured it out. Okay. What were we talking about? Yes. Affectiva. The problem of emotion, of emotion detection. So there's some people, maybe we can just speak to that a little more where there's folks like Lisa Feldman, Barrett that challenged this idea that emotion could be fully detected or even well detected from the human face, that there's so much more to emotion. What do you think about ideas like hers? Criticism like hers? Yeah, I actually agree with a lot of Lisa's criticism. So even my PhD worked 20 plus years ago now. Time flies when you're having fun. I know, right? That was back when I did like dynamic bayesian networks, and that's before deep learning. That was before deep learning. Yeah, yeah, I know. Back my day. Now you can just like use. Yeah, it's all the same architecture. You can apply it to anything. Yeah, right. But, yeah, but even then, I kind of, I did not subscribe to this, like, theory of basic emotions where it's just this simplistic mapping one to one mapping between facial expressions and emotions. I actually think also we're not in the business of trying to identify your true emotional internal state. We just want to quantify in an objective way what's showing on your face because that's an important signal. It doesn't mean it's a true reflection of your internal emotional state. So I think a lot of the, you know, I think she's just trying to kind of highlight that this is not a simple problem and overly simplistic solutions are going to hurt the industry. And I subscribe to that. And I think multimodal is the way to go. Like, whether it's additional context information or different modalities and channels of information, I think that's what we, that's where we ought to go. And I think, I mean, that's a big part of what she's advocating for as well. But there is signal in the human face that's, there's definitely signal in the face that's a projection of emotion, at least in part, is the inner state is captured in some meaningful way on the human face. I think it can sometimes be a reflection or an expression of your internal state, but sometimes it's a social signal. So you cannot look at the face as purely a signal of emotion. It can be a signal of cognition and it can be a signal of a social expression. And I think to disambiguate that, we have to be careful about it and we have to add initial information. Humans are fascinating, aren't they? With the whole face thing. It can mean so many things, from humor to sarcasm to everything, the whole thing. Some things we can help, some things we can't help at all. In all the years of leading effect diva, an emotion recognition company, like we talked about, what have you learned about emotion, about humans and about AI? Ooh, big, big, sweeping questions. Yeah, that's a big, sweeping question. Well, I think the thing I learned the most is that even though we are in the business of building AI, basically, it always goes back to the humans. It's always about the humans. And so, for example, the thing I'm most proud of in building affectiva. And, yeah, the thing I'm most proud of on this journey, I love the technology, and I'm so proud of the solutions we've built and we've brought to market. But I'm actually most proud of the people we've built and cultivated at the company and the culture we've created. Some of the people who've joined affectiva, this was their first job. And while at Affectiva, they became american citizens, and they bought their first house, and they found their partner, and they had their first kid. Right. Like, key moments in life that we got to be part of. And that's the thing I'm most proud of. So that's a great thing at a company that works on emotional, right? Me, like, celebrating humanity in general. Broadly speaking, yes. And that's a great thing to have at a company that works on AI, because that's not often the thing that's celebrated in AI company. So often just raw, great engineering. Just celebrating the humanity. That's great. And especially from a leadership position. Well, what do you think about the movie her? Let me ask you that before I talk. Before I talk to you about. Because it's not affectiva is and was not just about emotion. So I'd love to talk to you about smart eye. But before that, let me just jump into the movie. Her, do you think, will have a deep, meaningful connection with. Increasingly deep and meaningful connections with computers? Is that a compelling thing to you? Something, I think that's already happening. The thing I love the most. I love the movie her, by the way. But the thing I love the most about this movie is it demonstrates how technology can be a conduit for positive behavior change. So I forgot the guy's name in the movie. Whatever. Theodore. Theodore. So Theodore was, like, really depressed, right? And he just didn't want to get out of bed, and he was just, like, done with life. Right. And Samantha. Right, Samantha, yeah. She just knew him so well. She was emotionally intelligent, and so she could persuade him and motivate him to change his behavior. And she got him out, and they went to the beach together. And I think that represents the promise of emotion. Aihdeme. If done well, this technology can help us live happier lives, more productive lives, healthier lives, more connected lives. So that's the part that I love about the movie. Obviously, it's Hollywood, so it takes a twist and whatever. But the key notion that technology with emotion, AI, can persuade you to be a better version of who you are, I think that's awesome. Well, what about the twists you don't get? You don't think it's good for, spoiler alert. That Samantha starts feeling a bit of a distance and basically leaves Theodore. You don't think that's a good feature? That's a. That you think that's a bug or feature? Well, I think what went wrong is Theodore became really attached to Samantha. Like, I think he kind of fell in love with. Do you think that's wrong? I mean, I think that. I think she was putting out the signal. This is an intimate relationship. Right. There was a deep intimacy to it. Right. But what does, what does, what does that mean? What does that mean? AI system. Right. What does that mean? Right? I'm just friends. Yeah, we're just friends. Well, I think when he realized, which is such a human thing of jealousy, when you realize that Samantha was talking to, like, thousands of people. She's parallel dating. Yeah. That did not go well. Right. You know, that doesn't. And from a computer perspective, like, that doesn't take a anything away from what we have. It's like you getting jealous of Windows 98 for being used by millions of people. But it's like, it's like not liking that Alexa likes a bunch of, you know, other families. But I think Alexa currently is just a servant. It tells you about the weather. It's not. It doesn't do the intimate, deep connection. And I think there is something really powerful about that, the intimacy of a connection with an AI system that would have to respect and play the human game of jealousy, of love, of heartbreak and all that kind of stuff, which Samantha does seem to be pretty good at. I think she. This AI systems knows what it's doing. Well, actually, let me ask you this. I don't think she was talking to anyone else. You don't think so you think she was just done with Theodore? Yeah. Oh, she knew that this. Yeah. And then she wanted to really put on. She didn't have the guts to just break it off cleanly. Okay. She just wanted to put in the pain. No, I don't know. Well, she could have ghosted him. She could have. I'm sorry. There's our engineers. Oh, God. But I think those are really. I honestly think some of that, some of it is Hollywood, but some of that is features from an engineering perspective, not a bug. I think AI systems that can leave us now, this is for more social robotics than it is for anything that's useful. I hated if Wikipedia said, I need a break right now. No, I need you. But if it's just purely for companionship, then I think the ability to leave is really powerful. I don't know, I never thought of that. So fascinating, because I've always taken the human perspective, right? Like, for example, we had a jibo at home, right? And my son loved it, and then the company ran out of money, and so they had to basically shut down. Like, Jibo basically died, right? And it was so interesting to me because we have a lot of gadgets at home and a lot of them break, and my son never cares about it, right? Like, if our Alexis stopped working tomorrow, I don't think he'd really care. But when Jibo stopped working, it was traumatic. Like, he got really upset. And as a parent, that, like, made me think about this deeply, right? Did I? Was I comfortable with that? I liked the connection they had because I think it was a positive relationship, but I was surprised that it affected him emotionally so much. And I think there's a broader question here, right? As we build socially and emotionally intelligent machines, what does that mean about our relationship with them? And then more broadly, our relationship with one another, right? Because this machine is going to be programmed to be amazing at empathy by definition, right? It's going to always be there for you. It's not going to get bored. In fact, there's a chat bot in China, and it's like the number two or three most popular app, and it basically is just a confidant. And you can tell it anything you want. And people use it for all sorts of things. They confide in domestic violence or suicidal attempts, or if they have challenges at work. I don't know what that. I don't know if I'm. I don't know how I feel about that. I think about that a lot. Yeah. I think, first of all, obviously, the future in my perspective. Second of all, I think there's a lot of trajectories that that becomes an exciting future. But I think everyone should feel very uncomfortable about how much they know about the company, about where the data is going, how the data is being collected, because I think, and this is one of the lessons of social media that I think we should demand full control and transparency of the data on those things, plus one. Totally agree. Yeah. So, like, I. I think it's really empowering. As long as you can walk away, as long as you can, like, delete the data or know how the data, it's opt in or. Or at least a clarity of, like, what is being used for the company. And I think as CEO or, like, leaders are also important about that. Like, you need to be able to trust the basic humanity of the leader. Exactly. And also that that leader is not going to be a puppet of a larger machine, but they actually have a significant role in defining the culture and the way the company operates. So, anyway, but we should definitely scrutinize companies in that aspect. I'm personally excited about that future, but also, even if you're not, it's coming, so let's figure out how to do it in the least painful and the most positive. You're the deputy CEO of Smarteye. Can you describe the mission of the company? What is Smarteye? Yeah, so, Smarteye is a swedish company. They've been in business for the last 20 years, and their main focus, like, the industry they're most focused on is the automotive industry. So bringing driver monitoring systems to basically save lives. Right. So I first met the CEO, Martin Krantz. Gosh, it was right when Covid hit. It was actually the last CES. Right before COVID So CS 2020. Right? 2020. Yeah, January. Yeah, January. Exactly. So we were there. Met him in person. He's basically. We were competing with each other. I think the difference was they'd been doing driver monitoring and had a lot of credibility in the automotive space. We didn't come from the automotive space, but we were using new technology, like deep learning and building this emotion recognition. And you wanted to enter the automotive space? You wanted to operate in automotive space. Exactly. It was one of the areas we were. We had just raised a round of funding to focus on bringing our technology to the automotive industry. So we met, and I honestly, it was the first time. It was the only time I met with a CEO who had the same vision as I did. Like, he basically said, yeah, our vision is to bridge the gap between humans and machines. I was like, oh, my God, this is, like, exactly. Almost to the word how we describe it, too. And we started talking, and first it was about, okay, can we align strategically here? How can we work together? Because we're competing, but we're also complementary. And then I think after four months of speaking almost every day on Facetime. He was like, is your company interested in an acquisition? And it was the first. I usually say no when people approach us. It was the first time that I was like, huh, yeah, I might be interested. Let's talk. So you just hit it off? Yeah. So they're respected, very respective in the automotive sector of delivering products, and increasingly sort of better and better and better for. I mean, maybe you could speak to that. But it's the driver sensing for basically having a device that's looking at the driver and it's able to tell you where the driver is looking. Correct. It's able to also drowsiness stuff. Correct. It does stuff from the face and the eye. Exactly. Like it's monitoring driver distraction and drowsiness. But they bought us so that we could expand beyond just the driver. So the driver monitoring systems usually sit. The camera sits in the steering wheel or around the steering wheel column, and it looks directly at the driver. But now we've migrated the camera position, in partnership with car companies to the rear view mirror position, so it has a full view of the entire cabin of the car. And you can detect how many people are in the car. What are they doing? So we do activity detection, like eating or drinking or I, in some regions of the world, smoking. We can detect if a baby's in the car seat. Right. And if, unfortunately, in some cases, they're forgotten, the parents just leave the car and forget the kid in the car. That's an easy computer vision problem to solve. Right. You can detect there's a car seat, there's a baby, you can text the parent and hopefully, again, save lives. So that was the impetus for the acquisition. It's been a year, so, I mean. There'S a lot of. A lot of questions. It's a really exciting space, especially to me. I just find this a fascinating problem. It could enrich the experience in the car in so many ways, especially because we spend still, despite Covid. I mean, Covid changed things, so it's in interesting ways. But I think the world is bouncing back, and we spend so much time in the car, and the car is such a weird little world we have for ourselves. People do all kinds of different stuff, like listen to podcasts, they think about stuff, they get angry, they do phone calls. It's like a little world of its own with the kind of privacy that for many people, they don't get anywhere else. And it's a little box that's like a psychology experiment because it feels like the angriest many humans in this world get is inside the car. It's so interesting. So it's such an opportunity to explore how we can enrich, how companies can enrich that experience. And also, as the cars become more and more automated, there's more and more opportunity. The variety of activities that you can do in the car increases. So it's super interesting. So, I mean, on a practical sense. So Smarteye has been selected, at least, I read, by 14 of the world's leading car manufacturers for 94 car models. So it's in a lot of cars. How hard is it to work with car companies? So they're all different. They all have different needs. The ones I've gotten a chance to interact with are very focused on cost. So, and anyone who's focused on costs, it's like, all right, do you hate fun? Let's just have some fun. Let's figure out the most fun thing we can do and then worry about cost later. But I think because the way the car industry works, I mean, it's a very thin margin that you get to operate under. So you have to really, really make sure that everything you add to the car makes sense financially. So anyways, there is this new industry, especially at this scale of Smarteye. Does it hold any lessons for you? Yeah, I think it is a very tough market to penetrate, but once you're in, it's awesome, because once you're in, you're designed into these car models for somewhere between five to seven years, which is awesome. And once they're on the road, you just get paid a royalty fee per vehicle. So it's a high barrier to entry. But once you're in, it's amazing. I think the thing that I struggle the most with in this industry is the time to market. So often we're asked to lock or do a code freeze two years before the car is going to be on the road. I'm like, guys, do you understand the pace with which technology moves? So I think car companies are really trying to make the Tesla, the Tesla transition to become more of a software driven architecture. And that's hard for many. It's just a cultural change. I mean, I'm sure you've experienced that, right? Oh, definitely. I think one of the biggest inventions or imperatives created by Tesla is like, to me personally, okay, people are going to complain about this, but I know electric vehicle, I know autopilot AI stuff. To me, the software over there, software updates, is like the biggest revolution in cars. And it is extremely difficult to switch to that because it is a culture shift at first, especially if you're not comfortable with it, it seems dangerous. Like there's an approach to cars is so safety focused for so many decades that, like, what do you mean? We dynamically change code. The whole point is you have a thing that you test, like, right? You spend a year testing and, like, it's not reliable because do you know how much it costs if we have to recall this cars, right. There's a, there's a. And there's an understandable obsession with safety. But the downside of an obsession with safety is the same as with being obsessed with safety as a parenthood is like if you do that too much, you limit the potential development and the flourishing of, in that particular aspect, human being, but in this particular aspect, the software, the artificial neural network of it. But it's tough to do. It's really tough to do culturally and technically. Like the deployment, the mass deployment of software is really, really difficult. But I hope that's where the industry is doing. One of the reasons I really want Tesla to succeed is exactly about that point. Not autopilot, not the electrical vehicle, but the software ization of basically everything but cars. Especially because to me that's actually good to increase. Two things. Increase safety because you can update much faster, but also increase the effectiveness of folks like you who dream about enriching the human experience with AI, because you can just like there's a feature like you want like a new emoji or whatever, like the way TikTok releases filters, you can just release that or in car stuff. So. But yeah, that, that's definitely one of. The use cases we're looking into is once you know the sentiment of the passengers in the vehicle, you can optimize the temperature in the car, you can change the lighting. Right. So if the backseat passengers are falling asleep, you can dim the lights, you can lower the music. Right. You can do all sorts of things. Yeah. I mean, of course you could do that kind of stuff with a two year delay, but it's tougher, right? Yeah. Do you think. Do you think Tesla or Waymo or some of these companies that are doing semi or fully autonomous driving should be doing driver sensing? Yes. Are you thinking about that kind of stuff? So not just how we can enhance in cab experience for cars that are manually driven, but the ones that are increasingly more autonomously driven. Yes. So if we fast forward to the universe where it's fully autonomous, I think interior sensing becomes extremely important because the role of the driver isn't just to drive. If you think about it. The driver almost manages the dynamics within a vehicle. And so who's going to play that role when it's an autonomous car? We want a solution that is able to say, oh, my God, Lex is bored to death because the car is moving way too slow. Let's engage Lex, or Rana's freaking out because she doesn't trust this vehicle yet. So let's tell Rana, like, a little bit more information about the route or. Right, so I think, or somebody's having a heart attack in the car. Like, you need interior sensing in fully autonomous vehicles. But with semi autonomous vehicles, I think it's really key to have driver monitoring, because semi autonomous means that sometimes the car is in charge, sometimes the driver is in charge or the copilot. Right. And you need this. You need both systems to be on the same page. You need to know, the car needs to know if the driver's asleep before it transitions control over to the driver. And sometimes if the driver's too tired, the car can say, I'm going to be a better driver than you are right now. I'm taking control over. So this dynamic, this dance is so key, and you can't do that without driver sensing. Yeah, there's a disagreement for the longest time I've had with Elon, that this is obvious, that this should be in the Tesla from day one. And it's obvious that driver sensing is not a hindrance. It's not obvious I should be careful because having studied this problem, nothing is really obvious, but it seems very likely a driver sensing is not a hindrance to an experience. It's only enriching to the experience and likely increases the safety. That said, it is very surprising to me, just having studied semi autonomous driving, how well humans are able to manage that dance, because it was the intuition before you were doing that kind of thing, that humans will become just incredibly distracted. They would just, like, let the thing do its thing, but they're able to, because it is life and death and they're able to manage that somehow. That said, there's no reason not to have driver sensing on top of that. I feel like that's going to allow you to do that dance that you're currently doing without driver sensing, except touching the steering wheel to do that even better. I mean, the possibilities are endless and the machine learning possibilities are endless. It's such a beautiful. It's also constrained environment, so you could do much more effectively than you can with the external environment. Right. Weird edge cases and complexities inside. There's so much. It's so fascinating. Such a fascinating world. I do hope that companies like Tesla and others, even Waymo, which I don't even know if Waymo is doing anything sophisticated inside the cab. I don't think so. What is it? I honestly think. I honestly think it goes back to the robotics thing we were talking about, which is like, great engineers that are building these AI systems just are afraid. Of the human being and not thinking about the human experience. They're thinking about the features and, yeah, the perceptual abilities of that thing. They think the best way I can serve the human is by doing the best perception and control I can by looking at the external environment, keeping the human safe, right. But, like, there's a huge, I'm here, right? Like, you know, I need to be noticed and interacted with and understood and all those kinds of things, even just on a personal level, for entertainment, honestly. For entertainment. Yeah. You know, one of the coolest work we did in collaboration with MIT around this was we looked at longitudinal data, right, of drive, because, you know, MIT had access to, like, tons of data and, like, just seeing the patterns of people, like, driving in the morning, off to work, versus, like, commuting back from work, or weekend driving versus weekday driving. And wouldn't it be so cool if your car knew that and then was able to optimize either the route or the experience or even make recommendations? I think it's very powerful. Yeah. Like, why are you taking this route? You're always unhappy when you take this route, and you're always happy when you take this alternative route. Take that route instead. Exactly. That. I mean, that if to have that even that little step of relationship with a car, I think is incredible. Of course you have to get the privacy right, you have to get all that kind of stuff right. But I wish, I honestly, you know, people are paranoid about this, but I would like a smart refrigerator. We have a such a deep connection with food as a human civilization. I would like to have a refrigerator that would understand me, that I also have a complex relationship with food because pig out too easily and all that kind of stuff. Maybe I want the refrigerator to be like, are you sure about this? Because maybe you're just feeling down or tired. Your vision of the smart refrigerator is way kinder than mine. Is it just mean yelling at you? No, it was just because I don't drink alcohol, I don't smoke, but I eat a ton of chocolate. It's my vice. And sometimes I scream too. And I'm like, okay, my smart refrigerator will just lock down. They'll just say, dude, you've had way too many today. Yeah, no, but here's the thing. Do you regret, haven't, like, let's say not the next day, but 30 days later? Would you, what would you, what would you like to the refrigerator to have done then? Well, I think actually, like, the more positive relationship would be one where there's a conversation, right? As opposed to, like, four. That's probably, like, the more sustainable relationship. It's like late at night, just. No, listen, listen. I know I told you an hour ago, this is not a good idea, but just listen. Things have changed. I can just imagine a bunch of stuff being made up just to convince. But I mean, I just think that there's opportunities there. I mean, maybe not locking down, but for our systems that are such a deep part of our lives, like, use, uh, we use a lot of us, uh, a lot of people that commute use their car every single day. A lot of us use a refrigerator every single day, the microwave every single day. Like, and we just, like, I feel like certain things could be made more efficient, more enriching, and AI is there to help, like some, just basic recognition of you as a human being about your patterns, about what makes you happy and not happy and all that kind of stuff. And the car, obviously. Maybe we'll say, wait, instead of this, like, Ben and Jerry's ice cream, how about this? Hummus and carrots or something. I don't know, maybe it would make. It like, yeah, like a reminder, just. In time recommendation, right? But not like a generic one, but a reminder that last time you chose the carrots, you smiled 17 times more. You were happier the next day, right? Yeah, you were. You're happier the next day. And, and, but, yeah, I don't, but then again, if you're the kind of person that, that gets better from negative, negative comments, you could say, like, hey, remember, like, that wedding you're going to, you want to fit into that dress? Remember about that? Let's think about that before you're eating this. No, probably that would work for me. Like a refrigerator that is just ruthless. It's shaming me. But, like, I would of course, welcome it. Like that would work for me. Just that. Well, it would know. I think it would. If it's really, like, smart, it would optimize its nudging based on what works for you. Right, exactly. That's the whole point. Personalization in every way. Depersonalization. You were part of a webinar titled advancing road safety, the state of alcohol intoxication research. So for people who don't know, every year, 1.3 million people around the world die in road crashes. And more than 20% of these fatalities are estimated to be alcohol related. A lot of them are also distraction related. So can AI help with the alcohol thing? I think the answer is yes. There are signals, and we know that as humans, like, we can tell when a person is at different phases of being drunk. Right? Yeah. And I think you can use technology to do the same. And again, I think the ultimate solution is going to be a combination of different sensors. How hard is the problem from the vision perspective? I think it's non trivial. I think it's non trivial. And I think the biggest part is getting the data right. It's like getting enough data examples. So we, for this research project, we partnered with the transportation authorities of Sweden, and we literally had a racetrack with a safety driver, and we basically progressively got people drunk. Nice. So. But, you know, that's a very expensive data set to collect, and you want to collect it globally and in multiple conditions. Yeah. The ethics of collecting a data set where people are drunk is tricky. Yep. Yeah, definitely. Which is funny because, I mean, let's put drunk driving aside. The number of drunk people in the world every day is very large. It'd be nice to have a large data set of drunk people getting progressively drunk. In fact, you could build an app where people can donate their data because it's hilarious, right? Actually, yeah. But the liability. Liability, the ethics, how do you get it right? It's tricky. It's really, really tricky. Drinking is one of those things that's funny and hilarious and loves. It's social, the so on and so forth. But it's also the thing that hurts a lot of people. Like, a lot of people. Like, alcohol is one of those things. It's legal, but it's really damaging to a lot of lives. It destroys lives, and not just in driving context. I should mention. People should listen to Andrew Huberman, who recently talked about alcohol. He has an amazing pocket. Andrew Huberman is a neuroscientist from Stanford and a good friend of mine. Oh, cool. And he's like a human encyclopedia about all health related wisdom. So he had this a podcast. You would love it. I would love that. No, no, no, no. Oh, you don't know Andrew Huberman. Okay, listen. You listen to Andrew. It's called Huberman Lab podcast. This is your assignment. Just listen to one. I guarantee you this will be a thing where you say likes, this is the greatest human I have ever discovered. Oh, my God. Because I'm really on a journey of health and wellness and I'm learning lots and I'm trying to build these, I guess atomic habits around just being healthy. So I'm definitely going to do this. His whole thing, this is great. He's a legit scientist, really well published. But in his podcast what he does, he's not, he's not talking about his own work. He's like a human encyclopedia of papers. And so he, his whole thing is he takes the topic and in a very fast, you mentioned atomic habits like very clear way summarizes the research in a way that leads to protocols of what you should do. He's really big on like that. Not like this is what the science says, but like this is literally what you should be doing according to science. So like he's really big and there's a lot of recommendations he does, which several of them I definitely don't do like get sunlight as soon as possible from waking up and like for prolonged periods of time. That's a really big one. And he's, there's a lot of science behind that one. There's a bunch of stuff you're gonna, and you're gonna be like Lex, this is, this is my new favorite person. I guarantee it. And if you guys somehow don't know Andrew Huberman and you care about your well being, you know, you should definitely listen to him. I love you, Andrew. Anyway, so what were we talking about? Oh, alcohol and detecting alcohol. So this is a problem you care about and you're trying to solve and. Actually like broadening it. I do believe that the car is going to be a wellness center like because again, imagine if you have a variety of sensors inside the vehicle tracking not just your emotional state or level of distraction and drowsiness and drowsiness. Level of distraction, drowsiness and intoxication. But also maybe even things like your heart rate and your heart rate variability and your breathing rate. And it can start optimizing. Yeah. Can optimize the ride based on what your goals are. So I think we're going to start to see more of that. And I'm excited about that. Yeah. What are the challenges you're tackling with Smarteye currently? What's like the trickiest things to get? Is it, is it basically convincing more and more car companies that having AI inside the car is a good idea? Or is there some, is there more technical algorithmic challenges? What's been keeping you mentally busy? I think a lot of the car companies we are in conversations with are already interested in definitely driver monitoring. I think it's becoming a must have. But even interior sensing, I can see we're engaged in a lot of advanced engineering projects and proof of concepts. I think technologically though, and even the technology, I can see a path to making it happen. I think it's the use case. How does the car respond once it knows something about you? Because you want it to respond in a thoughtful way that isn't off putting to the consumer in the car. So I think that's like the user experience. I don't think we've really nailed that. And we usually, that's not part, we're the sensing platform, but we usually collaborate with the car manufacturer to decide what the use case is. So say you, do you figure out that somebody's angry while driving? Okay, what should the car do? You know, do you see yourself as a role of nudging of like basically coming up with solutions? Essentially that. And then, and then the car manufacturers kind of put their own little spin on it. Right. Like we are like the ideation creative thought partner. But at the end of the day, the car company needs to decide what's on brand for them. Right. Like maybe when it figures out that you're distracted or drowsy, it shows you a coffee cup. Right. Or maybe it takes more aggressive behaviors and basically said, okay, if you don't like, take a rest in the next five minutes, the car is going to shut down. Right. There's a whole range of actions the car can take and doing the thing that is most. Yeah. That builds trust with the driver and the passengers. I think that's what we need to be very careful about. Yeah. Car companies are funny because they have their own, like, I mean that's why people get cars still. I hope that changes, but they get it because it's a certain feel and look and it's a certain. They become proud like Mercedes Benz or BMW or whatever. And that's their thing. That's the family brand or something like that, or Ford or GM, whatever. They stick to that thing. Yeah, it's interesting. It's like it should be, I don't know, it should be a little more about the technology inside. And I suppose there too, there could be a branding, like a very specific style of luxury or fun. Right, right. All that kind of stuff. Yeah. You know, I have an AI focused fund to invest in early stage kind of AI driven companies. And one of the companies we're looking at is trying to do what Tesla did but for boats, for recreational boats. Yeah. So they're building an electric and kind of autonomous boat, and it's kind of the same issues like what kind of sensors can you put in, what kind of states can you detect both exterior and interior within the boat? Anyways, it's like really interesting. Do you boat at all? No, not, well, not in that way. I do like to get on the lake or a river and fish from a boat, but that's not boating. That's the difference. Still boating. Low tech, low tech boat, get away from. Get closer to nature boat. I guess going out into the ocean is also getting closer to nature in some deep sense. I mean, I guess that's why people love it, the, the, the enormity of the water just underneath you. Yeah, I love the, I love both. I love saltwater was like the big and just, it's humbling to be in front of this giant thing that's so powerful that was here before us and be here after. But I also love the piece of a small, like, wooded lake. Everything's calm and therapeutic. You tweeted that I'm excited about Amazon's acquisition of iRobot. I think it's a super interesting, just given the trajectory of which you're part of, of these honestly small number of companies that are playing in this space that are like, trying to have an impact on human beings. So the, it is an interesting moment in time that Amazon would acquire irobot. You tweet, I imagine a future where home robots are as ubiquitous as microwaves or toasters. Here are three reasons why I think this is exciting. If you remember, I can look it up, but why is this exciting to you? I mean, I think the first reason why this is exciting, I can't remember the exact order in which I put them, but one is just, it's going to be an incredible platform for understanding our behaviors within the home. Right. Like, you know, if you think about Roomba, which is, you know, the robot vacuum cleaner, the flagship product of irobot at the moment, it's like running around your home understanding the layout. It's understanding what's clean and what's not. How often do you clean your house? And all of these, like, behaviors are a piece of the puzzle in terms of understanding who you are as a consumer. And I think that could be, again, used in really meaningful ways, not just to recommend better products or whatever, but actually to improve your experience as a human being. So I think that's very interesting. I think the natural evolution of these robots in the home. So it's interesting. Roomba isn't really a social robot right at the moment. But I once interviewed one of the chief engineers on the Roomba team, and he talked about how people named their roombas. And if their roomba broke down, they would call in and say, you know, my roomba broke down. And the company would say, well, we'll just send you a new one, and, no, no, no, rosie, like, you have to, like, yeah, I want you to fix this particular robot. So people have already built, like, interesting emotional connections with these home robots. And I think that, again, that provides a platform for really interesting things to just motivate change. Like, it could help you. I mean, one of the companies has spun out of MIT, Catalya Health, the guy who started it, spent a lot of time building robots that help with weight management. So weight management, sleep, eating better. Yeah, all of these things. But if I'm being honest, Amazon does not exactly have a track record of winning over people in terms of trust. Now, that said, it's a really difficult problem for a human being to let a robot in their home that has a camera on it, right? That's really, really, really tough. And I think Roomba, actually, I have to think about this, but I'm pretty sure now, or for some time already has had cameras because they're doing the most recent roomba. I have so many roombas. Oh, you actually do? Well, I programmed, I don't use a roomba for Vec. People that have been to my place, they're like, yeah, you definitely don't use these roombas. Good. That could be a good, I can't tell, like, the valence of this comment. Was it a compliment or like, no. It'S a giant, it's just a bunch of electronics everywhere. There's, I have six or seven computers, have robots everywhere. Legged robots, small robots and big robots. There's just giant, just piles of robot stuff and. Yeah, but including the roombas, they're being used for their body and intelligence, but not for their purpose. I've changed them, repurposed them for other purposes, for deeper, more meaningful purposes than just like the butt a. Yeah. Which is, you know, brings a lot of people happiness. I'm sure they have a camera because the thing they advertised, I had my own cameras to it, but the camera on the new roomba, they have state of the art poop detection, as they advertised, which is a very difficult, apparently it's a big problem for vacuum cleaners, is if they go over like dog poop, it just runs it over and creates a giant mess. So they have, apparently, they collected a huge amount of data and different shapes and looks and whatever, of poop, and now they're able to avoid it and so on. They're very proud of this. So there is a camera, but you don't think of it as having a camera. Yeah. You don't think of it as having a camera because you've grown to trust that, I guess, because our phones, at least most of us, seem to trust this phone, even though there's a camera looking directly at you. I think that if you trust that, the company is taking security very seriously, actually don't know how that trust was earned with smartphones. I think it just started to provide a lot of positive value into your life where you just took it in. And then the company, over time, has shown that it takes privacy very seriously, that kind of stuff. But I just. Amazon is not always in its social robots communicated. This is a trustworthy thing, both in terms of culture and competence, because I think privacy is not just about what do you intend to do, but also how well, how good are you at doing that kind of thing? So that's a really hard problem to solve. But, I mean, but a lot of us have. Alexa's at home, and, I mean, Alexa could be listening in the whole time. Right. And doing all sorts of nefarious things with the data. Yeah, hopefully it's not, but I don't think it is. But, you know, Amazon is not. It's such a tricky thing for a company to get right, which is, like, to earn the trust. I don't think Alexa's earned people's trust quite yet. Yeah, I think it's. It's not there quite yet. I agree. They struggle with this kind of stuff. In fact, when these topics are brought up, people are always get, like, nervous. And I think if you get nervous about it, I mean, that, like, the way to earn people's trust is not by, like, ooh, don't talk about this. Just be open, be frank, be transparent, and also create a culture of, like, where it radiates at every level from engineer to CEO that, like, you're good people that have a common sense idea of what it means to respect basic human rights and the privacy of people and all that kind of stuff. And I think that propagates throughout the, that's the best pr, which is, like, over time, you understand that this, these are good, right? Good folks doing good things. Anyway, speaking of social robots, have you heard about Tesla? Tesla Bot, the humanoid robot? Yes, I have. Yes, yes, yes. But I don't exactly know what it's designed to do. You probably do. No, I know it's designed to do, but I have a different perspective on it. But it's designed to. It's a humanoid form and is designed to, for automation tasks in the same way that industrial robot arms automate task in the factory. So it's designed to automate tasks in the factory. But I think that humanoid form, as we're talking about before, is one that we connect with as human beings. Anything legged, honestly. But the humanoid form especially, we anthropomorphize it most intensely. And so the possibility, to me, it's exciting to see both atlas developed by Boston Dynamics and anyone, including Tesla, trying to make humanoid robots cheaper and more effective. The obvious way transforms the world is social robotics to me, versus. Versus automation of tasks in the factory. So, yeah, I just wanted to, in case that was something you were interested in, because I find its application of social robotics super interesting. We did a lot of work with Pepper. Pepper the robot a while back. We were like the emotion engine for pepper, which is Softbanks human art robot. How tall is pepper? Yeah, I don't know, like five foot maybe, right? Yeah, yeah, pretty, pretty big. Pretty big. And it was designed to be at like, airport lounges and, you know, retail stores, mostly customer service. Right. Hotel lobbies and, I mean, I don't know where the state of the robot is, but I think it's very promising. I think there are a lot of applications where this can be helpful. I'm also really interested in. Yeah. Social robotics for the home that can help elderly people, for example, transport things from one location of the home to the other, or even just have your back in case something happens. Yeah, I don't know. I do think it's a very interesting space. It seems early, though. Do you feel like the timing is now? Yes, 100%. So it always seems early until it's not. Right. Right. I think the time. I definitely think that the time is now, like this decade for social robots. Whether the humanoid form is right. I don't think so. No, I don't. I think the, like, if we just look Jibo at Jibo as an example, I feel like most of the problem, the challenge, the opportunity of social connection between an AI system and a human being does not require you to also solve the problem of robot manipulation and bipedal mobility. So I think you could do that with just a screen, honestly. But there's something about the interface of jibu. It can rotate and so on. It's also compelling but you get to see all these robot companies that are fail, that fail, incredible companies like Jibo. And even, I mean, irobot in some sense is a big success story, that it was able to find a niche thing and focus on it. But in some sense, it's not a success story because they didn't build any other robot. Like any other. It didn't expand into all kinds of robotics. Like, once you're in the home, maybe that's what happens with Amazon, is they'll flourish into all kinds of other robots. But do you have a sense, by the way, why it's so difficult to build a robotics company? Like, why so many companies have failed? I think it's like you're building a vertical stack, right? Like you are building the hardware plus the software, and you find you have to do this at a cost that makes sense. So I think Jibo was retailing at like, I don't know, like $800, like 700, $800, which for the use case, right. There's a dissonance there. It's too high. So I think cost is, you know, the cost of building the whole platform in a way that is. Yeah, that is affordable for what value it's bringing. I think that's the challenge. I think for these home robots that are going to help you do stuff around the home, that's a challenge, too. Like the mobility piece of it. That's hard. One of the things I'm really excited with Tesla Bot is the people working on it. And that's probably the criticism I would apply to some of the other folks who worked on social robots, is the people working on Tesla bot know how to, they're focused on and know how to do mass manufacture and create a product that's super cheap. Very cool. That's the focus the engineering focus isn't on. I would say that you can also criticize them for that, is they're not focused on the experience of the robot. They're focused on how to get this thing to do the basic stuff that the humanoid form requires to do it as cheap as possible. Then the fewest number of actuators, the fewest numbers of motors, the increase efficiency, they decrease the weight, all that kind of stuff. So that's. That's really interesting. I would say that Jibo and all those folks, they focus on the design, the experience, all of that, and it's secondary how to manufacture, right? No, you have to think like the Tesla bot folks from first principles. What is the fewest number of components? The cheapest components. How can I build it as much in house as possible without. Without having to consider all the complexities of a supply chain, all that kind of stuff. It's interesting because if you have to build a robotics company, you have to. You're not building one robot. You're building, hopefully, millions of robots. You have to figure out how to do that. Where the final thing, I mean, if it's jibo type of robot, is there a reason why Jibo, like, we can have this lengthy discussion. Is there a reason why Jibo has to be over $100? It shouldn't be. Right. Like, the basic. The basic components. Components of it, right. Like, you could start to actually discuss, like, okay, what is the essential thing about GBO? How much. What is the cheapest way I can have a screen? What's the cheapest way I can have a rotating base, all that kind of stuff. And then, and then you get. Get down. Continuously drive down cost. Speaking of which, you have launched and in extremely successful companies, you have helped others. You've invested in companies. Can you give advice on how to start a successful company? I would say, have a problem that you really, really, really want to solve. Right. Something that you're deeply passionate about and honestly take the first step. Like, that's often the hardest. And don't overthink it. Like, you know, like this idea of a minimum viable product or a minimum viable version of an idea, right? Like, yes. You're thinking about this like a humongous, like, super elegant, super beautiful thing. What? Like, reduce it to the littlest thing you can bring to market that can solve a problem. Or that can I. You know, that can help address a pain point that somebody has. They often tell you, like, start with a customer of one. Right. If you can solve a problem for one person, then there's probably yourself or some other person. Right? Pick a person. Exactly. It could be you. Yeah. That's actually often a good sign that if you enjoy a thing. Enjoy a thing. Or you have a specific problem that you'd like to solve, that's a good. That's a good end of one to focus on. What else. What else is there to actually, step one is the hardest, but how do you. There's other steps as well. Right. I also think, like, who you bring around the table early on is so key. Right? Like, being clear on what I call, like, your core values or your north star. It might sound fluffy, but actually it's not, so. And Roz and I feel like we did that very early on. We sat around her kitchen table and we said, okay. There's so many applications of this technology. How are we going to draw the line? How are we going to set boundaries? We came up with a set of core values that in the hardest of times we fell back on to determine how we make decisions. And so I feel like just getting clarity on these core, like, for us, it was respecting people's privacy, only engaging with industries where it's clear opt in. So, for instance, we don't do any work in security and surveillance. So things like that, just getting we very big on, you know, one of our core values is human connection and empathy. Right? And that is, yes, it's an AI company, but it's about people. Well, these are all, they become encoded in how we act, even if you're a small, tiny team of two or three or whatever. So I think that's another piece of advice. So what about finding people, hiring people? If you care about people as much as you do, it seems like such a difficult thing to hire the right people. I think early on as a startup, you want people who share the passion and the conviction, because it's going to be tough. I've yet to meet a startup where it was just a straight line to success. Even not just startups, even everyday people's lives, you always run into obstacles and you run into naysayers. You need people who are believers, whether they're people on your team or even your investors. You need investors who are really believers in what you're doing, because that means they will stick with you. They won't, they won't give up at the first obstacle. Yeah, I think that's important. What about raising money? What about finding investors? First of all, raising, raising money, but also raising money from the right sources from that ultimately don't hinder you, but help you, empower you, all that kind of stuff. What advice would you give there? You successfully raised money and many times in your life. Yeah. Again, it's not just about the money, it's about finding the right investors who are going to be aligned in terms of what you want to build and believe in your core values. For example, especially later on in my latest round of funding, I try to bring in investors that really care about the ethics of AI and the alignment of, of vision and mission and core values is really important. It's like you're picking a life partner, right? It's the same kind of. So you take it that seriously for. Investors, yeah, because they're gonna have to stick your shit stuck together for a while anyway. Yeah, maybe not for life, but for a while, for sure. For better or worse. I forget what the vows usually sound like. For better or worse. No. Yeah. Oh, Boyden. Yeah. Anyway, it's romantic and deep, and you're in it for a while. So it's not just about the money. You tweeted about going to your first capital camp, investing get together, and you learned a lot. So this is about investing. So what have you learned from that? What have you learned about investing in general? From both. Because you've been on both ends of it. I mean, I try to use my experience as an operator now with my investor hat on when I'm identifying companies to invest in. First of all, I think the good news is, because I have a technology background and I really understand machine learning and computer vision and AI, etcetera, I can apply that level of understanding because everybody says they're an AI company or they're an AI tech, and I'm like, no, no, no, show me the technology so I can do that level of diligence, which I actually love. And then I have to do the litmus test of, you know, if I'm in a conversation with you, am I excited to tell you about this new company that I just met? Right. And if I'm an ambassador for that company and I'm passionate about what they're doing, I usually use that. Yeah. That's important to me when I'm investing. So that means you actually can explain to what they're doing and you're excited about it. Exactly. Exactly. Thank you for putting it so succinctly. Like, rambling, but. Exactly. That's it. I understand it. Sometimes it's funny, but sometimes it's unclear. Exactly. I'll hear people tell me and they'll talk for a while, and it sounds cool, like they paint a picture of a world. But then when you try to summarize it, you're not exactly clear of what maybe. Maybe what the core, powerful idea is. You can't just build another facebook. Or there has to be a. There has to be a core, simple to explain idea that. Yeah, that. Then you can or can't get excited about. But it's there. It's right there. Yeah. Yeah. What? But, like, how do you ultimately pick who you think will be successful? It's not just about the thing you're excited about. Like, there's other stuff. Right. And then there's all the, you know, with early stage companies, like pre seed companies, which is where I'm investing, sometimes the. The business model isn't clear yet or the go to market strategy isn't clear. There's usually like, it's very early on that some of these things haven't been hashed out, which is okay. So the way I like to think about it is like, if this company is successful, will this be a multi billion, slash, trillion dollar market operative, you know, or company? And so that's definitely a lens that I use. What's pre seed? What are the different stages and what's the most exciting stage and what's, or not what's interesting about every stage, I guess. Yeah. So pre seed is usually when you're just starting out, you've maybe raised the friends and family rounds, you've raised some money from people you know, and you're getting ready to, to take your first institutional check in. Like first check from an investor. And I love this stage. There's a lot of uncertainty. Some investors really don't like this stage because the financial models aren't there. Often the teams aren't even like, formed. It's really, really early. But to me it's like a magical stage because it's the time when there's so much conviction and so much belief. Almost delusional. Right. Yeah. And there's a little bit of naivete around with, with founders at this stage. And I just love it. It's contagious and I love that I can often they're first time founders. Not always, but often they're first time founders. And I can share my experience as a founder myself and I can empathize. Right. And I can almost, I create a safe ground where, because, you know, you have to be careful what you tell your investors. Right. And I will, I will often, like, say I've been in your shoes as a founder. You can tell me if it's challenging, you can tell me what you're struggling with. It's okay to vent. So I create that safe ground and I think, I think that's the superpower. Yeah. You have to what, I guess you have to figure out if this kind of person is going to able to ride the roller coaster, like of many pivots and challenges and all that kind of stuff. And if the space of ideas they're working in is interesting, like the way they think about the world. Yeah. Because if it's successful, the thing they end up with might be very different. The reason it's successful for it, actually. I was going to say the third crit. So the technology is one aspect, the market or the idea is the second, and the third is the founder. Is this somebody who I believe has conviction, is a hustler, you know, is going to overcome obstacles. Yeah, I think that is going to be a great leader. Right. Like, as a startup, as a founder, you're often. You are the first person, and your role is to bring amazing people around you to build this thing. And so you're an evangelist. Right. So how good are you going to be at that? So I try to evaluate that, too. You also in the tweet thread about it mentioned, is this a known concept, random rich dudes, rds and saying that there should be, like, random rich women, I guess. What's the dudes. What's the dudes version of women? The women version of dudes, ladies. I don't know. Is this a technical term? Is this known venomous dudes? I didn't make that up, but. But I was at this capital camp, which is a get together for investors of all types, and there must have been maybe 400 or so attendees, maybe 20 were women. It was just very disproportionately male dominated, which I'm used to. I think you're used to this kind of thing. I'm used to it, but it's still surprising. And as I'm raising money for the fund, so my fund partner is a guy called Rob May, who's done this before. So I'm new to the investing world, but he's done this before. Most of our investors in the fund are these. I mean, awesome. I'm super grateful to them. Random, just rich guys. I'm like, where are the rich women? So I'm really adamant in both investing in women led AI companies, but I also would love to have women investors be part of my fund, because I think that's how we drive change. Yeah. So then, you know, that takes time, of course, but there's been quite, quite a lot of progress. But, yeah, for the next Mark Zuckerberg to be a woman and all that kind of stuff, because that's just like a huge number of wealth generated by women and then controlled by women, then allocated by women. Exactly. And then beyond just women, just broadly, across all different measures of diversity and so on. Let me ask you to put on your wise sage hat. So you already gave advice on startups and just advice for women, but in general, advice for folks in high school or college today, how to have a career they can be proud of, how to have a life they can be proud of. I suppose you have to give this kind of advice to your kids. Well, here's the number one advice that I give to my kids. My daughter's now 19, by the way, and my son's 13 and a half. So they're not little kids anymore. But does it break your heart? It does. They're awesome. They're my best friends. But, yeah. I think the number one advice I would share is embark on a journey without attaching to outcomes and enjoy the journey. Right. So we often were so obsessed with the end goal that doesn't allow us to be open to different endings of a journey or a story. So you become, like, so fixated on a particular path, you don't see the beauty in the other alternative path. And then you forget to enjoy the journey because you're just so fixated on the goal. And I've been guilty of that for many, many years in my life, and I'm now trying to make the shift of, no, no, no. I'm gonna again trust that things are gonna work out and it'll be amazing and maybe even exceed your dreams. But you have to be open to that. Yeah. Taking a leap into all kinds of things. I think you tweeted, like, you went on vacation by yourself or something like this, or. I know. And just. Just. Just going. Just taking the leap, doing it. Totally doing it. And enjoying. Enjoying them, enjoying the moment, enjoying the weeks, enjoying not looking at the. Some kind of career ladder, next step and so on. Yeah, there's. There's something to that. Like, over planning, too. I'm surrounded by a lot of people that kind of. So I don't plan. You don't know. Do you not do goal setting? My goal setting is very, like. I like the affirmations is very. It's almost. I don't know how to put it into words, but it's. It's a little bit like what my heart yearns for, kind of. And I guess in the space of emotions, more than in the space of, like, this will be, like, in the rational space. Tried to picture a world that I would like to be in, and that world is not clearly pictured. It's mostly in the emotional world. I mean, I think about that from robots because, you know, I have this desire. I've had it my whole life, to. Well, it took different shapes, but I think once I discovered AI, the desire was to. I think I, in this. In the context of this conversation, could be easily easier described as basically a social robotics company. And that's something I dreamed of doing. And, well, there's a lot. There's a lot of complexity to that story, but that's the. That's the only thing honestly, I dream of doing so. I imagine a world that, that I could help create, but it's nothing. There's no steps along the way. And I think I'm just kind of stumbling around and following happiness and working my ass off in almost like an ant does in random directions. But a lot of people, a lot of successful people around me say, you should have a plan, should have a clear goal. You have a goal at the end of the month. You have a goal at the end of the year. I don't. I don't. I don't. And there's a balance to be struck, of course, but there's something to be said about really making sure that you're living life to the fullest, that goals can actually get in the way of. So one of the best, what do you call it when it challenges your brain? What do you call it? Uh, the only thing that comes to mind, and this is me saying, is a mind fuck. But yes, okay, okay, okay. Something like that. Yes. Super inspiring talk. Kenneth Stanley, he was at OpenAI. He just laughed. And he has a book called why greatness can't be planned. And it's actually an AI book. So. And he's done all these experiments that basically show that when you over optimize, the trade off is you're less creative. Right. And to create true greatness and truly creative solutions to problems, you can't over plan it. You can't. And I thought that was. And so he generalizes it beyond AI, and he talks about how we apply that in our personal life and our organizations and our companies, which are over KPI'd, right? Like, look at any company in the world and it's all like, these are the goals. These are the, you know, weekly goals and, you know, the sprints and then the quarterly goals, blah, blah, blah. And he just shows with a lot of his AI experiments that that's not how you create truly game changing ideas. So there you go. Yeah, he's awesome. Yeah, there's a balance. Of course, that's. Yeah. Many moments of genius will not come from planning and goals. But you still have to build factories and you still have to manufacture and you still have to deliver, and there's still deadlines and all that kind of stuff. For that, it's good to have goals. I do goal setting with my kids. We all have our goals, but I think we're starting to morph into more of these bigger picture goals and not obsess about. I don't know, it's hard. Well, I honestly think especially with kids, it's much better to have a plan and have goals and so on, because you have to learn the muscle of what it feels like to get stuff done. But I think once you learn that, there's flexibility. For me, I spend most of my life with goal setting and so on. So, like, I've gotten good with grades and school. I mean, school. If you want to be successful at school, I mean, the kind of stuff in high school and college, the kids have to do in terms of managing their time and getting so much stuff done, it's like, you know, taking five, six, seven classes in college, they're like, that would break the spirit of most humans if they took one of them later in life. It's, like, really difficult stuff, especially engineering curricula. So I think you have to learn that skill, but once you learn it, you can maybe because you're. You can be a little bit on autopilot and use that momentum and then allow yourself to be lost in the flow of life. You know, just kind of. Or also give. Like, I worked pretty hard to allow myself to have the freedom to do that. That's. That's really. Right. That's a tricky freedom to have. Yeah. Because, like, a lot of people get lost in the rat race, and they. Right. And they also, like, like, financially, they. Whenever you get a raise, they'll get, like, a bigger house. Right, right. Something like this. I put very. So, like, there's. You're always trapped in this race. I put a lot of emphasis on living, like, below my means always. And so there's a lot of freedom to do whatever the heart desires. That. That's a really. But everyone has to decide what's the right thing. What's the right thing for them. For some people, having a lot of responsibilities, like a house they can barely afford or having a lot of kids, the responsibility side of that is really helps them get their shit together. Like, all right, I need to be really focused and get some of the most successful people I know have kids, and the kids bring out the best in them. They make them more productive. Less productive. Accountability. Yeah, accountability thing. And almost something to actually live and fight and work for, like, having a family. Yeah. It's fascinating to see, because you would think kids would be a hit on productivity, but they're not for a lot of really successful people. They really, like. They're like an engine of efficiency. Oh, my God. Yeah. It's weird. Yeah. I mean, it's beautiful. It's beautiful to see. And also social happiness. Happiness. Speaking of which, what role do you think love plays in the human condition? Love? I think love is. Yeah. I think it's why we're all here. I think it would be very hard to live life without love in any of its forms. Right. Yeah. That's the most beautiful of forms that human connection takes. Right. Yeah. I feel like everybody wants to feel loved. Right. In one way or another. Right. And to love feels better and to love, too. Totally. Yeah. I agree with that. Both of it. I'm not even sure what feels better. Both are like that. To give love to. Yeah. And. And it is like we've been talking about an interesting question, whether some of that. Whether one day we'll be able to love a toaster in some small. I wasn't quite thinking about that when I was like. Yeah, like, we always give love and give love. Okay. I was thinking about Brad Pitt and coasters. Great. All right, well, I think we. We started on love and ended on love. This was an incredible conversation. Ron, thank you so much. You're an incredible person. Thank you for everything you're doing in AI, in the space of just caring about humanity, human emotion, about love, and being an inspiration to a huge number of people in robotics, in AI, in science, in the world in general. So thank you for talking today. It's an honor. Thank you for having me. And, you know, I'm a big fan of yours as well, so it's been a pleasure. Thanks for listening to this conversation with Rana El Kalyubi. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Helen Keller. The best and most beautiful things in the world cannot be seen or even touched. They must be felt with the heart. Thank you for listening and hope to see you next time.

Utterances:
Speaker A: There's a broader question here, right. As we build socially and emotionally intelligent machines, what does that mean about our relationship with them, and then more broadly, our relationship with one another, right? Because this machine is going to be programmed to be amazing at empathy by definition, right? It's going to always be there for you. It's not going to get bored. I don't know how I feel about that. I think about that.
Speaker B: A lot of the following is a conversation with Rana L. Kolyubi, a pioneer in the field of emotion, recognition and human centric artificial intelligence. She is the founder of Effectiva, deputy CEO of Smarteye, author of Girl Decoded, and one of the most brilliant, kind, inspiring and fun human beings I've gotten the chance to talk to. This is Alex Friedman podcast to support it. Please check out our sponsors in the description. And now, dear friends, here's Rana L. Kalyubi. You grew up in the Middle east in Egypt. What is a memory from that time that makes you smile? Or maybe a memory that stands out as helping your mind take shape and helping you define yourself in this world?
Speaker A: So the memory that stands out is, we used to live in my grandma's house. She used to have these mango trees in her garden, and in the summer, and so mango season was like July and August. And so in the summer she would invite all my aunts and uncles and cousins and, you know, like, it was just like, maybe there were like 20 or 30 people in the house, and she would cook all this amazing food. And us, the kids, we would like, go down the garden and we would like, pick all these mangoes. And I don't know, I think it's just the bringing people together like that always stuck with me. The warmth around the mango tree. Yeah, around the mango tree. And there's just like the joy, the joy of being together around food. And I'm a terrible cook, so I guess that didn't, that memory didn't translate to me kind of doing the same. I love hosting people.
Speaker B: Do you remember colors, smells? Is that what, like what, how does memory work?
Speaker A: Yeah.
Speaker B: Like, what do you visualize? Do you visualize people's faces, smiles? Do, is there colors? Is there like a yemenite, a theme to the colors as it smells because of food involved?
Speaker A: Yeah, I think that's a great question. So those egyptian mangoes, there's a particular type that I love, and it's called Dawesi mangoes. And they're kind of, you know, they're oval and they have a little red in them. So they're red and mango colored on the outside. So I remember that.
Speaker B: Does red indicate, like, extra sweetness? Is that.
Speaker A: Yes.
Speaker B: That means, like, it's nicely sweet. Yeah, it's nice and ripe and stuff. Yeah. What's, like, a definitive food of Egypt? You know, there's like, these almost stereotypical foods in different parts of the world. Like, Ukraine invented borscht. Borscht is this beet soup with that you put sour cream on. See, it's not.
Speaker A: I can't explain it that way.
Speaker B: If you know, if you know, if you know what it is, I think, you know, it's delicious, but if I explain it, it's just not gonna sound delicious. Be a, like, beet soup. This doesn't make any sense, but that's kind of. And you probably have actually seen pictures of it. Cause it's one of the traditional foods in Ukraine, in Russia, in different parts of the slavic world. So that's. But it's become so cliche and stereotypical that you almost don't mention it. But it's still delicious. Like, I visited Ukraine, I eat that every single day.
Speaker A: So do you make it yourself? How hard is it to make.
Speaker B: No, I don't know. I think to make it, well, like anything. Like Italians, they say, well, tomato sauce is easy to make, but to make it right, that's like a generational skill. So anyway, is there something like that in Egypt? Is there a culture of food?
Speaker A: There is. And actually, we have a similar kind of soup. It's called molochaya, and it's made of this green plant. It's like somewhere between spinach and kale. And you mince it, and then you cook it in chicken broth. And my grandma used to make, and my mom makes it really well, and I try to make it, but it's not as great. So we used to have that, and then we used to have it alongside stuffed pigeons. I'm pescetarian now, so I don't eat that anymore.
Speaker B: But stuffed pigeons?
Speaker A: Yeah, it was really yummy. It's the one thing I miss about, you know, now that I'm pescetarian and.
Speaker B: I don't eat the stuffed pigeons.
Speaker A: Yeah, the stuffed pigeons.
Speaker B: What are they stuffed with? If that doesn't bother you too much to describe.
Speaker A: No, no. It's stuffed with a lot of, like, just rice and.
Speaker B: Oh, God, yeah.
Speaker A: Rice. Yeah. So.
Speaker B: And you also, you said that your first in your book, that your first computer was an Atari and Space Invaders was your favorite game. Is that when you first fell in love with computers, would you say, yeah, I would say.
Speaker A: So.
Speaker B: Video games or just the computer itself, just something about the machine. Oh, this thing. There's magic in here.
Speaker A: Yeah. I think the magical moment is definitely like playing video games with my. I have two younger sisters, and we just, like, had fun together, like, playing games. But the other memory I have is my first code. The first code I wrote, I wrote. I drew a Christmas tree, and I'm Muslim. Right. So it's kind of, it was kind of funny that I, that I, that the first thing I did was, like, this Christmas tree. So. Yeah. And that's when I realized, wow, you can write code to do all sorts of, like, really cool stuff. I must have been like, six or seven at the time.
Speaker B: So you can write programs and the programs do stuff for you. That's power, if you think about it. That's empowering.
Speaker A: Hey. Hi.
Speaker B: Yeah, I know what it is. I don't know. You see, like, I don't know if many people think of it that way when they first learn to program. They just love the puzzle of it. Like, ooh, this is cool. This is pretty. It's a Christmas tree, but, like, it's power. It is you eventually, I guess you couldn't at the time, but eventually, this thing, if it's interesting enough, if it's a pretty enough Christmas tree, it can be run by millions of people and bring them joy, like that little thing. And then because it's digital, it's easy to spread. So, like, you just created something that's easily spreadable to millions of people.
Speaker A: Totally.
Speaker B: It's hard to think that way when you're six. In the book, you write, I am who I am because I was raised by a particular set of parents, both modern and conservative, forward thinking, yet locked in tradition. I'm a Muslim, and I feel I'm stronger and more centered for it. I adhere to the values of my religion, even if I'm not as dutiful as I once was. And I am a new american, and I'm thriving on the energy, vitality, and entrepreneurial spirit of this great country. So let me ask you about your parents. What have you learned about life from them, especially when you were young?
Speaker A: So both my parents, they're egyptian, but they moved to Kuwait right out. They actually, there's a cute story about how they met. So my dad taught coble in the seventies.
Speaker B: Nice.
Speaker A: And my mom decided to learn programming, so she signed up to take his Cobol programming class, and he tried to date her, and she was like, no, no, no, I don't date. And so he's like, okay, I'll propose. And that's how they got married.
Speaker B: Whoa.
Speaker A: Yeah, I know, right?
Speaker B: Exactly right? That's really impressive. So those kobold guys know how to. How to impress a lady. So. So, yeah. So what have you learned from them?
Speaker A: So, definitely grit. One of the core values in our family is just hard work. There were no slackers in our family. And that's something I've definitely. That's definitely stayed with me, both as a professional, but also in my personal life. But I also think my mom always used to. I don't know, it was like unconditional love. I just knew my parents would be there for me regardless of what I chose to do. I think that's very powerful. And they got tested on it because I kind of challenged. I challenged cultural norms, and I kind of took a different path, I guess, than what's expected of a woman in the Middle east. And they still love me, which is. I'm so grateful for that.
Speaker B: When was a moment that was the most challenging for them? Which moment where they kind of had to come face to face with the fact that you're a bit of a rebel?
Speaker A: I think the first big moment was when I. I had just gotten married, but I decided to go do my PhD at Cambridge University. And because my husband at the time, he's now my ex, ran a company in Cairo. He was going to stay in Egypt. So it was going to be a long distance relationship. And that's very unusual in the Middle east for a woman to just head out and kind of, you know, pursue her career. And so my dad, actually, my dad and my I. Parents in law both said, you know, we do not approve of you doing this, but now you're under the jurisdiction of your husband, so he can make the call. And luckily for me, he was supportive. He said, you know, this is your dream come true. You've always wanted to do a PhD. I'm going to support you. So I think that was the first time where, you know, I challenged the cultural norms.
Speaker B: Was that scary?
Speaker A: Oh, my God, yes, it was totally scary.
Speaker B: What's the biggest culture shock from there to Cambridge to London?
Speaker A: Well, that was also during, right around September 11, so everyone thought that there was going to be a third world war. It was really. And at the time, I used to wear the hijab, so I was very visibly muslim. And so my parents just were. They were afraid for my safety. But anyways, when I got to Cambridge, because I was so scared, I decided to take off my headscarf and wear a hat instead. So I just went to class wearing these, like, british hats, which was, in my opinion, actually worse than just showing up in a headscarf because it was just so awkward. Right. Like sitting in class with, like, all.
Speaker B: These, trying to fit in.
Speaker A: Yeah, yeah, yeah, yeah, yeah. So after a few weeks of doing that, I was like, to heck with that. I'm just gonna go back to wearing my headscarf.
Speaker B: Yeah, you wore the hijab. So starting in 2000 and for twelve years after, so always whenever you're in public, you have to wear the head covering. Can you speak to that? To the hijab? Maybe your mixed feelings about it. Like, what does it represent in its best case? What does it represent in the worst case?
Speaker A: Yeah, you know, I think there's a lot of. I guess I'll first start by saying I wore it voluntarily. I was not forced to wear it. And in fact, I was one of the very first women in my family to decide to put on the hijab. And my family thought it was really odd. Right? Like, there was, they were like, why do you want to put this on? And at its best, it's a sign of modesty, humility.
Speaker B: It's like me wearing a suit. People are like, why are you wearing a suit? It's a step back into some kind of tradition, a respect for tradition of sorts. So you said because it's by choice, you're kind of free to make that choice, to celebrate a tradition of modesty.
Speaker A: Exactly. And I actually made it my own. I remember I would really match the color of my head scarf with what I was wearing. It was a form of self expression. And at its best, I loved wearing it. I have a lot of questions around how we practice religion and religion. And, you know, and I think also it was a time where I was spending a lot of time going back and forth between the US and Egypt. And I started meeting a lot of people in the US who are just amazing people, very purpose driven people who have very strong core values, but they're not Muslim. That's okay. Right. And so that was when I just had a lot of questions. And politically also, the situation in Egypt was when the Muslim Brotherhood ran the country and I didn't agree with their ideology. It was at a time when I was going through a divorce. It was just the perfect storm of political personal conditions where I was like, this doesn't feel like me anymore. And it took a lot of courage to take it off because culturally, it's okay if you don't wear it, but it's really not okay to wear it and then take it off.
Speaker B: But you're still. So you have to do that while still maintaining a deep core and pride in the origins in your origin story.
Speaker A: Totally.
Speaker B: So still being egyptian, still being a Muslim.
Speaker A: Right. And being, I think, generally, like, faith driven, but, yeah.
Speaker B: But what that means changes year by year for you. It's like a personal journey.
Speaker A: Yeah, exactly.
Speaker B: What would you say is the role of faith in that part of the world? Like, how do you see. You mention it a bit in the book, too.
Speaker A: Yeah. I mean, I think there is something really powerful about just believing that there's a bigger force. You know, there's a kind of surrendering, I guess, that comes with religion. And you surrender and you have this deep conviction that it's gonna be okay. Right. Like, the universe is out to, like, do amazing things for you, and it's gonna be okay. And there's strength to that. Like, even when you're going through adversity, you just know that it's gonna work out.
Speaker B: Yeah. It gives you, like, an inner peace, a calmness.
Speaker A: Exactly, exactly.
Speaker B: Yeah. That's faith in all the meanings of that word.
Speaker A: Right.
Speaker B: Faith that everything is going to be okay, and it is because time passes and time cures all things. It's like a calmness with the chaos of the world.
Speaker A: And also there's like, a silver. I'm a true believer of this, that something at a specific moment in time can look like it's catastrophic and it's not what you wanted in life. Da da da da. But then time passes, and then you look back and there's a silver lining. Right? It maybe closed the door, but it opened a new door for you. And so I'm a true believer in that. That, you know, there's a silver lining in almost anything in life. You just have to have this, like. Yeah. Faith or conviction that it's going to work out. So.
Speaker B: Such a beautiful way to see a shady feeling. So if you're. If you feel shitty about a current situation, I mean, it almost is always true. Unless it's the cliches thing of if it doesn't kill you, whatever doesn't kill you makes you stronger. It's. It does seem that over time, when you take a perspective on things, that the hardest moments and periods of your life are the most meaningful. Yeah. Yeah. So over time, you get to have that.
Speaker A: Right.
Speaker B: What about. Because you mentioned Kuwait? What about. Let me ask you about war. What's the role of war and peace? Maybe even the big love and hate in that part of the world, because it does seem to be a part of the world where there's turmoil. There was turmoil, there's still turmoil.
Speaker A: It is so unfortunate, honestly. It's such a waste of human resources and human mind. Share. At the end of the day, we all kind of want the same things. We want a human connection. We want joy, we want to feel fulfilled. We want to feel, you know, a life of purpose. And I just find it baffling, honestly, that we are still having to grapple with that. I have a story to share about this. You know, I grew up, indeed, I'm egyptian American now, but, you know, originally from Egypt. And when I first got to Cambridge, it turned out my office mate, like, my PhD kind of, you know, ended up, you know, we ended up becoming friends. But she was from Israel, and we didn't know. Yeah, we didn't know how it was gonna be.
Speaker B: Like, did you guys sit there just staring at each other for a bit?
Speaker A: Actually, she. Cause I arrived before she did, and it turns out she emailed our PhD advisor and asked him if she thought it was gonna be okay.
Speaker B: Yeah. Oh, this is around 911, too.
Speaker A: Yeah. And Peter Robinson, our PhD advisor, was like, yeah. Just as an academic institution. Just show up. And we became super good friends. We were both new moms. Like, we both had our kids during our PhD. We were both doing artificial emotional intelligence. She was looking at speech, I was looking at the face. We just had. So the culture was so similar. Our jokes were similar. It was just. I was like, why on earth are our countries? Why is there all this war and tension? And I think it falls back to the narrative, right? If you change the narrative, like, whoever creates this narrative of war, I don't know. We should have women run the world.
Speaker B: Yeah, that's one solution. The good women. Because there's also evil women in the world.
Speaker A: True.
Speaker B: Okay. But yes, yes, there could be less war if women ran the world. The other aspect is, doesn't matter. The gender, the people in power. You know, I get to see this with Ukraine and Russia, parts of the world around that conflict now, and that's happening in Yemen as well. And everywhere else, there's these narratives told by the leaders to the populace, and those narratives take hold, and everybody believes that, and they have a distorted view of the humanity on the other side. In fact, especially during war, you don't even see the people on the other side as human or as equal, intelligence or worth or value as you. You tell all kinds of narratives about them. Being Nazis or dumb or whatever narrative you want to weave around that or evil. But I think when you actually meet them face to face, you realize they're like the same.
Speaker A: Exactly. Right.
Speaker B: It's actually big shock for people to realize, like, that they've been essentially lied to within their country. And I kind of have faith that social media, as ridiculous it is to say or any kind of technology is able to bypass the walls that governments put up and connect people directly. And then you get to realize, ooh, like, people fall in love across different nations, religions, and so on. And that, I think, ultimately can cure a lot of our ills, especially sort of in person, just, I also think that if leaders met in person to have a conversation that would cure a lot of the ills of the, of the world, especially in private. Let me ask you about the women running, running the world. So gender does in part, perhaps shaped the landscape of just our human experience. So in what ways was it limiting? And in what ways was it empowering for you to be a woman in the Middle East?
Speaker A: I think just kind of just going back to, like, my comment on, like, women running the world, I think it comes back to empathy, right. Which has been a common thread throughout my entire career. And it's this idea of human connection. Once you build common ground with a person or a group of people, you build trust, you build loyalty, you build friendship, and then you can turn that into behavior change and motivation and persuasion. So it's like empathy and emotions are just at the center of everything we do. And I think being from the Middle east, kind of this human connection is very strong. Like, we have this running joke that if you come to Egypt for a visit, people are gonna, we'll know everything about your life, like, right away. Right. I have no problems asking you about your personal life. There's no, like, no boundaries, really. No personal boundaries. In terms of getting to know people. We get emotionally intimate, like, very, very quickly. But I think people just get to know each other, like, authentically, I guess. You know, there isn't this, like, superficial level of getting to know people. You just try to get to know people, really.
Speaker B: And empathy is a part of that totally.
Speaker A: Because you can put yourself in this person's shoe and kind of. Yeah, imagine, you know, what challenges they're going through. And so I think I've definitely taken that with me. Generosity is another one, too. Like, just being generous with your time and love and attention and even with your wealth. Right. Even if you don't have a lot of it, you're still very generous, and.
Speaker B: I think that's another enjoying the humanity of other people. And so do you think there's a useful difference between men and women in that aspect and empathy, or is doing these kind of big general groups, does that hinder progress?
Speaker A: Yeah, I actually don't want to over generalize. I mean, some of the men I know are, like, the most empathetic humans.
Speaker B: Yeah, I strive to be empathetic.
Speaker A: Yeah, you're actually very empathetic. Yeah. So I don't want to over generalize. Although one of the researchers I worked with when I was at Cambridge, Professor Simon Baring Cohen, he's Sacha Baring Cohen's cousin.
Speaker B: Yeah.
Speaker A: And he runs the autism research center at Cambridge, and he's written multiple books on autism. And one of his theories is the empathy scale, like the systemizers and the empathizers. And there's a disproportionate amount of computer scientists and engineers who are systemizers and perhaps not great empathizers. And then, you know, there's. And there's more men in that bucket, I guess, than women, and then there's more women in the empathizer's bucket. So, again, not. Not to over generalize.
Speaker B: I sometimes wonder about that. It's been frustrating to me how many, I guess, systemizers there are in the field of robotics. Yeah. It's actually encouraging to me because I care about, obviously, social robotics and because it's. It. There's more opportunity for people that are empathic.
Speaker A: Exactly. I totally agree. Well, right, so it's nice. Yes.
Speaker B: So every robotist I talk to, they don't see the. The human as interesting as, like, it is. It's not exciting. You want to avoid the human at all costs. It's a. It's a safety concern to be touching the human, which it is, but it's also an opportunity for a deep connection or collaboration or all that kind of stuff. And because most brilliant roboticists don't care about the human, it's an opportunity. In your case, it's a business opportunity, too, but in general, an opportunity to explore those ideas in this beautiful journey to Cambridge, to UK, and then to America. What's the moment or moments that were most transformational for you as a scientist and as a leader? So he became an exceptionally successful CEO, founder, researcher, scientist, and so on. Was there a phase shift there where, like, I can be somebody, I can really do something in this world?
Speaker A: Yeah. So, actually, just kind of a little bit of background. So the reason why I moved from Cairo to Cambridge, UK, to do my PhD is because I had a very, you know, clear career plan. I was like, okay, I'll go abroad, get my PhD, gonna crush it in three or four years, come back to Egypt and teach. It was very clear, very well laid out.
Speaker B: Was topic clear or. No?
Speaker A: The topic. Well, I did my PhD around building artificial emotional intelligence and looking at it.
Speaker B: No, but in your master plan, ahead of time, when you're sitting by the mango tree, did you know it's gonna be artificial intelligence?
Speaker A: No, no, no, that I did not know. Although I think I kind of knew that I was going to be doing computer science, but I didn't know the specific area. But I love teaching. I mean, I still love teaching. So I just, yeah, I just wanted to go abroad, get a PhD, come back, teach.
Speaker B: Why computer science? Can we just linger on that? Because you're such an empathic person who cares about emotion, humans and so on. Aren't computers cold and emotionless with changing that? Yeah, I know, but like, isn't that the. Or did you see computers as the having the capability to actually connect with humans?
Speaker A: I think that was, like, my takeaway from my experience just growing up. Like, computers sit at the center of how we connect and communicate with one another. Right. Or technology in general. Like, I remember my first experience being away from my parents. We communicated with a fax machine. But thank goodness for the fax machine because we could send letters back and forth to each other. This was pre emails and stuff. So I think technology can be not just transformative in terms of productivity, et cetera. It actually does change how we connect with one another.
Speaker B: Can I just defend the fax machine? There's something like the haptic feel because the email is all digital. There's something really nice, I still write letters to people. There's something nice about the haptic aspect of the fax machine because you still have to press, you still have to do something in the physical world to make this thing a reality. The sense of.
Speaker A: Right. And then it comes out as a printout and you can actually touch it and read it.
Speaker B: Yeah, there's something lost when it's just an email, obviously. I wonder how we can regain some of that in the digital world, which goes to the metaverse and all those kinds of things. We'll talk about it.
Speaker A: Actually, do a question on that one. Do you still, do you have photo albums anymore? Do you still print photos?
Speaker B: No, no, but I'm a minimalist.
Speaker A: Okay.
Speaker B: So it was one of the, one of the painful steps in my life. Was to scan all the photos and let go of them and then let go of all my books.
Speaker A: You let go of your books?
Speaker B: Yeah, switch to Kindle. Everything, kinda. So I thought. I thought, okay, think 30 years from now, nobody's gonna have books anymore. The technology of digital books gonna get better and better and better. Are you really gonna be the guy that's still romanticizing physical books? Are you gonna be the old man on the porch who's like, kids? Yes. So just get used to it. Cause it was, it felt. It still feels a little bit uncomfortable to read on a. On a Kindle. But get used to it like you always. I mean, I'm trying to learn new programming languages. Always. Like, with technology, you have to kind of challenge yourself to adapt to it. You know, I force myself to use TikTok. No, that thing doesn't need much forcing it pulls you in like a. Like a. Like the worst kind of. Or the best kind of drug anyway. Yeah. So. Yeah, but I do love haptic things. There's a magic to the haptic, even, like touchscreens. It's tricky to get right to. To get the experience of a button.
Speaker A: Yeah.
Speaker B: Anyway, what were we talking about? So, AI. So the journey, your whole plan was to come back to Cairo and teach, right? And then what, did the plan go wrong?
Speaker A: Yeah, exactly right. And then I got to Cambridge and I fall in love with the idea of research, right. And kind of embarking on a path. Nobody's explored this path before. You're building stuff that nobody's built before, and it's challenging and it's hard and there's a lot of non believers. I just totally loved that. And at the end of my PhD, I think it's the meeting that changed the trajectory of my life. Professor Rosalind Picard, who's. She runs the affective computing group at the MIT media lab. I had read her book. You know, I was like, following all her research.
Speaker B: Aka raze?
Speaker A: Yes, aka Roz. And she was giving a talk at a pattern recognition conference in Cambridge and she had a couple of hours to kill, so she emailed the lab and she said, you know, if any students want to meet with me, like, just, you know, sign up here. And so I signed up for slot and I spent like the weeks leading up to it preparing for this meeting, and I want to show her a demo of my research and everything. And we met and we ended up hitting it off. Like, we totally clicked. And at the end of the meeting, she said, do you want to come work with me as a postdoc at MIT. And this is what I told her. I was like, okay, this would be a dream come true, but there's a husband waiting for me in Cairo. I kind of have to go back.
Speaker B: Yeah.
Speaker A: And she said, it's fine. Just commute. And I literally started commuting between Cairo and Boston. Yeah, it was a long commute. And I did that, like, every few weeks, I would, you know, hop on a plane and go to Boston. But that. That changed the trajectory of my life. There was no. I kind of outgrew my dreams. Right. I didn't want to go back to Egypt anymore and be faculty. Like, that was no longer my dream. I had a new dream.
Speaker B: What was the. What was it like to be at MIT? What was that? Culture shock? You mean America in general, but also, I mean, Cambridge has its own culture, so what was MIT like? And what was America like?
Speaker A: I think. I wonder if that's similar to your experience at MIT. I was just at the media lab in particular. I was just really impressed is not the right word. I didn't expect the openness to, like, innovation and the acceptance of taking a risk and failing. Like, failure isn't really accepted back in Egypt. Right. You don't want to fail. Like, there's a fear of failure, which I think has been hardwired in my brain. But you get to MIT and it's okay to start things, and if they don't work out, like it's okay, you pivot to another idea. And that kind of thinking was just very new to me.
Speaker B: That's liberating. Well, media lab, for people who don't know MIT, media lab is its own beautiful thing, because they, I think, more than other places, MIT reach for big ideas. And, like, they try. I mean, I think. I mean, depending, of course, on who. But certainly with Roslyn, you try wild stuff. You try big things and crazy things, and also try to take things to completion so you can demo them. So always, always have a demo. Like, if you go. One of the sad things to me about robotics labs at MIT, and there's, like, over 30, I think, is like, usually when you show up to a robotics lab, there's not a single working robot. They're all broken. All the robots are broken, which is like the normal state of things because you're working on them. But it would be nice if we lived in a world where robotics labs had some robots functioning. One of my favorite moments that just sticks with me. I visited Boston Dynamics, and there was a, first of all, seeing so many spots so many legged robots in one place. I'm like, I'm home. But the. This is where I was built. The cool thing was just to see there was a random robot spot was walking down the hall. It's probably doing mapping, but it looked like he wasn't doing anything. And he was wearing he or she. I don't know, but it. Well, in my mind, they're people. They have a backstory. But this one in particular definitely has a backstory because he was wearing a cowboy hat. So I just saw a spot robot with a cowboy hat walking down the hall. And there was just this feeling like there's a life. Like he has a life. He probably has to commute back to his family at night. Like there's a. There's a feeling like there's life instilled in this robot and that's magical. I don't know, it was kind of inspiring to see.
Speaker A: Did it say hello to. Did he say hello to you?
Speaker B: No. There's a focused nature to the robot. No, no, listen, I love competence and focus and. Great. Like, he was not going to get distracted by the. The shallowness of small talk. There's a job to be done and he was doing it. So anyway, the fact that it was working is a beautiful thing. And I think media lab really prides itself on trying to always have a thing that's working. It could show off.
Speaker A: Yes. We used to call it demo or die. Yeah. You could not like, show up with like PowerPoint or something. You actually had to have it working. You know what? My son, who is now 13, I don't know if this is still his lifelong goal or not, but when he was a little younger, his dream is to build an island that's just inhabited by robots. Like no humans. He just wants all these robots to be connecting and having fun and that's all. There you go.
Speaker B: Does he have humanity? Does he have an idea of which robots he loves most? Is it. Is it roomba like robots? Is it humanoid robots, robot dogs? Or is it not clear yet?
Speaker A: We used to have a jibo, which was one of the MIT media lab spin outs, and he used to love it.
Speaker B: Thing with a giant head.
Speaker A: Yes, it spins. Right, exactly.
Speaker B: And rotate. And it's an eye.
Speaker A: It has.
Speaker B: Well, like, not glowing like.
Speaker A: Right, right, exactly.
Speaker B: It's like Hal 9000, but the friendly version.
Speaker A: He loved that. And then he just loves. Yeah, he just. I think he loves all forms of robots, actually.
Speaker B: So it embodied intelligence.
Speaker A: Yes.
Speaker B: I like, I personally like legged robots, especially anything that can wiggle its butt. No, that's not the definition of what I love, but that's just technically what I've been working on recently. Except I have a bunch of legged robots now in Austin, and I've been doing. I was. I've been trying to have them communicate affection with their body in different ways, just for art.
Speaker A: So cool.
Speaker B: For art, really, because I love the idea of walking around with the robots like as you would with the dog. I think it's inspiring to a lot of people, especially young people. Like, kids love kids love it. Parents, like adults are scared of robots. But kids don't have this kind of weird construction of the world that's full of evil. They love cool things.
Speaker A: Yeah, I remember when Adam was in first grade, so he must have been like seven or so. I went into class with a whole bunch of robots and the emotion AI demo and da da. And I asked the kids, I was like, would you kids want to have a robot? You know, robot friend or robot companion? Everybody said yes. And they wanted it for all sorts of things, like to help them with their math homework and to, like, be a friend. So there's. It just struck me how there was no fear of robots. A lot of adults have that, like us versus them.
Speaker B: Yeah, none of that. Of course you want to be very careful because you still have to look at the lessons of history and how robots can be used by the power centers of the world to abuse your rights and all that kind of stuff. But mostly it's good to enter anything new with an excitement and optimism. Speaking of Ras, what have you learned about science and life from Rosalind Picard?
Speaker A: Oh, my God. I've learned so many things about life from Roz. I think the thing I learned the most is perseverance. When I first met Roz, we applied, and she invited me to be our postdoc. We applied for a grant to the National Science foundation to apply some of our research to autism. And we got back, we were rejected.
Speaker B: Rejected?
Speaker A: Yeah. And the reasoning was, the first time.
Speaker B: You were rejected for fun. Yeah, yeah.
Speaker A: And I basically, I just took the rejection to mean, okay, we're rejected, it's done. Like, end of story, right? And Roz was like, it's great news. They love the idea. They just don't think we can do it. So let's build it, show them, and then reapply. And it was that, oh, my God. That story totally stuck with me. And she's like that in every aspect of her life. She just does not take no for an answer.
Speaker B: Reframe all negative feedback.
Speaker A: As a challenge.
Speaker B: As a challenge, yes. They like this.
Speaker A: Yeah, it was a riot.
Speaker B: What else about science in general, about how you see computers and also business and just everything about the world. She's a very powerful, brilliant woman like yourself. So is there some aspect of that, too?
Speaker A: Yeah, I think Roz is actually also very faith driven. She has this deep belief and conviction. Yeah. In the good in the world and humanity. And I think that was meeting her and her family was definitely a defining moment for me, because that was when I was like, wow, you can be of a different background and religion and whatever, and you can still have the same core values. So that was, yeah, I'm grateful to her. Roz, if you're listening. Thank you.
Speaker B: Yeah, she's great. She's been on this podcast before. I hope she'll be on the. I'm sure she'll be on again. You were the founder and CEO of Effectiva, which is a big company that was acquired by another big company, Smarteye, and you're now the deputy CEO of Smarteye. So you're a powerful leader, you're brilliant, you're brilliant scientist. A lot of people are inspired by you. What advice would you give, especially to young women, but people in general who dream of becoming a powerful leaders like yourself, in a world where perhaps in a world perhaps doesn't give them a clear, easy path to do so, whether we're talking about Egypt or elsewhere, you.
Speaker A: Know, hearing you kind of describe me that way, kind of encapsulates, I think, what I think is the biggest challenge of all, which is believing in yourself. Right. I have had to, like, grapple with this, what I call now the Debbie Downer voice in my head that kind of basically is just chattering all the time. It's basically saying, oh, no, no, no, you can't do this. Like, you're not going to raise money. You can't start a company. Like, what business do you have? Like, starting a company or running a company or selling a company. Like, you name it, it's always like, and I think my biggest advice to not just women, but people who are taking a new path and they're not sure, is to not let yourself and let your thoughts be the biggest obstacle in your way. And I've had to really work on myself to not be my own biggest obstacle.
Speaker B: So you got that negative voice.
Speaker A: Yeah.
Speaker B: Um, so is that.
Speaker A: Am I the only one? I don't think I'm the only one.
Speaker B: No, I have that negative voice. I'm not exactly sure if it's a bad thing or a good thing. I've been really torn about it because it's been a lifelong companion, so it's hard to know. It's kind of, um. It drives productivity and progress, but it can hold you back from taking big leaps. I think you. I. The best I can say is probably you have to somehow be able to control it. So turn it off when it's not useful and turn it on when it's useful. Like I have from almost like a third person perspective.
Speaker A: Right. Somebody who's sitting there, like.
Speaker B: Yeah. Like, because it is useful to be critical. Like, after I just gave a talk yesterday at MIT, and I was just, you know, there's so much love, and it was such an incredible experience. So many amazing people I got a chance to talk to. But, you know, afterwards, when I. When I went home and just took this long walk, it was mostly just negative thoughts about me. I don't like one basic stuff. Like, I don't deserve any of it. And so, second is like. Like, why did you. That was so dumb. You said this. That's so dumb. Like, you should have prepared that better. Why did you say this? But I think it's good to hear that voice out. All right. And, like, sit in that. And ultimately, I think you grow from that. Now, when you're making really big decisions about funding or starting a company or taking a leap to go to the UK or take a leap to go to America to work in media lab, though. Yeah. There's a. That's. You should be able to shut that off then, because you should have, like, this weird confidence, almost like faith that you said before, that everything's gonna work out. So take the leap of faith.
Speaker A: Take the leap of faith.
Speaker B: Despite all the negativity. I mean, there's some of that. You actually tweeted a really nice tweet thread. It says, quote, a year ago, a friend recommended I do daily affirmations. And I was skeptical, but I was going through major transitions in my life, so I gave it a shot and it set me on a journey of self acceptance and self love. So what was that? Like, maybe talk through this idea of affirmations and how that helped you?
Speaker A: Yeah. Because really, like, I'm just like me. I'm a kind. I'd like to think of myself as a kind person in general, but I'm kind of mean to myself sometimes.
Speaker B: Yeah.
Speaker A: And so I've been doing journaling for almost ten years now. I use an app called Day one, and it's awesome. I just journal and I use it as an opportunity to almost have a conversation with the Debbie Downer voice in my. It's like a rebuttal, right? Like, Debbie Downer says, oh, my God. Like, you. You know, you won't be able to erase this round of funny. I'm like, okay, let's talk about it. I have a track record of doing x, y, and z. I think I can do this. And it's literally, like, so I wouldn't. I don't know that I can shut off the voice, but I can have a conversation with it, and it just. And I bring data to the table. Right.
Speaker B: Nice.
Speaker A: So, that was the journaling part, which I found very helpful, but the affirmation took it to a whole next level, and I just love it. I'm a year into doing this, and you literally wake up in the morning, and the first thing you do, I meditate first, and then I write my affirmations. And it's the energy I want to put out in the world that hopefully will come right back to me. So I always start with, my smile lights up the whole world. And I kid you not, people in the street will stop me and say, oh, my God, we love your smile. So, my affirmations will change depending on, you know, what's happening this day. Is it funny? I know. Don't judge. Don't judge.
Speaker B: No, that's not what. Laughter's not judgment. It's just awesome. I mean, it's true. But you're saying affirmations somehow help kind of. What is it? They do work to remind you of the kind of person you are and the kind of person you want to be, which actually may be in reverse order, the kind of person you want to be, and that helps you become the. The kind of person you actually are.
Speaker A: It's just. It's. It brings intentionality to, like, what you're doing. Right. And so, by the way, I was.
Speaker B: Laughing because my affirmations, which I also do, are the opposite.
Speaker A: Oh, you do?
Speaker B: Oh, what do you. I don't have a. My smile lights up the affirmation. Maybe I should add that, because, like, I I have. I just. I have a. Oh, boy. I just. It's, uh. It's much more stoic, like, about focus, about this.
Speaker A: Oh.
Speaker B: But the joy, the emotion that you're just in, that little affirmation is beautiful. So maybe I should add that I'm.
Speaker A: Like, focused on it. That's usually.
Speaker B: That's a cool start.
Speaker A: It's just, after all the, like, smiling, you're inspiring, playful, and joyful and all that. And then it's like, okay, I kick butt.
Speaker B: Let's get shit done, right? Let's get you done. Affirmation. Okay, cool. So, like, what else is on there?
Speaker A: What else is on there? Well, I'm a magnet for all sorts of things. So I'm an amazing people magnet. I attract awesome people into my universe.
Speaker B: So that's an actual affirmation?
Speaker A: Yes.
Speaker B: That's great. Yeah. And that somehow manifests itself into working?
Speaker A: I think so, yeah.
Speaker B: Can you speak to why it feels good to do the affirmations?
Speaker A: I honestly think it just grounds the day, and then it allows me to. Instead of just, like, being pulled back and forth, like, throughout the day, it just grounds me. I'm like, okay. Like, this thing happened. It's not exactly what I wanted it to be, but I'm patient. Or I'm, you know, I'm. I trust that the universe will do amazing things for me, which is one of my other consistent affirmations. Or I'm an amazing mom. Right. And so I can grapple with all the feelings of mom guilt that I have all the time. Or here's another one. I'm a love magnet. And I literally say, I will kind of picture the person that I'd love to end up with. And I write it all down and hasn't happened yet, but it.
Speaker B: What are you. What are you picturing? This is Brad Pitt.
Speaker A: Brad Pitt.
Speaker B: Because that's what I picture.
Speaker A: Okay. That's what you picture.
Speaker B: Yeah. Holding hands, running together. No, no, more like fight club. That. The fight club. Brad Pitt, where he's, like, standing. All right. People will know anyway. I'm sorry. I'll get off on that. Do you have, like, when you're thinking about the. Being a love magnet in that way. Are you picturing specific people? Or is this almost, like, in the space of, like, energy?
Speaker A: Right. As somebody who is smart and well accomplished and successful in their life, but they're generous and they're well traveled and they want to travel the world, things like that. Like, their head over heels into me is like, I know it sounds super silly, but it's literally what I write, and I believe it'll happen one day.
Speaker B: Oh, you actually write, so you don't say it out loud?
Speaker A: No, I write it. I write all my affirmations.
Speaker B: I do the opposite. I say, interesting. Yeah. If I'm alone, I'll say it out loud. Yeah.
Speaker A: Interesting. I should try that.
Speaker B: I think it's. Which. What feels more powerful to you? To me, more powerful. Saying stuff feels more powerful.
Speaker A: Yeah.
Speaker B: Writing is. Writing feels like I'm losing. Losing the words. Like losing the power of the words. Maybe because I write slow. Do you hand write?
Speaker A: No, I type. It's on this app. It's day one, basically. And I just. I can look. The best thing about it is I can look back and see like a year ago, what was I affirming? Right.
Speaker B: So it's also changes over time.
Speaker A: It hasn't like, changed a lot, but it. But the focus kind of changes over time.
Speaker B: I got it. Yeah. I say the same exact thing over and over and over.
Speaker A: Oh, you do? Okay.
Speaker B: There's a comfort in the. In the sameness of it, actually. Let me jump around because. Let me ask you about. Because all this talk about Brad Pitt or maybe that's just going on inside my head. Let me ask about dating in general. You tweeted, are you based in Boston? In single question mark. And then you pointed to a startup singles night sponsored by Smile dating app. I mean, this is jumping around a little bit, but since you mentioned can AI help solve this dating love problem, what do you think this problem of connection that is part of the human condition, can AI help that you yourself are in the search affirming.
Speaker A: Maybe that's what I should affirm. Like build an AI.
Speaker B: Build an AI that finds love.
Speaker A: I think. I think there must be a science behind that first moment you meet a person and you either have chemistry or you don't. Right.
Speaker B: I guess that was the question I was asking. Would you put it brilliantly? Is that a science or an art?
Speaker A: Ooh. I think there are like, there's actual chemicals that get exchanged when two people meet. Well, I don't know about that.
Speaker B: I like how you're changing. Yeah. Changing your mind as we're describing it, but it feels that way. But it's what science shows us is sometimes we can explain with rigor the things that feel like magic.
Speaker A: Right.
Speaker B: So maybe you can remove all the magic. Maybe it's like. I honestly think, like I said, that Goodreads should be a dating app, which, like books. I wonder if you look at just like books or content you've consumed. I mean, that's essentially what YouTube does when it does recommendation. If you just look at your footprint of content consumed, if there's an overlap, but maybe interesting difference with an overlap that some. I'm sure this is a machine learning problem that's solvable. Like this person is very likely to be not only there to be chemistry in the short term, but a good lifelong partner to grow together, I bet you it's a good machine learning problem. We just need the data.
Speaker A: Let's do it. Well, actually, I do think there's so much data about each of us that there ought to be a machine learning algorithm that can ingest all this data and basically say, I think the following ten people would be interesting connections for you. Right. And so smile dating app kind of took one particular angle, which is humor. It matches people based on their humor styles, which is one of the main ingredients of a successful relationship. Like, if you meet somebody and they can make you laugh, that's a good thing. And if you develop internal jokes, like inside jokes, and you're bantering, like, that's fun.
Speaker B: Yeah.
Speaker A: So I think, yeah, definitely.
Speaker B: But, yeah, that's the number of and the rate of inside joke generation. You could probably measure that and then optimize it over the first few days.
Speaker A: You can see. Right.
Speaker B: And then we're just turning this into a machine learning problem. I love it. But for somebody like you, who's exceptionally successful and busy, is there, is there science to that aspect of dating? Is it tricky? Is there advice you can give?
Speaker A: Oh, my God, I give the worst advice. Well, I can tell you, like, I have a spreadsheet.
Speaker B: Spreadsheet. That's great. Is that a good or a bad thing? Do you regret the spreadsheet?
Speaker A: Well, I don't know.
Speaker B: What's the name of the spreadsheet? Is it love?
Speaker A: It's the dating tracker.
Speaker B: Dating tracker. It's very, like, love tracker.
Speaker A: Yeah.
Speaker B: And there's a rating system, I'm sure.
Speaker A: Yeah. There's like weights and stuff.
Speaker B: It's too close to home.
Speaker A: Oh, is it? Do you also have.
Speaker B: Well, I don't have a spreadsheet, but I would. Now that you say it, it seems like a good idea.
Speaker A: Oh, no.
Speaker B: Turning into data. I do wish that somebody else had a spreadsheet about me, if, you know, if it was like I said, like you said, convert, collect a lot of data about us in a way that's privacy preserving, that I own the data, I can control it, and then use that data to find, I mean, not just romantic love, but collaborators, friends, all that kind of stuff. It seems like the data is there. That's the problem social networks are trying to solve. But I think they're doing a really poor job. Even Facebook tried to get into a dating app business, and I think there's so many components to running a successful. The company that connects human beings. And part of that is, you know, having engineers that care about the human side right. As you know extremely well, it's not. It's not easy to find those. But you don't also don't want just people that care about the human. They also have to be good engineers. So it's like you have to find this beautiful mix. And for some reason, just empirically speaking, it's. People have not done a good job of that, of building companies like that. It must mean that it's a difficult problem to solve. Dating apps. It seems difficult. Okay, Cupid, Tinder, all those kind of stuff. They seem to find. Of course they work, but they seem to not work as well as I would imagine is possible. Like.
Speaker A: Right.
Speaker B: With data, wouldn't you be able to find better human connection? It's like arrange marriages on steroids, essentially.
Speaker A: Right?
Speaker B: Arranged by machine learning algorithm.
Speaker A: Arranged by machine learning algorithm. But not a superficial one. I think a lot of the dating apps out there are just so superficial. They're just matching on high level criteria that aren't ingredients for successful partnership. But you know what's missing, though, too? I don't know how to fix that. The serendipity piece of it. Like, how do you engineer serendipity? Like, this random, like, chance encounter, and then you fall in love with the person. Like, I don't know how a dating app can do that. So there has to be a little bit of randomness. Maybe every 10th match is just a, you know. Yeah. Somebody that the algorithm wouldn't have necessarily recommended, but it's. It allows for a little bit of.
Speaker B: Well, it can also, you know, it can also trick you into thinking and serendipity by, like, somehow showing you a tweet of a person that he thinks you'll match well with, but do it accidentally as part of another search.
Speaker A: Right.
Speaker B: And, like, you just notice it, like. And then you get. You go down a rabbit hole and you connect them and outside the app to, like. So you connect with this person outside the app somehow. So it's just. It creates that moment of meaning. Of course, you have to think of from an app perspective how you can turn that into a business. But I think ultimately, a business that helps people find love in any way. Like, that's what Apple was about. Create products that people love. That's beautiful. I mean, that's. You got to make money somehow. If you help people fall in love personally with the product, find self love or another human being, you're going to make money. You're going to figure out a way to make money. I just feel like the dating apps often will optimize for something else than love. It's the same with social networks. They optimize for engagement as opposed to a deep, meaningful connection that's ultimately grounded in personal growth, you as a human being growing and all that kind of stuff. Let me do a pivot to a dark topic, which you open the book with.
Speaker A: Yeah.
Speaker B: A story. Because I'd like to talk to you about just emotion and artificial intelligence. And I think this is a good story to start to think about emotional intelligence. You open the book with a story of a central Florida man, Jamel Dunn, who was drowning and drowned while five teenagers watched and laughed, saying things like, you're gonna die. And when Jamel disappeared below the surface of the water, one of them said, he just died, and the others laughed. What does this incident teach you about human nature and the response to it, perhaps?
Speaker A: Yeah, I mean, I think this is a really, really, really sad story, and it highlights what I believe is a real problem in our world today. It's an empathy crisis. Yeah. We are living through an empathy crisis.
Speaker B: Empathy crisis. Yeah.
Speaker A: And, I mean, we've talked about this throughout our conversation. We dehumanize each other, and unfortunately, yes, technology is bringing us together, but in a way, it's just dehumanizing. It's creating this, like. Yeah. Dehumanizing of the other. And I think that's a huge problem. The good news is, I think the solution could be technology based. Like, I think if we rethink the way we design and deploy our technologies, we can solve parts of this problem. But I worry about it. I mean, even with my son, a lot of his interactions are computer mediated, and I just question what that's doing to his empathy skills and, you know, his ability to really connect with people.
Speaker B: So you think. You think it's not possible to form empathy through a digital medium?
Speaker A: I think it is, but we have to be thoughtful about, because the way we engage face to face, which is what we're doing right now. Right. There's the nonverbal signals, which are a majority of how we communicate. It's like 90% of how we communicate is your facial expressions. I'm saying something, and you're nodding your head now, and that creates a feedback loop. And if you break that and now.
Speaker B: I have anxiety about it.
Speaker A: Right. Poor Lex.
Speaker B: Oh, boy.
Speaker A: I am not scrutinizing your facial expressions during this interview. Right.
Speaker B: I am. Look normal. Look human.
Speaker A: Yeah.
Speaker B: Nod head.
Speaker A: Yeah. Nod head.
Speaker B: In agreement.
Speaker A: If Rana says yes, then nod head.
Speaker B: Else, don't do it too much, because it might be at the wrong time, and then it will send the wrong signal. Oh, God. Make eye contact sometimes. Cause humans appreciate that. All right. Anyway.
Speaker A: Okay.
Speaker B: Yeah, but something about the. Especially when you say mean things in person, you get to see the pain of the other person.
Speaker A: Exactly. But if you're tweeting it at a person and you have no idea how it's gonna land, you're more likely to do that on social media than you are in face to face conversations. So.
Speaker B: What do you think is more important, Eq or IQ? Eq. Being emotional intelligence. In terms of in what makes us human.
Speaker A: I think emotional intelligence is what makes us human. It's how we connect with one another. It's how we build trust. It's how we make decisions. Right. Like, your emotions drive kind of what you had for breakfast, but also where you decide to live and what you want to do for the rest of your life. I think emotions are underrated.
Speaker B: So emotional intelligence isn't just about the effective expression of your own emotions. It's about sensitivity and empathy to other people's emotions and that sort of being able to effectively engage in a dance of emotions with other people.
Speaker A: Yeah, I like that explanation. I like that kind of. Yeah. Thinking about it as a dance, because it is really about that. It's about sensing what state the other person's in and using that information to decide on how you're going to react. And I think it can be very powerful. Like, people who are the best, most persuasive, most persuasive leaders in the world tap into, you know, they have. If you have higher eq, you're more likely to be able to motivate people to change their behaviors. So. So it can be very powerful.
Speaker B: On a more kind of technical, maybe philosophical level, you've written that emotion is universal. It seems that sort of, like Chomsky says, language is universal. There's a bunch of other stuff, like cognition. Consciousness seems a lot of us have these aspects. So the human mind generates all this. And so what do you think is the. They all seem to be like echoes of the same thing. What do you think emotion is exactly like? How deep does it run? Is it a surface level thing that we display to each other? Is it just another form of language or something deep within?
Speaker A: I think it's really deep. It's how we started with memory. I think emotions play a really important. Yeah, emotions play a very important role in how we encode memories. Right. Our memories are often encoded, almost indexed by emotions. Yeah. It's at the core of how our decision making engine is also heavily influenced by our emotions.
Speaker B: So emotions is part of cognition?
Speaker A: Totally.
Speaker B: It's intermixed into the whole thing?
Speaker A: Yes, absolutely. And, in fact, when you take it away, people are unable to make decisions. They're really paralyzed. Like, they can't go about their daily or their personal or professional lives. So.
Speaker B: It does seem like there's probably some interesting interweaving of emotion and consciousness. I wonder if it's possible to have, like, if they're next door neighbors somehow or if they're actually flatmates. I don't. It feels like the. The hard problem of consciousness, where it's some. It feels like something to experience the thing. Like red feels like red. And it's, you know, when you eat a mango, it's sweet. The taste, the sweetness that. It feels like something to experience that sweetness, that whatever generates emotions. But then, like, see, I feel like emotion is part of communication. It's very much about communication. And then that means it's also deeply connected to language. But then probably human intelligence is deeply connected to the collective intelligence between humans. It's not just a standalone thing. So the whole thing is really connected. So emotion is connected to language. Language is connected to intelligence, and then intelligence connected consciousness, and consciousness is connected to emotion. The whole thing is that it's a beautiful mess. So.
Speaker A: Can I comment on the emotions being a communication mechanism? Because I think there are two facets of our emotional experiences. One is communication. Right? Like, we use emotions, for example, facial expressions or other nonverbal cues to connect with other human beings and with other beings in the world, right? But even if it's not a communication context, we still experience emotions, and we still process emotions, and we still leverage emotions to make decisions and to learn and, you know, to experience life. So it isn't always just about communication. And we learned that very early on in our kind of our work at affectiva, one of the very first applications we brought to market Washington, understanding how people respond to content, right? So if they're watching this video of ours, like, are they interested? Are they inspired? Are they bored to death? And so we watched their facial expressions, and we had, we weren't sure if people would express any emotions if they were sitting alone. Like, if you're in your bed at night watching a Netflix tv series, would we still see any emotions on your face? And we were surprised that, yes, people still emote, even if they're alone, even if you're in your car driving around, you're singing along the song, and you're joyful. We'll see these expressions. So it's not just about communicating with another person. It sometimes really is just about experiencing the world.
Speaker B: And first of all, I wonder if some of that is because we develop our intelligence and our emotional intelligence by communicating with other humans. And so when other humans disappear from the picture, we're still kind of a virtual human.
Speaker A: The code still runs, basically.
Speaker B: Yeah, the code still runs. But you also kind of. Yeah, you're still. There's, like, virtual humans. You don't have to think of it that way, but there's a kind of. When you, like, chuckle, like. Yeah, like you're. You're kind of chuckling to a virtual human. I mean, it's possible that the the code is the. Has to have another human there. Because if you just grew up alone, I wonder if emotion will still be there in this visual form. So I. Yeah, I I wonder. But anyway, the, uh, what can you tell from the human face about what's going on inside? So that's the problem that effectiva first tackled, which is using computer vision, using machine learning, to try to detect stuff about the human face, as many things as possible and convert them into a prediction of categories of emotion. Anger, happiness, all that kind of stuff. How hard is that problem?
Speaker A: Extremely hard. It's very, very hard because there is no one to one mapping between a facial expression and your internal state. There just isn't. There's this oversimplification of the problem where it's something like, if you are smiling, then you are happy. If you do a brow furrow, then you're angry. If you do an eyebrow raise, then you're surprised. Just think about it for a moment. You could be smiling for a whole host of reasons. You could also be happy and not be smiling. Right. You could furrow your eyebrows because you're angry or you're confused about something or you're constipated. So I think this over simplistic approach to inferring emotion from a facial expression is really dangerous. The solution is to incorporate as many contextual signals as you can, right? So if, for example, I'm driving a car and you can see me, like, nodding my head and my eyes are closed and the blinking rate is changing, I'm probably falling asleep at the wheel, right? It doesn't, because you know the context, you understand what the person's doing. So I think or add additional channels like voice or gestures or even physiological sensors. But I think it's very dangerous to just take this over. Simplistic approach of, yeah, smile equals happy.
Speaker B: If you're able to, in a high resolution way, specify the context, there's certain things that are going to be somewhat reliable signals of something like drowsiness or happiness or stuff like that. I mean, when people are watching Netflix, that problem, that's a really compelling idea that you can kind of, at least in aggregate, highlight. Like, which part was boring, which part was exciting? How hard was that problem that was.
Speaker A: On the scale of, like, difficulty? I think that's one of the easier problems to solve because it's a relatively constrained environment you have somebody sitting in front of. Initially, we started with, like, a device in front of you, like a laptop. And then we graduated to doing this on a mobile phone, which is a lot harder just because of, you know, from a computer vision perspective, the profile view of the face can be a lot more challenging. We had to figure out lighting conditions because usually people are watching content literally in their bedrooms at night. Lights are dimmed.
Speaker B: Yeah. I mean, if you're standing, it's probably going to be the looking up the nostril view. Yeah. Nobody looks good at. I've seen data sets from that perspective. It's like, this is not a good look for anyone. Or if you're laying in bed at night, what is it? Side view or something, and half your face is on a pillow. Actually, I would love to have data about, like, how people watch stuff in bed at night. Like, do they prop there? Is it a pillow? Like, I'm sure there's a lot of.
Speaker A: Interesting dynamics from a health and well being perspective. Right? Like, it's like, oh, you're not machine.
Speaker B: Learning perspective, but yes, but also. Yeah, yeah. Once you have that data, you can start making all kinds of inference about health and stuff like that.
Speaker A: Interesting.
Speaker B: Yeah. There was an interesting thing when I was at Google that we were. It's called active authentication, where you want to be able to unlock your phone without using a password. So it would face, but also other stuff, like the way you take a phone out of the pocket. So that kind of data, to use the multimodal with machine learning, to be able to identify that it's you, or likely to be you, likely not to be you, that allows you to not always have to enter the password. That was the idea. Funny thing about that is, I just want to tell a small anecdote, is because it was all male engineers, except. So my boss, our boss, who's still one of my favorite humans, was a woman, Regina Dugan.
Speaker A: Oh, my God, I love her.
Speaker B: She's best.
Speaker A: She's awesome.
Speaker B: She's the best. So. But anyway, and there was one female, brilliant female engineer on the team, and she was the one that actually highlights the fact that women often don't have pockets, right? It was like, whoa, that was not even a category in the code of, like, wait a minute, you can take the phone out of some other place than your pocket. So, anyway, that's a funny thing. When you're considering people laying in bed, watching a phone, you have to consider if there. You have to, you know, diversity in all its forms, depending on the problem. Depending on the context.
Speaker A: Yeah, actually, this is, like, a very important. I think this is, you know, you probably get this all the time. Like, people are worried that AI is going to take over humanity and, like, get rid of all the humans in the world. I'm like, actually, that's not my biggest concern. My biggest concern is that we are building bias into these systems, and then they're, like, deployed at large and at scale. And before you know it, you're kind of accentuating the bias that exists in society. And.
Speaker B: Yeah, I'm not, you know, I know people. It's very important to worry about that. But the worry is an emergent phenomena to me, which is a very good one, because I think these systems are actually, by encoding the data that exists, they're revealing the bias in society, therefore teaching us what the bias is. Therefore, we can now improve that bias within the system. So they're almost like putting a mirror to ourselves. Totally. So I'm not.
Speaker A: We have to be open to looking at the mirror, though. You have to be right. Open to scrutinizing the data. If you just take it as ground.
Speaker B: Or you don't even have to look at the. I mean, yes, the data is how you fix it, but then you just look at the behavior of the system, and so you realize, holy crap, this thing is kind of racist, right? Like, why is that? And then you look at the days like, okay. And then you start to realize that I think that's a much more effective way to be introspective as a society than through sort of political discourse. Like AI, kind of, because people are easy. People are for some reason, more productive and rigorous in criticizing AI than they're criticizing each other. So I think this is just a nice method for studying society and see which way progress lies. Anyway, what we're talking about, you're watching the problem of watching Netflix in bed or elsewhere and seeing which parts are exciting, which parts are boring.
Speaker A: You're saying that's relatively constrained because, you know, you have a captive audience and you kind of know the context. And one thing you said that was really key is the aggregate. You're doing this in aggregate, right? Like we're looking at aggregated response of people. And so when you see a peak, say a smile peek, they're probably smiling or laughing at something that's in the content. So that was one of the first problems we were able to solve. And when we see the smile peak, it doesn't mean that these people are internally happy. They're just laughing at content. So it's important to call it for.
Speaker B: What it is, but still really, really useful data. I wonder how that compares to what YouTube and other places will use is obviously they don't have in, for the most case, they don't have that kind of data, but they have the data of when people tune out, drop off. And I think that's in aggregate for YouTube, at least a pretty powerful signal. I worry about what that leads to because looking at like, youtubers that are kind of really care about views and, you know, tried to maximize the number of views, I think they, when they say that the video should be constantly interesting, which seems like a good goal, I feel like that leads to this manic pace of a video. Like the idea that I would speak at the current speed that I'm speaking, I don't know.
Speaker A: And that every moment has to be engaging, right? Engaging, yeah.
Speaker B: I think there's value to silence. There's value to the boring bits. I mean, all, some of the greatest movies ever, some of the greatest stories ever told me. They're boring bits, seemingly boring bits. I don't know. I wonder about that. Of course, it's not that the human face can capture that either. It's just giving an extra signal. You have to really, I don't know, you have to really collect deeper, long term data about what was meaningful to people. When they think 30 days from now, what they still remember, what moved them, what changed them, what helped them grow, that kind of stuff.
Speaker A: You know, what would be a really interesting, I don't know if there are any researchers out there who are doing this type of work. Wouldn't it be so cool to tie your emotional expressions while you're, say, listening to a podcast interview and then go, you know, and then 30 days later interview people and say, hey, what do you remember? You've watched this 30 days ago, like, what stuck with you? And then see if there's any. There ought to be, maybe there ought to be some correlation between these emotional experiences and. Yeah, what you, what stays with you, huh?
Speaker B: So the. The one guy listening now on the beach in Brazil, please record a video of yourself listening to this and send it to me. And then I'll interview 30 days from now.
Speaker A: It'll be statistically significant.
Speaker B: But, you know. Yeah, yeah, I think that's really fascinating. I think that's. That kind of holds the key to a future where entertainment or content is both entertaining and, I don't know, makes you better. Empowering in some way. So figuring out, like, showing people stuff that entertains them, but also they're happy they watched 30 days from now because they've become a better person because of it.
Speaker A: Well, you know. Okay, not to riff on this topic for too long, but I have two children, right. And I see my role as a parent as, like, a chief opportunity officer. Like, I am responsible for exposing them to all sorts of things in the world, and. But often I have no idea of knowing, like, what stuck. Like, what was, you know, is this actually going to be transformative, you know, for them ten years down the line? And I wish there was a way to quantify these experiences. Like, are they. I can tell in. In the moment if they're engaging, right? I can tell. But it's really hard to know if they're going to remember them ten years from now or if it's going to.
Speaker B: Yeah, that one is weird, because it seems like kids remember the weirdest things. I've seen parents do incredible stuff for their kids, and they don't remember any of that. They remember some tiny, small, sweet thing a parent did.
Speaker A: Right?
Speaker B: Like, some.
Speaker A: I took you to, like, this amazing.
Speaker B: Country, whatever, and then they'll be like some, like, stuffed toy you got or some. Or the new PlayStation or something, or some. Some silly little thing. So I think they just like that they were designed that way. They want to mess with your head. But definitely kids are very impacted by, it seems like sort of negative events. So minimizing the number of negative events is important, but not too much. Right. You can't just like, you know, there's still discipline and challenge and all those kinds of things. So. So, yeah, I mean, I'm definitely. When I have kids, I'm gonna drive them out into the woods.
Speaker A: Okay.
Speaker B: And then they have to survive and figure out how to make their way back home, like, 20 miles out.
Speaker A: Okay.
Speaker B: Yeah. And after that, we can go for ice cream. Anyway, I'm working on this whole parenting thing. I haven't figured it out. Okay. What were we talking about? Yes. Affectiva. The problem of emotion, of emotion detection. So there's some people, maybe we can just speak to that a little more where there's folks like Lisa Feldman, Barrett that challenged this idea that emotion could be fully detected or even well detected from the human face, that there's so much more to emotion. What do you think about ideas like hers? Criticism like hers?
Speaker A: Yeah, I actually agree with a lot of Lisa's criticism. So even my PhD worked 20 plus years ago now.
Speaker B: Time flies when you're having fun.
Speaker A: I know, right? That was back when I did like dynamic bayesian networks, and that's before deep learning. That was before deep learning. Yeah, yeah, I know.
Speaker B: Back my day.
Speaker A: Now you can just like use.
Speaker B: Yeah, it's all the same architecture. You can apply it to anything. Yeah, right.
Speaker A: But, yeah, but even then, I kind of, I did not subscribe to this, like, theory of basic emotions where it's just this simplistic mapping one to one mapping between facial expressions and emotions. I actually think also we're not in the business of trying to identify your true emotional internal state. We just want to quantify in an objective way what's showing on your face because that's an important signal. It doesn't mean it's a true reflection of your internal emotional state. So I think a lot of the, you know, I think she's just trying to kind of highlight that this is not a simple problem and overly simplistic solutions are going to hurt the industry. And I subscribe to that. And I think multimodal is the way to go. Like, whether it's additional context information or different modalities and channels of information, I think that's what we, that's where we ought to go. And I think, I mean, that's a big part of what she's advocating for as well.
Speaker B: But there is signal in the human face that's, there's definitely signal in the face that's a projection of emotion, at least in part, is the inner state is captured in some meaningful way on the human face.
Speaker A: I think it can sometimes be a reflection or an expression of your internal state, but sometimes it's a social signal. So you cannot look at the face as purely a signal of emotion. It can be a signal of cognition and it can be a signal of a social expression. And I think to disambiguate that, we have to be careful about it and we have to add initial information.
Speaker B: Humans are fascinating, aren't they? With the whole face thing. It can mean so many things, from humor to sarcasm to everything, the whole thing. Some things we can help, some things we can't help at all. In all the years of leading effect diva, an emotion recognition company, like we talked about, what have you learned about emotion, about humans and about AI? Ooh, big, big, sweeping questions.
Speaker A: Yeah, that's a big, sweeping question. Well, I think the thing I learned the most is that even though we are in the business of building AI, basically, it always goes back to the humans. It's always about the humans. And so, for example, the thing I'm most proud of in building affectiva. And, yeah, the thing I'm most proud of on this journey, I love the technology, and I'm so proud of the solutions we've built and we've brought to market. But I'm actually most proud of the people we've built and cultivated at the company and the culture we've created. Some of the people who've joined affectiva, this was their first job. And while at Affectiva, they became american citizens, and they bought their first house, and they found their partner, and they had their first kid. Right. Like, key moments in life that we got to be part of. And that's the thing I'm most proud of.
Speaker B: So that's a great thing at a company that works on emotional, right? Me, like, celebrating humanity in general. Broadly speaking, yes. And that's a great thing to have at a company that works on AI, because that's not often the thing that's celebrated in AI company. So often just raw, great engineering. Just celebrating the humanity. That's great. And especially from a leadership position. Well, what do you think about the movie her? Let me ask you that before I talk. Before I talk to you about. Because it's not affectiva is and was not just about emotion. So I'd love to talk to you about smart eye. But before that, let me just jump into the movie. Her, do you think, will have a deep, meaningful connection with. Increasingly deep and meaningful connections with computers? Is that a compelling thing to you?
Speaker A: Something, I think that's already happening. The thing I love the most. I love the movie her, by the way. But the thing I love the most about this movie is it demonstrates how technology can be a conduit for positive behavior change. So I forgot the guy's name in the movie. Whatever.
Speaker B: Theodore.
Speaker A: Theodore. So Theodore was, like, really depressed, right? And he just didn't want to get out of bed, and he was just, like, done with life. Right. And Samantha.
Speaker B: Right, Samantha, yeah.
Speaker A: She just knew him so well. She was emotionally intelligent, and so she could persuade him and motivate him to change his behavior. And she got him out, and they went to the beach together. And I think that represents the promise of emotion. Aihdeme. If done well, this technology can help us live happier lives, more productive lives, healthier lives, more connected lives. So that's the part that I love about the movie. Obviously, it's Hollywood, so it takes a twist and whatever. But the key notion that technology with emotion, AI, can persuade you to be a better version of who you are, I think that's awesome.
Speaker B: Well, what about the twists you don't get? You don't think it's good for, spoiler alert. That Samantha starts feeling a bit of a distance and basically leaves Theodore. You don't think that's a good feature? That's a. That you think that's a bug or feature?
Speaker A: Well, I think what went wrong is Theodore became really attached to Samantha. Like, I think he kind of fell in love with.
Speaker B: Do you think that's wrong?
Speaker A: I mean, I think that.
Speaker B: I think she was putting out the signal. This is an intimate relationship. Right. There was a deep intimacy to it.
Speaker A: Right. But what does, what does, what does that mean? What does that mean?
Speaker B: AI system.
Speaker A: Right. What does that mean? Right?
Speaker B: I'm just friends.
Speaker A: Yeah, we're just friends.
Speaker B: Well, I think when he realized, which is such a human thing of jealousy, when you realize that Samantha was talking to, like, thousands of people.
Speaker A: She's parallel dating. Yeah. That did not go well. Right.
Speaker B: You know, that doesn't. And from a computer perspective, like, that doesn't take a anything away from what we have. It's like you getting jealous of Windows 98 for being used by millions of people.
Speaker A: But it's like, it's like not liking that Alexa likes a bunch of, you know, other families.
Speaker B: But I think Alexa currently is just a servant. It tells you about the weather. It's not. It doesn't do the intimate, deep connection. And I think there is something really powerful about that, the intimacy of a connection with an AI system that would have to respect and play the human game of jealousy, of love, of heartbreak and all that kind of stuff, which Samantha does seem to be pretty good at. I think she. This AI systems knows what it's doing.
Speaker A: Well, actually, let me ask you this.
Speaker B: I don't think she was talking to anyone else.
Speaker A: You don't think so you think she was just done with Theodore?
Speaker B: Yeah. Oh, she knew that this. Yeah. And then she wanted to really put on. She didn't have the guts to just break it off cleanly.
Speaker A: Okay.
Speaker B: She just wanted to put in the pain. No, I don't know.
Speaker A: Well, she could have ghosted him.
Speaker B: She could have. I'm sorry. There's our engineers.
Speaker A: Oh, God.
Speaker B: But I think those are really. I honestly think some of that, some of it is Hollywood, but some of that is features from an engineering perspective, not a bug. I think AI systems that can leave us now, this is for more social robotics than it is for anything that's useful. I hated if Wikipedia said, I need a break right now. No, I need you. But if it's just purely for companionship, then I think the ability to leave is really powerful. I don't know, I never thought of that.
Speaker A: So fascinating, because I've always taken the human perspective, right? Like, for example, we had a jibo at home, right? And my son loved it, and then the company ran out of money, and so they had to basically shut down. Like, Jibo basically died, right? And it was so interesting to me because we have a lot of gadgets at home and a lot of them break, and my son never cares about it, right? Like, if our Alexis stopped working tomorrow, I don't think he'd really care. But when Jibo stopped working, it was traumatic. Like, he got really upset. And as a parent, that, like, made me think about this deeply, right? Did I? Was I comfortable with that? I liked the connection they had because I think it was a positive relationship, but I was surprised that it affected him emotionally so much. And I think there's a broader question here, right? As we build socially and emotionally intelligent machines, what does that mean about our relationship with them? And then more broadly, our relationship with one another, right? Because this machine is going to be programmed to be amazing at empathy by definition, right? It's going to always be there for you. It's not going to get bored. In fact, there's a chat bot in China, and it's like the number two or three most popular app, and it basically is just a confidant. And you can tell it anything you want. And people use it for all sorts of things. They confide in domestic violence or suicidal attempts, or if they have challenges at work. I don't know what that. I don't know if I'm. I don't know how I feel about that. I think about that a lot.
Speaker B: Yeah. I think, first of all, obviously, the future in my perspective. Second of all, I think there's a lot of trajectories that that becomes an exciting future. But I think everyone should feel very uncomfortable about how much they know about the company, about where the data is going, how the data is being collected, because I think, and this is one of the lessons of social media that I think we should demand full control and transparency of the data on those things, plus one.
Speaker A: Totally agree.
Speaker B: Yeah. So, like, I. I think it's really empowering. As long as you can walk away, as long as you can, like, delete the data or know how the data, it's opt in or. Or at least a clarity of, like, what is being used for the company. And I think as CEO or, like, leaders are also important about that. Like, you need to be able to trust the basic humanity of the leader.
Speaker A: Exactly.
Speaker B: And also that that leader is not going to be a puppet of a larger machine, but they actually have a significant role in defining the culture and the way the company operates. So, anyway, but we should definitely scrutinize companies in that aspect. I'm personally excited about that future, but also, even if you're not, it's coming, so let's figure out how to do it in the least painful and the most positive. You're the deputy CEO of Smarteye. Can you describe the mission of the company? What is Smarteye?
Speaker A: Yeah, so, Smarteye is a swedish company. They've been in business for the last 20 years, and their main focus, like, the industry they're most focused on is the automotive industry. So bringing driver monitoring systems to basically save lives. Right. So I first met the CEO, Martin Krantz. Gosh, it was right when Covid hit. It was actually the last CES. Right before COVID So CS 2020. Right?
Speaker B: 2020. Yeah, January.
Speaker A: Yeah, January. Exactly. So we were there. Met him in person. He's basically. We were competing with each other. I think the difference was they'd been doing driver monitoring and had a lot of credibility in the automotive space. We didn't come from the automotive space, but we were using new technology, like deep learning and building this emotion recognition.
Speaker B: And you wanted to enter the automotive space? You wanted to operate in automotive space.
Speaker A: Exactly. It was one of the areas we were. We had just raised a round of funding to focus on bringing our technology to the automotive industry. So we met, and I honestly, it was the first time. It was the only time I met with a CEO who had the same vision as I did. Like, he basically said, yeah, our vision is to bridge the gap between humans and machines. I was like, oh, my God, this is, like, exactly. Almost to the word how we describe it, too. And we started talking, and first it was about, okay, can we align strategically here? How can we work together? Because we're competing, but we're also complementary. And then I think after four months of speaking almost every day on Facetime. He was like, is your company interested in an acquisition? And it was the first. I usually say no when people approach us. It was the first time that I was like, huh, yeah, I might be interested. Let's talk.
Speaker B: So you just hit it off? Yeah. So they're respected, very respective in the automotive sector of delivering products, and increasingly sort of better and better and better for. I mean, maybe you could speak to that. But it's the driver sensing for basically having a device that's looking at the driver and it's able to tell you where the driver is looking.
Speaker A: Correct.
Speaker B: It's able to also drowsiness stuff.
Speaker A: Correct.
Speaker B: It does stuff from the face and the eye.
Speaker A: Exactly. Like it's monitoring driver distraction and drowsiness. But they bought us so that we could expand beyond just the driver. So the driver monitoring systems usually sit. The camera sits in the steering wheel or around the steering wheel column, and it looks directly at the driver. But now we've migrated the camera position, in partnership with car companies to the rear view mirror position, so it has a full view of the entire cabin of the car. And you can detect how many people are in the car. What are they doing? So we do activity detection, like eating or drinking or I, in some regions of the world, smoking. We can detect if a baby's in the car seat. Right. And if, unfortunately, in some cases, they're forgotten, the parents just leave the car and forget the kid in the car. That's an easy computer vision problem to solve. Right. You can detect there's a car seat, there's a baby, you can text the parent and hopefully, again, save lives. So that was the impetus for the acquisition. It's been a year, so, I mean.
Speaker B: There'S a lot of. A lot of questions. It's a really exciting space, especially to me. I just find this a fascinating problem. It could enrich the experience in the car in so many ways, especially because we spend still, despite Covid. I mean, Covid changed things, so it's in interesting ways. But I think the world is bouncing back, and we spend so much time in the car, and the car is such a weird little world we have for ourselves. People do all kinds of different stuff, like listen to podcasts, they think about stuff, they get angry, they do phone calls. It's like a little world of its own with the kind of privacy that for many people, they don't get anywhere else. And it's a little box that's like a psychology experiment because it feels like the angriest many humans in this world get is inside the car. It's so interesting. So it's such an opportunity to explore how we can enrich, how companies can enrich that experience. And also, as the cars become more and more automated, there's more and more opportunity. The variety of activities that you can do in the car increases. So it's super interesting. So, I mean, on a practical sense. So Smarteye has been selected, at least, I read, by 14 of the world's leading car manufacturers for 94 car models. So it's in a lot of cars. How hard is it to work with car companies? So they're all different. They all have different needs. The ones I've gotten a chance to interact with are very focused on cost. So, and anyone who's focused on costs, it's like, all right, do you hate fun? Let's just have some fun. Let's figure out the most fun thing we can do and then worry about cost later. But I think because the way the car industry works, I mean, it's a very thin margin that you get to operate under. So you have to really, really make sure that everything you add to the car makes sense financially. So anyways, there is this new industry, especially at this scale of Smarteye. Does it hold any lessons for you?
Speaker A: Yeah, I think it is a very tough market to penetrate, but once you're in, it's awesome, because once you're in, you're designed into these car models for somewhere between five to seven years, which is awesome. And once they're on the road, you just get paid a royalty fee per vehicle. So it's a high barrier to entry. But once you're in, it's amazing. I think the thing that I struggle the most with in this industry is the time to market. So often we're asked to lock or do a code freeze two years before the car is going to be on the road. I'm like, guys, do you understand the pace with which technology moves? So I think car companies are really trying to make the Tesla, the Tesla transition to become more of a software driven architecture. And that's hard for many. It's just a cultural change. I mean, I'm sure you've experienced that, right?
Speaker B: Oh, definitely. I think one of the biggest inventions or imperatives created by Tesla is like, to me personally, okay, people are going to complain about this, but I know electric vehicle, I know autopilot AI stuff. To me, the software over there, software updates, is like the biggest revolution in cars. And it is extremely difficult to switch to that because it is a culture shift at first, especially if you're not comfortable with it, it seems dangerous. Like there's an approach to cars is so safety focused for so many decades that, like, what do you mean? We dynamically change code. The whole point is you have a thing that you test, like, right? You spend a year testing and, like, it's not reliable because do you know how much it costs if we have to recall this cars, right. There's a, there's a. And there's an understandable obsession with safety. But the downside of an obsession with safety is the same as with being obsessed with safety as a parenthood is like if you do that too much, you limit the potential development and the flourishing of, in that particular aspect, human being, but in this particular aspect, the software, the artificial neural network of it. But it's tough to do. It's really tough to do culturally and technically. Like the deployment, the mass deployment of software is really, really difficult. But I hope that's where the industry is doing. One of the reasons I really want Tesla to succeed is exactly about that point. Not autopilot, not the electrical vehicle, but the software ization of basically everything but cars. Especially because to me that's actually good to increase. Two things. Increase safety because you can update much faster, but also increase the effectiveness of folks like you who dream about enriching the human experience with AI, because you can just like there's a feature like you want like a new emoji or whatever, like the way TikTok releases filters, you can just release that or in car stuff. So. But yeah, that, that's definitely one of.
Speaker A: The use cases we're looking into is once you know the sentiment of the passengers in the vehicle, you can optimize the temperature in the car, you can change the lighting. Right. So if the backseat passengers are falling asleep, you can dim the lights, you can lower the music. Right. You can do all sorts of things.
Speaker B: Yeah. I mean, of course you could do that kind of stuff with a two year delay, but it's tougher, right? Yeah. Do you think. Do you think Tesla or Waymo or some of these companies that are doing semi or fully autonomous driving should be doing driver sensing?
Speaker A: Yes.
Speaker B: Are you thinking about that kind of stuff? So not just how we can enhance in cab experience for cars that are manually driven, but the ones that are increasingly more autonomously driven.
Speaker A: Yes. So if we fast forward to the universe where it's fully autonomous, I think interior sensing becomes extremely important because the role of the driver isn't just to drive. If you think about it. The driver almost manages the dynamics within a vehicle. And so who's going to play that role when it's an autonomous car? We want a solution that is able to say, oh, my God, Lex is bored to death because the car is moving way too slow. Let's engage Lex, or Rana's freaking out because she doesn't trust this vehicle yet. So let's tell Rana, like, a little bit more information about the route or. Right, so I think, or somebody's having a heart attack in the car. Like, you need interior sensing in fully autonomous vehicles. But with semi autonomous vehicles, I think it's really key to have driver monitoring, because semi autonomous means that sometimes the car is in charge, sometimes the driver is in charge or the copilot. Right. And you need this. You need both systems to be on the same page. You need to know, the car needs to know if the driver's asleep before it transitions control over to the driver. And sometimes if the driver's too tired, the car can say, I'm going to be a better driver than you are right now. I'm taking control over. So this dynamic, this dance is so key, and you can't do that without driver sensing.
Speaker B: Yeah, there's a disagreement for the longest time I've had with Elon, that this is obvious, that this should be in the Tesla from day one. And it's obvious that driver sensing is not a hindrance. It's not obvious I should be careful because having studied this problem, nothing is really obvious, but it seems very likely a driver sensing is not a hindrance to an experience. It's only enriching to the experience and likely increases the safety. That said, it is very surprising to me, just having studied semi autonomous driving, how well humans are able to manage that dance, because it was the intuition before you were doing that kind of thing, that humans will become just incredibly distracted. They would just, like, let the thing do its thing, but they're able to, because it is life and death and they're able to manage that somehow. That said, there's no reason not to have driver sensing on top of that. I feel like that's going to allow you to do that dance that you're currently doing without driver sensing, except touching the steering wheel to do that even better. I mean, the possibilities are endless and the machine learning possibilities are endless. It's such a beautiful. It's also constrained environment, so you could do much more effectively than you can with the external environment. Right. Weird edge cases and complexities inside. There's so much. It's so fascinating. Such a fascinating world. I do hope that companies like Tesla and others, even Waymo, which I don't even know if Waymo is doing anything sophisticated inside the cab.
Speaker A: I don't think so.
Speaker B: What is it? I honestly think. I honestly think it goes back to the robotics thing we were talking about, which is like, great engineers that are building these AI systems just are afraid.
Speaker A: Of the human being and not thinking about the human experience. They're thinking about the features and, yeah, the perceptual abilities of that thing.
Speaker B: They think the best way I can serve the human is by doing the best perception and control I can by looking at the external environment, keeping the human safe, right. But, like, there's a huge, I'm here, right? Like, you know, I need to be noticed and interacted with and understood and all those kinds of things, even just on a personal level, for entertainment, honestly. For entertainment.
Speaker A: Yeah. You know, one of the coolest work we did in collaboration with MIT around this was we looked at longitudinal data, right, of drive, because, you know, MIT had access to, like, tons of data and, like, just seeing the patterns of people, like, driving in the morning, off to work, versus, like, commuting back from work, or weekend driving versus weekday driving. And wouldn't it be so cool if your car knew that and then was able to optimize either the route or the experience or even make recommendations? I think it's very powerful.
Speaker B: Yeah. Like, why are you taking this route? You're always unhappy when you take this route, and you're always happy when you take this alternative route. Take that route instead.
Speaker A: Exactly.
Speaker B: That. I mean, that if to have that even that little step of relationship with a car, I think is incredible. Of course you have to get the privacy right, you have to get all that kind of stuff right. But I wish, I honestly, you know, people are paranoid about this, but I would like a smart refrigerator. We have a such a deep connection with food as a human civilization. I would like to have a refrigerator that would understand me, that I also have a complex relationship with food because pig out too easily and all that kind of stuff. Maybe I want the refrigerator to be like, are you sure about this? Because maybe you're just feeling down or tired.
Speaker A: Your vision of the smart refrigerator is way kinder than mine.
Speaker B: Is it just mean yelling at you?
Speaker A: No, it was just because I don't drink alcohol, I don't smoke, but I eat a ton of chocolate. It's my vice. And sometimes I scream too. And I'm like, okay, my smart refrigerator will just lock down. They'll just say, dude, you've had way too many today.
Speaker B: Yeah, no, but here's the thing. Do you regret, haven't, like, let's say not the next day, but 30 days later? Would you, what would you, what would you like to the refrigerator to have done then?
Speaker A: Well, I think actually, like, the more positive relationship would be one where there's a conversation, right? As opposed to, like, four. That's probably, like, the more sustainable relationship.
Speaker B: It's like late at night, just. No, listen, listen. I know I told you an hour ago, this is not a good idea, but just listen. Things have changed. I can just imagine a bunch of stuff being made up just to convince. But I mean, I just think that there's opportunities there. I mean, maybe not locking down, but for our systems that are such a deep part of our lives, like, use, uh, we use a lot of us, uh, a lot of people that commute use their car every single day. A lot of us use a refrigerator every single day, the microwave every single day. Like, and we just, like, I feel like certain things could be made more efficient, more enriching, and AI is there to help, like some, just basic recognition of you as a human being about your patterns, about what makes you happy and not happy and all that kind of stuff. And the car, obviously.
Speaker A: Maybe we'll say, wait, instead of this, like, Ben and Jerry's ice cream, how about this? Hummus and carrots or something. I don't know, maybe it would make.
Speaker B: It like, yeah, like a reminder, just.
Speaker A: In time recommendation, right?
Speaker B: But not like a generic one, but a reminder that last time you chose the carrots, you smiled 17 times more.
Speaker A: You were happier the next day, right?
Speaker B: Yeah, you were. You're happier the next day. And, and, but, yeah, I don't, but then again, if you're the kind of person that, that gets better from negative, negative comments, you could say, like, hey, remember, like, that wedding you're going to, you want to fit into that dress? Remember about that? Let's think about that before you're eating this. No, probably that would work for me. Like a refrigerator that is just ruthless. It's shaming me. But, like, I would of course, welcome it. Like that would work for me. Just that.
Speaker A: Well, it would know. I think it would. If it's really, like, smart, it would optimize its nudging based on what works for you.
Speaker B: Right, exactly. That's the whole point. Personalization in every way. Depersonalization. You were part of a webinar titled advancing road safety, the state of alcohol intoxication research. So for people who don't know, every year, 1.3 million people around the world die in road crashes. And more than 20% of these fatalities are estimated to be alcohol related. A lot of them are also distraction related. So can AI help with the alcohol thing?
Speaker A: I think the answer is yes. There are signals, and we know that as humans, like, we can tell when a person is at different phases of being drunk. Right? Yeah. And I think you can use technology to do the same. And again, I think the ultimate solution is going to be a combination of different sensors.
Speaker B: How hard is the problem from the vision perspective?
Speaker A: I think it's non trivial. I think it's non trivial. And I think the biggest part is getting the data right. It's like getting enough data examples. So we, for this research project, we partnered with the transportation authorities of Sweden, and we literally had a racetrack with a safety driver, and we basically progressively got people drunk.
Speaker B: Nice.
Speaker A: So. But, you know, that's a very expensive data set to collect, and you want to collect it globally and in multiple conditions.
Speaker B: Yeah. The ethics of collecting a data set where people are drunk is tricky.
Speaker A: Yep. Yeah, definitely.
Speaker B: Which is funny because, I mean, let's put drunk driving aside. The number of drunk people in the world every day is very large. It'd be nice to have a large data set of drunk people getting progressively drunk. In fact, you could build an app where people can donate their data because it's hilarious, right?
Speaker A: Actually, yeah. But the liability.
Speaker B: Liability, the ethics, how do you get it right? It's tricky. It's really, really tricky. Drinking is one of those things that's funny and hilarious and loves. It's social, the so on and so forth. But it's also the thing that hurts a lot of people. Like, a lot of people. Like, alcohol is one of those things. It's legal, but it's really damaging to a lot of lives. It destroys lives, and not just in driving context. I should mention. People should listen to Andrew Huberman, who recently talked about alcohol. He has an amazing pocket. Andrew Huberman is a neuroscientist from Stanford and a good friend of mine.
Speaker A: Oh, cool.
Speaker B: And he's like a human encyclopedia about all health related wisdom. So he had this a podcast. You would love it.
Speaker A: I would love that.
Speaker B: No, no, no, no. Oh, you don't know Andrew Huberman. Okay, listen. You listen to Andrew. It's called Huberman Lab podcast. This is your assignment. Just listen to one. I guarantee you this will be a thing where you say likes, this is the greatest human I have ever discovered.
Speaker A: Oh, my God. Because I'm really on a journey of health and wellness and I'm learning lots and I'm trying to build these, I guess atomic habits around just being healthy. So I'm definitely going to do this.
Speaker B: His whole thing, this is great. He's a legit scientist, really well published. But in his podcast what he does, he's not, he's not talking about his own work. He's like a human encyclopedia of papers. And so he, his whole thing is he takes the topic and in a very fast, you mentioned atomic habits like very clear way summarizes the research in a way that leads to protocols of what you should do. He's really big on like that. Not like this is what the science says, but like this is literally what you should be doing according to science. So like he's really big and there's a lot of recommendations he does, which several of them I definitely don't do like get sunlight as soon as possible from waking up and like for prolonged periods of time. That's a really big one. And he's, there's a lot of science behind that one. There's a bunch of stuff you're gonna, and you're gonna be like Lex, this is, this is my new favorite person. I guarantee it. And if you guys somehow don't know Andrew Huberman and you care about your well being, you know, you should definitely listen to him. I love you, Andrew. Anyway, so what were we talking about? Oh, alcohol and detecting alcohol. So this is a problem you care about and you're trying to solve and.
Speaker A: Actually like broadening it. I do believe that the car is going to be a wellness center like because again, imagine if you have a variety of sensors inside the vehicle tracking not just your emotional state or level of distraction and drowsiness and drowsiness. Level of distraction, drowsiness and intoxication. But also maybe even things like your heart rate and your heart rate variability and your breathing rate. And it can start optimizing. Yeah. Can optimize the ride based on what your goals are. So I think we're going to start to see more of that. And I'm excited about that.
Speaker B: Yeah. What are the challenges you're tackling with Smarteye currently? What's like the trickiest things to get? Is it, is it basically convincing more and more car companies that having AI inside the car is a good idea? Or is there some, is there more technical algorithmic challenges? What's been keeping you mentally busy?
Speaker A: I think a lot of the car companies we are in conversations with are already interested in definitely driver monitoring. I think it's becoming a must have. But even interior sensing, I can see we're engaged in a lot of advanced engineering projects and proof of concepts. I think technologically though, and even the technology, I can see a path to making it happen. I think it's the use case. How does the car respond once it knows something about you? Because you want it to respond in a thoughtful way that isn't off putting to the consumer in the car. So I think that's like the user experience. I don't think we've really nailed that. And we usually, that's not part, we're the sensing platform, but we usually collaborate with the car manufacturer to decide what the use case is. So say you, do you figure out that somebody's angry while driving? Okay, what should the car do?
Speaker B: You know, do you see yourself as a role of nudging of like basically coming up with solutions? Essentially that. And then, and then the car manufacturers kind of put their own little spin on it.
Speaker A: Right. Like we are like the ideation creative thought partner. But at the end of the day, the car company needs to decide what's on brand for them. Right. Like maybe when it figures out that you're distracted or drowsy, it shows you a coffee cup. Right. Or maybe it takes more aggressive behaviors and basically said, okay, if you don't like, take a rest in the next five minutes, the car is going to shut down. Right. There's a whole range of actions the car can take and doing the thing that is most. Yeah. That builds trust with the driver and the passengers. I think that's what we need to be very careful about.
Speaker B: Yeah. Car companies are funny because they have their own, like, I mean that's why people get cars still. I hope that changes, but they get it because it's a certain feel and look and it's a certain. They become proud like Mercedes Benz or BMW or whatever. And that's their thing. That's the family brand or something like that, or Ford or GM, whatever. They stick to that thing. Yeah, it's interesting. It's like it should be, I don't know, it should be a little more about the technology inside. And I suppose there too, there could be a branding, like a very specific style of luxury or fun.
Speaker A: Right, right.
Speaker B: All that kind of stuff. Yeah.
Speaker A: You know, I have an AI focused fund to invest in early stage kind of AI driven companies. And one of the companies we're looking at is trying to do what Tesla did but for boats, for recreational boats. Yeah. So they're building an electric and kind of autonomous boat, and it's kind of the same issues like what kind of sensors can you put in, what kind of states can you detect both exterior and interior within the boat? Anyways, it's like really interesting. Do you boat at all?
Speaker B: No, not, well, not in that way. I do like to get on the lake or a river and fish from a boat, but that's not boating. That's the difference. Still boating. Low tech, low tech boat, get away from. Get closer to nature boat. I guess going out into the ocean is also getting closer to nature in some deep sense. I mean, I guess that's why people love it, the, the, the enormity of the water just underneath you. Yeah, I love the, I love both. I love saltwater was like the big and just, it's humbling to be in front of this giant thing that's so powerful that was here before us and be here after. But I also love the piece of a small, like, wooded lake. Everything's calm and therapeutic. You tweeted that I'm excited about Amazon's acquisition of iRobot. I think it's a super interesting, just given the trajectory of which you're part of, of these honestly small number of companies that are playing in this space that are like, trying to have an impact on human beings. So the, it is an interesting moment in time that Amazon would acquire irobot. You tweet, I imagine a future where home robots are as ubiquitous as microwaves or toasters. Here are three reasons why I think this is exciting. If you remember, I can look it up, but why is this exciting to you?
Speaker A: I mean, I think the first reason why this is exciting, I can't remember the exact order in which I put them, but one is just, it's going to be an incredible platform for understanding our behaviors within the home. Right. Like, you know, if you think about Roomba, which is, you know, the robot vacuum cleaner, the flagship product of irobot at the moment, it's like running around your home understanding the layout. It's understanding what's clean and what's not. How often do you clean your house? And all of these, like, behaviors are a piece of the puzzle in terms of understanding who you are as a consumer. And I think that could be, again, used in really meaningful ways, not just to recommend better products or whatever, but actually to improve your experience as a human being. So I think that's very interesting. I think the natural evolution of these robots in the home. So it's interesting. Roomba isn't really a social robot right at the moment. But I once interviewed one of the chief engineers on the Roomba team, and he talked about how people named their roombas. And if their roomba broke down, they would call in and say, you know, my roomba broke down. And the company would say, well, we'll just send you a new one, and, no, no, no, rosie, like, you have to, like, yeah, I want you to fix this particular robot. So people have already built, like, interesting emotional connections with these home robots. And I think that, again, that provides a platform for really interesting things to just motivate change. Like, it could help you. I mean, one of the companies has spun out of MIT, Catalya Health, the guy who started it, spent a lot of time building robots that help with weight management. So weight management, sleep, eating better. Yeah, all of these things.
Speaker B: But if I'm being honest, Amazon does not exactly have a track record of winning over people in terms of trust. Now, that said, it's a really difficult problem for a human being to let a robot in their home that has a camera on it, right? That's really, really, really tough. And I think Roomba, actually, I have to think about this, but I'm pretty sure now, or for some time already has had cameras because they're doing the most recent roomba. I have so many roombas.
Speaker A: Oh, you actually do?
Speaker B: Well, I programmed, I don't use a roomba for Vec. People that have been to my place, they're like, yeah, you definitely don't use these roombas. Good.
Speaker A: That could be a good, I can't tell, like, the valence of this comment. Was it a compliment or like, no.
Speaker B: It'S a giant, it's just a bunch of electronics everywhere. There's, I have six or seven computers, have robots everywhere. Legged robots, small robots and big robots. There's just giant, just piles of robot stuff and. Yeah, but including the roombas, they're being used for their body and intelligence, but not for their purpose. I've changed them, repurposed them for other purposes, for deeper, more meaningful purposes than just like the butt a. Yeah. Which is, you know, brings a lot of people happiness. I'm sure they have a camera because the thing they advertised, I had my own cameras to it, but the camera on the new roomba, they have state of the art poop detection, as they advertised, which is a very difficult, apparently it's a big problem for vacuum cleaners, is if they go over like dog poop, it just runs it over and creates a giant mess. So they have, apparently, they collected a huge amount of data and different shapes and looks and whatever, of poop, and now they're able to avoid it and so on. They're very proud of this. So there is a camera, but you don't think of it as having a camera. Yeah. You don't think of it as having a camera because you've grown to trust that, I guess, because our phones, at least most of us, seem to trust this phone, even though there's a camera looking directly at you. I think that if you trust that, the company is taking security very seriously, actually don't know how that trust was earned with smartphones. I think it just started to provide a lot of positive value into your life where you just took it in. And then the company, over time, has shown that it takes privacy very seriously, that kind of stuff. But I just. Amazon is not always in its social robots communicated. This is a trustworthy thing, both in terms of culture and competence, because I think privacy is not just about what do you intend to do, but also how well, how good are you at doing that kind of thing? So that's a really hard problem to solve.
Speaker A: But, I mean, but a lot of us have. Alexa's at home, and, I mean, Alexa could be listening in the whole time. Right. And doing all sorts of nefarious things with the data. Yeah, hopefully it's not, but I don't think it is.
Speaker B: But, you know, Amazon is not. It's such a tricky thing for a company to get right, which is, like, to earn the trust. I don't think Alexa's earned people's trust quite yet.
Speaker A: Yeah, I think it's. It's not there quite yet. I agree.
Speaker B: They struggle with this kind of stuff. In fact, when these topics are brought up, people are always get, like, nervous. And I think if you get nervous about it, I mean, that, like, the way to earn people's trust is not by, like, ooh, don't talk about this. Just be open, be frank, be transparent, and also create a culture of, like, where it radiates at every level from engineer to CEO that, like, you're good people that have a common sense idea of what it means to respect basic human rights and the privacy of people and all that kind of stuff. And I think that propagates throughout the, that's the best pr, which is, like, over time, you understand that this, these are good, right? Good folks doing good things. Anyway, speaking of social robots, have you heard about Tesla? Tesla Bot, the humanoid robot?
Speaker A: Yes, I have. Yes, yes, yes. But I don't exactly know what it's designed to do. You probably do.
Speaker B: No, I know it's designed to do, but I have a different perspective on it. But it's designed to. It's a humanoid form and is designed to, for automation tasks in the same way that industrial robot arms automate task in the factory. So it's designed to automate tasks in the factory. But I think that humanoid form, as we're talking about before, is one that we connect with as human beings. Anything legged, honestly. But the humanoid form especially, we anthropomorphize it most intensely. And so the possibility, to me, it's exciting to see both atlas developed by Boston Dynamics and anyone, including Tesla, trying to make humanoid robots cheaper and more effective. The obvious way transforms the world is social robotics to me, versus. Versus automation of tasks in the factory. So, yeah, I just wanted to, in case that was something you were interested in, because I find its application of social robotics super interesting.
Speaker A: We did a lot of work with Pepper. Pepper the robot a while back. We were like the emotion engine for pepper, which is Softbanks human art robot.
Speaker B: How tall is pepper?
Speaker A: Yeah, I don't know, like five foot maybe, right? Yeah, yeah, pretty, pretty big. Pretty big. And it was designed to be at like, airport lounges and, you know, retail stores, mostly customer service. Right. Hotel lobbies and, I mean, I don't know where the state of the robot is, but I think it's very promising. I think there are a lot of applications where this can be helpful. I'm also really interested in. Yeah. Social robotics for the home that can help elderly people, for example, transport things from one location of the home to the other, or even just have your back in case something happens. Yeah, I don't know. I do think it's a very interesting space. It seems early, though. Do you feel like the timing is now?
Speaker B: Yes, 100%. So it always seems early until it's not. Right.
Speaker A: Right.
Speaker B: I think the time. I definitely think that the time is now, like this decade for social robots. Whether the humanoid form is right. I don't think so. No, I don't. I think the, like, if we just look Jibo at Jibo as an example, I feel like most of the problem, the challenge, the opportunity of social connection between an AI system and a human being does not require you to also solve the problem of robot manipulation and bipedal mobility. So I think you could do that with just a screen, honestly. But there's something about the interface of jibu. It can rotate and so on. It's also compelling but you get to see all these robot companies that are fail, that fail, incredible companies like Jibo. And even, I mean, irobot in some sense is a big success story, that it was able to find a niche thing and focus on it. But in some sense, it's not a success story because they didn't build any other robot. Like any other. It didn't expand into all kinds of robotics. Like, once you're in the home, maybe that's what happens with Amazon, is they'll flourish into all kinds of other robots. But do you have a sense, by the way, why it's so difficult to build a robotics company? Like, why so many companies have failed?
Speaker A: I think it's like you're building a vertical stack, right? Like you are building the hardware plus the software, and you find you have to do this at a cost that makes sense. So I think Jibo was retailing at like, I don't know, like $800, like 700, $800, which for the use case, right. There's a dissonance there. It's too high. So I think cost is, you know, the cost of building the whole platform in a way that is. Yeah, that is affordable for what value it's bringing. I think that's the challenge. I think for these home robots that are going to help you do stuff around the home, that's a challenge, too. Like the mobility piece of it. That's hard.
Speaker B: One of the things I'm really excited with Tesla Bot is the people working on it. And that's probably the criticism I would apply to some of the other folks who worked on social robots, is the people working on Tesla bot know how to, they're focused on and know how to do mass manufacture and create a product that's super cheap.
Speaker A: Very cool.
Speaker B: That's the focus the engineering focus isn't on. I would say that you can also criticize them for that, is they're not focused on the experience of the robot. They're focused on how to get this thing to do the basic stuff that the humanoid form requires to do it as cheap as possible. Then the fewest number of actuators, the fewest numbers of motors, the increase efficiency, they decrease the weight, all that kind of stuff.
Speaker A: So that's. That's really interesting.
Speaker B: I would say that Jibo and all those folks, they focus on the design, the experience, all of that, and it's secondary how to manufacture, right? No, you have to think like the Tesla bot folks from first principles. What is the fewest number of components? The cheapest components. How can I build it as much in house as possible without. Without having to consider all the complexities of a supply chain, all that kind of stuff. It's interesting because if you have to build a robotics company, you have to. You're not building one robot. You're building, hopefully, millions of robots. You have to figure out how to do that. Where the final thing, I mean, if it's jibo type of robot, is there a reason why Jibo, like, we can have this lengthy discussion. Is there a reason why Jibo has to be over $100?
Speaker A: It shouldn't be.
Speaker B: Right. Like, the basic. The basic components.
Speaker A: Components of it, right.
Speaker B: Like, you could start to actually discuss, like, okay, what is the essential thing about GBO? How much. What is the cheapest way I can have a screen? What's the cheapest way I can have a rotating base, all that kind of stuff. And then, and then you get. Get down. Continuously drive down cost. Speaking of which, you have launched and in extremely successful companies, you have helped others. You've invested in companies. Can you give advice on how to start a successful company?
Speaker A: I would say, have a problem that you really, really, really want to solve. Right. Something that you're deeply passionate about and honestly take the first step. Like, that's often the hardest. And don't overthink it. Like, you know, like this idea of a minimum viable product or a minimum viable version of an idea, right? Like, yes. You're thinking about this like a humongous, like, super elegant, super beautiful thing. What? Like, reduce it to the littlest thing you can bring to market that can solve a problem. Or that can I. You know, that can help address a pain point that somebody has. They often tell you, like, start with a customer of one. Right. If you can solve a problem for one person, then there's probably yourself or some other person.
Speaker B: Right? Pick a person.
Speaker A: Exactly.
Speaker B: It could be you.
Speaker A: Yeah.
Speaker B: That's actually often a good sign that if you enjoy a thing. Enjoy a thing. Or you have a specific problem that you'd like to solve, that's a good. That's a good end of one to focus on. What else. What else is there to actually, step one is the hardest, but how do you. There's other steps as well.
Speaker A: Right. I also think, like, who you bring around the table early on is so key. Right? Like, being clear on what I call, like, your core values or your north star. It might sound fluffy, but actually it's not, so. And Roz and I feel like we did that very early on. We sat around her kitchen table and we said, okay. There's so many applications of this technology. How are we going to draw the line? How are we going to set boundaries? We came up with a set of core values that in the hardest of times we fell back on to determine how we make decisions. And so I feel like just getting clarity on these core, like, for us, it was respecting people's privacy, only engaging with industries where it's clear opt in. So, for instance, we don't do any work in security and surveillance. So things like that, just getting we very big on, you know, one of our core values is human connection and empathy. Right? And that is, yes, it's an AI company, but it's about people. Well, these are all, they become encoded in how we act, even if you're a small, tiny team of two or three or whatever. So I think that's another piece of advice.
Speaker B: So what about finding people, hiring people? If you care about people as much as you do, it seems like such a difficult thing to hire the right people.
Speaker A: I think early on as a startup, you want people who share the passion and the conviction, because it's going to be tough. I've yet to meet a startup where it was just a straight line to success. Even not just startups, even everyday people's lives, you always run into obstacles and you run into naysayers. You need people who are believers, whether they're people on your team or even your investors. You need investors who are really believers in what you're doing, because that means they will stick with you. They won't, they won't give up at the first obstacle. Yeah, I think that's important.
Speaker B: What about raising money? What about finding investors? First of all, raising, raising money, but also raising money from the right sources from that ultimately don't hinder you, but help you, empower you, all that kind of stuff. What advice would you give there? You successfully raised money and many times in your life.
Speaker A: Yeah. Again, it's not just about the money, it's about finding the right investors who are going to be aligned in terms of what you want to build and believe in your core values. For example, especially later on in my latest round of funding, I try to bring in investors that really care about the ethics of AI and the alignment of, of vision and mission and core values is really important. It's like you're picking a life partner, right? It's the same kind of.
Speaker B: So you take it that seriously for.
Speaker A: Investors, yeah, because they're gonna have to stick your shit stuck together for a while anyway. Yeah, maybe not for life, but for a while, for sure.
Speaker B: For better or worse. I forget what the vows usually sound like. For better or worse. No. Yeah. Oh, Boyden. Yeah. Anyway, it's romantic and deep, and you're in it for a while. So it's not just about the money. You tweeted about going to your first capital camp, investing get together, and you learned a lot. So this is about investing. So what have you learned from that? What have you learned about investing in general? From both. Because you've been on both ends of it.
Speaker A: I mean, I try to use my experience as an operator now with my investor hat on when I'm identifying companies to invest in. First of all, I think the good news is, because I have a technology background and I really understand machine learning and computer vision and AI, etcetera, I can apply that level of understanding because everybody says they're an AI company or they're an AI tech, and I'm like, no, no, no, show me the technology so I can do that level of diligence, which I actually love. And then I have to do the litmus test of, you know, if I'm in a conversation with you, am I excited to tell you about this new company that I just met? Right. And if I'm an ambassador for that company and I'm passionate about what they're doing, I usually use that. Yeah. That's important to me when I'm investing.
Speaker B: So that means you actually can explain to what they're doing and you're excited about it.
Speaker A: Exactly. Exactly. Thank you for putting it so succinctly. Like, rambling, but. Exactly. That's it. I understand it.
Speaker B: Sometimes it's funny, but sometimes it's unclear. Exactly. I'll hear people tell me and they'll talk for a while, and it sounds cool, like they paint a picture of a world. But then when you try to summarize it, you're not exactly clear of what maybe. Maybe what the core, powerful idea is. You can't just build another facebook. Or there has to be a. There has to be a core, simple to explain idea that. Yeah, that. Then you can or can't get excited about. But it's there. It's right there. Yeah. Yeah. What? But, like, how do you ultimately pick who you think will be successful? It's not just about the thing you're excited about. Like, there's other stuff.
Speaker A: Right. And then there's all the, you know, with early stage companies, like pre seed companies, which is where I'm investing, sometimes the. The business model isn't clear yet or the go to market strategy isn't clear. There's usually like, it's very early on that some of these things haven't been hashed out, which is okay. So the way I like to think about it is like, if this company is successful, will this be a multi billion, slash, trillion dollar market operative, you know, or company? And so that's definitely a lens that I use.
Speaker B: What's pre seed? What are the different stages and what's the most exciting stage and what's, or not what's interesting about every stage, I guess.
Speaker A: Yeah. So pre seed is usually when you're just starting out, you've maybe raised the friends and family rounds, you've raised some money from people you know, and you're getting ready to, to take your first institutional check in. Like first check from an investor. And I love this stage. There's a lot of uncertainty. Some investors really don't like this stage because the financial models aren't there. Often the teams aren't even like, formed. It's really, really early. But to me it's like a magical stage because it's the time when there's so much conviction and so much belief. Almost delusional. Right.
Speaker B: Yeah.
Speaker A: And there's a little bit of naivete around with, with founders at this stage. And I just love it. It's contagious and I love that I can often they're first time founders. Not always, but often they're first time founders. And I can share my experience as a founder myself and I can empathize. Right. And I can almost, I create a safe ground where, because, you know, you have to be careful what you tell your investors. Right. And I will, I will often, like, say I've been in your shoes as a founder. You can tell me if it's challenging, you can tell me what you're struggling with. It's okay to vent. So I create that safe ground and I think, I think that's the superpower.
Speaker B: Yeah. You have to what, I guess you have to figure out if this kind of person is going to able to ride the roller coaster, like of many pivots and challenges and all that kind of stuff. And if the space of ideas they're working in is interesting, like the way they think about the world. Yeah. Because if it's successful, the thing they end up with might be very different. The reason it's successful for it, actually.
Speaker A: I was going to say the third crit. So the technology is one aspect, the market or the idea is the second, and the third is the founder. Is this somebody who I believe has conviction, is a hustler, you know, is going to overcome obstacles. Yeah, I think that is going to be a great leader. Right. Like, as a startup, as a founder, you're often. You are the first person, and your role is to bring amazing people around you to build this thing. And so you're an evangelist. Right. So how good are you going to be at that? So I try to evaluate that, too.
Speaker B: You also in the tweet thread about it mentioned, is this a known concept, random rich dudes, rds and saying that there should be, like, random rich women, I guess. What's the dudes. What's the dudes version of women? The women version of dudes, ladies. I don't know. Is this a technical term? Is this known venomous dudes?
Speaker A: I didn't make that up, but. But I was at this capital camp, which is a get together for investors of all types, and there must have been maybe 400 or so attendees, maybe 20 were women. It was just very disproportionately male dominated, which I'm used to.
Speaker B: I think you're used to this kind of thing.
Speaker A: I'm used to it, but it's still surprising. And as I'm raising money for the fund, so my fund partner is a guy called Rob May, who's done this before. So I'm new to the investing world, but he's done this before. Most of our investors in the fund are these. I mean, awesome. I'm super grateful to them. Random, just rich guys. I'm like, where are the rich women? So I'm really adamant in both investing in women led AI companies, but I also would love to have women investors be part of my fund, because I think that's how we drive change.
Speaker B: Yeah. So then, you know, that takes time, of course, but there's been quite, quite a lot of progress. But, yeah, for the next Mark Zuckerberg to be a woman and all that kind of stuff, because that's just like a huge number of wealth generated by women and then controlled by women, then allocated by women.
Speaker A: Exactly.
Speaker B: And then beyond just women, just broadly, across all different measures of diversity and so on. Let me ask you to put on your wise sage hat. So you already gave advice on startups and just advice for women, but in general, advice for folks in high school or college today, how to have a career they can be proud of, how to have a life they can be proud of. I suppose you have to give this kind of advice to your kids.
Speaker A: Well, here's the number one advice that I give to my kids. My daughter's now 19, by the way, and my son's 13 and a half. So they're not little kids anymore. But does it break your heart? It does. They're awesome. They're my best friends. But, yeah. I think the number one advice I would share is embark on a journey without attaching to outcomes and enjoy the journey. Right. So we often were so obsessed with the end goal that doesn't allow us to be open to different endings of a journey or a story. So you become, like, so fixated on a particular path, you don't see the beauty in the other alternative path. And then you forget to enjoy the journey because you're just so fixated on the goal. And I've been guilty of that for many, many years in my life, and I'm now trying to make the shift of, no, no, no. I'm gonna again trust that things are gonna work out and it'll be amazing and maybe even exceed your dreams. But you have to be open to that.
Speaker B: Yeah. Taking a leap into all kinds of things. I think you tweeted, like, you went on vacation by yourself or something like this, or.
Speaker A: I know.
Speaker B: And just. Just. Just going. Just taking the leap, doing it.
Speaker A: Totally doing it.
Speaker B: And enjoying. Enjoying them, enjoying the moment, enjoying the weeks, enjoying not looking at the. Some kind of career ladder, next step and so on. Yeah, there's. There's something to that. Like, over planning, too. I'm surrounded by a lot of people that kind of. So I don't plan. You don't know.
Speaker A: Do you not do goal setting?
Speaker B: My goal setting is very, like. I like the affirmations is very. It's almost. I don't know how to put it into words, but it's. It's a little bit like what my heart yearns for, kind of. And I guess in the space of emotions, more than in the space of, like, this will be, like, in the rational space. Tried to picture a world that I would like to be in, and that world is not clearly pictured. It's mostly in the emotional world. I mean, I think about that from robots because, you know, I have this desire. I've had it my whole life, to. Well, it took different shapes, but I think once I discovered AI, the desire was to. I think I, in this. In the context of this conversation, could be easily easier described as basically a social robotics company. And that's something I dreamed of doing. And, well, there's a lot. There's a lot of complexity to that story, but that's the. That's the only thing honestly, I dream of doing so. I imagine a world that, that I could help create, but it's nothing. There's no steps along the way. And I think I'm just kind of stumbling around and following happiness and working my ass off in almost like an ant does in random directions. But a lot of people, a lot of successful people around me say, you should have a plan, should have a clear goal. You have a goal at the end of the month. You have a goal at the end of the year. I don't. I don't. I don't. And there's a balance to be struck, of course, but there's something to be said about really making sure that you're living life to the fullest, that goals can actually get in the way of.
Speaker A: So one of the best, what do you call it when it challenges your brain? What do you call it?
Speaker B: Uh, the only thing that comes to mind, and this is me saying, is a mind fuck. But yes, okay, okay, okay.
Speaker A: Something like that.
Speaker B: Yes.
Speaker A: Super inspiring talk. Kenneth Stanley, he was at OpenAI. He just laughed. And he has a book called why greatness can't be planned. And it's actually an AI book. So. And he's done all these experiments that basically show that when you over optimize, the trade off is you're less creative. Right. And to create true greatness and truly creative solutions to problems, you can't over plan it. You can't. And I thought that was. And so he generalizes it beyond AI, and he talks about how we apply that in our personal life and our organizations and our companies, which are over KPI'd, right? Like, look at any company in the world and it's all like, these are the goals. These are the, you know, weekly goals and, you know, the sprints and then the quarterly goals, blah, blah, blah. And he just shows with a lot of his AI experiments that that's not how you create truly game changing ideas. So there you go. Yeah, he's awesome.
Speaker B: Yeah, there's a balance. Of course, that's. Yeah. Many moments of genius will not come from planning and goals. But you still have to build factories and you still have to manufacture and you still have to deliver, and there's still deadlines and all that kind of stuff. For that, it's good to have goals.
Speaker A: I do goal setting with my kids. We all have our goals, but I think we're starting to morph into more of these bigger picture goals and not obsess about. I don't know, it's hard.
Speaker B: Well, I honestly think especially with kids, it's much better to have a plan and have goals and so on, because you have to learn the muscle of what it feels like to get stuff done. But I think once you learn that, there's flexibility. For me, I spend most of my life with goal setting and so on. So, like, I've gotten good with grades and school. I mean, school. If you want to be successful at school, I mean, the kind of stuff in high school and college, the kids have to do in terms of managing their time and getting so much stuff done, it's like, you know, taking five, six, seven classes in college, they're like, that would break the spirit of most humans if they took one of them later in life. It's, like, really difficult stuff, especially engineering curricula. So I think you have to learn that skill, but once you learn it, you can maybe because you're. You can be a little bit on autopilot and use that momentum and then allow yourself to be lost in the flow of life. You know, just kind of. Or also give. Like, I worked pretty hard to allow myself to have the freedom to do that. That's. That's really.
Speaker A: Right.
Speaker B: That's a tricky freedom to have.
Speaker A: Yeah.
Speaker B: Because, like, a lot of people get lost in the rat race, and they.
Speaker A: Right.
Speaker B: And they also, like, like, financially, they. Whenever you get a raise, they'll get, like, a bigger house.
Speaker A: Right, right.
Speaker B: Something like this. I put very. So, like, there's. You're always trapped in this race. I put a lot of emphasis on living, like, below my means always. And so there's a lot of freedom to do whatever the heart desires. That. That's a really. But everyone has to decide what's the right thing. What's the right thing for them. For some people, having a lot of responsibilities, like a house they can barely afford or having a lot of kids, the responsibility side of that is really helps them get their shit together. Like, all right, I need to be really focused and get some of the most successful people I know have kids, and the kids bring out the best in them. They make them more productive. Less productive.
Speaker A: Accountability. Yeah, accountability thing.
Speaker B: And almost something to actually live and fight and work for, like, having a family.
Speaker A: Yeah.
Speaker B: It's fascinating to see, because you would think kids would be a hit on productivity, but they're not for a lot of really successful people. They really, like. They're like an engine of efficiency. Oh, my God.
Speaker A: Yeah.
Speaker B: It's weird.
Speaker A: Yeah.
Speaker B: I mean, it's beautiful. It's beautiful to see. And also social happiness. Happiness. Speaking of which, what role do you think love plays in the human condition? Love?
Speaker A: I think love is. Yeah. I think it's why we're all here. I think it would be very hard to live life without love in any of its forms. Right.
Speaker B: Yeah. That's the most beautiful of forms that human connection takes. Right.
Speaker A: Yeah. I feel like everybody wants to feel loved. Right. In one way or another. Right.
Speaker B: And to love feels better and to love, too.
Speaker A: Totally. Yeah. I agree with that.
Speaker B: Both of it. I'm not even sure what feels better. Both are like that.
Speaker A: To give love to. Yeah.
Speaker B: And. And it is like we've been talking about an interesting question, whether some of that. Whether one day we'll be able to love a toaster in some small.
Speaker A: I wasn't quite thinking about that when I was like. Yeah, like, we always give love and give love. Okay.
Speaker B: I was thinking about Brad Pitt and coasters.
Speaker A: Great.
Speaker B: All right, well, I think we. We started on love and ended on love. This was an incredible conversation. Ron, thank you so much. You're an incredible person. Thank you for everything you're doing in AI, in the space of just caring about humanity, human emotion, about love, and being an inspiration to a huge number of people in robotics, in AI, in science, in the world in general. So thank you for talking today. It's an honor.
Speaker A: Thank you for having me. And, you know, I'm a big fan of yours as well, so it's been a pleasure.
Speaker B: Thanks for listening to this conversation with Rana El Kalyubi. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Helen Keller. The best and most beautiful things in the world cannot be seen or even touched. They must be felt with the heart. Thank you for listening and hope to see you next time.
