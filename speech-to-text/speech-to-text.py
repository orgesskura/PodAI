"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hnsr8t22BDAdeNRCP-l2o6W6-0PVE0kH
"""

#Remeber to pip install --q git+https://github.com/m-bain/whisperx.git

import whisperx
import gc
import torch
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

device = "cuda" if torch.cuda.is_available() else "cpu"
batch_size = 4  # reduce if low on GPU mem
compute_type = "float16" if device == "cuda" else "int8"
audio_file = "downloads/Pieter Levels： Programming, Viral AI Startups, and Digital Nomad Life ｜ Lex Fridman Podcast #440.mp3"

try:
    # Load audio and transcribe
    audio = whisperx.load_audio(audio_file)
    model = whisperx.load_model("large-v2", device, compute_type=compute_type)

    # Transcribe with language detection
    result = model.transcribe(audio, batch_size=batch_size)
    detected_language = result["language"]
    print(f"Detected language: {detected_language}")

    # Align whisper output
    model_a, metadata = whisperx.load_align_model(language_code=detected_language, device=device)
    result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)

    # Perform diarization
    diarize_model = whisperx.DiarizationPipeline(use_auth_token="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", device=device)
    diarize_segments = diarize_model(audio, min_speakers=2, max_speakers=2)

    # Assign speakers to words
    result = whisperx.assign_word_speakers(diarize_segments, result)

    # Generate transcript
    def create_transcript(result, output_file="transcript.txt"):
        with open(output_file, "w", encoding="utf-8") as f:
            current_speaker = None
            for segment in result["segments"]:
                speaker = segment.get("speaker", "Unknown")
                if speaker != current_speaker:
                    current_speaker = speaker
                    f.write(f"\nSpeaker {current_speaker}:\n")
                f.write(f"{segment['text'].strip()} ")
        print(f"Transcript saved to {output_file}")

    # Create the transcript
    create_transcript(result)

    print("Transcript generation complete.")

except Exception as e:
    print(f"An error occurred: {str(e)}")

finally:
    # Clean up to free memory
    try:
        del model, model_a, diarize_model
    except:
        pass
    gc.collect()
    if device == "cuda":
        torch.cuda.empty_cache()

